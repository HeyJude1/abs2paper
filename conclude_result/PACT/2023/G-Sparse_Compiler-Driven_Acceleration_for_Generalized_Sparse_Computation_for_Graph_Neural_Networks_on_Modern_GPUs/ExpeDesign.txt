实验设计总结：

1、核心目标:  
- 验证g-SpMM和g-SDDMM内核的性能优势（对比现有GPU实现如DGL、FeatGraph等）  
- 评估优化技术（2-D共享内存分块、行平衡、1-D步长寄存器分块等）对性能的影响  
- 测试自动调优系统（基于成本模型和遗传算法）的有效性与泛化能力  

2、数据集:  
- **REDDIT**：节点为论坛帖子，边表示用户对两帖子的评论行为（高度不平衡，平均每行非零元素多）  
- **OGBN-PROTEINS**：节点为蛋白质，边为生物关联（中等规模稀疏图）  
- **OGBN-PRODUCTS**：节点为亚马逊商品，边表示共同购买关系（大规模稀疏图）  

3、关键设置:  
- **硬件环境**：NVIDIA V100 GPU（16GB显存），CUDA 11.1  
- **数据格式**：稀疏张量使用32位索引，密集张量为32位浮点行优先格式  
- **对比基线**：DGL v0.9.1、FeatGraph、Sputnik、cuSPARSE  
- **特征长度范围**：1至1024（2的倍数递增），覆盖不同计算负载场景  
- **自动调优**：每个实验耗时8-20秒，搜索线程数/寄存器分配等参数，结果可复用  
- **端到端模型**：GCN/GAT/GraphSage（2层隐藏层，每层256维），全批量训练  

结构化补充说明：  
- **性能验证方法**：通过Python接口调用内核以统一测量开销，集成至DGL框架对比原始实现。  
- **泛化性验证**：成本模型在REDDIT上训练，但在其他数据集测试（g-SpMM推理相关系数0.74）。  
- **代码效率**：G-Sparse核心代码行数仅为DGL的23%-28%，同时实现1.45×至3.2×加速。