相关工作总结：

1、现有方法一：基于词法标记的代码表示方法
核心思想: 早期研究主要依赖源代码的词法标记（lexical tokens）进行代码表示，通过解析代码的文本特征来支持优化决策。
主要局限性: 无法有效捕捉代码的语义信息，导致对程序行为的理解存在本质性缺陷。

2、现有方法二：基于LLVM IR的表示学习方法
核心思想: 新一代方法利用LLVM中间表示（IR）提取代码语义特征，为深度学习模型提供结构化程序信息。
主要局限性: 需要为每个独立任务设计复杂的图神经网络（GNN）建模，缺乏可迁移的通用表示能力。

3、现有方法三：非神经网络的机器学习方法
核心思想: 采用传统机器学习（如贝叶斯优化）进行参数自动调优，典型应用包括OpenMP调优和在线调优任务。
主要局限性: 
- 严重依赖领域特定知识，泛化能力差
- 需要多次执行目标代码来评估参数性能
- 计算开销仍然显著

4、现有方法四：基于搜索的自动调优技术
核心思想: 使用爬山算法、随机搜索、Nelder-Mead等搜索空间优化技术替代暴力搜索，代表工作包括ActiveHarmony和OpenTuner。
主要局限性:
- 采样过程产生巨大开销
- 仍需大量实际执行来评估参数配置

研究缺口：
1. 现有代码表示方法在语义捕获与模型通用性之间存在矛盾：词法方法缺乏语义，而LLVM IR方法又过度依赖任务特定建模。
2. 传统调优方法普遍存在"执行依赖"问题，需要反复运行目标程序来验证参数效果。
3. 当前缺乏能够同时满足以下要求的解决方案：
   - 跨任务可迁移的预训练表示
   - 避免目标程序执行的预测能力
   - 支持轻量级下游建模的通用嵌入

（注：根据论文内容，作者提出的多模态预训练方法正是针对上述缺口，通过可迁移的LLVM IR表示和免执行的预测能力来解决这些核心问题。）