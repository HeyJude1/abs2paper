### 核心挑战总结：

#### 挑战一：**跨栈优化的复杂性**  
**分析**:  
- **具体内容**: 论文指出，深度神经网络（DNN）的加速需要协调机器学习（如模型架构、压缩技术）和系统（如算法、硬件）多个层次的优化，但各层之间的选择存在强耦合性。例如，硬件资源限制（如CPU内存）要求模型压缩和软件算法必须适配，而新型DNN操作（如深度可分离卷积）需要定制的硬件支持。  
- **根源**: 这种复杂性源于DNN部署的“全栈”特性：每一层的优化（如模型剪枝）需依赖下层支持（如稀疏计算算法），而现有研究往往孤立探索单层优化，缺乏跨层协同设计的通用框架。此外，不同领域（机器学习与系统）的研究者缺乏共同语言，导致优化脱节。

#### 挑战二：**设计空间爆炸与评估成本高昂**  
**分析**:  
- **具体内容**: 论文通过实验发现，即使少量参数组合（如4种模型×3种压缩技术×2种硬件）也会产生大量结果，且性能表现非线性。例如，MobileNetV2在特定硬件上最优算法从GEMM变为直接卷积仅因引入“调优”参数。  
- **根源**: DNN加速涉及多个NP难问题（如稀疏化、量化、调度搜索），每增加一个优化维度（如新硬件或数据格式），设计空间呈指数级增长。现有评估方法（如固定部分参数）难以捕捉跨层交互效应，而穷举实验又受限于计算资源（如AutoTVM调优需140小时/模型）。

#### 挑战三：**稀疏与量化优化的实际收益受限**  
**分析**:  
- **具体内容**: 模型优化技术（如剪枝、低精度量化）的理论优势常因系统支持不足而无法实现。例如：  
  - 稀疏化在GPU上因不规则计算难以利用并行性，速度提升低于预期；  
  - int8量化在部分硬件上因编译器未充分优化指令生成，未能达到理想加速比（4×）。  
- **根源**: 技术瓶颈来自两方面：  
  1. **算法-硬件失配**：稀疏数据格式（如CSR）的存储开销和访问不规则性抵消了计算节省；  
  2. **工具链局限**：编译器（如TVM）默认面向密集计算设计，对新兴技术（如块稀疏、混合精度）支持不足。

---

### 补充说明：  
论文通过提出DLAS框架结构化上述挑战，强调需联合优化六层栈（模型、算法、硬件等）。根本矛盾在于：**DNN追求通用性与部署要求高效性之间的张力**，而现有技术栈的割裂加剧了这一矛盾。例如，EfficientNet的架构创新使其对量化敏感，需跨模型设计、压缩、编译三层协同改进才能有效部署。