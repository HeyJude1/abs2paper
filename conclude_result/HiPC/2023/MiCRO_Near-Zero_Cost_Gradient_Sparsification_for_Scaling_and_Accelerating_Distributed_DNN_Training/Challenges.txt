### 核心挑战总结：

#### 挑战一：**梯度堆积（Gradient Build-up）**
**分析**:  
梯度堆积指在分布式训练中，由于各工作节点（workers）选择的梯度重叠度低，导致通信聚合的梯度数量远超过单个节点选择的梯度数量（最高可达用户设定密度的n倍，n为工作节点数）。  
**根源**:  
- **问题本质特性**: 所有工作节点共享相同的梯度选择范围，但各自独立选择梯度时缺乏协调机制，导致非重叠梯度被重复传输。  
- **技术限制**: 现有稀疏化方法未对梯度选择空间进行分区或优化，无法避免跨节点的冗余通信。

#### 挑战二：**高计算成本（High Computational Cost of Gradient Selection）**
**分析**:  
基于排序的稀疏化方法（如Top-k）需要对整个梯度向量排序，其计算复杂度为O(n log k)，且难以利用GPU的并行计算能力。  
**根源**:  
- **算法复杂度**: 排序操作本身的高计算复杂度成为性能瓶颈。  
- **硬件适配性**: 现有算法设计未充分优化以适配GPU的流处理器并行架构。

#### 挑战三：**阈值预测不准确（Inaccurate Threshold Estimation）**
**分析**:  
基于阈值的稀疏化方法需动态预测阈值以满足用户设定的密度要求，但现有方法难以准确估计阈值，导致实际通信密度远高于设定值。  
**根源**:  
- **动态性挑战**: 训练过程中梯度分布不断变化，静态或简单动态阈值策略无法适应。  
- **误差累积**: 不准确的阈值会引发压缩比误差（实际密度与用户设定密度的偏差），进一步增加通信负载。

---

### 补充说明：
1. **技术矛盾点**：论文指出误差反馈（Error Feedback）虽可减少因丢弃梯度导致的模型保真度损失，但为实现低误差需选择更多梯度（高密度），反而加剧通信瓶颈（见公式1与实验分析）。  
2. **关联性**：上述挑战共同导致分布式DNN训练无法同时实现高效通信和计算加速，凸显了现有稀疏化方法的可扩展性缺陷。