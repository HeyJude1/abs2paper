问题背景总结：  
1、研究领域: 分布式深度神经网络（DNN）训练优化，具体聚焦于梯度稀疏化（gradient sparsification）技术。  
2、核心问题: 现有梯度稀疏化方法（排序法和阈值法）存在**计算复杂度高**（排序法）或**通信流量控制不足**（阈值法）的问题，且均因梯度堆积（gradient build-up）导致分布式训练的可扩展性受限。  
3、研究动机:  
   - **理论价值**：解决梯度稀疏化中计算效率与通信效率的权衡问题，提出一种同时降低计算开销和精确控制通信流量的方法。  
   - **实践价值**：提升分布式DNN训练的加速比和可扩展性，尤其适用于通信带宽受限的大规模集群环境。  
4、潜在应用:  
   - 大规模分布式深度学习模型训练（如多GPU/多节点场景）。  
   - 边缘计算或带宽受限环境下的协同模型训练。  

（注：总结严格基于原文对现有方法局限性、挑战及研究目标的描述，未引入外部信息。）