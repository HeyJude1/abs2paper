实验结果分析总结：

1、主要发现:
- 在混合并行训练中，CAPTURE相比Alpa显著降低了峰值内存使用：
  - 使用1F1B调度时，内存降低18.36%-43.92%（GPT-3、MoE和Wide-ResNet）
  - 使用GPipe调度时，内存降低5.64%-21.38%
- 纯流水线并行场景下：
  - 1F1B调度获得19.38%-25.26%内存增益（除MoE外）
  - GPipe调度对Wide-ResNet实现12.53%内存增益
- 内存预测准确性：
  - 44.8%的单GPU预测误差在2%内，97.1%在11%内
  - 45.4%的整计划预测误差在2%内，97.8%在11%内
- 硬件资源效率：
  训练Wide-ResNet(1B)时，CAPTURE可在6 GPU上完成Alpa需要16 GPU的任务，且吞吐量提升36.3%

2、消融研究结论:
（注：原文未明确进行传统消融实验，但通过以下对比揭示了关键设计）
- 层合并器(Layer Merger)的作用：
  - 将MoE和GPT-3的运行时减少50%
  - Wide-ResNet因本身16层结构不受影响
- 剪枝器(Pruner)的有效性：
  对MoE和Wide-ResNet效果显著，但对GPT-3因层结构相似性效果有限

3、其他分析洞察:
- 内存-吞吐量权衡：
  1F1B调度下内存增益18.4%-43.9%对应吞吐损失11.5%-42.4%
  特殊案例：MoE(7.1B)在GPipe下获得5.6%内存增益但吞吐损失超60%
- 预测误差来源：
  1F1B调度中未考虑流水线位置对内存的影响
  GPipe调度中邻居参数n取值有限导致部分误判
- 扩展性限制：
  纯流水线并行无法扩展到超过模型层数的GPU数量（如MoE仅支持16GPU）
  混合并行是实现更大规模扩展的必要条件