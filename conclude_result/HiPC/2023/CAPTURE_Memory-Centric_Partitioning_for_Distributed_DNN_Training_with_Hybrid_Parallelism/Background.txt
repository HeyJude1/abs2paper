问题背景总结：
1、研究领域: 分布式深度学习训练优化（特别是混合并行训练系统）

2、核心问题: 如何为混合并行（流水线/数据/张量并行）的DNN训练设计内存优化的模型分区和并行化方案，以突破现有吞吐量导向型分区方法的内存瓶颈。

3、研究动机: 
- 理论价值：现有混合并行系统（如Alpa/Varuna）的分区方法仅考虑训练吞吐量，导致GPU间内存使用不均衡，限制了可训练模型规模。
- 实践价值：降低峰值内存使用可实现在相同硬件上训练更大模型（提升43.9%），或使用更少资源完成训练（硬件需求减少2倍以上），显著降低大模型训练成本。

4、潜在应用:
- 大规模NLP/视觉模型的分布式训练
- 在廉价云实例（如spot-VMs）上实现经济高效的大模型训练
- 支持Transformer等内存密集型模型的扩展训练