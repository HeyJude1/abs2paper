实验结果分析总结：

1、主要发现:  
- DeltaSPARSE在双GPU配置下显著优于单GPU基线方法（cuSPARSE v11.7、bhSPARSE、spECK、NSPARSE）。在计算密集型矩阵（如'm_t1'、'nd3k'等）上，相比次优方法spECK，加速比达1.68x-2.20x。  
- 最大加速比分别达7.30x（NSPARSE）、2.20x（spECK）、4.49x（bhSPARSE）和19.46x（cuSPARSE），几何平均加速比为3.57x、1.86x、2.39x和9.83x。  
- 多GPU扩展性表现出近线性增长：2-6 GPU配置下平均加速比分别为1.64x-3.02x，峰值加速比达1.82x-3.91x，尤其在高计算复杂度矩阵上表现突出。

2、消融研究结论:  
- 未明确提及传统消融实验，但通过运行时分解揭示了关键组件贡献：  
  - 数值计算阶段内核占54.0%总耗时，符号计算阶段占26.5%，验证了计算核心的高效性。  
  - 任务调度开销极低（符号阶段3.22%，数值阶段0.46%），证明分层调度策略的有效性。  
  - 异步内存管理（cudaStream）显著降低内存分配与拷贝开销。

3、其他分析洞察:  
- **负载均衡性**：4-GPU配置下，符号阶段和数值阶段的GPU执行时间变异系数（CV）分别低至0.006-0.096和0.001-0.114，即使对不规则稀疏矩阵（如'smt'）仍保持优异平衡性。  
- **参数敏感性**：算法性能与矩阵稀疏模式强相关，高计算复杂度矩阵更易从多GPU硬件获益，但极端不规则稀疏可能导致并行度降低。  
- **可视化验证**：运行时分解柱状图直观显示各阶段耗时占比，佐证了算法设计中对计算/通信/调度开销的优化效果。  

注：实验基于SuiteSparse的16个代表性稀疏方阵，硬件配置为6×NVIDIA V100 GPU。局限性在于当前实现仅针对单节点多GPU环境，但设计可扩展至分布式集群。