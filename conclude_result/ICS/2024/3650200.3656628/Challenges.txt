核心挑战总结：

挑战一：**边缘设备计算资源有限与Transformer模型高计算需求的矛盾**  
分析:  
- 具体内容：Transformer模型（如BERT-Large、ViT-Large）需要极高的计算资源（例如336M参数、478B FLOPs/推理），而边缘设备（如Raspberry Pi 4的ARM Cortex-A72）通常缺乏GPU支持且算力有限，导致推理延迟无法满足服务级别目标（SLOs）。  
- 根源：问题本质是**模型复杂度与硬件能力的错配**。Transformer的自注意力机制（含MatMul、GeMM等密集运算）和MLP层的计算强度远超传统CNN，而边缘设备的处理器设计未针对此类负载优化。  

挑战二：**现有并行化方法对Transformer架构的适配性不足**  
分析:  
- 具体内容：现有工作（如CoEdge、EdgeFlow）针对CNN的卷积操作设计任务划分策略（如基于特征图分块），但Transformer的自注意力机制和MLP层具有完全不同的计算模式（如多头注意力独立计算、GeMM操作的分片逻辑），导致传统分片方法无法直接应用。  
- 根源：**架构差异导致的算法局限性**。CNN的Conv操作具有局部感受野和规则分片特性，而Transformer的自注意力需要全局交互和动态权重计算，现有方法缺乏对这类结构的针对性优化（论文标注为"Lack of (1)"）。  

挑战三：**异构边缘环境的动态性与静态分片策略的矛盾**  
分析:  
- 具体内容：Megatron-LM等方案假设同构GPU环境，其静态模型并行策略无法适应边缘设备的异构性（如内存容量、算力差异）和网络动态变化（带宽波动）。例如，预定义的分片策略可能因设备资源不足或网络拥塞而失效。  
- 根源：**环境假设不匹配现实约束**。边缘场景中设备类型多样（CPU/GPU混合）、网络状态时变，但现有方法缺乏实时 profiling 和动态调整能力（论文指出缺失网络状态和设备能力的协同考量）。  

补充说明：  
- **数据维度挑战**隐含在挑战二中：自注意力机制的多头输出合并（GeMM）和MLP层的分片通信（如all-reduce操作）引入了额外开销，进一步加剧了异构环境下的延迟问题。