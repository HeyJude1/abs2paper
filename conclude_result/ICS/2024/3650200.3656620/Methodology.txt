方法概述：
1、方法名称: MEATTEN (ARM多核CPU上的自注意力优化库)

2、核心思想: 通过层次化数据复用和算子融合策略，在ARM多核CPU上高效实现Transformer的自注意力模块。核心创新点包括：(1) 基于外积公式的微内核设计，(2) 将softmax分解并前向/反向融合到矩阵乘法中，(3) 自适应并行算法实现批处理MM的负载均衡。

3、主要流程/组件
组件/步骤一: 双阶段微内核设计
- Q×Kᵀ微内核：计算查询-键矩阵乘积，同时融合scale/mask操作和softmax的max子步骤
- S×V微内核：计算注意力-值矩阵乘积，融合softmax的exp/sum/norm子步骤

组件/步骤二: 七层嵌套循环结构
- L6/L7为最内层微内核循环（寄存器级优化）
- L5/L5'实现L1缓存重用（在线数据打包）
- L4/L4'管理L2缓存数据块
- L1/L2处理批处理和注意力头维度（并行化维度）

组件/步骤三: 自适应并行调度
- 动态划分计算空间（𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒×ℎ×𝑠𝑒𝑞_𝑙𝑒𝑛）
- 基于NUMA感知的线程映射策略
- 通过不等式约束(3)-(5)自动调整分块参数(𝑏1,𝑏2,𝑏3)

组件/步骤四: 分析模型驱动的参数推导
- 基于缓存容量约束(CL1/CL2)推导分块大小
- 寄存器分配模型(公式7-8)优化计算内存比(CMR)
- 支持fp32/fp16/fp64等多种精度

关键创新点：
1. "online softmax"将softmax分解为四个子步骤(max/exp/sum/norm)，分别融合到前后两个矩阵乘法中；
2. 通过外积微内核设计实现寄存器级融合，减少内存访问开销；
3. 层次化缓存管理策略(L1/L2重用)结合NUMA感知的任务映射。