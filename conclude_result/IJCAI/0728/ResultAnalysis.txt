实验结果分析总结：

1、主要发现:  
- TaD方法在多项选择任务（TruthfulQA）和开放式生成任务（数学推理、常识推理）中均显著提升了微调后LLMs的性能，且具有通用性（适用于不同模型和PEFT方法）。  
- 在TruthfulQA中，尽管训练数据仅包含<Question, Best Answer>对，TaD仍能同时提升MC1、MC2和MC3指标，表明其对知识向量的对齐有效。  
- 数学推理任务中TaD的性能增益更大（因输出语义密度高），而常识推理任务因输出格式简单（如单token答案）增益较小，但仍保持稳定改进。  
- 对比基线方法（CD和DoLa），TaD在多数情况下表现更优，尤其在GSM8K和MultiArith上避免了CD的性能下降问题，且能一致保持或提升微调模型的性能。

2、消融研究结论:  
- **知识向量方向验证**：从预训练模型指向微调模型的方向（如7b→7b*）能显著提升性能，反向设置则导致性能下降（表c），验证了知识向量定义的科学性。  
- **模型规模与知识适应对比**：基于微调学习的知识向量方向（预训练→微调模型）比基于模型规模的方向（小模型→大模型）更有效（表e），且二者结合增益有限（表f）。  
- **训练数据比例影响**：数据越少，TaD的改进幅度越大；此时需增大权重参数μ以强化知识向量对齐（图4），表明TaD在低数据场景下的有效性。

3、其他分析洞察:  
- **解码策略兼容性**：TaD与不同基础解码策略兼容，均能提升数学推理任务的表现。  
- **参数敏感性**：μ的选择与训练数据量负相关，低数据时需更高μ值以优化对齐效果。  
- **鲁棒性验证**：5次实验的标准差分析显示，TaD的改进稳定且标准差小于原始模型性能波动，证明其鲁棒性。  
- **全参数微调实验**：TaD在FPFT模型中同样有效，甚至表现优于PEFT场景，进一步验证其普适性。  

关键结论：TaD通过捕捉微调前后概率分布差异构建的知识向量，能稳定增强下游任务性能，尤其在数据稀缺或复杂输出任务中优势显著。