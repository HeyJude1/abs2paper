2 BACKGROUND 2.1 Architecture of Transformer-based DNNs
The transformer model, a prominent deep learning architecture in natural language processing, uses a self-attention mechanism . This mechanism computes the quantified relevance for each input element based on its relationships with all other elements in the input sequence. The self-attention is employed in parallel, creating multi-heads per input tensor. It allows the model to consider contextual information, ensuring an improved understanding of long-range connections of the input sequence. Figure depicts the basic structure of the transformer. It is composed of a multi-head self-attention block and a feed-forward block such as MLP. Both layer normalization and residual connection exist at the beginning and end of the self-attention and MLP blocks, respectively. The details of the components are as follows.
R 2 R 1 L LR 2 LR 1 m k n 1 n 2 k (a) R column-wise MatMul k n R L 1 R L 2 R m 1 m 2 L 1 L 2 k (b) L row-wise MatMul L 1 R 1 k 2 k 1 k 1 k 2 n R 1 R 2 L 2 L 1 m m n L 2 R 2 m n LR (
Multi-head Self-Attention. In the multi-head self-attention block, Query (Q), Key (K), and Value (V) are obtained by multiplying the input with each corresponding weight matrix and splitting them by the number of heads. For each head, attention scores are calculated by performing MatMul for Q and K. Next, the softmax function is used for these scores, generating attention weights (U). These weights are used to compute a weighted sum of the V, generating an output representation of each head (UV). After the outputs of each head are merged (Y), the final multi-head selfattention output is produced through GeMM.
Multi-Layer Perceptron. The input of the MLP block is the output that comes from the self-attention mechanism. This input undergoes a linear transformation using GeMM operation. Subsequently, the result is passed through an activation function, such as the GeLU . The activation function introduces non-linearity to the model, allowing it to capture features in the data. Lastly, the final output is generated through another GeMM operation, aiming to capture the higher-level features within the input.
Layer Normalization and Residual Connection. The final output of the self-attention and MLP block is added to the original input (residual connection) and then normalized across all channels (layer normalization) within the input. It helps stabilize training and prevents vanishing gradient problems.
2.2 Prior Works for DNN Model Parallelism
Within the domain of DNN model parallelism, the common approach involves distributing tensor operations across multiple computational devices. An example of such an approach is the fused-tile partitioning (FTP) technique . The FTP disassembles Conv layers into sub-tasks for distribution among edge devices. The FTP process entails fusing layers and backpropagating the input's receptive field to compute the output tensor for each task. It can reduce the memory traffic when accessing tensor data for fused Conv layers. However, an issue arises due to the potential input tensor overlap, leading to redundant computations, such as halo cells within tiled convolutions. Moreover, the FTP is limited to CNN models, diverging notably from transformer architectures.
In contrast, the state-of-the-art approaches have been proposed for parallelizing both multi-head self-attention and MLP blocks of the transformer model within either multiple GPUs or TPUs, respectively. For multi-head self-attention, they distribute the 1-dimensional tiled (1D-tiled) weight parameters along with a column for MatMul operations associated with Q, K, and V across computing devices (depicted as (a) in Figure ). This design allows each MatMul calculation corresponding to each attention head to be performed independently on a GPU, eliminating the need for inter-GPU communication for self-attention. For the last GeMM operation within self-attention, it is confirmed that further splitting the weight into 2-dimension (2D-tiled) makes more efficient in terms of communication cost according to the study .
In the case of MLP of the Megatron-LM, the weight of the first GeMM is split by columns in an R column-wise manner, while the weight of the second GeMM is split by rows in a block-wise manner (illustrated as (c) in Figure ). The design allows the second GeMM independently on each GPU using the output of the first GeMM without communication. In the case of MLP of the study , the weight of the first GeMM is 2D-tiled. The weight of the second GeMM is also 2D-tiled and the activation tensor is 1D-tiled by columns (illustrated as (c) in Figure with the 2D-tiled weight in the case). Compared to the Megatron-LM, in which partial-sum must be transferred between devices, this method is more efficient in terms of communication costs because it requires only exchanges for partial-sum involved in tiled areas of the final output. However, they unavoidably need all-reduce (i.e., collective communication consisting of reduce-scatter and all-gather operations) operation for the intermediate results among devices to perform the subsequent operation in self-attention and MLP block. Their model parallelism also assumes homogeneous GPUs or TPUs rather than heterogeneous environments. Thus, they cannot determine the acceptable partitioning for distributing workloads in heterogeneous edge environments. This highlights the need for a more practical parallelism strategy to achieve acceptable distributed DNN inference across diverse edge devices.
2.3 Heterogeneity-aware Workload Partitioning
The workload of the neural network encompasses the tensor operations needed for layer execution. In previous research , various partitioning methods have been explored to offload specific tensor operations within CNN workloads onto different heterogeneous devices, enabling parallel processing. The primary aim of these methods is to satisfy the service-level objectives (SLOs), such as latency metrics. To achieve this goal, they collect device-specific profiled data and use this information to determine how workloads should be partitioned before inference. The profiled data includes several parameters such as computing intensity, CPU frequency, maximum available memory capacity for each device, and network status. Based on the profiled data, they make optimal partitioning decisions for parallel inference on heterogeneous devices. Specifically, they determine how to divide the input tensor along the height dimension in CNN models, allowing each device to execute  tiled Conv operations. This process is only activated during the setup phase whenever a DNN application is launched. Once the decision is laid out, their DNN inference runtime engines statically adhere to the predefined partitioning decision until the entire task is completed. However, when dealing with transformers that integrate both self-attention and MLP components, applying conventional partitioning methodologies dedicated to Conv operation is not immediately feasible. Furthermore, if the actual values of profiled data change during the inference phase, it can fail to meet the SLOs. Since existing partitioning algorithms operate under the assumption of static network or device capabilities, they do not change predetermined decisions.