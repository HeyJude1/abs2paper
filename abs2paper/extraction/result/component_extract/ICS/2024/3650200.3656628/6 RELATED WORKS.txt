6 RELATED WORKS
DeepThings proposed a method of distributing input tensor data to enable independent model inference on Convolution (Conv) layers. This method aims to measure and distribute the receptive field of the input in order to compute the output data with a specific range across homogeneous edge devices. MoDNN partitioned the input tensor data of Conv layers and fully-connected layers on a mobile computing environment. In MoDNN, the workload is divided into segments using a greedy approach, and these segments are allocated more heavily to devices that possess higher computational capacity. CoEdge proposed a framework for parallel inference for Conv-centric CNN architecture across heterogeneous edge devices. CoEdge introduced an adaptive workload partitioning technique that considers not only available computing resources but also network bandwidth among devices. This workload partitioning aims to reduce both inference latency and energy consumption. EdgeFlow redesigned and extended existing partitioning methods to suit the directed acyclic graph model. This adaptation was rooted in the assumption that the architecture of the DNN model is represented as a graph rather than a linear chain. Specifically, the partitioning algorithm of EdgeFlow comprehends the model graph's input-output relationship to allocate specific layer operations to each edge device. These studies successfully achieved the parallelism of CNN model inference. Hepti performs parallel inference following the CoEdge approach for CNN models.
In contrast, Megatron-LM and several studies proposed model parallelism for transformer models. By leveraging Mat-Mul parallelism, Megatron-LM examined the operational behavior of the transformer model. In Megatron-LM, a partitioning algorithm that takes into account the network conditions and computational capabilities of each heterogeneous device has not been proposed. Unlike an intra-layer partitioning-based transformer parallelism of these studies, an inter-layer model parallelism was suggested in PipeEdge . Specifically, PipeEdge divides the batch of the input into multiple micro-batches, and pipelines the execution of these micro-batches across multiple edge devices.