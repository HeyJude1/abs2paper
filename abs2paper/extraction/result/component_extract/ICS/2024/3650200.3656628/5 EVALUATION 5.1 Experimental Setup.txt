5 EVALUATION 5.1 Experimental Setup
Our evaluation was conducted on a Raspberry Pi 4 board and non-GPU server as a heterogeneous edge computing platform. To increase heterogeneity, we leveraged cpulimit software tool to limit the CPU utilization and tc tool to change network bandwidth between devices. The detailed configurations of the testbed are described in Table . We used PyTorch to implement the deep learning inference engine of Hepti and PyTorch Distributed communication package as a distributed backend framework. All experiments are initiated from the Raspberry Pi 4 board, which acts as a role of the primary edge device. The primary edge device also performs a dynamic workload partitioning algorithm based on IBM DOcplex , which provides a library for linear programming. Our transformer model workloads include BERT-large and MobileBERT, representing BERT variants to examine models that have a fundamental transformer architecture. Additionally, our approach extends to other models sharing basic transformer structures such as Whisper , and Vision Transformers without significant modifications. However, models necessitating a fundamentally different attention mechanism lie beyond the scope of our approach. We compare and evaluate Hepti with the following baselines by processing text input with 256 tokens in a single batch. First, we compare our parallel inference approach against BERT-large (940Mbps)  In BERT-large with WS manner, at 940 Mbps, 87.5% of the workload was offloaded, while at 470 Mbps, 93.7% of the workload was offloaded.
the parallelism approach of previous studies , while keeping the partitioning decision the same. We also compare our dynamic workload partitioning algorithm approach against the partitioning approach of CoEdge , while our parallel inference methods for the transformer are equally provided.
5.2 Parallel Inference Performance Comparison
To see the effectiveness of our parallel inference method, we measured and compared the end-to-end latency of the Hepti with the approaches of local inference and 1D-and 2D-tiled WS mannerbased inferences which are taken by previous studies .
In this experiment, we control our partitioning algorithm to be performed only once before the inference to determine the workload partitioning ratio. A memory size of 1.63 GB was allocated to the secondary device to ensure that model parameters could be loaded. Figure shows the average end-to-end latency for each inference methodology when the network bandwidth is 940 Mbps and 470 Mbps.
According to the results of the 940 Mbps environment of Figure , serial execution takes 8 and 0.95 seconds for BERT-large and MobileBERT, respectively. On the other hand, the 2D-tiled WS methodology takes only 6.4 and 0.77 seconds, respectively. This method performs parallel inference, minimizing the number of data communications between the two devices and saving 21.6% of the memory of the secondary device. Similarly, the 1D-tiled WS methodology takes only 5.4 and 0.71 seconds respectively. The 1D-tiled WS approach reduces 16.6% of the end-to-end latency compared to the 2D-tiled WS approach because of the number of devices. 2D-tiled WS manner demands more weight-slicing operations and more communication points than the 1D-tiled WS manner. However, as revealed in the previous study , the 2D-tiled WS approach can reduce communication overhead as the number of devices increases, leading to performance improvements. Consequently, this enables faster parallel inference compared to the 1D-tiled WS method in such environments.
We found that the parallel inference method in a WS manner significantly improves performance, taking only 4.07 seconds for BERT-large and 0.61 seconds for MobileBERT. This reduction in time corresponds to a decrease of 37.1% and 20.4% in total overhead compared to 2D-tiled WS-based inference. Also, compared to the parallel execution with 1D-tiled WS manner, Hepti's WS manner Even when the network bandwidth decreases to 470 Mbps, Hepti's WS manners still outperform serial execution for BERTlarge and MobileBERT as shown in Figure . Particularly, we confirm that Hepti, operating in a WS manner, reduces the latency for BERT-large by around 46.6% and 29.8% compared to 2D-and 1D-tiled WS manner each. However, the 2D-tiled WS and the 1Dtiled WS of the Hepti show longer end-to-end latency compared to the serial execution for MobileBERT. This is due to communication overhead outweighing performance gains from parallelization. Unlike large models, MobileBERT's lightweight nature contributes to low computational latency. Applying 1D-or 2D-tiled WS-based inference techniques to lightweight models actually incurs significant communication overhead, leading to overall performance degradation. More specifically, MobileBERT includes additional feed-forward network (FFN) layers that contain two MatMuls each like the MLP block. The 1D-and 2D-tiled WS manner requires communications in every FFN layer because they use the block-wise parallel MatMul. On the contrary, Hepti can execute all FFN layers without communication as WS manner uses L row-wise MatMul.
To identify the underlying factors influencing the overall inference performance of Hepti's parallelization strategies, we conducted measurements of both computational and communication overheads in each transformer component. Table compares the amount of data transfer during the parallel inference in 1D-tiled WS and Hepti. In the case of 1D-tiled WS, we observed that 1.05 GB of data is transferred per device at the beginning of every layer normalization, which requires exchanges for the entire tensor. However, Hepti's WS manner uses L row-wise parallel MatMul operations. Thus, it reduces primary device data transfer to just 245.8 KB, an 88.3% reduction compared to the 1D-tiled WS manner. We have confirmed that the WS manner is more efficient in terms of indevice communication compared to other methods. Because our experimental setup uses only two devices, the case of the 2D-tiled WS manner is the same as the 1D-tiled WS manner in terms of communication points and transferred tensor size. To analyze computational overhead, we calculated the number of element-wise multiplications and additions required for MatMul and GeMM operations in self-attention and MLP for both 1D-tiled WS manner and WS manner of the Hepti with BERT-large. Our experiments revealed that the total number of element-wise multiplications was the same for 1D-tiled WS, and Hepti. However, 1D-tiled WS manner required 1,048,576 more element-wise additions compared to Hepti for each transformer layer. This is because 1D-tiled WS manner uses block-wise parallel MatMul method during the last GeMM of self-attention and the second GeMM of MLP, necessitating additional addition operations for sub-matrices. In contrast, Hepti in a WS manner, only uses L row-wise MatMul and does not require additional addition operations. Next, both 1D-tiled WS manner and Hepti perform layer normalization for the tensor data each device possesses. In the case of 1D-tiled WS, since both primary and secondary devices share the entire tensor data, they perform an equal number of normalization operations, which increases with the number of devices. This approach aims to minimize inter-device communication and the model size by partitioning weights of MatMul and GeMM operations. On the other hand, Hepti conducts layer normalization solely for the submatrices split in the row direction on both devices, regardless of the number of devices, resulting in a constant total workload. In the case of 2D-tiled WS, the total number of element-wise multiplications is the same as in the 1D-tiled WS approach. Also, the number of total addition operations is the same as in the 1D-tiled WS approach due to the experimental setting. Generally, the entire number of element-wise multiplications is identical in 1D-and 2D-tiled WS manners. In contrast, in the 2D-tiled WS method, the number of element-wise addition operations is influenced by the number of devices dividing each dimension of weights. In our experimental setup, the dimensions of weights are partitioned by the same devices as in the 1D-tiled WS.
5.3 Adaptability to Heterogeneous Settings
In this experiment, we used BERT-large to investigate the impact of Hepti's dynamic partitioning algorithm on parallel inference within a heterogeneous environment, comparing it to CoEdge. We considered variations in network bandwidth and available memory size on the secondary device. Since CoEdge lacks support for parallel inference with transformer models, we configured it to follow Hepti's parallel inference approach in WS manner. Figure presents recorded parallel processing latency during the transformer layer computation point across different heterogeneous scenarios. In Segment 1, the network bandwidth is 800-940 Mbps, and there is sufficient memory space on the secondary device to accommodate model parameters. Specifically, a minimum of 1.63GB of memory is guaranteed to enable parallel inference in the WS manner on the secondary device. Therefore, both CoEdge and Hepti can perform parallel inference in WS mode during Segment 1.
Moving to Segment 2, the network bandwidth drops to 200-300 Mbps, causing CoEdge to experience a 12.42% latency increase compared to Hepti. The reason is that CoEdge sticks to the partitioning decision fixed during the setup phase before the inference initiation, with no modifications during the inference phase. Conversely, Hepti continues to deliver optimal performance because it operates a dynamic partitioning algorithm. This algorithm allows Hepti to adapt to changes in network bandwidth and generate optimal partitioning decisions during the inference phase.
Transitioning from Segment 2 to Segment 3, the available memory on the secondary device decreases from 1.63 GB to 1 GB. In this case, WS transformer parallel processing becomes infeasible due to limited memory space. In the case of CoEdge, the inference program halts due to out-of-memory issues. This is because CoEdge is unable to adjust its partitioning decisions during inference. In contrast, Hepti can dynamically switch to the 1D-tiled WS manner, which requires only 757 MB of memory space on the secondary device in the experiment. Thus, Hepti can sustain inference even in environments with reduced memory usage. Our experiment demonstrates that Hepti exhibits superior inference robustness in heterogeneous edge environments compared to CoEdge. Note that in a scenario where network bandwidth is also heavily constrained, the performance may degrade compared to serial inference. For example, when limited to a network bandwidth of less than 100 Mbps, Hepti's 1D-tiled WS experiences a 1.9% latency increase compared to the serial execution. This was attributed to communication overhead outweighing the computational gains from parallelization.
5.4 Overhead of Dynamic Workload Partitioning Algorithm
Hepti's dynamic workload partitioning algorithm operates iteratively, even during inference. To assess its impact on overall inference latency, we measured the performance overhead of this dynamic partitioning algorithm. Figure illustrates how the partitioning algorithm's overhead varies as the number of participating devices in parallel inference increases. The algorithm's behavior is influenced by the values of computing intensity and network bandwidth in the input, so we conducted experiments with a hundred randomly sampled instances. Figure reveals that as the number of devices increases up to ten, Hepti's average latency for the partitioning algorithm becomes negligible. For example, with ten devices, the average latency is only 11.89 milliseconds Compared to the previous end-to-end latency of Hepti for BERT-large at 940 Mbps network bandwidth (as shown in Figure ), which was 4.07 seconds, the partitioning algorithm latency contributes to only 0.29%. However, the maximum execution time can reach 44.9 milliseconds, particularly when using ten devices. It occurs when a specific 𝜆 𝑖 , a constituent of 𝜋, becomes zero, resulting in some devices being excluded from inference. This necessitates a rerun of the partitioning algorithm. Even so, the partitioning algorithm is iterated every 24 iterations of the BERT-large transformer, adding about 1.076 seconds of parallel inference delay. The total latency is still significantly less than the 8 seconds, a latency of local inference time for BERT-large at 940 Mbps bandwidth (as shown in Figure ).