3 MOTIVATION
Lack of Understanding of Transformer Parallelism. Existing workload partitioning algorithms designed for parallel inference on heterogeneous devices lack an understanding of the transformer architecture. These algorithms were initially tailored for partitioning CNN-based models, primarily relying on dividing tensor by height dimension for Conv operation. In contrast, the transformer architecture heavily relies on MatMul and GeMM operations. This primitive operation is frequently used in self-attention and MLP. Therefore, as depicted in Figure , MatMul-dedicated parallelization strategies are required based on matrix split in a row or column direction. In order to quantitatively analyze how the partitioning approach in Figure contributes to the actual performance gain of MatMul operation in an edge environment, we conduct experiments for the following scenarios. The edge device that triggers DNN inference is a primary device and another is a secondary device. The detailed experimental setup is described in Section 5.
• Scenario 1: When MatMul operation is serialized on a single primary edge device • Scenario 2: When MatMul operation is parallelized on two heterogeneous edge devices (Case (b) in Figure )
Figure (a) shows the execution time of each scenario. We conducted MatMul of two matrices with sizes [m, 1024] and [1024, 1024] by varying the value of m to change the matrix size. The network bandwidth and the offloading ratio from primary to secondary devices are fixed at 940 Mbps and at 90%, respectively. Because selfattention involves five MatMul operations in total, we performed MatMul five times in this experiment. In Scenario 1, the entire Mat-Mul takes place only on a primary device, where the computation delay itself accounts for the total latency of MatMul. In contrast, in Scenario 2, the execution time is a summation of the maximum computation delay for divided MatMul operations and the communication delay for tensor offloading. According to Figure (a), the execution time of Scenario 2 decreased by at least 25.1% and by as much as 64.2% compared to Scenario 1. This is mainly due to the significant reduction in computation delay in Scenario 2 compared to the communication delay incurred. For example, when the size of the MatMul is [256, 1024], Scenario 1 had a computation delay of 109.3 ms. On the other hand, in Scenario 2 where Mat-Mul is parallelized, each device's computation delay decreased by a maximum of 34.2 ms. The communication delay showed only 20.8 ms. We observed the total execution time eventually shortened to 55.1 ms. The performance gains obtained in the parallel processing of matrices between heterogeneous devices remain consistent even when the size of the MatMul operation varies. We recognized the need to devise a parallelization approach for the transformer model by applying the parallel operation method of MatMul to the conventional inference workflow.
Insufficient Workload Partitioning for Heterogeneous Devices. The DNN workload partitioning involves deciding how tensor operations within a layer should be divided and offloaded to different devices for processing. However, earlier approaches to transformer model parallelism focused solely on parallelizing layer execution within homogeneous environments. The absence of practical workload partitioning logic is crucial because the overall performance relies on the computing capabilities of individual devices involved in parallel execution, as well as the network bandwidth. Figure (b) provides a quantitative analysis of overall latency fluctuations while varying offloading ratios for five iterations of MatMul operation. In this experiment, we gradually adjusted the MatMul workload on the secondary device in 10% increments, upon the two network bandwidths (240 and 940 Mbps) and two computing capabilities (100% and 25% CPU utilization) of the secondary device. The value of m of the entire MatMul was kept fixed at 256. The detailed experimental setup is described in Section 5.
At a network bandwidth of 940 Mbps and CPU utilization of 100%, we observed that increasing the offloaded tensor operation from the primary device to the secondary device reduced computation delay while increasing communication delay. To illustrate, at a 10% offloading ratio, computation delay accounted for 89.3% of the total latency, while communication delay constituted 10.69%. On the other hand, at a 100% offloading ratio point, computation delay and communication delay made up 57.5% and 42.5%, respectively. We identified the best offloading point, resulting in a total latency of only 55.1 ms at a 90% offloading ratio. When the CPU utilization is limited to 25%, while keeping the same network bandwidth, the optimal offloading ratio shifted from 90% to 50%. This is because the performance gain from the secondary device compared to the communication delay has decreased. A similar phenomenon was also observed with a 240 Mbps network bandwidth. Specifically, if CPU utilization decreases from 100% to 25%, the optimal offloading point shifts from 70% to 30%. This experimental result underscores the fact that determining the optimal offloading ratio depends on the unique computing capabilities of each device. Next, when the network bandwidth is changed from 940 Mbps to 240 Mbps while keeping CPU utilization of 100%, we observed a change in the optimal offloading ratio from 90% to 70%. This is because each device's computation delay relative to the communication delay shifted as the network bandwidth decreased. We found that network bandwidth and computing capability variations also affect the optimal offloading ratio. These observations and implications motivated our research into the optimal parallel inference methods for MatMulcentric transformer models, taking into account the computing capabilities and network status of heterogeneous devices.