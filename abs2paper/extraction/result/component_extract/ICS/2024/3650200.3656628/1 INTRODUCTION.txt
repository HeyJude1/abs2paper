1 INTRODUCTION
Edge AI is a paradigm of operating AI workflows near the users at the network's edge, close to data sources. Unlike the traditional AI workflow, which sends data to the cloud, Edge AI benefits from low latency and low network traffic . Edge AI enables GPU-trained models to be performed in various edge device environments , including non-GPU devices, expanding DNN models to real-world applications. This expansion is expected to contribute to the growth of the Edge AI market, which is projected to reach $3.1 billion by 2027 .
Recently, state-of-the-art DNNs, especially transformer-based models, have revolutionized intelligent tasks in various fields, such as computer vision and natural language processing . The overwhelming effectiveness of transformer models has made their deployment in edge environments crucial, shaping the landscape of upcoming Edge AI. For instance, such integration can increase real-time productivity and personalized interactions like detecting heartbeat frequency and voice recognition .
However, deploying transformer-based DNN models in practical edge environments poses a critical challenge due to performance delays, which fail to meet service-level objectives (SLOs). This is primarily attributed to the limited computing resources of most edge hardware, while transformer model inference processes demand significant computational resources . A case in point is the BERT-Large model , which is a representative NLP transformer model occupying 336 million parameters . Similarly, the vision transformer (e.g., ViT-Large) requires approximately 478 billion FLOPs for a single inference . On the other hand, non-GPU edge devices commonly used for Internet-of-Thing (IoT) devices typically integrate low-performance processors, such as the ARM Cortex-A72 in the Raspberry Pi 4 board .
One promising approach to ensuring SLOs is to offload partial DNN model onto multiple nearby edge devices and parallelize the inference process . The essence of DNN inference parallelization lies in accurately distributing computational tasks across edge devices . Attaining an optimal workload distribution requires a comprehensive understanding of the factors: The architecture of the target DNN model; The available network status for communication among devices; The computing resources available on each device. If an excessive number of tasks are assigned to resource-limited local edge devices or a substantial workload is offloaded through limited network bandwidth to remote edge devices, there is a high likelihood of failing to meet the SLOs.
There have been prior works that involved parallel and distributed inference of DNN models in heterogeneous environments with varying computing resources . For example, both CoEdge and EdgeFlow proposed workload partitioning algorithms to parallelize CNN-based model inference in diverse edge environments. In CNN-based models, the output feature map of the convolution (Conv) operation is typically used as input for the next Conv, resulting in a significant volume of Conv operations. These algorithms were mainly designed to divide computation tasks specifically for these Conv operations. However, the architecture of the transformer model is more complex, featuring a distinct self-attention mechanism that significantly differs from Conv operations. For a BERT example, it consists of self-attention, layer normalization, and Multi-Layer Perceptron (MLP). As shown in Figure , the self-attention structure performs MatMul, GeMM, and SoftMax, rather than a sequence of Conv operations. CoEdge and EdgeFlow algorithms are not tailored for handling the structure of transformer models (Lack of (1)). Consequently, they cannot be directly applied to parallelize transformer-based model inference.
On the other hand, Megatron-LM proposed a strategy for the transformer model parallelism across multiple same GPUs. Specifically, Megatron-LM reported the model parallelism mechanism for large language models by considering the operational logic of selfattention and MLP layers together while also taking into account the memory constraints of GPUs. However, it should be noted that Megatron-LM operates under the assumption of homogeneous environments. Thus, the predefined model parallelism in Megatron-LM cannot guarantee optimization when faced with varying memory constraints of heterogeneous edge environments. Furthermore, it does not provide workload partitioning algorithms that consider network dynamics and computing capabilities of different computing devices (Lack of ( ) and ( )). Hence, the proposed solution is still less practical for achieving best distributed DNN inference in diverse edge environments.
In this paper, we propose Hepti (Heterogeneous Edges' Parallel Transformer Inference), an inference framework, which supports heterogeneity-conscious parallelization of the transformer model on edge environments. Our main contributions are the following:
(1) Hepti supports three different parallel inference strategies by understanding the transformer architecture to cope with varying edge devices' memory status. (2) Hepti generates the acceptable workload partitioning during inference by considering the network status and computing capabilities of heterogeneous devices. (3) We have confirmed that Hepti reduces the latency of transformer model inference by up to 30.7% compared to existing transformer model parallelism methods. (4) In the face of changing network conditions and computing capabilities, Hepti exhibits greater robustness than existing workload partitioning methodologies in heterogeneous edge environments.