4 HEPTI DESIGN AND WORKFLOW
Hepti is a framework that supports both parallel inference of transformer model ( § 4.2) and a partitioning engine for dynamic workload partitioning ( § 4.3) in a heterogeneous environment. Hepti considers transformer architecture but provides three inference modes depending on how the weight is stationary for each edge device. It also takes into account variations in network status and computing capabilities of each edge device for acceptable workload partitioning decisions. With these technical functionalities, Hepti autonomously determines and employs the parallel execution strategies for transformer DNN models.
4.1 Overall Behavior of Hepti
When the inference is requested, then Hepti transitions to the profiling stage to identify each edge device available for inference and collect accurate profiles for the computing resources of each device. The profiling targets statistics data about the CPU frequency, available memory size, and network bandwidth of other devices. These factors are summarized in Table (a) and will be leveraged for partitioning transformer workload.
Once the profiling is completed, the Hepti initiates the parallel inference. During the runtime process, Hepti can repeatedly transition between the partitioning and actual parallel inference stages. At the partitioning stage, Hepti determines how much workload each participating device will handle for computation. At the parallel inference stage, Hepti performs the parallel inference executions. The repeated transitions between these two stages are intended to maintain the acceptable workload partitioning decision, which can be changed during the inference process. Specifically, Hepti finds the acceptable workload partitioning that can minimize the delay of computation and communication between devices by using the proposed dynamic workload partitioning. If partitioning is performed in the middle of the inference process, Hepti does not transition to the profiling stage again. Instead, the primary device identifies the available memory size and computing intensity of the secondary devices. The size of memory information is conveyed at every moment when the secondary device returns intermediate inference results or exchanges tensors with the primary device. The primary device also estimates the computing intensity of the secondary device based on the time taken until the previously requested parallel inference results are returned and finally approximately estimates the network bandwidth. Whenever the workload partitioning decision for each device is made, the primary device offloads the allocated tensor workload to other devices and initiates their inferences. Simultaneously, the inference of the primary device begins.
The profiling and partitioning are triggered by the primary device while the runtime process of model inference involves parallel execution by both the primary and secondary devices. Any edge device can perform dual roles as the primary and the secondary device, owing to its capability for in-device transformer inference.
4.2 Hepti Inference Workflow
Once the workload partitioning decision is made, Hepti can perform parallel inference using the primary and the secondary devices. Figure illustrates the three types of parallel inference workflows of Hepti. If the available memory on the secondary devices for offloading operations is sufficient to load the weight parameters, Hepti operates in a Weight-stationary (WS) manner. Motivated by Megatron-LM and to cope with the case where the available memory of the secondary devices is severely limited, Hepti also supports 2D-tiled Weight-stationary (2D-tiled WS), and 1D-tiled Weight-stationary (1D-tiled WS). These manners are based on a method of splitting weight parameters to fit within the available memory size of secondary devices. According to the study , it has been observed and validated that the 2D-tiled WS manner exhibits superior performance when the number of computing devices (or cores) exceeds 16. Otherwise, the 1D-tiled WS manner outperforms. Consequently, in the design strategy adopted by  Hepti, the 2D-tiled WS manner is prioritized in the case where a scalable inference design is needed due to a substantial number of devices and limited memory capacity. Conversely, the 1D-tiled WS manner is employed in alternative scenarios. The mode of Hepti is determined during the dynamic workload partitioning algorithm, described in Section 4.3.
Weight-stationary (WS) manner. First, in Hepti, the initial input data generated near the edge is transformed into matrix form. Then, it is copied to each secondary device to perform transformer operations in parallel. Afterward, Hepti performs R column-wise MatMul for K, Q, and V in self-attention with input matrix. It allows for facilitating subsequent splitting and merging operations on the resulting matrix, which are carried out with respect to attention heads. Then, for the last GeMM operation in self-attention, the WS uses the L row-wise MatMul. Accordingly, certain portions of matrices from each device are interchanged to construct input matrices for GeMM, organized in a row-wise partitioned manner. When using L row-wise MatMul, each device can independently perform computations up to the final layer normalization of the transformer. Figure (a) illustrates how the matrix tensors of each device are exchanged and processed in Hepti's WS mode. In Figure (a), each device exchanges data using all-gather communication for the final GeMM operation in self-attention. Consequently, each device acquires all tensors along with the row dimension, enabling L row-wise MatMul operations. Thus, each operation can be performed independently during the next layer normalization and MLP without further exchanges. After the iteration of the transformer, all tensors held by each device are propagated to prepare for the next iteration.
1D-tiled weight-stationary (1D-tiled WS) manner. Once all devices occupy the initial input matrix, the operation process of self-attention is identical to the Weight-stationary (WS) manner. To leverage the inherent parallelism in multi-head attention operations within self-attention, Hepti performs an R column-wise MatMul, enabling the matrix results corresponding to each attention head to be used as inputs directly on each device. However, the 1D-tiled WS manner uses block-wise MatMul to perform the final GeMM of selfattention. This approach divides the weight parameters, which are often larger than the input size. After the MatMul computation, collective communication such as all-reduce, involving reduce-scatter and all-gather operations between devices and additional matrix addition operations are needed, as illustrated in Figure (b). After self-attention, and during the layer normalization operation, both devices must perform the same operation for the entire matrix. The reason for redundant layer normalization operations on each device is to prepare for the subsequent R column-wise parallel MatMul operation in the MLP. The R column-wise parallel MatMul also divides weight parameters across devices to save memory. The final GeMM in MLP is again performed using block-wise MatMul, necessitating an all-gather operation for communicating tensors at the end.
2D-tiled weight-stationary (2D-tiled WS) manner. The 2Dtiled WS manner still follows the above process of the self-attention except the last GeMM operation. For the last GeMM, this approach takes block-wise MatMul as a basic but divides weight parameters along with both row and column. Specifically, to perform block-wise MatMul when weights are split on a row-wise basis, it is necessary to gather the columns of the tensor corresponding to the portion of the tensor operated on the rows of the weight. For instance, assuming four devices utilize 2D-tiled WS, as depicted in Figure , the first two devices share tensors, while the remaining two devices share tensors through an all-gather operation, as illustrated in Figure ). As a result, each device generates partial sums for the block-wise MatMul. To obtain the final result of the block-wise MatMul, an all-reduce operation is subsequently performed on the partial sums, concluding the GeMM operation of self-attention. This all-reduce process is also essential for the two GeMM operations in the MLP. Although the implementation of the 2D-tiled WS manner is more difficult compared to the 1D-tiled WS manner, it not only allows for a further reduction in the size of the weight split across devices but also proves to be communication-efficient due to the minimized amount of tensor movement between devices.
Hepti's WS manner involves relatively fewer tensor exchanges and computations compared to the 1D-and 2D-tiled WS manners. This is because the L row-wise parallel MatMul approach is used instead of block-wise MatMul. However, if the available memory on the secondary device significantly decreases during inference in Hepti's WS mode, it leads to an out-of-memory issue, forcing the inference to terminate prematurely. To prevent this, Hepti continuously monitors the available memory status of devices and dynamically switches between three modes. The memory status information is conveyed during intermediate tensor exchanges between the primary and the secondary device during inference. With the received memory information, Hepti can repeatedly execute the dynamic workload partitioning algorithm when the tensors are fully merged on the primary device at the specific step of the inference workflow.
4.3 Dynamic Workload Partitioning
The goal of Hepti is to meet the SLO by controlling the amount of workload distribution from a latency perspective for parallel inference. To achieve this, Hepti needs to solve the problem of finding an acceptable workload partitioning decision, considering the computational capacity of each device and the network status. To address this optimization problem, Hepti's partitioning engine employs a dynamic workload partitioning algorithm. This algorithm autonomously makes an acceptable decision during the runtime inference phase. For example, Hepti can make a new partitioning decision at the beginning of self-attention, as the primary device possesses the entire tensor at that point. In our dynamic workload partitioning algorithm, we set the mathematical model which is composed of constraints based on profiled data and solves an objective function to meet the latency deadline of SLO. In the following, we define a problem formulation using notations in Table . Then, we present the dynamic workload partitioning algorithm.
Problem Formulation. Consider e edge devices and the MatMul operation between matrices of dimensions [m, k] and [k, n] where the m, k, and n are integers. There are several numerical constraints when deciding the partitioning size of each device. Equations ( ) and (2) represent size constraints when splitting tensors. Specifically, these require that the 𝑎 𝑖 values in 𝜋 must be non-negative integers. The concatenation size of all partitioned tensors, along with the split direction, should match the length of the specific direction. For example, in the case of L row-wise parallel MLP of Hepti, the sum of 𝑎 𝑖 values must equal the m value.
∑︁ 𝑖 ∈𝑁 𝑎 𝑖 =          m if L row-wise MatMul n if R column-wise MatMul k if Block-wise MatMul (1) 𝑎 𝑖 ≥ 0, 𝑖 ∈ 𝑁 (2)
Then, using Equation (3), we determine the workload size as the magnitude of elements involved in the dot-product operation for MatMul. The size of workload 𝑟 𝑏,𝑖 determined by 𝑎 𝑖 is constrained by the Equation ( ). This equation restricts the workload size of the transformer block to ensure that it does not exceed the available memory of the device.
𝑟 𝑏,𝑖 =          𝑘 • 𝑛 • 𝑎 𝑖 if L row-wise MatMul 𝑘 • 𝑚 • 𝑎 𝑖 if R column-wise MatMul 𝑎 𝑖 • 𝑚 • 𝑛 if Block-wise MatMul (3) 𝑟 𝑏,𝑖 ≤ 𝑠 𝑖 , 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁 (4)
While executing a single transformer block, Hepti experiences delays in both computation and communication aspects. We estimate each delay by approximating the computing cycles of specific partitioned tensor operations. The total computation latency of the 𝑖-th partition is evaluated using Equation ( ) and . To estimate the required CPU clock cycles for processing 𝑟 𝑏,𝑖 , we calculate 𝜌 𝑖 • 𝑟 𝑏,𝑖 . Then, we divide it by 𝑓 𝑖 to calculate the overall computation time.
We evaluate the total communication latency corresponding to the 𝑖-th device based on Equation . The 𝑐 𝑏,𝑖 can be derived from 𝑎 𝑖 . For example, 𝑐 𝑏,𝑖 is (m -𝑎 𝑖 ) • n • 4 (byte size) for the last layer normalization block in a WS manner. By dividing 𝑐 𝑏,𝑖 by network bandwidth, we measured the time required for the necessary communication. The latency, 𝑇 ′ 𝑏𝑖 , required for collective communication (e.g., all-gather and reduce-scatter) is aggregated across devices, and the resulting sum is denoted as 𝑇 𝑥 𝑏 . Equation ( ) defines the total latency 𝑇 , which is a sum of computation and communication latency. The 𝑇 represents the longest time taken to perform a single inference. Finally, our target objective is to decide on an optimal 𝜋 solution, minimizing the 𝑇 so that it does not exceed the execution deadline 𝐷.
𝑇 𝑐 𝑏𝑖 = 𝜌 𝑖 • 𝑟 𝑏,𝑖 𝑓 𝑖 ( 5
)
𝑇 𝑥 𝑏 = ∑︁ 𝑖 ∈𝑁 𝑇 ′ 𝑏𝑖 , 𝑇 ′ 𝑏𝑖 = 𝑐 𝑏,𝑖 𝐵𝑊 𝑖,𝑗 𝑏 ∈ 𝐵 𝑖, 𝑗 ∈ 𝑁 (6) 𝑇 = ∑︁ 𝑏 ∈𝐵 max 𝑖 ∈𝑁 (𝑇 𝑐 𝑏𝑖 + 𝑇 𝑥 𝑏 ) (7)
Hence, our defined Hepti's inference optimization can be formulated as the following Equation ( ).
P1 : min𝑇 𝑠.𝑡 . 𝑇 ≤ 𝐷, (1), ( ), ( )
Linear Programming-based Approximation. Solving P1 is not practical in terms of time complexity since P1 has non-convex optimization constraints and objective functions. Particularly, it is challenging due to the discrete nature of the integer variable 𝑎 𝑖 in Equation Thus, by introducing continuous real numeric variable 𝜆 to the P1, we transform it into a linearly approximated problem.
First, we set the variable 𝜆 to the 𝑎 𝑖 as shown in Equation ( ). The range of 𝜆 is constrained by Equation . Then, optimization of our dynamic partitioning algorithms can be rewritten as the following Equation . The objective function and other constraints are still linear with 𝑎 𝑖 , which is now continuous. For other numerical constraints of Equation , they have still linear relationships. Hence, P2 is a mixed integer linear programming problem.
𝑎 𝑖 =          𝜆 𝑖 • m if L row-wise MatMul 𝜆 𝑖 • n if R column-wise MatMul 𝜆 𝑖 • k if Block-wise MatMul (9) ∑︁ 𝑖 ∈𝑁 𝜆 𝑖 = 1, 𝑠.𝑡 . 0 ≤ 𝜆 𝑖 ≤ 1, 𝑖 ∈ 𝑁 ( 10
)
P2 : min𝑇 𝑠.𝑡 . 𝑇 ≤ 𝐷, (4), ( ), (10)
Algorithms. Our dynamic workload partitioning engine takes as inputs the (𝑁 , 𝐵, 𝐷) tuple, as well as (𝜌, 𝑓 𝑖 , 𝑠 𝑖 ) for each device and 𝐵𝑊 𝑖,𝑖+1 between devices. If all secondary devices are unavailable, then Hepti executes serial transformer inference. Otherwise, the engine checks 𝑠 𝑖 and determines the Hepti's operation mode. The Hepti uses 1D-tiled WS when there are fewer than 16 devices, and 2D-tiled WS otherwise. Then, the engine uses a mixed-ILP solver to find 𝜆 𝑖 of 𝜋 that can solve P2. If there are elements with 𝜆 𝑖 equal to zero in 𝜋, the corresponding devices do not participate in parallel inference. In this case, we remove the device from 𝑁 and re-evaluate the partitioning decision for the remaining devices. For the obtained 𝑎 𝑖 values, rounding to the nearest integer is applied to determine the final element of 𝜋. The resulting 𝜋 from our partitioning finally contains a decision on how much transformer model inference workload is allocated to each device. In the case of the 2D-tiled WS manner, the aforementioned specific-axis partitioning algorithm is simply executed twice, once for the row and once for the column direction to split the matrix in 2-dimension.