5 MICRO-KERNEL DESIGN
MEATTEN has two types of micro-kernels designed for computing Q × K T and S × V while fusing non-matmul steps such as softmax. Micro-kernels are built upon the ARMv8 architecture, which provides 32 128-bit vector registers (V0-V31) and fused multiplyaccumulate instructions (FMA). The micro-kernels aim to fully exploit the modern CPU's out-of-order instruction execution ability by employing classic instruction-level parallelism techniques such as loop unrolling and instruction scheduling.
5.1 Online Softmax
The softmax introduces intricate data dependencies because it can only be performed once all elements in a row of S are computed. We introduce an online softmax method and then decompose its process into four substeps, seamlessly integrating them forward and backward into the computation of Q × K T and S × V, as illustrated in Figure . Considering a simple scenario with five matrix blocks: Q, (K (1) ) T , (K (2) ) T , V (1) , and V (2) , we seek to calculate softmax(Q
× (K (1) ) T (K (2) ) T ) × V (1)
V (2) to obtain the output O. This process can be decomposed into two stages. In the first stage, we calculate the product S (1) of Q and (K (1) ) T , simultaneously identifying the local maximum m (1) for each row in S (1) . Then, we exponentiate the matrix S (1) in place and obtain the sum of each row, denoted as sum (1) . Finally, the matrix multiplication of S (1)  and V (1) yields O (1) . It is notable that we do not normalize each row of O (1) by dividing by the corresponding elements of sum (1) . Therefore, the first stage is formulated as follows:
𝐒 𝟏 𝐒 𝟐 (𝐊 𝟏 ) 𝐓 (𝐊 𝟐 ) 𝐓 Q 𝐞 𝐒 𝟏 −𝐦 𝟏 𝐞 𝐒 𝟐 −𝐦 𝟐 𝐕 𝟏 O 𝐦 𝟏 𝐦 𝟐 𝐬𝐮𝐦 𝟏 𝐬𝐮𝐦 𝟐 𝐕 𝟐
𝑖, 𝑘 × V
(1) 𝑘, 𝑗
In the second stage, a similar computation is performed for Q and (K (2) ) T , resulting in the product S (2) . While reducing the value m (2) for each row of S (2) , a comparison is made with the values of m (1) to determine the global maximum. The exponentiated result of S (2) is then multiplied by V (2) to obtain O (2) . Note that each element of sum (2) is the sum of each row of S (2) , augmented by the updated sum (1) . Thus, the final output O is computed as O (1) multiplied by e m (1) −m (2)  plus O (2) , then divided by sum (2) . Therefore, we have:
S (2) 𝑖, 𝑗 = ∑︁ 𝑘 Q 𝑖, 𝑘 × (K (2) ) T 𝑘, 𝑗 , m (2) 𝑖 = 𝑚𝑎𝑥 (m (1)
𝑖 , 𝑚𝑎𝑥 𝑗 S
(2)
𝑖, 𝑗 ) S (2) 𝑖, 𝑗 = 𝑒 S (2) 𝑖, 𝑗 −m (2) 𝑖 , sum (2)
𝑖 = sum (1) 𝑖 × 𝑒 m (1) 𝑖 −m (2) 𝑖 + ∑︁ 𝑗 S (2) 𝑖, 𝑗 O (2) 𝑖, 𝑗 = ∑︁ 𝑘 S (2) 𝑖, 𝑘 × V (2) 𝑘, 𝑗 O 𝑖, 𝑗 = (O (1)
𝑖, 𝑗 × 𝑒 m (1)
𝑖 −m (2) 𝑖 + O (2)
𝑖, 𝑗 )/sum
(2) 𝑖
5.2 Micro Kernels
We have designed two types of micro-kernels based on the functionalities and memory access characteristics of loops L6 and L7, as indicated by the dashed boxes in Figures 5. They can be regarded as the minimal computational unit that partitions the computation space constructed by the nested loop. As shown in Figure , we interpret the design with a 32-bit floating-point data type (FP32), with each vector register capable of storing four FP32 data. The micro-kernels are coded with the ARMv8 ISA with NEON extension .
5.2.1 Q × K T .
We allocate 𝑚𝑟 and 𝑛𝑟 /4 registers for reading Q and K T , requiring a total of 𝑚𝑟 × 𝑛𝑟 /4 registers to store the product results. Notably, the data of matrix K T is packed into a contiguous buffer to facilitate vectorization. Additionally, three registers should be reserved for holding the value of 1
d k
, loading the value of the mask matrix, and storing the row maximum value. It is desirable for the value of 𝑚𝑟 to be as close as possible to 𝑛𝑟 to maximize the computation-to-memory ratio (CMR) of the micro-kernels . Ultimately, we set 𝑚𝑟 = 5, 𝑛𝑟 = 16, utilizing all 32 vector registers. In summary, the usage of registers needs to satisfy:
𝑛𝑟 mod 4 == 0 (7)
𝐶𝑀𝑅 = 2 × 𝑚𝑟 × 𝑛𝑟 𝑚𝑟 + 𝑛𝑟 (8)
The detailed computation process is outlined in Algorithm 1. We calculate the product of Q and K T using the outer product formula and FMA instructions. Before writing the product back to memory, we scale it by 1   d k and perform addition with the loaded mask matrix data. Crucially, we also use FMAX instructions to derive the maximum value in a row-wise manner (max), which is the first step of softmax. Our micro-kernel allows for a series of memory-intensive operations performed on the product stored in registers, thus can significantly reduce memory access overhead.
The results are typically written back to memory in a row-major or column-major fashion in classical dense linear algebra libraries. To facilitate the computation of S × V in the subsequent microkernel, we store the result data as tiles of size mr × 4. The advantage of this data storage format lies in the fact that when the subsequent micro-kernel uses mr registers to read the matrix S, the data will be accessed continuously. This eliminates the need for data packing and facilitates data access using vector instructions.
5.2.2 S×V.
The second micro-kernel exponentiates the values of S with the base e prior to performing matrix multiplication, as shown in Algorithm 2. Here, S represents the output of the preceding Q × K T , expected to reside in the L2 cache. Exponential operations impose high register demands, preventing their integration into the matrix multiplication computation. However, when computing the exponentiation (exp) of S by loading data into vector registers, we can fuse the computation of row sums (sum) before writing the exponential values back to memory. Subsequently, we calculate the product of S and V, denoted as O. A condition must be introduced to ensure normalization is performed only on the last iteration of loop L3. As scalar multiplication (norm) is a memory-intensive point-wise operation, we similarly fuse this step before writing the values of O stored in registers back to memory. Therefore, the four steps of softmax, except for exp, which can only achieve cachelevel fusion, max, sum, and norm, can be fused at the register level, mitigating expensive memory access overhead.
Algorithm 1: Micro-kernel for Q × K T 1 for kk = 0 → 𝑑 𝑘 step 4 do 2 (V0 -V4) ← Q(ii : ii+5, kk : kk+4) 3 (V8 -V11) ← K T (kk, jj : jj+16) 4 (V12 -V31) ← FMA((V0 -V4)[0], (V8 -V11)) ⊲ Outer Product 5 • • • ⊲ Loop unroll 6 (V8 -V11) ← K T (kk+3, jj : jj+16) 7 (V12 -V31) ← FMA((V0 -V4)[3], (V8 -V11)) 8 V5 ← BCAST(1 / d k ) 9 V12 ← FMUL(V12, V5) ⊲ Scale 10 V6 ← LDR(mask(ii, jj : jj+4)), V12 ← FADD(V12, V6) ⊲ Mask 11 V7 ← BCAST(max[ii]), V7 ← FMAX(V12, V7) ⊲ Max 12 • • • 13 V31 ← FMUL(V31, V5) 14 V6 ← LDR(mask(ii+4, jj+12 : jj+16)), V31 ← FADD(V31, V6) 15 V7 ← BCAST(max[ii+4]), V7 ← FMAX(V31, V7) Algorithm 2: Micro-kernel for S × V 1 Exponentiate S[ii : ii+5, p : p+b3] if jj == 0 ⊲ Exp and Sum 2 for kk = p → 𝑝 + 𝑏3 step 4 do 3 (V0 -V4) ← S(ii : ii+5, kk : kk+4) 4 (V8 -V11) ← V(kk, jj : jj+16) 5 (V12 -V31) ← FMA((V0 -V4)[0], (V8 -V11)) ⊲ Outer Product 6 • • • ⊲ Loop unroll 7 (V8 -V11) ← V(kk+3, jj : jj+16) 8 (V12 -V31) ← FMA((V0 -V4)[3], (V8 -V11)) 9 if 𝑗 == 𝑠𝑒𝑞 𝑙𝑒𝑛 − 𝑏2 and 𝑝 == 𝑗 + 𝑏2 − 𝑏3 then 10 V5 ← BCAST(1 / sum[ii]) 11 V12 ← FMUL(V12, V5) ⊲ Norm 12 • • • 13 V5 ← BCAST(1 / sum[ii+4]) 14 V31 ← FMUL(V31, V5)