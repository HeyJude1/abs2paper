7 EXPERIMENTAL SETUP 7.1 Platform Details
We have conducted a comprehensive performance evaluation on three representative ARMv8 multi-core processors: Phytium 2000+ , KP920 , and ThunderX2 . Table details the hardware specifications for each platform. Notably, Phytium 2000+ employs a shared 2MB L2 cache for every four cores. In contrast, the L2 cache in the other two platforms is individually assigned to each core, with a collective utilization of a large-capacity L3 cache shared among 64 cores. We measure the performance of the SDPA operator and conduct end-to-end inference.
7.2 Competitive Works
We evaluate MEATTEN by comparing it with four existing works, namely: XNN_F. XNNPACK is a high-performance and prevailing handtuned library offered by Google . It utilizes techniques such as thread pooling to exploit hardware parallelism and achieve load balancing. XNNPACK includes a fused SDPA operator, referred to here as XNN_F. XNN_NF. Since SDPA is a complex operator formed by stacking multiple sub-steps, we have constructed an SDPA operator using various routines provided by XNNPACK, denoted as XNN_NF here.
Each sub-step is implemented by calling an individual parallel routine without fusion.
Ansor. Ansor , a submodule of the deep learning compiler TVM , is responsible for generating schedules for operators automatically. We run auto-tuning up to 1000 measurement trails per test case, as per the default configuration of Ansor. For an entire deep neural network, we set the number of measurement trials to 1000×𝑛, where n is the number of subgraphs. It is typically sufficient for the search or auto-tuning to converge. We use TVM 0.12.0. BLIS. BLIS is a classical dense linear algebra library that provides high-performance GEMM routines. As BLIS does not offer softmax routines, we invoke relevant functions from XNNPACK. We parallelize the 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 and h dimensions using OpenMP, with individual GEMM relying on BLIS as the backend.
PyTorch. We use PyTorch version 2.0.0 with BLIS as the backend. This serves as the baseline for our end-to-end experiments.
7.3 Workloads
The number of attention heads h and the hidden dimension 𝑑 𝑚𝑜𝑑𝑒𝑙 serve as hyperparameters for transformers. In the Bert-base model , with h=12 and 𝑑 𝑚𝑜𝑑𝑒𝑙 =768, the resulting 𝑑 𝑘 is 64. This parameter configuration remains fixed during our evaluation. The work has statistically analyzed sequence lengths in 8 corpora of the GLUE benchmark , revealing that, excluding certain outliers, sequence lengths predominantly concentrate within the interval [0, 1024]. We have conducted tests on sequences with lengths distributed in the range [160, 1600] to ensure good coverage. We mainly utilize batch sizes of 32 and 64.
7.4 Evaluation Methodology
We perform a warm-up to load the code into the instruction cache for each experimental case; the time taken for this warm-up is not considered. We conduct three repeated tests for each case and calculate the arithmetic mean of the three results to obtain stable performance. We measure performance using GFLOPS as the metric and only consider the floating-point operations of the two batched matrix multiplication in SDPA. Therefore, we have: sequences. We reduce data packing overhead by using optimized loop layouts and a newly designed data format. Secondly, unlike MEATTEN, it does not adopt multidimensional loop tiling, failing to leverage on-chip hierarchical cache storage architecture fully. BLIS achieves performance comparable to that of XNN_NF, and on Phytium 2000+, BLIS even slightly outperforms XNN_NF. However, BLIS and XNN_NF do not exploit fusion strategies and fail to achieve optimal performance. Additionally, compared to the other two processors, MEATTEN achieves the highest performance speedup on Phytium 2000+, which is attributed to our thread mapping strategy that can efficiently utilize the shared L2 cache.
𝐺𝐹 𝐿𝑂𝑃𝑆 = 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 × ℎ × (4 × 𝑠𝑒𝑞 𝑙𝑒𝑛 × 𝑠𝑒𝑞 𝑙𝑒𝑛 × 𝑑 𝑘 ) 𝑡𝑖𝑚𝑒 𝑐𝑜𝑠𝑡 × 1.0𝑒9 (14)
Figure shows the throughput of the SDPA with a batch size of 64. MEATTEN outperforms the optimal method and delivers average speedups of 1.75×, 1.36×, and 1.30× on Phytium 2000+, KP920, and ThunderX2, respectively. The performance results presented in Figure closely align with those in Figure , which demonstrates the excellent performance of our approach across various batch sizes.