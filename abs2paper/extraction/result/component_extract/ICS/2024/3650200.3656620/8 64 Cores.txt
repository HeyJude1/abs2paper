8 64 Cores
All 64 cores are employed in this experiment to leverage hardware parallelism fully. This implies that KP920 and ThunderX2 utilize two NUMA nodes, posing a challenge to the scalability of the tested methods.
As shown in Figure , with a batch size of 32, MEATTEN improves computational throughput by 2.17×, 2.67×, and 2.12× on Phytium 2000+, KP920, and ThunderX2, respectively. For a batch size of 64, MEATTEN achieves acceleration ratios of 1.72×, 2.35×, and 2.07×. The performance of the best-performing XNN_F does not significantly grow with an increase in the number of cores, indicating a notable impact on its performance due to cross-NUMAnode memory access. We emphasize that all methods use the numactl command to specify identical memory allocation strategies. MEATTEN, utilizing fusion and thread mapping techniques, effectively reduces the demand for memory bandwidth and remote memory access, achieving excellent scalability.
8.3 Comparison with Ansor
To our knowledge, Ansor currently does not support dynamic shapes. Therefore, we perform 1000 tuning trials for each instance of the SDPA operator with different 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 and 𝑠𝑒𝑞 𝑙𝑒𝑛 . This indicates that we have allocated sufficient budget for Ansor's tuning. In these experiments, we utilize all 64 cores. Ansor does not achieve optimal performance because its search method cannot generate efficient fusion strategies, as shown in Figure .
8.4 Scalability
We assess the scalability performance of each method using varying CPU cores, with a single thread assigned to each core. As depicted in Figure , MEATTEN and XNN_F exhibit significant performance gains with increasing cores, and MEATTEN demonstrates better performance than XNN_F. We observe sub-optimal scalability of XN-NPACK on Phytium 2000+, primarily due to this processor having 8 NUMA nodes, posing significant challenges for memory access optimization. On the other two processors, XNNPACK's performance demonstrates almost linear growth with increased cores when using only one NUMA node. It is when the cores exceeds 32, leading to cross-NUMA-node memory access, that XNNPACK's performance even starts to decline with more parallelism.
8.5 Ablation Study
In this experiment, we analyze the impact of each optimization step on performance, as shown in Figure . We focus on analyzing four steps: softmax fusion (V2), MM fusion (V3), tuning of the parameter 𝑏2 (V4), and buffer S reuse (MEATTEN). V1 refers to not using any optimization, while MEATTEN uses all four mentioned steps. Compared to V1, V2 decomposes the softmax into several substeps and fuses them forward and backward into the two matrix multiplications. This significantly reduces memory access overhead, as sub-steps can be fused at the register level. V3 further combines the two matrix multiplications into the same loop, as illustrated in the L3 loop of Figure , referred to as MM fusion. This practice effectively exploits the locality between producers and consumers, thus significantly improving performance. V4 fine-tunes the parameter 𝑏2. By adjusting the value of 𝑏2, we aim to keep the temporarily generated block S in the L2 cache. Finally, the buffer reuse's impact has been evaluated. Experimental results demonstrate that buffer reuse reduces memory consumption and yields a noticeable performance improvement.
8.6 End-to-end Performance
We use Bert-base as a study case and perform end-to-end performance benchmarks on ThunderX2, as shown in Figure . We have  We load the Bert-base model from PyTorch and subsequently optimize the entire network using Ansor. We currently consider two batch sizes, 32 and 64, with a sequence length set to 480. Our approach outperforms Ansor by 1.27× and 1.23×, respectively.
We have also benchmarked the performance of Vision Transformer . As shown in Table , our method achieved a 2.76 × and 2.85 × acceleration compared to PyTorch.