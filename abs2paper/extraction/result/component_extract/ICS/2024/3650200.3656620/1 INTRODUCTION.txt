1 INTRODUCTION
Recently, the field of deep learning (DL) has achieved significant success in natural language processing with Transformer-based models such as Bert and GPT . Transformers use a self-attention mechanism that simultaneously computes the dependencies between any two tokens in a sequence . This feature endows the models with exceptional algorithmic parallelism and accuracy while mitigating gradient vanishing and exploding issues .
As shown in Figure , the execution time of the attention module accounts for 70% of the overall time when inferencing the Bertbase model. This demonstrates that the module is a significant performance bottleneck of the Transformer-based models. It mainly comprises two batched matrix multiplication (MM) operators and a softmax positioned between them . In real model deployment, the performance optimization of the attention poses three considerable challenges. Firstly, the MM workload in attention is typically small and irregular-shaped, with the second MM having the opportunity to benefit from the first MM when fusing them . This is because the sequence lengths of Transformers are variable and small . For instance, the MM has ùëÄ = ùëÅ = 512, ùêæ = 64 in the Bert-base model, which is considered a non-common matrix shape and hard to optimize . Secondly, the softmax operator is memory-intensive and exhibits complex data dependencies, necessitating the meticulous design to fuse it into the two MM while ensuring accuracy. Lastly, the current optimization support for batched MM remains rudimentary on CPUs .
Existing dense linear algebra libraries have adopted highly optimized strategies for large-scale MM and can achieve about 80% of a processor's peak performance . However, these libraries exhibit poor performance for the matrix sizes commonly used in Transformer models. To accelerate small and irregular-shaped MM, LIBXSMM and LIBSHALOM have designed new computation micro-kernels targeted at X86 and ARM platforms, respectively. LIBXSMM utilizes cache and vector-friendly data layouts, which makes it unable to be directly integrated into DL frameworks (e.g., PyTorch ). These works focus solely on optimizing MM operations and do not consider operator fusion across operations. To overcome the limitation, XNNPACK [2] fuses the MM and softmax operations within the attention module to reduce the overhead imposed by the softmax operation. Nevertheless, it does not carefully design the micro-kernel and batched parallelization strategy for the MM operation, resulting in its inability to utilize the processor's cores and SIMD units fully. Ansor employs an automated tuning approach to optimize the attention module and is unable to perceive the details of the underlying hardware, thus delivering inferior performance compared to a hand-written library . The library FLASHATTENTION achieves highly promising acceleration on GPUs. GPUs exhibit numerous architectural differences, and further exploration is still needed to optimize the attention module on CPUs. ARM multi-cores are widely used in high-performance clusters and data centers . For instance, Japan's Fugaku supercomputer is a homogeneous system ranking at the top 500 list and entirely constructed based on the Fujitsu A64FX ARM CPU . Recent research also indicates that utilizing ARM multi-cores for DL workloads is highly suitable . This motivates us to develop an efficient attention library on the ARM architecture.
To this end, the paper presents MEATTEN , an open-source library dedicated to optimizing the self-attention module on ARM multi-core CPUs. Novel computation micro-kernel, data format, and parallelization strategies have been implemented. Specifically, we develop micro-kernels based on outer product and vector instructions while utilizing an online strategy for the register-level fusion pattern of "softmax to MM". We design a data format suitable for matrix multiplication and judicious loop layouts to reduce memory access overhead and improve data locality. Informed by an analytical model, our decisions on loop permutation, tiling, and parallelization are tailored to the characteristics of workloads and the hierarchical memory system. Intra-and inter-MM parallelism are exploited while achieving load balance. We share our experience that the core insight of the proposed holistic optimizations lies in leveraging data reuse to mitigate the expensive overhead associated with memory read and write operations. We demonstrate the advantages of MEATTEN by applying it to three modern ARM multi-core CPUs, Phytium 2000+ , Kunpeng 920 (KP920) , and ThunderX2 . We compare it with the state-of-the-art solutions, including both library [2, 31] and compilation methods . Experimental results indicate that our approach delivers optimal performance across various scenarios, including batch size, sequence length, and thread number. To further show the benefits of MEATTEN on end-to-end inference, we integrate it into the DL framework PyTorch . Our integration shows that MEATTEN achieves a speedup of over 3√ó on the representative Bert-base model.
The contributions of the paper can be summarized as follows:
‚Ä¢ It provides an analytical model that guides data reuse and derives the algorithmic parameters for different architectures and workloads (Section 4). ‚Ä¢ It presents a new method to develop fused micro-kernels, reducing memory access overhead (Section 5). ‚Ä¢ It designs a bathed parallelization algorithm, enabling intraand inter-MM parallelism (Section 6).