2 BACKGROUND 2.1 Transformer-based Models
Transformer-based language models typically consist of four modules: embedding, encoder, decoder, and output. The encoder and decoder dominate the execution time of a model and are a key optimization focus. The encoder is formed by stacking multiple encoder blocks with the same structure but independently trained parameters such as weights and biases. The construction of the decoder follows a similar approach. Different networks exhibit distinct structures; for instance, Bert-base comprises only an encoder of 12 blocks but without a decoder. Additionally, the number of blocks is configurable. For simplicity, in Figure , we only illustrate the architecture of a single encoder block, which mainly includes two sub-layers: a multi-head self-attention and a fully connected feed-forward network. The simple feed-forward sub-layer consists of two linear transformation operators and an activation operator between them, while the multi-head self-attention sub-layer involves a series of operators with a more intricate structure.
2.2 Attention Module
The self-attention mechanism serves as a core building block in Transformers . It computes the representation of a sequence by capturing relationships between tokens at different positions. Table summarizes the parameters used in the paper. Given a sequence that undergoes tokenization and embedding, a matrix of shape 𝑠𝑒𝑞 𝑙𝑒𝑛 × 𝑑 𝑚𝑜𝑑𝑒𝑙 is produced, where 𝑠𝑒𝑞 𝑙𝑒𝑛 is the number of tokens, and 𝑑 𝑚𝑜𝑑𝑒𝑙 is the length of a word vector. This matrix is then fed into the attention module, undergoing a three-step calculation, as illustrated in Figure . 1) It requires the multiplication of three weight matrices, W Q , W K , and W V , to generate queries (Q), keys (K), and values (V). 2) Following Formula 1 , the process involves: ① computing the dot product of matrices Q and K T to obtain the attention matrix , ② applying a mask to the attention matrix to conceal certain token-to-token relationships, ③ softmax normalization along the rows of the matrix to derive the attention probability matrix , and ④ multiplying the matrix by V to obtain the output. This process is conducted in parallel with h attention heads, where 𝑑 𝑘 = 𝑑 𝑚𝑜𝑑𝑒𝑙 / h after splitting.
3) The data from multiple attention heads is concatenated and linearly transformed to yield the attention module's output.
𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(Q, K, V) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 ( QK T √︁ 𝑑 𝑘 )V (1)
2.3 Characteristic of Workloads
Modern software systems commonly adopt a modular and hierarchical design approach to reduce code coupling and development costs . They typically decompose the workflow of the attention module into several segments and seek support from operator libraries that are highly optimized for specific architectures. As depicted in Figure , we categorize the matrix operations involved in the module into four classes and aim to analyze the characteristics of the workloads.
2.3.1 Matrix Multiplication.
The linear transformation illustrated in steps 1) and 3) essentially involves a GEMM (GEneral Matrix Multiplication) routine. The optimization of GEMM is a well-explored area, with classical dense linear algebra libraries such as OpenBLAS and BLIS providing high-performance implementations. Small values of 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 and 𝑠𝑒𝑞 𝑙𝑒𝑛 may lead to small or irregularshaped GEMM , as shown in Figure , and recent research, including works like LIBSHALOM and LIBXSMM , has delved into this issue thoroughly. Therefore, the paper's focus is not the acceleration of a single large-scale or irregular matrix multiplication operator.
× × × (a) Generate Q, K, V in step 1) (b) Q × K T in step 2) (c) QK T × V in step 2) batch size seq len d model d model 3 W Q W K W V seq len batch size h d k seq len d k Q K T V × × × Figure 3
: Key workloads in the attention module.
2.3.2 Batched MM.
Transformers use h attention heads, allowing the model to learn representations from different subspaces , as seen in models like GPT-2, where h=12 . Considering the batch processing technique during model inference, the process represented by Formula 1 is computed 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 × h times. Batched matrix multiplication dominates the overall computation, with the shape being illustrated in Figures . Unlike the matrix workload in linear transformation, developing parallelism solely within a single MM (intra-MM) is insufficient for utilizing multicore CPUs. Particularly in natural language processing tasks, the range of 𝑠𝑒𝑞 𝑙𝑒𝑛 variation can be substantial , and small-scale MMs generated by short sequences result in very low computational efficiency. Therefore, batch parallelization of MMs (inter-MM) is highly essential. Moreover, in the case of variable sequence lengths, the on-chip cache requirements for MMs vary, posing a challenge in kernel fusion and devising parallel algorithms.
2.3.3
Point-wise Operation. Mask and Scale are typically implemented as matrix addition and scalar multiplication, respectively. They are both memory-intensive point-wise operations and lack opportunities for data reuse. Previous work has optimized them through straightforward fusion strategies .
𝑦 𝑖 = 𝑒 𝑥 𝑖 −𝑚𝑎𝑥 𝑠𝑢𝑚 = 𝑒 𝑥 𝑖 −𝑚𝑎𝑥 𝑠𝑒𝑞 𝑙𝑒𝑛 𝑗=1 𝑒 𝑥 𝑗 −𝑚𝑎𝑥
(2) 2.3.4 Reduction. Softmax reduces the values of the attention matrix to the interval [0, 1], where the shape of each attention matrix is 𝑠𝑒𝑞 𝑙𝑒𝑛 × 𝑠𝑒𝑞 𝑙𝑒𝑛 . For each row vector X of length 𝑠𝑒𝑞 𝑙𝑒𝑛 in the matrix, softmax is performed according to Formula 2. Here, max is the maximum element in the vector, and sum is the row sum after taking the exponentials of the vector elements. Four iterations are required on X to obtain each output 𝑦 𝑖 : find the row maximum, exponentiate each element, accumulate the row sum, and normalize by dividing by sum. It is evident that the softmax operator features significant memory access overhead, and its serial workflow introduces complex and strict data dependencies.
2.4 Hierarchical Memory System
High-performance multi-core processors typically adopt a hierarchical on-chip cache architecture, leading to inconsistent access latency. We illustrate this design using the Phytium 2000+ processor, which serves as a building block for China's next-generation exascale computer as an example. As depicted in Figure , this processor organizes every four cores into a core group, where each core has an L1 cache size of 32KB, and the four cores share a 2MB L2 cache . Eight cores form a panel, and the whole chip is composed of eight panels. Accessing the local L1 and L2 caches yields the fastest speeds, with a latency difference of 4 to 5 times between them . When data is not in the local L2 cache, the access latency depends on the distance between the data and the CPU core. For instance, accessing data in panel 6 from the CPU core in panel 0 incurs the highest on-chip data access overhead, approximately 10 times that of accessing the local L2 cache . When data is off-chip, the memory access latency becomes significantly higher.