6 PARALLELIZATION SCHEME
Our adaptive parallel algorithm aims to exploit intra-and inter-MM parallelism while maintaining load balance and data locality. We employ the widely used shared-memory programming model OpenMP for parallelization. Users can set the thread count T through OMP_NUM_THREADS, and our algorithm evenly distributes tasks among T threads to harness hardware parallelism. Simultaneously, the environment variable GOMP_CPU_AFFINITY is utilized to establish thread-to-core affinity, which is a crucial step for performance enhancement. Below, we will elucidate the parallelization methodology from three perspectives.
6.1 Task Partition
We parallelize the dimensions ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ , h, and ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› , as depicted in loops L1 and L2 of Figure . Loops L3 and L4' are not parallelized because the reduction step of matrix multiplication would introduce write-after-write data dependencies at these two loops. It necessitates costly synchronization operations to ensure consistency. We partition the computation space of each SDPA head into âŒˆğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› / ğ‘1âŒ‰ partitions. Therefore, in the case of batch processing, we generate parts partitions, where parts equals âŒˆğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ Ã— â„ Ã— ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› / ğ‘1âŒ‰ . When parts is not divisible by T, we iteratively reduce the b1 by a step of 1, ensuring an even workload distribution among T threads. This means our parallel algorithm adaptively adjusts the number of parallel threads for each dimension based on ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ , ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› , and â„. Therefore, we have:
ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘  = âŒˆ ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ Ã— â„ Ã— ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› ğ‘1 âŒ‰ (9)
ğ‘ğ‘ğ‘Ÿğ‘¡ğ‘  mod ğ‘‡ == 0 (10)
6.2 Task Distribution
We assign sequential indices starting from 0 to all partitions, resulting in a ğ‘ğ‘ğ‘Ÿğ‘¡ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ of range [0, parts âˆ’ 1]. Based on the index of each partition, we determine the corresponding thread index number ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ of the thread that is responsible for the partition's execution. The calculation of the ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ is as follows:
ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ = ğ‘ğ‘ğ‘Ÿğ‘¡ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ mod ğ‘‡ (11)
Next, we need to determine the positional information for each partition to facilitate locating the corresponding matrix data during task execution. We introduce two variables: ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ and ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ . Here, ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ signifies the task index within a attention head, while ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ indicates which attention head the task belongs. Therefore, we have:
ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ = ğ‘ğ‘ğ‘Ÿğ‘¡ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ mod âŒˆ ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› ğ‘1 âŒ‰ ( 12
)
ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ = ğ‘ğ‘ğ‘Ÿğ‘¡ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ âŒˆ ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› ğ‘1 âŒ‰ (13)
Consider a simplified scenario with parameters set as ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ =1, h=2, and ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› =200. Assuming b1=100, then two attention heads will generate four partitions designated by the index range [0, 3]. The partition with index 2 is assigned to thread 2 by our method when using four parallel threads. The ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ for this partition is 1, signifying its affiliation with the second attention head. Meanwhile, the ğ‘–ğ‘›ğ‘¡ğ‘Ÿğ‘ ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ is 0, reflecting that this partition is the first partition within the attention head.
6.3 Thread Mapping
Configuring thread-to-core affinity helps mitigate the performance degradation caused by thread migration across CPU cores. This is because when a CPU core undergoes a thread switch, there is a need to reload the data required by the current thread into the on-chip cache. Moreover, mapping multiple threads that share data to multiple cores with a shared cache can reduce the redundancy of data copying and the overhead of remote data access.
Within a single attention head, multiple partitions can share K T and V. Therefore, we map the parallel threads within a single attention head to multiple closer CPU cores, such as the CPU cores within the same panel of a Phytium 2000+. There is no opportunity for data sharing among multiple attention heads in SDPA, requiring no special treatment.