6 PARALLELIZATION SCHEME
Our adaptive parallel algorithm aims to exploit intra-and inter-MM parallelism while maintaining load balance and data locality. We employ the widely used shared-memory programming model OpenMP for parallelization. Users can set the thread count T through OMP_NUM_THREADS, and our algorithm evenly distributes tasks among T threads to harness hardware parallelism. Simultaneously, the environment variable GOMP_CPU_AFFINITY is utilized to establish thread-to-core affinity, which is a crucial step for performance enhancement. Below, we will elucidate the parallelization methodology from three perspectives.
6.1 Task Partition
We parallelize the dimensions 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 , h, and 𝑠𝑒𝑞 𝑙𝑒𝑛 , as depicted in loops L1 and L2 of Figure . Loops L3 and L4' are not parallelized because the reduction step of matrix multiplication would introduce write-after-write data dependencies at these two loops. It necessitates costly synchronization operations to ensure consistency. We partition the computation space of each SDPA head into ⌈𝑠𝑒𝑞 𝑙𝑒𝑛 / 𝑏1⌉ partitions. Therefore, in the case of batch processing, we generate parts partitions, where parts equals ⌈𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 × ℎ × 𝑠𝑒𝑞 𝑙𝑒𝑛 / 𝑏1⌉ . When parts is not divisible by T, we iteratively reduce the b1 by a step of 1, ensuring an even workload distribution among T threads. This means our parallel algorithm adaptively adjusts the number of parallel threads for each dimension based on 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 , 𝑠𝑒𝑞 𝑙𝑒𝑛 , and ℎ. Therefore, we have:
𝑝𝑎𝑟𝑡𝑠 = ⌈ 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 × ℎ × 𝑠𝑒𝑞 𝑙𝑒𝑛 𝑏1 ⌉ (9)
𝑝𝑎𝑟𝑡𝑠 mod 𝑇 == 0 (10)
6.2 Task Distribution
We assign sequential indices starting from 0 to all partitions, resulting in a 𝑝𝑎𝑟𝑡 𝑖𝑛𝑑𝑒𝑥 of range [0, parts − 1]. Based on the index of each partition, we determine the corresponding thread index number 𝑡ℎ𝑟𝑒𝑎𝑑 𝑖𝑛𝑑𝑒𝑥 of the thread that is responsible for the partition's execution. The calculation of the 𝑡ℎ𝑟𝑒𝑎𝑑 𝑖𝑛𝑑𝑒𝑥 is as follows:
𝑡ℎ𝑟𝑒𝑎𝑑 𝑖𝑛𝑑𝑒𝑥 = 𝑝𝑎𝑟𝑡 𝑖𝑛𝑑𝑒𝑥 mod 𝑇 (11)
Next, we need to determine the positional information for each partition to facilitate locating the corresponding matrix data during task execution. We introduce two variables: 𝑖𝑛𝑡𝑟𝑎 𝑖𝑛𝑑𝑒𝑥 and 𝑖𝑛𝑡𝑒𝑟 𝑖𝑛𝑑𝑒𝑥 . Here, 𝑖𝑛𝑡𝑟𝑎 𝑖𝑛𝑑𝑒𝑥 signifies the task index within a attention head, while 𝑖𝑛𝑡𝑒𝑟 𝑖𝑛𝑑𝑒𝑥 indicates which attention head the task belongs. Therefore, we have:
𝑖𝑛𝑡𝑟𝑎 𝑖𝑛𝑑𝑒𝑥 = 𝑝𝑎𝑟𝑡 𝑖𝑛𝑑𝑒𝑥 mod ⌈ 𝑠𝑒𝑞 𝑙𝑒𝑛 𝑏1 ⌉ ( 12
)
𝑖𝑛𝑡𝑒𝑟 𝑖𝑛𝑑𝑒𝑥 = 𝑝𝑎𝑟𝑡 𝑖𝑛𝑑𝑒𝑥 ⌈ 𝑠𝑒𝑞 𝑙𝑒𝑛 𝑏1 ⌉ (13)
Consider a simplified scenario with parameters set as 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 =1, h=2, and 𝑠𝑒𝑞 𝑙𝑒𝑛 =200. Assuming b1=100, then two attention heads will generate four partitions designated by the index range [0, 3]. The partition with index 2 is assigned to thread 2 by our method when using four parallel threads. The 𝑖𝑛𝑡𝑒𝑟 𝑖𝑛𝑑𝑒𝑥 for this partition is 1, signifying its affiliation with the second attention head. Meanwhile, the 𝑖𝑛𝑡𝑟𝑎 𝑖𝑛𝑑𝑒𝑥 is 0, reflecting that this partition is the first partition within the attention head.
6.3 Thread Mapping
Configuring thread-to-core affinity helps mitigate the performance degradation caused by thread migration across CPU cores. This is because when a CPU core undergoes a thread switch, there is a need to reload the data required by the current thread into the on-chip cache. Moreover, mapping multiple threads that share data to multiple cores with a shared cache can reduce the redundancy of data copying and the overhead of remote data access.
Within a single attention head, multiple partitions can share K T and V. Therefore, we map the parallel threads within a single attention head to multiple closer CPU cores, such as the CPU cores within the same panel of a Phytium 2000+. There is no opportunity for data sharing among multiple attention heads in SDPA, requiring no special treatment.