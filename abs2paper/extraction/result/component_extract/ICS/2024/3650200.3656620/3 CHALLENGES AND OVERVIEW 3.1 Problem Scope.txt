3 CHALLENGES AND OVERVIEW 3.1 Problem Scope
Given the insights from the preceding analysis, we concentrate our optimization efforts on the procedure illustrated in step 2) of Figure . Termed scaled dot-product attention (SDPA) , this process is encapsulated as a function primitive in deep learning frameworks like PyTorch , serving developers in constructing transformer-based neural networks. Based on our comprehension of existing research, we summarize three technical perspectives for optimizing and enhancing the performance of the SDPA operator.
3.2 Optimization Challenges
3.2.1 How do we devise computation kernels for SDPA with operator fusion techniques. A fundamental direction is using the outer product formula to construct a GEMM kernel as a starting point. Subsequently, while ensuring the correctness of data dependencies, we gradually integrate functionalities such as softmax into the GEMM kernel. Specifically, we split these steps into sub-steps based on their memory access characteristics and fuse them into the GEMM kernel, generating multiple computational kernels with different functionalities. This allows us to use these customized kernels to describe the computational logic of SDPA. Additionally, we typically use data packing techniques to rearrange data into contiguous buffers to ensure continuous data access during kernel computation. A combination of on-the-fly packing techniques and thoughtful data format design is necessary to mitigate associated costs. Therefore, we face multiple challenges, such as data formats, complex memory access patterns of operators, and data dependencies.
3.2.2 How do we exploit intra-and inter-MM parallelism based on the characteristics of the workload. The computational complexity of the SDPA varies with different sequence lengths and batch sizes. This results in varying intra-and inter-MM parallelism. Recent research has initiated preliminary attempts at optimizing batched GEMM on CPUs, often imposing constraints on the matrix shape.
For instance, LIBXSMM requires the three dimensions of an MM to satisfy 3 âˆš ğ‘€ğ‘ ğ¾ â‰¤ 32, a condition rarely met in practical scenarios. Other approaches either lack the capability to dynamically adjust task partitioning strategies for varying sequence lengths [2], thus failing to effectively utilize on-chip caches, or resort to table lookup methods that cannot cover all cases . This indicates the need for an adaptive parallel algorithm capable of allocating tasks and parallelizing threads in real-time based on workload characteristics. Simultaneously, leveraging the hierarchical memory system for locality-aware task mapping is crucial.
How can we derive algorithmic parameters using analytical models to achieve portability. Deducing parameters rationally based on architectural disparities and workload variations is foundational for ensuring high algorithmic performance. Additionally, even though the kernel is coded in assembly language on a specific instruction set architecture, it is essential to abstract critical parameters of the algorithm and guide its design using analytical models to avoid binding parallel algorithms to specific platforms.
3.3 Overview
3.3.1 High-level view. Figure illustrates a high-level perspective of MEATTEN's algorithm design. The algorithm's inputs Q, K, V, and output O are all four-dimensional tensors with a shape of ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ Ã— h Ã— ğ‘ ğ‘’ğ‘ ğ‘™ğ‘’ğ‘› Ã— ğ‘‘ ğ‘˜ . We assume that each tensor is stored in row-major order, aligning with the data format conventions of mainstream deep learning frameworks such as PyTorch . MEATTEN draws inspiration from Goto's blocked algorithm and is primarily composed of a seven-layer nested loop, with the innermost loop, known as the micro-kernel, being written in assembly instructions. The crucial optimizations of the algorithm encompass micro-kernel, loop layouts, and parallelization, aiming to enhance data reuse while improving parallelism.
3.3.2 Roadmap.
In the subsequent sections, we will elaborate on the technical details of MEATTEN. We begin by elucidating how data reuse is achieved on the hierarchical cache architecture through loop tiling and permutation (Section 4). An analytical model is provided to guide the optimization of the loop layout. Next, we introduce the design principles of micro-kernels within the loop, complemented by the enhancement in data format (Section 5). Finally, we discuss the parallelization techniques, encompassing task partition, distribution, and thread mapping (Section 6). We describe the algorithm's design using single floating-point data (fp32), but the methodology can be applied to other precisions such as fp16, fp64, and int16.