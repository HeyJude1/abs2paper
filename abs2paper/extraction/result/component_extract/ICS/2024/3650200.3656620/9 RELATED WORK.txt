9 RELATED WORK
The self-attention mechanism , serving as a core building block in Transformer-based models , is commonly identified as a bottleneck during model inference. The DL stack usually seeks hand-tuned libraries or compilers for low-level optimizations , aiming to harness hardware capabilities efficiently. This section delineates prior efforts related to this paper from three technical perspectives. Loop optimization. Loop permutation and tiling are crucial loop optimization techniques with significant implications for enhancing data locality. Chimera and MOpt formalize the data movement volume of a nested loop through analytic models and employ constraint-solving methods to derive the optimal optimization strategy under constraints. Ansor employs an automatic tuning approach, generating extensive schedules for an operator and then evaluating their performance using a cost model to select the most efficient one. Inspired by Goto's blocked algorithm , MEATTEN analyzes how to organize the loop layout for data reuse on the on-chip memory hierarchy, from registers to L1 and L2 caches. Micro-kernel design. Classical dense linear algebra libraries such as OpenBLAS and BLIS typically adopt outer-productbased micro-kernels that can achieve exceptional performance for large-scale matrices. LIBXSMM and LIBSHALOM have undertaken aggressive optimizations for small or irregular-shaped matrices, including key techniques like data packing hiding and instruction scheduling. However, these approaches are limited to MMs. By dissecting the memory access patterns of each step in the attention module, MEATTEN has devised highly efficient microkernels capable of fusing multiple memory-intensive steps. Its micro-kernels reduce the need for data packing by coupling it with innovative data formats, thus significantly enhancing performance. Parallel strategy. XNNPACK [2] utilizes thread pooling to exploit multi-cores. However, its task partition strategy overlooks the hierarchical cache structure and leads to sub-optimal locality. The work employs a table-based method to determine the thread allocation strategy for different workloads, making a tradeoff between performance and portability. While LIBXSMM achieves batched parallelism for operators, it imposes restrictions on the shape of operators. MEATTEN's adaptive parallel algorithm can dynamically explore intra-and inter-MM parallelism based on workload characteristics while maintaining data locality.