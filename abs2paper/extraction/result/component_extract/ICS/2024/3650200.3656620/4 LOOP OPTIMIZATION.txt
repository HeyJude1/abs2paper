4 LOOP OPTIMIZATION
Loop tiling and permutation are two paramount loop optimization techniques with crucial implications for improving data reuse . For conciseness, Figure exclusively presents one head of SDPA, with its workflow and components mirroring Figure , excluding the L1 loop. Our analysis initiates from the two micro-kernels of loop L6 and L7 in Figure , progressively elucidating how we determine the order of the nested loop. Subsequently, we deduce the tiling parameters of the algorithm based on an analytical model.
4.1 Loop Order
4.1.1 Q × K T .
Kernel one primarily computes the product of Q and K T , simultaneously fusing two point-wise operations, namely scale and mask, along with a substep of the softmax involving finding the row-wise maximum (max). Detailed design considerations regarding the functionality of kernels will be explained in Section 5. Here, our focus is solely on how to manipulate data reuse.
In the L6 loop, we access two panels from Q and K T , each of size 𝑚𝑟 ×𝑑 𝑘 and 𝑑 𝑘 ×𝑛𝑟 , as depicted by the dashed box in Figure . During the loop, we load data from main memory for Q and K T , compute their product, and store the product to a tile of size 𝑚𝑟 × 𝑛𝑟 directly in registers. Along the reduction dimension 𝑑 𝑘 , this tile accumulates partial results generated by outer-product computations for each 𝑚𝑟 × 1 column slice and 1 × 𝑛𝑟 row slice, achieving registerlevel data reuse. Transitioning to the L5 loop, it is noteworthy that in the first iteration of the L5 loop, we opt to pack the elements of the 𝑑 𝑘 × 𝑛𝑟 panel from K T to a linear buffer. Employing an online packing method , we achieve instruction overlap between data packing and micro-kernel computation, thereby amortizing the overhead of memory access instructions of data packing. 𝑑 𝑘 is typically a small value (64 in Bert ), and nr is not large due to register constraints. Therefore, the 𝑑 𝑘 × 𝑛𝑟 buffer is much smaller than the capacity of the L1 cache. Hence, in subsequent iterations of the L5 loop, we can reuse this buffer along the b1 dimension, fetching each 𝑚𝑟 × 𝑑 𝑘 panel of Q from main memory until the 𝑏1 × 𝑑 𝑘 block Q fills the L1 cache. Following this, in the L4 loop, we reuse block Q from the L1 cache along the b2 dimension until  In iterations of the L4 loop, we pack each 𝑑 𝑘 × 𝑛𝑟 panel of K T into the previously allocated buffer to achieve low memory space consumption. Notably, the old buffer can reside in the L1 cache. It significantly reduces memory write overhead, as there is no need to pack data into a newly allocated cold buffer outside the cache. Meanwhile, we allocate a contiguous memory space of 𝑏1 × 𝑏2 for the runtime-generated data block S. The space serves as the output for kernel one and is subsequently fed into kernel two as input. It is expected to be held in the L2 cache and reused in each iteration of the L3 loop. Moreover, this buffer can also be reused along the 𝑏𝑎𝑡𝑐ℎ 𝑠𝑖𝑧𝑒 and h dimensions for a thread when dealing with different attention heads. We empirically demonstrate that buffer reuse has a highly positive impact on performance.
4.1.2 S × V. Kernel two computes the product of S and V, concurrently fusing the remaining three substeps of softmax, namely exponentiation (exp), the sum of rows (sum), and normalization (norm). At the L7 loop, we access panels of S and V, each of size 𝑚𝑟 × 𝑏3 and 𝑏3 × 𝑛𝑟 , respectively. The 𝑚𝑟 × 𝑏3 panel is respected to reside in the L2 cache, while the 𝑏3 × 𝑛𝑟 panel needs to be loaded from main memory. At the L6' loop, we traverse the block V of size 𝑏3 × 𝑑 𝑘 , ensuring that the L1 cache can hold the block. Unlike Goto's algorithm, we do not pack each discontinuous 𝑏3 × 𝑛𝑟 panel of V. Although we cannot continuously read data at the L7 and L6' loops, we can reuse the data block V residing in the L1 cache during each L5' loop iteration. Our approach avoids any performance degradation due to discontinuous memory access. Instead, it leverages the L1 cache efficiently without introducing any memory write overhead.
4.2 Tiling Size
The preceding research indicates that the key to selecting the block algorithm's parameters lies in the effective utilization of L2 and L1 cache. We will provide the choices for the parameters mr and nr in Section 5, which are related to the micro-kernel design and the register capacity. At the L2 loop, Q is partitioned into blocks of 𝑏1 ×𝑑 𝑘 , which can be accommodated in the L1 cache when iterating loop L4. Meanwhile, space needs to be reserved for K T and S of size 𝑑 𝑘 × 𝑛𝑟 and 𝑏1 × 𝑛𝑟 , respectively. Thus, we have inequality 3.
Similarly, at the L3 loop, K T and S are divided into blocks of size 𝑑 𝑘 × 𝑏2 and 𝑏1 × 𝑏2. We expect the L2 cache to hold these blocks, thus imposing constraint 4. The L4' loop partitions V into blocks of size 𝑏3 × 𝑑 𝑘 that can not exceed the size of the L1 cache and be reused during the iteration of loop L5', satisfying constraint 5. The constraint inequalities 3-5 derive 𝑏1, 𝑏2, and 𝑏3 boundary values, respectively. We need minor empirical fine-tuning of them, which is consistent with BLIS .
𝑏1 × 𝑑 𝑘 + 𝑑 𝑘 × 𝑛𝑟 + 𝑏1 × 𝑛𝑟 < 𝐶 𝐿1 (3)
𝑏1 × 𝑑 𝑘 + 𝑑 𝑘 × 𝑏2 + 𝑏1 × 𝑏2 < 𝐶 𝐿2 (4)
𝑏3 × 𝑑 𝑘 + 𝑚𝑟 × 𝑏3 + 𝑚𝑟 × 𝑑 𝑘 < 𝐶 𝐿1 (5)