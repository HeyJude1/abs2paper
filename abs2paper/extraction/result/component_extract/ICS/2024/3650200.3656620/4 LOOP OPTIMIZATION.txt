4 LOOP OPTIMIZATION
Loop tiling and permutation are two paramount loop optimization techniques with crucial implications for improving data reuse . For conciseness, Figure exclusively presents one head of SDPA, with its workflow and components mirroring Figure , excluding the L1 loop. Our analysis initiates from the two micro-kernels of loop L6 and L7 in Figure , progressively elucidating how we determine the order of the nested loop. Subsequently, we deduce the tiling parameters of the algorithm based on an analytical model.
4.1 Loop Order
4.1.1 Q Ã— K T .
Kernel one primarily computes the product of Q and K T , simultaneously fusing two point-wise operations, namely scale and mask, along with a substep of the softmax involving finding the row-wise maximum (max). Detailed design considerations regarding the functionality of kernels will be explained in Section 5. Here, our focus is solely on how to manipulate data reuse.
In the L6 loop, we access two panels from Q and K T , each of size ğ‘šğ‘Ÿ Ã—ğ‘‘ ğ‘˜ and ğ‘‘ ğ‘˜ Ã—ğ‘›ğ‘Ÿ , as depicted by the dashed box in Figure . During the loop, we load data from main memory for Q and K T , compute their product, and store the product to a tile of size ğ‘šğ‘Ÿ Ã— ğ‘›ğ‘Ÿ directly in registers. Along the reduction dimension ğ‘‘ ğ‘˜ , this tile accumulates partial results generated by outer-product computations for each ğ‘šğ‘Ÿ Ã— 1 column slice and 1 Ã— ğ‘›ğ‘Ÿ row slice, achieving registerlevel data reuse. Transitioning to the L5 loop, it is noteworthy that in the first iteration of the L5 loop, we opt to pack the elements of the ğ‘‘ ğ‘˜ Ã— ğ‘›ğ‘Ÿ panel from K T to a linear buffer. Employing an online packing method , we achieve instruction overlap between data packing and micro-kernel computation, thereby amortizing the overhead of memory access instructions of data packing. ğ‘‘ ğ‘˜ is typically a small value (64 in Bert ), and nr is not large due to register constraints. Therefore, the ğ‘‘ ğ‘˜ Ã— ğ‘›ğ‘Ÿ buffer is much smaller than the capacity of the L1 cache. Hence, in subsequent iterations of the L5 loop, we can reuse this buffer along the b1 dimension, fetching each ğ‘šğ‘Ÿ Ã— ğ‘‘ ğ‘˜ panel of Q from main memory until the ğ‘1 Ã— ğ‘‘ ğ‘˜ block Q fills the L1 cache. Following this, in the L4 loop, we reuse block Q from the L1 cache along the b2 dimension until  In iterations of the L4 loop, we pack each ğ‘‘ ğ‘˜ Ã— ğ‘›ğ‘Ÿ panel of K T into the previously allocated buffer to achieve low memory space consumption. Notably, the old buffer can reside in the L1 cache. It significantly reduces memory write overhead, as there is no need to pack data into a newly allocated cold buffer outside the cache. Meanwhile, we allocate a contiguous memory space of ğ‘1 Ã— ğ‘2 for the runtime-generated data block S. The space serves as the output for kernel one and is subsequently fed into kernel two as input. It is expected to be held in the L2 cache and reused in each iteration of the L3 loop. Moreover, this buffer can also be reused along the ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ ğ‘–ğ‘§ğ‘’ and h dimensions for a thread when dealing with different attention heads. We empirically demonstrate that buffer reuse has a highly positive impact on performance.
4.1.2 S Ã— V. Kernel two computes the product of S and V, concurrently fusing the remaining three substeps of softmax, namely exponentiation (exp), the sum of rows (sum), and normalization (norm). At the L7 loop, we access panels of S and V, each of size ğ‘šğ‘Ÿ Ã— ğ‘3 and ğ‘3 Ã— ğ‘›ğ‘Ÿ , respectively. The ğ‘šğ‘Ÿ Ã— ğ‘3 panel is respected to reside in the L2 cache, while the ğ‘3 Ã— ğ‘›ğ‘Ÿ panel needs to be loaded from main memory. At the L6' loop, we traverse the block V of size ğ‘3 Ã— ğ‘‘ ğ‘˜ , ensuring that the L1 cache can hold the block. Unlike Goto's algorithm, we do not pack each discontinuous ğ‘3 Ã— ğ‘›ğ‘Ÿ panel of V. Although we cannot continuously read data at the L7 and L6' loops, we can reuse the data block V residing in the L1 cache during each L5' loop iteration. Our approach avoids any performance degradation due to discontinuous memory access. Instead, it leverages the L1 cache efficiently without introducing any memory write overhead.
4.2 Tiling Size
The preceding research indicates that the key to selecting the block algorithm's parameters lies in the effective utilization of L2 and L1 cache. We will provide the choices for the parameters mr and nr in Section 5, which are related to the micro-kernel design and the register capacity. At the L2 loop, Q is partitioned into blocks of ğ‘1 Ã—ğ‘‘ ğ‘˜ , which can be accommodated in the L1 cache when iterating loop L4. Meanwhile, space needs to be reserved for K T and S of size ğ‘‘ ğ‘˜ Ã— ğ‘›ğ‘Ÿ and ğ‘1 Ã— ğ‘›ğ‘Ÿ , respectively. Thus, we have inequality 3.
Similarly, at the L3 loop, K T and S are divided into blocks of size ğ‘‘ ğ‘˜ Ã— ğ‘2 and ğ‘1 Ã— ğ‘2. We expect the L2 cache to hold these blocks, thus imposing constraint 4. The L4' loop partitions V into blocks of size ğ‘3 Ã— ğ‘‘ ğ‘˜ that can not exceed the size of the L1 cache and be reused during the iteration of loop L5', satisfying constraint 5. The constraint inequalities 3-5 derive ğ‘1, ğ‘2, and ğ‘3 boundary values, respectively. We need minor empirical fine-tuning of them, which is consistent with BLIS .
ğ‘1 Ã— ğ‘‘ ğ‘˜ + ğ‘‘ ğ‘˜ Ã— ğ‘›ğ‘Ÿ + ğ‘1 Ã— ğ‘›ğ‘Ÿ < ğ¶ ğ¿1 (3)
ğ‘1 Ã— ğ‘‘ ğ‘˜ + ğ‘‘ ğ‘˜ Ã— ğ‘2 + ğ‘1 Ã— ğ‘2 < ğ¶ ğ¿2 (4)
ğ‘3 Ã— ğ‘‘ ğ‘˜ + ğ‘šğ‘Ÿ Ã— ğ‘3 + ğ‘šğ‘Ÿ Ã— ğ‘‘ ğ‘˜ < ğ¶ ğ¿1 (5)