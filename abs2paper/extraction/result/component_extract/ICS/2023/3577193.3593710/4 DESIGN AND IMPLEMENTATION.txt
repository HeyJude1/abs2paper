4 DESIGN AND IMPLEMENTATION
This section illustrates the design and implementation of CMLCompiler, as shown in Fig.
4.2 Model Parser
Model Parser converts operator representations into an ECG. Operators in an operator representation are initialized as nodes in an ECG, the data structure of which is defined in Section 3.2. Operator.weights and operator.intermediate_results are set according to data dependencies, and edges are built between nodes. Operator.use_sparse and operator.dtype are set as False and Unknown, respectively. Operator.type is set according to operator type, which is defined in Table . Weights and intermediate_result are initialized after that. Weight.sparsity is set as the ratio of non-zero data and all data, known during compilation. Weight.smallest_dtype is set as the smallest dtype without accuracy loss, and weight.actual_dtype is initialized the same. Intermediate_result.sparsity and intermedi-ate_result.dtype are set according to operator. When all operators are visited, the ECG is established.
4.3 Graph Optimizer
Graph Optimizer performs graph-level optimizations, using a functionally equivalent transformation for ECGs. These optimizations are based on the features of CML models and do not influence accuracy. There are three specific graph rewriting optimizations: dtype rewriting, sparse operator replacing, and redundant elimination. As shown in Fig. , the top is the ECG of decision trees before optimization; many details are hidden. Weight 𝑊 3 represents the relationship between leaf nodes and internal nodes for decision trees, which is a matrix only containing 0 and 1. The smallest_dtype of 𝑊 3 is bool. The output of 𝑔𝑟𝑒𝑎𝑡𝑒𝑟 operator has a dtype of bool as well. So the following matrix multiplication (matmul) operator can use a dtype of bool rather than float32. Intel processors speed up int8 computation using AVX instruction, while bool cannot benefit from that feature. So we convert the dtype of matmul to int8 according to hardware specification. In Fig. , below is the ECG after graph rewriting. Those white weights and operators use float32, while gray weights and operators use int8. Now we introduce the dtype rewriting principle in detail. Algorithm 1 shows the procedure of dtype rewriting:
(1) Visit all operators in ECG. For each operator, dtype is set as the largest dtype of all inputs. After that, operator dtype is converted to the dtype which can utilize hardware's SIMD instructions best. We keep a list of hardware specifications to modulate operator dtype.
In order to guarantee accuracy, dtype cannot get smaller. Then we modulate operator implementation based on operator dtype.
(2) When operator dtype is fixed, we set the input dtype. The dtype of weights is set the same as the operator, reducing dtype conversion in runtime. The dtype of intermediate results cannot be converted during compilation. So we add dtype converting operator, .i.e, cast, before the operator.
We explain the differences between dtype rewriting for CML models and model quantization for DL models. Quantization is an approximate algorithm for DL models that causes a decrease in accuracy and brings extra computation, such as calibration. Dtype rewriting for CML models is based on the properties of CML, converting dtype of operators and weights with no accuracy decrease and extra computation. Replacing dense operators with sparse operations can speed up as well. The sparsity of input data can be known until runtime, while the sparsity of weights can be known during compilation. So we convert the data format of weights rather than input data. Different hardware devices have different support for sparse operators. For example, CPUs can benefit from sparse computation while GPUs have little effect. So we set a threshold based on hardware specification. If weight.sparsity is smaller than the threshold, we store it in a compressed sparse row (CSR) format. Then we convert the corresponding operator into a sparse implementation. An example is shown in Fig. , we convert 𝑊 1 and the corresponding matmul to sparse.
4.3.3 Redundant elimination.
Redundant elimination eliminates those operators who do not influence final results due to their mathematical properties. For example, a series of monotonic operators followed by an indices operator is mathematically equivalent to the indices operators alone. For each operator in ECGs, we check its operator type. If another monotonic operator follows a monotonic operator, we fuse them. We eliminate the monotonic operator if an indices operator follows it. An example is shown in Fig. , the softmax before argmax is eliminated.
4.4 Graph Translator
Graph Translator converts the optimized ECG into DL computational graph based on ECG topology and chooses the proper operator implementation. DL frameworks or compilers provide different implementations for the same operator. Graph Translator utilizes four properties: 𝑢𝑠𝑒_𝑠𝑝𝑎𝑟𝑠𝑒, 𝑡𝑦𝑝𝑒, 𝑑𝑡𝑦𝑝𝑒, and 𝐷𝐿_𝑜𝑝𝑒𝑟𝑎𝑡𝑜𝑟 in ECG to choose the most proper implementation. Operator implementation can also be modulated based on hardware information. We can utilize hardware features to optimize operators. If the hardware supports extended instructions like AVX, we use them to speed up operators. If the backend is TVM, we just pass the precise 𝑚𝑐𝑝𝑢 and 𝑚𝑎𝑡𝑡𝑟 information and utilize TVM to make operator-level optimizations. DL frameworks or compilers take DL computational graphs as input and make more optimizations, finally compiling them into executable modules.
4.5 Hybrid Deployment of CML and DL with a Unified Framework
We convert those CML and DL hybrid applications under a unified framework to reduce the cost of switching frameworks and provide an opportunity for end-to-end optimizations, as shown in Fig. . We load models from PyTorch and sklearn and convert them into ECG subgraphs. We build edges according to data dependency and merge those subgraphs in a single ECG. Then we can use optimizations both in our work and DL compilers. Finally, we compile and deploy it on various hardware devices.
4.6 Implementation
Due to the benefits in portability and performance, we implement CMLCompiler on the basis of TVM. The intermediate representations and transforms are all written in python. We read trained models from CML frameworks such as sklearn and convert them into operator representations, implementing them in the format of TVM relay functions and storing their weights in TVM arrays. We wrap those relay functions in the format of ECGs. After optimizations in Section 4.3, we convert ECGs into TVM's IRModules.
Then we utilize TVM to make more optimizations and compile to executable modules based on specific hardware targets. We use cross-compilation to support a broad spectrum of hardware devices. We deploy them on lightweight runtime based on TVM runtime and make inferences on various hardware devices.