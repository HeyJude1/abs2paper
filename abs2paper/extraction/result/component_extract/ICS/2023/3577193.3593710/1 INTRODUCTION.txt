1 INTRODUCTION
Deep learning (DL) and classical machine learning (CML), collectively called machine learning (ML), have played an increasingly critical role in recent years. DL refers to those neural network models, such as convolutional neural networks (CNNs) , recurrent neural networks (RNNs) , and generative adversarial networks (GANs) . Different from DL, CML represents a set of non-neural network models in ML, e.g., linear models , decision trees , random forests , and support vector machines . DL stands out because of its accuracy, while CML is still widely used for lower time and energy costs. Doris Xin et al. analyze 3000 production ML pipelines at Google and find that 40% of them use CML models. Besides, many real-world applications adopt hybrid deployments of CML and DL to guarantee high accuracy and low latency , e.g., DL models for feature embedding and CML models for classification or regression.
DL compilers, like TVM , provide a structural approach to tackle the portability issue and facilitates wide deployment of DL models on a broad spectrum of devices like GPUs, FPGAs, and IoT devices and guarantees an appreciable performance. DL compilers use computational graphs as high-level abstractions, supporting a large variety of DL models. Meanwhile, DL compilers propose low-level abstractions such as tensor representation to generate executable code. For newborn hardware, the vendor just needs to provide hardware primitives, instead of a sophisticated highperformance library that is prohibitively costly. Based on the tensor representation and computational graphs abstractions, many optimizations are proposed to boost performance, e.g., they provide sophisticated support for CPU processor architectures as the latter has different architectures, diverse core numbers, extended instructions, and cache sizes. However, despite its popularity and importance, CML suffers from severe portability and performance issues. State-of-the-practice and state-of-the-art CML frameworks provide ad-hoc solutions, implementing each CML model on every hardware device case by case due to the lack of unified abstractions. These ad-hoc solutions raise considerable difficulties in developing a generalpurpose framework and optimization techniques to achieve optimal performance for every model. They either lack the support or only partially support various hardware devices, such as GPUs, FPGAs, and IoT devices. In addition, adding support for a model on a new hardware device needs great effort, more than several thousands of lines of codes , let alone hundreds or thousands of models and devices. Moreover, they also face performance issues. Even on the CPUs -the most popular CML platform, the performance is unsatisfactory due to the lack of specific optimizations for advanced characteristics like multi-cores and SIMD. The hybrid deployment of CML and DL models faces more severe problems.
Our intuition is to enable CML to leverage DL's well-defined unified abstractions and highly mature compilers, optimization technologies, and frameworks. Unfortunately, it is not a trivial task. There are significant distinctions in operators and models between CML and DL. DL operators focus on tensors, while CML handles arrays, matrices, scalars, and tables. DL models are all neural network models, while CML models, such as decision trees, can hardly be represented as neural networks. Most DL models are expressible as flat sequences of operations without if-statements , but if-statements frequently occur in CML models. Existing DL abstractions, such as tensor representation and computational graphs, can not directly represent CML operators and models. Those distinctions determine CML can hardly leverage the DL ecosystems directly. Several efforts attempt to support CML models on DL frameworks, e.g., TensorFlow provides a CPU-based decision forest library TF-DF . However, these attempts do not solve the generality and portability issue. They only support a narrower range of models, lacking support for GPUs and IoT devices.
This paper focuses on CML inference for the first step, considering its great significance that occupies nearly half of the total cost and its wide applications in online serving, Internet of things (IoT), etc . We will extend our work to CML training in the near future. As illustrated in Fig. , we propose a unified compiler, CMLCompiler, for CML inference, which enables CML to leverage the mature DL ecosystems. At the core of CMLCompiler are two unified abstractions: operator representations and extended computational graphs (ECGs) and a compiler framework. Operator representations convert CML operators into tensor formats, while an ECG organizes these converted operators in an optimizationfriendly way. The two unified abstractions define how to convert and translate CML models into DL computational graphs, which can be recognized and executed by DL frameworks and compilers. The CMLCompiler framework consists of four modules -operator converter, model parser, graph optimizer, and graph translator. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized DL computational graph to DL compilers or frameworks. CMLCompiler can also optimize the mixed pipelines of CML and DL. As TVM provides portability and sophisticated optimizations, we choose to implement CMLCompiler on TVM. Currently, it supports up to 35 CML models.
This paper makes the following contributions:
â€¢ We propose two unified abstractions -operator representations and extended computational graphs-to represent CML operators and models. The remainder of the paper is organized as follows. Section 2 introduces the motivation. Section 3 introduces unified abstractions. Section 4 shows design and implementation. Section 5 presents our evaluation. Section 6 illustrates the related work. Finally, we draw a conclusion in Section 7.