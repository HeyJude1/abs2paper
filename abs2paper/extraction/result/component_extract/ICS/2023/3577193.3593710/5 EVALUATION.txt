5 EVALUATION
This section summarizes the evaluation. Section 5.1 shows the experimental setup. Section 5.2 evaluates the performance of graph rewriting optimizations based on ECGs. Section 5.3 compares our work with the state-of-the-art frameworks. Section 5.4 evaluates the hybrid deployment of CML and DL.
5.1 Experimental Setup
We deploy a server node equipped with two Xeon E5-2620 V3 (Haswell) CPUs, an Nvidia Titan RTX GPU, and 64 GB memory to conduct the experiments on CPU and GPU. Each CPU contains six physical cores. The GPU contains 4608 Cuda cores and 24 GB memory. The operating system is Ubuntu 16.04, and the other software includes TVM 0.8, PyTorch 1.8.1, hummingbird 0.3.1, scikit-learn 1.0.1, and CUDA 10.2. For the IoT experiments, we use Raspber-rypi4b with Raspbian 10 operating system and deploy the above software with the same version. We use YearPrediction as the dataset, with 515345 samples and 90 features. We use 80% data to train models and 20% data to make inferences. We run all the experiments five times and use the average as the final results. We test hummingbird using both two backends (PyTorch and TVM) and select their best results.
5.2 Optimizations
This section evaluates graph rewriting optimizations based on ECGs, as described in Section 4.3. These optimizations: dtype rewriting, sparse operator replacing, and redundant elimination, can work together and produce cumulative optimization effects. They can also coexist with the optimizations in TVM. We choose four typical tree models: DecisionTreeClassifier, RandomForestClassifier, ExtraTreeClassifier, and ExtraTreesClassifier, as well as two typical linear models: LogisticRegression and SGDClassifier. We evaluate the dtype rewriting and sparse operator replacing for tree models, Figure : Graph Rewriting Optimizations. Take sklearn as the baseline. "base" means our work without optimizations. "DR" means using dtype rewriting. "SOR" means using sparse operator replacing. "RE" means using redundant elimination. "√ó" means unsupported. For those models not supported by sklearn on IoT devices, take our result without optimzations as the baseline.
and redundant elimination for linear models according to their unique patterns.
Fig. shows the result on CPU. For tree models, using our work without optimizations has a 1.31x-2.54x speedup compared with sklearn; this is due to our abstractions which utilize optimizations of TVM, including better utilization of SIMD instructions and multi cores. Using dtype rewriting and sparse operator replacing bring 1x-1.21x and 1.26x-1.75x speedup, respectively, achieving 1.27x-2.11x speedup together, 1.84x-4.44x faster than sklearn. linear models, our work without optimizations runs slower than sklearn. However, using redundant elimination brings 1.22x-1.51x speedup; the result after our optimizations is 1.06x-1.14x faster than sklearn.
Fig. shows the result of IoT devices. Note that sklearn lacks enough support for IoT devices. For example, 64-bit tree models trained on servers cannot be executed on Raspberrypi4b with a 32-bit operating system. Retraining those models in 32-bit format on Raspberrypi4b from scratch takes more time, so we regard those models as unsupported, marked as cross. So we take our work without optimizations as the baseline. Using dtype rewriting and sparse operator replacing bring 1.01x-1.33x and 1.23x-2.3x speedup, respectively, achieving 1.49x-2.53x speedup together. For linear models, our work without optimizations achieves a 1.71x-1.84x speedup. Using redundant elimination brings 1.08x-1.14x more speedup, 1.95x-1.98x faster than sklearn. The computation part of GPU is less than 20%, so those optimizations play a limited role on GPU. In conclusion, CML models can benefit from both TVM's optimizations and our optimizations and achieve obvious speedup.
5.3 Overall Results
This section evaluates 14 typical CML algorithms covering preprocessing algorithms, linear models, tree-based models, and SVMs, on CPU, GPU, and IoT devices, compared with state-of-the-art frameworks including sklearn, intel extension for sklearn , and hummingbird. It contains two parts: batch experiments for all data and query experiments for a single record.
The differences between the results of CMLCompiler and sklearn are all less than 1√ó10 ‚àí5 , which means that our work does not affect the accuracy. The outputs on different hardware are all the same, so we focus on performance hereinafter. Table shows the performance of batch experiments. On CPU, our work reflects the best performance on 12 algorithms out of 14, achieving 1.02x-10.57x speedup compared with sklearn, 1.14x-4.38x speedup compared with hummingbird, and 1.44x-8.47x speedup compared with intel sklearn. On GPU, our work achieves competitive performance compared with hummingbird. Our work performs better on 11 algorithms out of 14, with a 1.11x-3.31x speedup. On an IoT device Raspberrypi4b, our work performs better on 13 algorithms out of 14, with a 1.28x-5.09x speedup.
Table shows the performance of query experiments for a single record. On CPU, our work achieves the best performance on 11 algorithms out of 14, with a 1.36x-170.68x speedup compared with sklearn, a 1.56x-4.47x speedup compared with hummingbird, and a 1.31x-169.43x speedup compared with intel sklearn. Our work has better performance on GPU on 10 algorithms out of 14 compared with hummingbird, with a 1.41x-4.64x speedup. Our latency on Raspberrypi4b does not differ much compared with sklearn. However, we perform better in model support.
In conclusion, we have advantages in both batch and query experiments for all three hardware devices. Many models in sklearn only support a single core and cannot fully utilize the SIMD instructions. We perform better than sklearn and intel sklearn due to better utilization of multi-cores and SIMD instructions through compilation. Hummingbird uses both PyTorch and TVM as backends, where TVM performs better in most cases of our evaluations. It implements models in PyTorch and converts them into TVM using ùëì ùëüùëúùëö_ùëùùë¶ùë°ùëúùëüùëê‚Ñé API. This conversion is not direct and efficient enough, causing a performance decrease. Besides, hardware information is missed during conversion, which limits the optimizations of TVM for hummingbird. We map ECGs into relay operators directly and select the most efficient implementation based on ECGs and hardware specification information. Additionally, our abstractions bring more optimizations, as described in Section 4.3, bringing up to 2.53x speedup, working together to achieve better performance.
5.4 Hybrid Deployment of CML and DL
This section shows three hybrid deployment cases of CML and DL. As the baselines, without a unified framework, a DL framework is used to implement DL algorithms, while a CML framework is used to implement CML algorithms. Our work converts CML and DL models into a single ECG, making optimizations and compiling them to various hardware devices. We test the latency of a single query, which is essential in real-world applications.
5.4.1 Sentence Sentiment Classification.
The first is a sentence sentiment classification case, which uses Bert to embed English sentences and logistic regression to make a classification . We use BERT-tiny as the pre-trained Bert model and SST2 as the dataset. The baseline implements BERT-tiny in pytorchtransformers and logistic regression in sklearn. The result is shown in Fig . . Our work achieves a 1.67x speedup on server CPUs. Pytorch-transformers cannot be installed on IoT devices, so the baseline cannot run on Raspberrypi4b. The latency of our work on Raspberrypi4b is 18 milliseconds, which is acceptable in most use cases.
5.4.2 Radiographic Image Analysis. The second case uses Deep
Hybrid Learning to analyze radiographic images, which uses simple DNN to make feature engineering and CML models such as  random forests to make a classification. We use CheXpert as the dataset. The baseline implements DNN in PyTorch and random forest in sklearn. The result is shown in Fig . . Our work achieves a 2.3x speedup on server CPUs. The pre-trained random forest cannot run on IoT devices by sklearn, while our work can support those devices through cross-compilation.
5.4.3
Click Through Rate Prediction. The third case is click-through rate prediction used in recommendation systems of our anonymous industry partners, using GBDT to extract features and the Wide and Deep models to make a prediction. We use avazu 1 as the dataset. The baseline implements GBDT in sklearn and Wide and Deep in PyTorch. The result is shown in Fig . . We achieve 3.04x speedup on the server CPUs. The GBDT model in the baseline cannot be executed on IoT devices, while our latency on IoT devices is only 5.06 ms.