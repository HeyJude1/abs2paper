3 THE UNIFIED ABSTRACTIONS
CMLCompiler takes CML models as input and returns DL computational graphs as output, utilizing DL frameworks or compilers to compile and deploy them. At the core of CMLCompiler are two unified abstractions. Operator representations are used to represent CML operators in tensor format, as shown in Section 3.1. Extend computational graph (ECG) organizes operator representations in an optimization-friendly way and can be used to represent CML models, as shown in Section 3.2. Section 3.3 shows the supported algorithms and extensions for other algorithms.
3.1 Operator Representation
An operator representation uses a combination of one or more DL operators with tensors as input and output to represent a CML operator. We convert CML operators into DL operators and wrap them in the format of operator representations. Data in CML has mainly four formats: arrays, matrices, scalars, and tables . Matrices and arrays are regarded as two types of tensors whose operators can naturally be converted into DL operators. When CML models deal with tables, they take numeric data from tables and operate it, which can also be regarded as scalars. Hereby, we focus on the operators on scalars.
3.1.1 Operator categories and corresponding representations. As shown in Table , we classify CML operators into six categories and provide operator representations, respectively.
(1) Assignment operators assign values to variables. If we assign n values 𝑣 1 , 𝑣 2 , ..., 𝑣 𝑛 to n variables 𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 , we organize these variables and values in two tensors 𝑋 = [𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ] and 𝑉 = [𝑣 1 , 𝑣 2 , ..., 𝑣 𝑛 ]. Then we assign tensor V to tensor X to replace n scalar assignments. Tensor assignments benefit memory copy which stores data in block.
(2) Swap operators swap two or more variables. These variables can be represented in a tensor format and use reorganization operators such as 𝑟𝑒𝑠ℎ𝑎𝑝𝑒 to swap the elements.
(3) Basic arithmetic operators refer to those arithmetic calculations based on scalars, such as 𝑎𝑑𝑑, 𝑠𝑢𝑏, 𝑚𝑢𝑙, and 𝑑𝑖𝑣. We use element-wise arithmetic operators based on tensors to replace them, which can utilize SIMD instructions better.
(4) Aggregation operators refer to operators that calculate aggregates among many scalars, such as 𝑚𝑖𝑛, 𝑚𝑎𝑥, 𝑠𝑢𝑚, and 𝑎𝑣𝑔. Reduction operators can be used to accomplish that.
(5) Comparison operators make a comparison between scalars and return True or False, such as 𝑙𝑒𝑠𝑠, 𝑒𝑞𝑢𝑎𝑙, and 𝑔𝑟𝑒𝑎𝑡𝑒𝑟 . Comparisons with the same operator can be represented in a tensor format and use an element-wise comparison to replace. Conditional operators are used to represent if-else statements, in the form of 𝑖 𝑓 (𝑒𝑥𝑝𝑟 1) 𝑒𝑥𝑝𝑟 2 𝑒𝑙𝑠𝑒 𝑒𝑥𝑝𝑟 3, where 𝑒𝑥𝑝𝑟 1 is a comparison operator. If 𝑒𝑥𝑝𝑟 2 and 𝑒𝑥𝑝𝑟 3 are all assignment or arithmetic operators, we convert all three expressions into tensors. However, the situation gets tricky if one of 𝑒𝑥𝑝𝑟 2 or 𝑒𝑥𝑝𝑟 3 is still a conditional operator. We call those operators sequential conditional operators. Sequential conditional operators may contain many conditions, where each element in a tensor may have quite different decision paths. The complexity of decision paths makes it difficult to convert those operators into tensor operators. Those frequent if-else statements perform poorly on hardware devices such as GPUs and ASICs. Sequential conditional operators are the most delicate, and we defer their discussion later.
3.1.2 Conditional operators representation.
We analyze those widely used CML models and find that sequential conditional operators mainly occur in tree-based models. So we use decision tree as an example to introduce the representation of conditional operators in detail, as shown in Fig. . We use the combination of DL operators to represent those sequential conditional operators.
The top-right shows the original decision tree. The input data is a list of samples; each has many features. 𝐼 refers to internal nodes, numbered in the order of Level Order Traversal. 𝐿 refers to leaf nodes, numbered in the order of In-Order Traversal. Each leaf node is an assignment operator, reaching which node determines the final result. The left shows the corresponding CML operator graph. Scalar-based assignments assign features 𝐹 𝑗 to temporary variables 𝑥 𝑖 , scalar-based comparisons compare 𝑥 𝑖 with thresholds 𝑇 𝑖 , and conditional operators determine the next nodes to be reached. These Table : The summary of operator representation. Each operator representation represents a CML operator. Scalars are marked as lower-case letters, while tensors are marked as upper-case letters. EW is short for element-wise.
Operator Representation in tensor format Operator Type Expressions Operator Type Expressions Assignment
𝑥 1 ← 𝑣 1 ; 𝑥 2 ← 𝑣 2 ; ...; 𝑥 𝑛 ← 𝑣 𝑛 Assignment 𝑋 = [𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ]; 𝑉 = [𝑣 1 , 𝑣 2 , ..., 𝑣 𝑛 ]; 𝑋 ← 𝑉 Swap 𝑥 1 ← 𝑥 2 ; 𝑥 2 ← 𝑥 1 ; Reorganization 𝑋 = [𝑥 1 , 𝑥 2 ]; 𝑟𝑒𝑠ℎ𝑎𝑝𝑒 (𝑋 ); Basic Arithmetic 𝑥 1 + 𝑦 1 ; 𝑥 2 + 𝑦 2 ; ...; 𝑥 𝑛 + 𝑦 𝑛 EW Arithmetic 𝑋 = [𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ]; 𝑌 = [𝑦 1 , 𝑦 2 , ..., 𝑦 𝑛 ]; 𝑋 + 𝑌 Aggregation 𝑠𝑢𝑚(𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ) Reduction 𝑋 = [𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ]; 𝑠𝑢𝑚(𝑋 ) Comparison 𝑥 1 < 𝑦 1 ; 𝑥 2 < 𝑦 2 ; ...; 𝑥 𝑛 < 𝑦 𝑛 EW Comparison 𝑋 = [𝑥 1 , 𝑥 2 , ..., 𝑥 𝑛 ]; 𝑌 = [𝑦 1 , 𝑦 2 , ..., 𝑦 𝑛 ]; 𝑋 < 𝑌 Conditional 𝑖 𝑓 (𝑒𝑥𝑝𝑟 1) 𝑒𝑥𝑝𝑟 2 𝑒𝑙𝑠𝑒 𝑒𝑥𝑝𝑟 3 Described in Section 3.1.2 F 5 < T 1 F 1 < T 2 F 4 < T 3 L 2 F 2 < T 4 L 1 L 3 L 4 L 5 True False I 1 I 2 I 3 I 4 F 5 < T 1 F 1 < T 2 F 4 < T 3 L 2 F 2 < T 4 L 1 L 3 L 4 L 5
True False
Operator Representation .
Table : The properties of weights in Fig. . 𝑁 𝑆 , 𝑁 𝐹 , 𝑁 𝐼 , and 𝑁 𝐿 refer to the number of samples, features, internal nodes, and leaf nodes, respectively. 𝐼𝑛𝑝𝑢𝑡 ∈ R 𝑁 𝑆 ×𝑁 𝐹 means 𝑁 𝑆 samples, each has 𝑁 𝐹 features. 𝑊 1 ∈ {0, 1} 𝑁 𝐹 ×𝑁 𝐼 captures the relationship between features and internal nodes. 𝑊 2 ∈ R 𝑁 𝐼 is the thresholds used in internal nodes. 𝑊 3 ∈ {0, 1} 𝑁 𝐼 ×𝑁 𝐿 represents the structure between internal nodes and leaf nodes. 𝑂𝑢𝑡𝑝𝑢𝑡 ∈ N 𝑁 𝑆 returns the leaf node index each sample reaches. Dtype is the data type of weights. Sparsity is the ratio of non-zero data to all data in weights.
Dtype Sparsity
𝑊 1 [𝑖] [ 𝑗] = 1, 𝐹 𝑖 ∈ 𝐶𝑜𝑛𝑑𝑖𝑡𝑖𝑜𝑛(𝐼 𝑗 ) 0, otherwise bool 1 𝑁 𝐹 𝑊 2 [𝑖] = 𝑇ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 (𝐼 𝑖 ) float32 1 𝑊 3 [𝑖] [ 𝑗] = 0, 𝐿 𝑗 ∈ 𝐿𝑒 𝑓 𝑡𝑆𝑢𝑏𝑇 𝑟𝑒𝑒 (𝐼 𝑖 ) 1, otherwise bool [ 1 2 , 1 − 1 𝑁 𝐿 ]
scalar-based CML operators are converted to operator representations, as shown in the bottom-right in Fig. , and the definitions and properties of weights are shown in Table . Multi Scalar-based assignments are converted to one Tensor-based assignment. ((1) in Fig. ) The data processing order differs from its layout in the tensor, we use the 𝑟𝑒𝑜𝑟𝑔𝑎𝑛𝑖𝑧𝑎𝑡𝑖𝑜𝑛 operator to change the tensor layout, which is replaced by 𝑚𝑎𝑡𝑚𝑢𝑙 with 0-1 matrix 𝑊 1 . ((1) in Fig. ) Multi Scalar-based comparisons are converted to one Tensor-based element-wise comparison. ((2) in Fig. ) Sequential conditional operators are represented by the combination of 𝑚𝑎𝑡𝑚𝑢𝑙 and 𝑎𝑟𝑔𝑚𝑎𝑥. ((3) in Fig. ) Output returns the leaf nodes every sample reaches. Finally, a decision tree is converted to the combination of three operator representations.
The features of CML operator representations. As described above, we represent CML operators in the format of operator representations. These operator representations have unique features different from operators in DL models. First, the weights of DL operators and CML operator representations have different meanings. The weights in DL models are all learnable parameters. Without approximate optimizations such as pruning and quantization, those weights are dense, and the data type (dtype) should be float32 to ensure accuracy. Many weights of CML operator representations have other meanings, such as representing the structure of conditional operators. Those weights are sparse and can naturally be expressed as low-precision dtypes such as bool. The natural sparse features bring optimizations described in Section 4.3.2.
Second, the frequent operators in DL and CML are not the same. Almost all operators in DL take float32 as input and return float32 as output. CML uses many comparison operators, such as 𝑙𝑒𝑠𝑠, 𝑒𝑞𝑢𝑎𝑙, and 𝑔𝑟𝑒𝑎𝑡𝑒𝑟 , which rarely occur in DL models. Those comparison operators take float or integer as input and return bool, bringing remarkable changes in the dtype of input and output, which can be used to make optimizations as described in Section 4.3.1. Both DL and CML models use indices operators, which compare input and returns indices, such as 𝑎𝑟𝑔𝑠𝑜𝑟𝑡 and 𝑎𝑟𝑔𝑚𝑎𝑥. Those indices operators have mathematical properties that can be used to make graph-level optimizations, as described in Section 4.3.3. These optimizations can be ignored in DL models with dozens or hundreds of layers but are helpful for those CML models with fewer layers.
3.2 Extended Computational Graph
This section introduces extended computational graph (ECG), which organizes operator representations in an optimization-friendly way and can be used to represent CML models. ECG is an extension based on DL computational graph. In general, a DL computational graph is represented as a directed graph where nodes represent operations on tensors or program inputs and edges represent data dependencies between operations . From the perspective of the DL frameworks and compilers, computational graphs are dense and float32 by default, such as neural network models. Using approximate optimizations like pruning and quantization brings sparse and low-precision data to all operators and weights. These optimizations cause a decrease in accuracy and bring extra computation, such as calibration. When we convert CML operators to operator representations, part of those converted operators and weights are sparse and low-precision naturally. Using DL computational graphs to represent CML models directly is not precise enough and ignores many optimization opportunities due to the data type and sparse features. So we extend the computational graph in the DL systems into extended computational graph (ECG) as the unified abstraction for CML models.
Before introducing ECG, first, we present more details about data type (dtype) and sparsity. We define the partial order relation for dtypes used in our work:
𝑓 𝑙𝑜𝑎𝑡32 > 𝑖𝑛𝑡32/𝑓 𝑙𝑜𝑎𝑡16 > 𝑖𝑛𝑡16 > 𝑖𝑛𝑡8 > 𝑖𝑛𝑡4 > 𝑏𝑜𝑜𝑙
The lower dtype can be converted into a higher dtype without accuracy loss, while a backward conversion with accuracy loss is forbidden. Using lower dtype computation, such as int8 matmul, can speed up and reduce memory usage. However, there are many limitations to dtype optimization. For example, the inputs of the same operator should have the same dtype; thus, the dtype of operators depends on the largest dtype of inputs. Besides, many hardware devices have extended instructions based on specific dtypes. For example, an Intel processor speeds up int8 computation using AVX Sparsity is defined as the ratio of non-zero data to all data. If data sparsity is relatively small, we take it as sparse data and store it in a compressed sparse row (CSR) format. Using sparse operators to handle those sparse data can perform better than dense operators. Taking advantage of sparsity influences optimization greatly, so we add sparsity as another property for ECG.
We classify the inputs of an operator into two categories: intermediate results and weights. Intermediate results are other operators' outputs and can only be handled during runtime. Input data is the first intermediate result in ECG, while output data is the last. Intermediate results are represented as {𝑠𝑝𝑎𝑟𝑠𝑖𝑡𝑦, 𝑑𝑡𝑦𝑝𝑒, 𝑡𝑒𝑛𝑠𝑜𝑟 }. If we want to change the dtype of intermediate results, we should add dtype converting operator in the ECG.
Weights are model parameters that can be loaded from trained models. Weights can be handled both during compilation and runtime, while a proper transformation during compilation can reduce runtime costs. Weights are represented as {𝑠𝑝𝑎𝑟𝑠𝑖𝑡𝑦, 𝑠𝑚𝑎𝑙𝑙𝑒𝑠𝑡_𝑑𝑡𝑦− 𝑝𝑒, 𝑎𝑐𝑡𝑢𝑎𝑙_𝑑𝑡𝑦𝑝𝑒, 𝑡𝑒𝑛𝑠𝑜𝑟 }. Smallest_dtype is the smallest dtype for weights without accuracy loss, actual_dtype is the dtype actually used. Smallest_dtype depends on the property of weights, while actual_dtype is fixed based on smallest_dtype and operators. As shown in Fig. , 𝑊 1 represents the relationship between input features and internal nodes for decision trees, which is a 0-1 matrix. The smallest_dtype of 𝑊 1 is bool. However, W1 is multiplied by input data with a dtype of float32. If we choose bool as the ac-tual_dtype, 𝑊 1 will be converted to float32 during runtime. To reduce the execution time in runtime, we should convert 𝑊 1 to float32 during compilation, so we set actual_dtype as float32 rather than bool.
Operators are represented in the form of {𝑤𝑒𝑖𝑔ℎ𝑡𝑠, 𝑖𝑛𝑡𝑒𝑟𝑚𝑒𝑑𝑖𝑎𝑡𝑒_ 𝑟𝑒𝑠𝑢𝑙𝑡𝑠, 𝑢𝑠𝑒_𝑠𝑝𝑎𝑟𝑠𝑒, 𝑡𝑦𝑝𝑒, 𝑑𝑡𝑦𝑝𝑒, 𝐷𝐿_𝑜𝑝𝑒𝑟𝑎𝑡𝑜𝑟 }. Weights and in-termediate_results are inputs of operators. Use_sparse is a flag of whether using the sparse operator or not, which is closely related to sparse operator replacing optimization described in Section 4.3.2. Operator type is the type of operator. As shown in Table , we divide operators used in ECG into five categories. Comparison operators refer to those operators that compare two tensors and return bool tensors. Indices operators refer to those operators that return tensors' indices based on specific conditions. Those two kinds of operators are dtype-lowering operators, the output dtype of which is smaller than the input. Models without those operators, such as most DL models, use the same dtype through the whole graphs, where dtype optimizations cannot be used without approximate optimization. CML models make much use of those operators, which
∀𝑥 1 ≤ 𝑥 2 =⇒ 𝑓 (𝑥 1 ) ≤ 𝑓 (𝑥 2 )
A series of monotonic operators followed by an indices operator is mathematically equivalent to the indices operators alone. Those properties provide more optimizations, as described in Section 4.3.3. Reduction operators calculate aggregates over input. Arithmetic operators refer to other arithmetic calculations. Operator dtype is the operators' data type, such as int8 matmul or float32 matmul. Operator dtype depends on the dtype of weights and intermedi-ate_results. DL_operator is the native definition of operators in DL computational graphs, which we use to translate ECG to DL computational graphs.
3.3 Supported Algorithms and Extension for Other Algorithms
CMLCompiler supports 35 CML algorithms nowadays, as shown in Table , covering most of the popular CML algorithms . Our work can also be extended to other algorithms, such as clustering and matrix decomposition. Most CML algorithms use operators categorized in Section 3.1.1, each of which can be converted to corresponding Operator Representations-our low-level abstractions, guaranteeing our extensibility. We take Kmeans as an example. Kmeans use basic arithmetic operators to calculate the distance between nodes, which can be converted to element-wise arithmetic operators and use aggregation operators to make clustering, which can be converted to reduction operators. When all operators of a CML algorithm are converted to Operator Representations, it can utilize our work to compile and make optimizations.
Operator