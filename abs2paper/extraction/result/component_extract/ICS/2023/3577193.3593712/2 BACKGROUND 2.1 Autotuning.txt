2 BACKGROUND 2.1 Autotuning
Autotuning is a process that efficiently evaluates a number of parameter configurations from a user-defined parameterized kernel or application to optimize a given objective such as performance (e.g., runtime, FLOPS). Here we provide a walkthrough with the Polybench kernel "3mm" as a concrete example of basic autotuning concepts. The kernel performs dense matrix multiplication with four matrices ğ´, ğµ, ğ¶, ğ· such that the output is (ğ´ Ã— ğµ) Ã— (ğ¶ Ã— ğ·).
Autotuning utilizes a finite budget (typically time or number of evaluations) to optimize a relationship ğ‘“ (ğ‘; ğ‘¡) âˆˆ R ğ‘‘ between a given parameter configuration ğ‘, out of all possible configurations ğ¶, a tuning task ğ‘¡, and ğ‘‘ objective outputs such that argmax ğ‘ ğ‘“ (ğ‘; ğ‘¡) âˆ€ğ‘ âˆˆ C. Each task ğ‘¡ is a specific instance from a set of related tasks T , which may have different configurations for optimum performance. Each objective ğ‘‘ is a real-valued metric that functionally depends on both the task and parameters according to ğ‘“ (ğ‘; ğ‘¡). The exact closed form of ğ‘“ (ğ‘; ğ‘¡) is unknown but is assumed to be a complex, nonlinear relationship.
An example task of tuning the 3mm kernel's runtime performance involves ğ‘› = 10 parameters in the form of source code annotations that affect loop tile sizes (i.e., 4, 8, 32), loop interchanges (the order loop iterators appear in nested loops), and memory management (the packing used for tile memory structures). Each evaluation of the objective requires annotating the source with parameter values, then compiling and executing it on the benchmark system to collect timing data, which incurs considerable cost even for small input matrices. There are 376,320 unique combinations of the ten parameters that define our tuning space for 3mm, which is prohibitively costly to brute-force with empirical searches. Autotuning uses more intelligent approaches to identify the configurations that achieve optimal performance.
Autotuning must differentiate input scales as different tasks because changing the input scale frequently induces drastic changes in the optimum configuration. As shown in Table , small sizes require the packed-array technique for matrices ğ´ and ğ¸, but medium-sized inputs do not. The degree of improvement can also vary between input scales, where small 3mm inputs can gain 1.13Ã— speedup from autotuning. However, medium-sized 3mm inputs gain 14.94Ã— speedup over the respective baselines.
2.2 Transfer Learning in Autotuning
Several search methods have been developed to reduce the number of evaluations required to find the best configuration for autotuning tasks. They can be classified into model-based and model-free methods. The former methods learn the relationship between the parameter configurations and the objective function through an incrementally updated surrogate model and leverage it to cheaply evaluate multiple points and minimize the number of actual evaluations. Examples include Bayesian optimization that employs Gaussian process regression and random forest and their variants. The latter methods optimize the objective function without such models. Examples include random search, grid search, genetic algorithms, and Nelder--Mead. The key advantage of the model-based methods is that they require significantly fewer evaluations than the model-free methods, especially for large search spaces . TL in autotuning is an emerging approach that leverages data from one autotuning task in related autotuning tasks to improve sample efficiency significantly. Related autotuning tasks are common in HPC applications, which include tuning different input sizes of the same kernel or application, tuning the same kernel across architectures, and tuning related kernels with the same computational signature. While the best configurations are often different for different autotuning tasks, TL is particularly effective when the related tasks share similar high-performing characteristics in the search space. Model-based search methods are promising for TL because the model can be pretrained or bootstrapped with the existing data from related tasks.
2.3 Gaussian Copula
The generative modeling-based TL approach that we propose is based on the GC, a well-known multivariate probability distribution in statistics literature. Let us consider a simple autotuning example with three variables: input scale, one tunable parameter, and the performance metric. After tuning several input scales, we can model the distribution of the values of the three variables independently. These are referred to as marginal distributions. The three variables are correlated, however, so we also model their interactions with one another using a joint probability distribution.
Copulas are a class of statistical modeling techniques that decompose a multivariate probability distribution into its marginal distributions and use a separate function to couple those distributions. This approach allows us to specify the correlation separately via a correlation matrix. GCs adopt probability integral transform, a technique that can transform any probability distribution into a uniform distribution and vice versa. GCs use the uniform and normal distribution as the intermediate distribution to model complex joint probability distribution. This is achieved as follows. Given the values of the three variables, a covariance matrix that models the correlation among the variables is computed. A multivariate normal distribution is then defined using the computed covariance matrix with a zero mean vector. The probability integral transform is applied to convert the marginals of the Gaussian distribution to uniform distributions. The uniform marginal distributions are then converted into the original distribution using the probability integral transform. We refer the reader to the work of Masarotto and Varin for a more detailed mathematical exposition of the statistical model and mechanics.