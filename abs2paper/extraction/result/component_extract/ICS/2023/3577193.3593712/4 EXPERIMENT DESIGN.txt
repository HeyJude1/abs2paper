4 EXPERIMENT DESIGN
We evaluate our method and several existing techniques in few-shot TL autotuning with a variety of benchmarks empirically evaluated on a real system. TL Autotuning Benchmarks. We use the Polybench 4.2 benchmark suite and several Exascale Computing Project (ECP) proxy mini-applications to evaluate our GC autotuning methodology. ECP benchmarks are multithreaded, and one (SWLite) is a GPU program. The selected applications are based on our ability to define valuable optimizations in our tuning spaces.
Polybench consists of numerical computation kernels extracted from various application domains. We utilize six of the most complex benchmarks spanning the application domains of linear algebra, image processing, stencils, and data Table : Tuning spaces for each benchmark alongside the GC's coverage and budget based on the top-30% of source evaluations. Specific parameters are described in Tables . mining: Syr2k, 3mm, Heat-3d, LU, Covariance, and Floyd-Warshall.
Benchmark
The ECP proxy applications represent essential computational kernels from high-performance computing programs, allowing for highly effective performance analyses and tuning without requiring the time-intensive execution of the entire application. We include four mini-applications-AMG, RSBench, XSBench, and SW4Lite-with different computememory access ratios and memory accesses patterns.
We parameterize each kernel with code modifications in performance-critical sections of the benchmark that may improve performance. These modifications include tile sizes, loop optimization techniques, parallelization and scheduling strategies, data allocation formats, and multiprocess synchronization frequencies. Table shows the number of unique parameters in each experimental benchmark as well as the combinatoric search space size of all possible parameter configurations. The largest search space has over 5 million potential configurations. The tuning spaces for Polybench and ECP applications are described in greater detail in Tables , respectively.
Source Tasks and Training Dataset. To form the prior knowledge for TL autotuning, we use offline autotuning through YTOPT to collect 200 evaluations in each of three nontarget tasks: small, medium, and large. Since 200 evaluations represent <5% of the search space, any chosen tuner must span performance for maximally effective TL. All of our YTOPT autotuning is performed by Bayesian optimization with random forests, tuned for 90% confidence in the 50 𝑡ℎ quantile of evaluated performance.
Table summarizes the tuning spaces of source tasks and includes the GC's predicted evaluation budget based on filtered source data. The prediction is based on the model's capability to identify one or more evaluations in the top 1% with 95% confidence, assuming that as much as 5% of pruned configurations are potentially optimal. A dash represents an unknown budget, where the overall problem size is reduced to such a degree that it is impossible to predict a budget requirement using Equation 1. In this case, the GC's tuning space coverage could fail to include the optimal region if the transfer relationship is poorly informed. Hence, we cautiously treat indeterminate budgets the same as few-shot TL for techniques that cannot determine their own budget, and we determine how well the GC can perform using the same budget as prior techniques.
Compared Approaches. We evaluate the following autotuning approaches to demonstrate our advantage:
• Baseline. Parameter values are taken directly from their respective sources; no parameter tuning is performed except compiler flag -O3. • Bayesian Optimization. Bayesian optimization (BO) without TL using YTOPT . The autotuner utilizes a random forest surrogate model and a hedged Gaussian process to evaluate the expected improvement of proposed configurations. • GPTune DTLA. GPTune with DTLA is a state-ofthe-art autotuner that is capable of utilizing TL using a neural joint model to combine Gaussian processes representing individual parameters.
• Gaussian Copula (ours). A GC is fit to the top 30%
performing data from source tasks, then conditionally sampled on the target task to generate configurations.
Autotuning Procedure. Each benchmark has three source task sizes (small, medium, and large) based on given magnitudes of performance-scaling input features. The three target task sizes are small-medium (SM), medium-large (ML), and extra-large (XL). The first two represent two interpolations between source tasks, and the last is an extrapolation beyond the scope of source tasks. Each target is tuned independently after the model is exposed to all source datasets. In order to permit the fairest possible comparison among different techniques, the same source tasks dataset is presented to each transfer-capable technique, but the GC filters source datasets for its benefit. In order to mitigate the variance of randomness employed by each technique, the few-shot tuning process is repeated with three random seeds, and results are reported using the average across all seeds.
We permit each autotuning technique a fixed budget of 30 evaluations per target task. Even when the GC can predict a viable budget of fewer than 30 evaluations shown in Table , we collect all 30 and specifically note the intermediate results when the predicted budget is exhausted. Since we expect TL techniques to extract some understanding of the problem from prior data, we evaluate success primarily based on the best-observed performance among limited evaluations.
For all evaluations of source and target tasks, the parameterized code is compiled once and executed three times. The autotuning objective is reported as the mean of the last two evaluations to minimize the impact of variance and uncontrolled noise in the execution environment. Timing data is collected with internal measurements in the benchmark source code to ensure that overheads such as process startup and data initialization are excluded.
Experimental Platform. All experiments are conducted on a Linux machine with 320 GB 2x AMD EPYC 7742 64-core processor (128 total cores) 1 TB DDR4 with Ubuntu 20.04.2 LTS. The machine also includes a 40 GB NVIDIA A100, which we use for evaluating the GPU-based SW4Lite ECP application. Measurements of elapsed time include time for sample generation, source code compilation using the Clang compiler, and program execution. Each benchmark internally measures empirical performance.
Because the tuning spaces we defined express optimizations through Polly , a loop optimizer for LLVM, we use a Clang compiler (version 13.0.0) for compilation. However, any compiler that supports Polly is suitable for replicating our experiments. Some Polly optimizations can be applied heuristically based on analysis of the LLVM intermediate representation, while others can be induced by programmersupplied #pragma directives in the source code. Currently, not all code transformations can be specified by directives, such as unroll-and-jam, loop fusion, and code motion. For this reason, two of our applications (3mm and LU) adopt heuristic optimizations.