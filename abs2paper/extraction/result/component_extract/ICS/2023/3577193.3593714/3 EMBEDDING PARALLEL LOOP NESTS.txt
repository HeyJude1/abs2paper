3 EMBEDDING PARALLEL LOOP NESTS
The basis of the similarity search is a representation of parallel loop nests which captures a rich set of performance-relevant properties. This representation should encode static properties such as the structure of loops, and the data accesses, but also reflect dynamic properties such as the bandwidth utilization, the thread imbalance, or the amount of mispredicted branches. In contrast to approaches solely focusing on runtime prediction for data-oblivious applications , the purpose of this representation is to provide a detailed description of performance for general parallel loop nests; the runtime itself does not expose information about the potential for optimization.
We compute the representation of parallel loop nests using neural networks based on both static and dynamic features, depicted in Figure . Dynamic features (performance counters) measured on representative inputs allow the model to treat input-specific aspects of a parallel loop nest as gaps in the static analysis. These features inform the model about the behavior of the loop nest via hardware metrics. For example, the load imbalance between threads is a direct result of a matrix's sparsity pattern in a sparse matrix multiplication.
3.1 Parallel Loop Nests
Before introducing the representation, the term parallel loop nest shall be defined in detail. A parallel loop defines a parallel iteration space and a (possibly empty) body of computations executed for each iteration. A parallel loop nest is an ordered tree where each node is a parallel loop nested inside the iteration space of the parent.
A program is considered a set of parallel loop nests, which are optimized independently. This assumes that optimizations on the full program have been determined beforehand, e.g., the identification of parallelism and the fusion of parallel loop nests. A fusion strategy based on similarity is briefly discussed in Section 8.
The computations and loop extents are not assumed to be known at compile-time. In particular, the body may comprise sequential loops and recursions whose function depends on input data. Compared with other models , this definition relaxes the requirements of compile-time known loop extents, operations, and memory access patterns.
3.2 Encoding
The encoding maps the parallel loop nest given in an intermediate representation (IR) to a set of features, which can be processed by a neural network. The encoding of parallel loop nests consists of two parts: a graph encoding of the static IR and an encoding of the dynamic profiling information in a single vector. A detailed list of the used static and dynamic features is presented in Appendix A.
Static Encoding. The basis of the static encoding is a parallel loop nest represented as a stateful dataflow multigraph (SDFG) . SDFGs combine state machines with dataflow graphs to represent complete programs, which makes them amenable for static analysis and simplifies the mapping to a graph encoding. However, the approach could equally be implemented with other IRs, e.g., LLVM IR .
At the outermost scope, the SDFG of a parallel loop nest is a dataflow graph comprising at least a single parallel loop, called map. As shown in Figure , the body of the map may comprise nested maps, tasklets (operations), or nested SDFGs. The components of an SDFG are mapped to a graph of nodes with features and edges as follows:
‚Ä¢ Access node: Access nodes represent data in the data-flow graph and are mapped to corresponding nodes in the encoding. These nodes are represented by features such as shape, total size, data type, and data layout.  in an SDFG defining the structure of the data accesses. Accordingly, memlets also define the edges of the encoding. In order to collect features for memlets, each edge is split into two edges and an intermediate node encodes the memlet itself. Data accesses are additionally encoded in an access matrix following the format of the polyhedral model .
Non-affine accesses are represented by an empty access matrix and a special flag indicating a non-affine access.
Dynamic Encoding. Processor hardware architectures provide facilities called performance counters to collect detailed statistics about the execution of a program. For example, counters for the total number of executed instructions, or the number of bytes transferred between different levels of the memory hierarchy. We encode the dynamic profile of a parallel loop nest in a single vector of performance counters for the entire parallel loop nest. In total, 19 counters are selected from 8 different categories: instructions, FP32, FP64, branching, main memory, L3 cache, L2 cache and DRAM controller. A detailed list of the counters can be found in the appendix. The selected counters are available on all modern high-performance CPUs, ensuring the portability of the approach. Each counter is measured for all threads during the profiling, and the statistics min, max, mean, std. deviation, and sum are computed over all threads. Hence, the resulting vector contains 95 different features.
3.3 Model
As illustrated in Figure , the two encodings are first processed in separate branches of the neural network. A linear embedding layer maps the dynamic encoding to a dynamic embedding. A graph neural  network (GNN) based on the graph transformer operator maps the static encoding to node embeddings, which are summarized into a graph embedding by an attentional pooling layer . Finally, the graph embedding is concatenated with the dynamic embedding and mapped by another MLP to an embedding of the entire parallel loop nest. The size of the embeddings is fixed to 128 for node and graph embeddings. In total, the model comprises 44 layers and 862,000 trainable parameters. The implementation of the model is written in PyTorch 1.13, using standard GNN layers from PyTorch Geometric.
Targets and Training. To train the model, we add another linear layer to the model, which predicts a target vector based on the embedding of the parallel loop nest. These targets comprise 20 standard performance metrics of the parallel loop nest summarized in Figure . This includes the runtime, the instructions per cycle, different bandwidths, cache miss ratios, and several rates of specific operations per total instructions. We choose the mean absolute error as the loss function and train the model for 20 epochs using Adam at a learning rate of 1e‚àí3. We do not specifically tune the hyperparameters of the model beyond manually setting an initial learning rate, and use an early stopping approach for the weights.
Dataset. We synthetically generate the training and validation set from standard kernels such as maps, reductions, and stencils. In particular, we include non-data-oblivious kernels such as boolean masks. The test set is extracted from real-world applications implemented in NPBench by automatically cutting out each parallel loop nest. The sizes of the training, validation, and test sets cover approximately 6,500, 2,000, and 1,000 parallel loop nests, respectively. In contrast to other models designed to predict the speedup of different schedules, we consider a single canonical schedule, which significantly reduces the input variation. The canonical schedule executes the outermost loop of the loop nest in parallel.
Target Architecture. The target architecture is an Intel Xeon Gold 6140 CPU with a base clock rate of 2.3 GHz and 768 GB of main memory. The entire dataset is labeled automatically with LIKWID , which defines groups of performance metrics that can be measured simultaneously. Each group of metrics is measured in two phases: In a warmup phase, the program is executed ùëõ ùë§ times, where ùëõ ùë§ is chosen such that the logical number of bytes moved corresponds to twice the size of the L2 cache but clipped to a maximum of 1,000 repetitions. In the measurement phase, the program is executed ten times and the median is taken over those measurements to convert the measurements into a single label. In general, most metrics report the measured mean over all threads. However, global throughput metrics such as bandwidths or the instruction per cycle are summed over the threads; the runtime is considered as the maximum over all threads.
3.4 Validation
Before evaluating the quality of embeddings on application-specific tasks, we validate the model on the prediction of the performance metrics. Figure lists the Pearson correlation coefficient between the targets and the model's predictions on the test set for the different performance metrics. In this figure, the performance metrics are ranked by their difficulty of prediction by our model in descending order. The minimum correlation of 0.60 is found for Instructions Per Cycle and the maximum correlation of 0.98 for the metric of Dropped Cache-Lines Bandwidth. For 17 out of 20 targets, the correlation is at least 0.80, indicating a strong correlation between the model prediction and the target labels. These results also correlate with the difficulty of prediction in general, as, e.g., the Instructions Per Cycle metric depends on multiple hardware and system factors.