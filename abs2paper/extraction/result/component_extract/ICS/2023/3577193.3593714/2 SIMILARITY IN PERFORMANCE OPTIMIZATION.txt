2 SIMILARITY IN PERFORMANCE OPTIMIZATION
Programs with different structural properties may still share similar performance characteristics, which allow them to be optimized in similar manners.
The following example shows a standard matrix multiplication and a min-plus matrix multiplication commonly used for shortestpath problems:
The loop nests are structurally identical, thus trivially sharing similar performance characteristics. Consequently, the min-plus matrix multiplication can be optimized using the same tiling, buffering, and vectorization strategies found in the literature for matrix-matrix mutliplication . Due to the structural similarity, existing autoschedulers based on the polyhedral model are able to detect this reliably , reaching a significant speedup over the na√Øve version.
The same potential for optimizations can, however, also be observed in a structurally different application, such as the sparsedense matrix multiplication shown below:
Compared to the regular, dense matrix multiplication from before, this loop nest is no longer data-oblivious since the innermost loop bounds are data-dependent. The sparsity pattern of the input thus determines the workload's characteristics (e.g., load balancing over multiple threads). Regardless of those characteristics, both programs exhibit a strided memory access to the dense matrix B, which can be resolved by interchanging the two innermost loops to improve performance. Existing auto-schedulers can apply this optimization to the original matrix multiplication, but can only transfer these optimizations to the sparse multiplication if their performance models indicate similar performance characteristics. Such static models must, however, make simplifying assumptions on the code, either assuming a fixed sparsity pattern and overapproximating the loop bounds , or using an inspector-executor model to produce code conditionally. Both assumptions hinder possible further optimizations with regard to load imbalance and dynamic characteristics.
A case where the structural differences are even more pronounced is shown below, where the first program computes a sparse matrixvector product, and the second program performs a prime number check on an array of 20,000 numbers: Despite their structural differences, both programs are inherently prone to an imbalanced distribution of work among different threads when parallelizing the outermost loop. In both cases, a dynamic assignment of work to threads yields significantly better performance for specific input distributions. While a purpose-built, dataspecific model can address this problem for the sparse matrixvector product, the same model cannot directly be applied to the structurally-different prime number filter. Hence, in order to identify similarities and transfer optimizations between data-dependent applications, the integration of a larger number of specialized models would be necessary.
In contrast, performance engineers are able to identify similarities between both data-oblivious and data-dependent applications treating data-dependent aspects as gaps, which are inferred through profiling. Performance embeddings adopt this observation by encoding both static and dynamic performance characteristics of parallel loop nests, enabling the transfer of optimizations across more general problems.