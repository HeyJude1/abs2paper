7 RELATED WORK
Automatic performance optimization and performance modeling for optimization has been studied by a variety of works. The following section summarizes prior related research.
Performance Modeling and Extrapolation. Several wo-rks focused on the automatic prediction of program and subprogram performance. One of the earlier instances of using machine learning for performance modeling was performed by Ipek et al. , who use an MLP to predict application performance. Carrington et al. and Siegmund et al. also provide performance prediction for tuning via heuristic means on an application-level, and Calotoiu et al. model and extrapolate runtime dependency on parameters of general codes via time measurement of multiple small experiments. Most such works do not focus on the optimization transformations and their choice, but rather on accurate execution time prediction.
Application-specific performance models introduce domain knowledge into the prediction and often use the generated communication or performance model to inform an optimization search without executing the program, which might be expensive due to running on distributed environments. . The embeddings are colored by the optimal scheduling type, i.e., static (purple ) and (orange ).
Polyhedral Compilers. The Pluto , PENCIL , and LLVM Polly compilers express performance optimizations as the solution of an integer linear program (ILP) with respect to a hand-crafted cost model of the target architecture. For reasons of tractability of the ILP, the cost model makes strong simplifying assumptions, often yielding sub-optimal results on complex architectures .
Deep Code Representations. inst2vec , GNN-CDFG , Pro-GraML , and IR2Vec are examples of neural code representations that map static code to embeddings. The embeddings are designed to solve typical compiler tasks and classify applications according to their semantics. In contrast, performance embeddings encode static and dynamic properties, aiming to capture performance aspects regardless of the underlying algorithm.
Optimizing Compilers. Optimizing compilers are subject to extensive research. Tiramisu , Halide and TVM introduce deep learning performance models based on static features, which guide the search in the scheduling space. Singh et al. extends these performance models to graph neural networks, improving the prediction's accuracy. Steiner et al. re-formulate the search problem as a Markov Decision Problem, which can be solved using reinforcement learning. Other works utilize input-specific and profiling features to optimize programs based on classification problems: For instance, Elafrou et al. train a neural network to choose between classes of optimizations for sparse linear algebra routines. Dutta et al. combine a pattern classifier and performance counters for selecting OpenMP configurations. Our approach separates the performance model from the optimization by introducing an offline optimization database. This allows the local search in the application space, which significantly reduces the complexity of the search and allows for the extension of the optimization space without re-training the model. In particular, our database-based approach is not limited to a fixed set of optimizations.
Transfer Tuning. Martins et al. cluster C functions based on static features to select the optimal compiler passes according to the cluster assignment. Gibson and Cano provide a constrained definition of the term transfer tuning as the reuse of optimizations found by auto-schedulers for specific operations in tensor programs. The discovered optimizations are matched by hand-crafted heuristics to other operations. Our approach extends this concept to intermediate representations and optimizations based on a fuzzy matching of node embeddings. The similarity of performance embeddings thereby generalizes hand-crafted transfer rules.