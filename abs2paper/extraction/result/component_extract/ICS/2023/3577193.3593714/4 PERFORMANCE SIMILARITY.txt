4 PERFORMANCE SIMILARITY
A similarity search for performance optimization requires that similar embeddings imply similar performance optimization potentials. For instance, if a parallel loop nest has a low memory bandwidth utilization, this loop nest should be mapped to an embedding that is similar to the embeddings of other parallel loop nests with low memory bandwidth utilization.
We evaluate this hypothesis based on the local variation of parallel loop nests under different performance metrics. Specifically, for each parallel loop nest in the test set, we query the 3-nearestneighbors based on the embedding distance and compute the relative standard deviation among these four loop nests for a specific performance metric. We define the mean of the local variations in the test set as the performance similarity of the model. Below, we discuss the similarity metrics we use for our evaluation, the state-of-the-art baselines we compare with, and analyze similarity on the NPBench dataset.
Assessing similarity. Since the cost for data movement is the dominant factor in performance optimization , we focus on memory-specific performance metrics for evaluation. The memory usage efficiency (MUE) combines the following two performance metrics to assess the optimization potential of a program:
‚Ä¢ Main / L3 / L2 Memory Bandwidth: The attained memory bandwidth on different levels of the memory hierarchy is a standard metric to identify optimization potentials in typical bound-and-bottleneck analyses (cf., Roofline model ). ‚Ä¢ Data Locality: Fuhrer et al. point out that an analysis based on solely the attained memory bandwidth ignores the intrinsic limitations of the algorithm. For instance, a loop nest with a strided memory access pattern and a loop nest with a random memory access pattern may both yield low memory bandwidths. However, the former may still be optimized through a loop interchange, while the latter already achieves its maximal bandwidth utilization. The data locality accounts for these algorithmic limitations and is defined as the ratio of the I/O lower bound Q of the algorithm and the measured transferred bytes from main memory D, in short, ùëÑ ùê∑ . Q is estimated automatically by SOAP-Analysis , which is based on the concept of the Red-Blue Pebble Game .
Baselines. To assess the model's performance, we compare the similarity of our embeddings with three other models that map parallel loop nests to embeddings, and perform ablation studies on the input features.
The reuse distance analysis is a traditional approach to loop nest analysis, which simulates the execution of the loop for a specified number of iterations on a simplified cache model. Using this simulation-based analysis, we map each loop nest to a fourdimensional vector of the cache miss ratio, the bytes read from and written to the memory, and the arithmetic intensity. The movement of bytes gives a strong indication of the efficiency of the memory access patterns, and the arithmetic intensity is typically used to estimate the performance of a program on a target architecture. Since the simulation of loop nests is expensive, we only simulate the first 500 iterations of the loop nest.
IR2Vec provides embeddings of programs based on LLVM IR. The embeddings are trained in an unsupervised manner and can be used for various machine learning tasks related to program properties and source code.
Baghdadi et al. introduce a state-of-the-art performance model for optimizing polyhedral programs. The model estimates the speedup of a schedule and a loop nest based on static features and a recurrent neural network. Since the model is designed to predict the speedup of a certain schedule, we remove the linear prediction layer and obtain the embedding of the parallel loop nest from the input of this last layer. Results. Table summarizes the performance similarity of the baseline feature extractors and our model. Our model has a strictly lower local variation for all performance metrics and thus yields a higher performance similarity. Hence, the performance optimization based on the local neighbors in our embedding space is more likely to resolve the actual performance bottlenecks of a parallel loop nest. Furthermore, we run ablation studies using only one set of the static/dynamic features. The studies show that the selected dynamic features are sufficient for reasoning over bandwidth. However, static features (such as array accesses) are crucial to understand memory access patterns for data locality and I/O complexity.
Similarly to text analogies for word embeddings, we additionally verify our representation through the use of several distance tests. For example, one of our tests implements three operations: linear copy of two arrays (denoted as ùëé), indirect copy with a random permutation on the indices (ùëè), and indirect copy with the identity index permutation (ùëê). In all of our learned embeddings, ùëë (ùëé, ùëê) < ùëë (ùëé, ùëè) for the cosine distance ùëë.
To further understand the similarity induced by our model, Figure visualizes the embeddings of the test set in a t-SNE plot . A t-SNE plot reduces high-dimensional data onto a 2D plane based on neighborhood minimization. In the figure, each sample is a point colored by its data locality; a plot that is separable by color, as our model's embedding space is (Figure ), indicates a strong influence of the performance metric in the representation of the sample. For comparison, Figure shows that the data locality is not an important factor for the representation of the sample, depicted by scattered clusters.
Evaluating importance of static features. Since the model has a rich set of dynamic features available, the question arises whether the static encoding is used by the model. To analyze this question, we analyze the structure of the node embeddings for the input array access nodes of a parallel loop nest. We extract the node embeddings of input arrays from 350 synthetically generated parallel loop nests. For each array, we measure the L2 load bandwidth of the isolated access to the array.
We programmatically isolate the access by modifying the parallel loop nests. For example, the isolated access to a matrix ùêµ in a matrixmatrix multiplication is shown below:
The resulting t-SNE plot of the node embeddings of input nodes is depicted in Figure . The samples are colored by the measured L2 load bandwidth showing that local groups of node embeddings are similar in the bandwidth of their access. This indicates that the model generates meaningful embeddings for these nodes based on static features such as the access's stride and the array's size.