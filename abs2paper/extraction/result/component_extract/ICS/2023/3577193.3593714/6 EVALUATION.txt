6 EVALUATION
We now evaluate transfer tuning in two case studies: In the first case study, the optimizations found by a state-of-the-art auto-scheduler for polyhedral applications are transfer tuned between applications from different domains such as image processing, numerical weather prediction, and linear algebra. In the second case study, dynamic scheduling decisions are transfer tuned between sparse matrix-matrix multiplication (SpMM) for matrices from suitesparse .
6.1 Case Study: Auto-Scheduler
Baghdadi et. al. train a speedup prediction model and use this model to guide the search of the Tiramisu auto-scheduler in a large scheduling space consisting of typical loop transformations such as loop interchange, tiling, parallelization, and vectorization. We show that transfer tuning the discovered optimizations between applications based on the performance embeddings reduces performance optimization to a local search. The evaluation set consists of 12 applications comprising approximately one hundred parallel loop nests.
Experimental Setup. To find a strong reference optimization for each parallel loop nest, we run the Tiramisu auto-scheduler's Monte-Carlo Tree Search (MCTS) for a larger number of epochs. Additionally, we test the 100 best hypotheses found by the search on the target architecture to determine the overall best-performing configuration. Hence, the optimization database comprises approximately one hundred schedules corresponding to the total number of parallel loop nests. The transfer tuned optimization for a parallel loop nest is found by a ùëò-nearest-neighbor search in the embedding space of all parallel loop nests except for the parallel loop nest to be tuned (leave-one-out). To apply the Tiramisu auto-scheduler to our graph IR, we implement a converter from SDFGs to the representation of programs used by this model. lists the results of the Tiramisu auto-scheduler's optimization of each application as well as the results obtained by transfer tuning for ùëò = 5 and ùëò = 10 neighbors. For the majority of applications, the transfer-tuned runtime is within 5% of the reference at a fraction of the search complexity, see MCTS Space column for the number of configurations tested by the auto-scheduler. Since the reference optimizations are found once and then stored in the database, transfer tuning enables exhaustive offline optimization of applications with a large scheduling space.
Results. Table
Daubechies Wavelet. In the embedding space, the neighbors of a parallel loop nest act as a of explored search paths based on slightly varied input conditions. The Daubechies wavelet benchmark is an example where this neighborhood yields a considerable speedup. The application consists of a single parallel loop nest, where the outermost loop iterates over the 3 channels of an image. Parallelizing over this loop induces a major performance bottleneck on a CPU with 36 cores since most cores are idling.
Upon inspecting the transferred transfer tuning results, we see that it optimized according to the Haar wavelet: MCTS fails to find an optimization maximizing the parallelism for the Daubechies wavelet, but succeeds in finding an optimization for the almost identical Haar wavelet. This also showcases an important feature of performance embeddings -as opposed to end-to-end neural networks, transfer tuning provides explainability for its optimization decisions.
Other examples are the Harris filter and the histogram filter, in which transfer tuning finds additional potential for applying the optimization found within the same benchmark. The runtime difference of transfer tuning for five and ten neighbors relative to the runtime of the Tiramisu autoscheduler for polyhedral applications. The auto-scheduler uses Monte-Carlo Tree Search (MCTS) to explore a large schedule space, whereas transfer tuning is a local search based on a few nearest neighbors.
Multi-Layer Perceptron (MLP). Although matmul and min-plus matrix multiplication are potential candidates for optimizing the layers in mlp, we see that transfer tuning performs worse for this particular benchmark. The matrix multiplications of matmul and mlp differ significantly in the dimensions of the matrices: while matmul multiplies a 1024 √ó 2048 and a 2048 √ó 1024 matrix, mlp multiplies weight matrices, which have a small leading dimension of 64 corresponding to the batch size. Hence, the matrix multiplications define different trade-offs of data locality and parallelization. This shows that the optimization database's density (i.e., the availability of similar neighbors) is an important hyperparameter of transfer tuning.
6.2 Case Study: Tailored Optimization
In the second case study, we demonstrate the extensibility of transfer tuning to custom optimizations by dynamically scheduling Sp-MMs for matrices from suitesparse . A typical performance bottleneck of SpMM is an imbalanced distribution of work among the threads, resulting from the distribution of the non-zero elements. The standard optimization is then to change the scheduling from a static assignment of work to threads to a dynamic assignment, which incurs some overhead for the execution.
Experimental Setup. To define an optimization database for the scheduling decision, we determine the optimal schedule for 42 sparse matrices from suitesparse by benchmarking OpenMP's default static schedule and a dynamic schedule of chunk size 8. The matrices are multiplied by a dense matrix of 512 columns filled with random values. We evaluate whether transfer tuning can decide the optimal schedule by splitting this set of matrices into a set that is stored in the optimization database and a test set. The scheduling of the test set matrices is then determined by a 1-nearest neighbor query to the database. The resulting runtime of the matrices is compared with the Intel MKL 2021.3 implementation of SpMM. Results. The t-SNE plot of the SpMM embeddings of all matrices is depicted in Figure , where the embeddings of the different matrices are colored by their optimal schedule. The separation of groups by colors already indicates the applicability of the 1-nearestneighbor approach to dynamic scheduling. Table summarizes the runtimes of both schedules, the runtime after transfer tuning as well as the Intel MKL baseline. Transfer tuning picks the correct scheduling decision for 8 out of the 10 test benchmarks. Furthermore, the comparison with the Intel MKL baseline shows a significant speedup of the optimal scheduling for a different subset of 8 out of 10 benchmarks.
BERT. The BERT transformer is a standard neural network architecture in natural language processing. The sparsification of the dense layers is a common technique to enable efficient inference by sacrificing a reasonable amount of accuracy . In order to show the cross-domain transfer of this knowledge, we repeat the above experiment for the sparse weights of a sparsified model , yielding a similarly separable embedding space for transfer tuning. The tSNE plot of the sparse weights is depicted in Figure .
In conclusion, transfer tuning yields comparable performance speedups on all tested cases, at times outperforming existing tools and libraries by inferring cross-application optimizations. It can adapt to additional insights gained by automated tools and tailored optimizations and can be inspected to explain its reasoning behind certain optimizations via the chosen neighbor.