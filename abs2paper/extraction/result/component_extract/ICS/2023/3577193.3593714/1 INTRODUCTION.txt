1 INTRODUCTION
Automatic performance optimization of programs for modern computing architectures is challenging. Even for smaller programs, the possibilities to schedule the operations and the data movement become infeasible to explore exhaustively. To efficiently navigate the optimization space, a performance model could be constructed as a surrogate to approximate the search; the searched parameters can be limited to a small number for brute-force tuning; or, more often than not, the program is optimized manually by a performance engineer.
Several performance models have been developed for specific program classes, notably Polyhedral subprograms . The polyhedral model has helped develop several automated tuning methods based on integer-linear programming and machine learning as well. Such methods primarily target optimizations on the loop level such as interchanging their order and tiling the iteration space. However, these techniques are limited in representing real-world applications due to the need for expressing programs with affine array accesses and simple loop bounds.
Methods for optimizing data-dependent applications, such as sparse linear algebra routines, must rely on specialized, inputspecific models . Because such models are hard to integrate into a general tuning framework, performance engineers often fall back to general profiling-based performance models, such as the roofline model , for custom applications. Since profiling-based models lack a connection to the algorithmic structure, their interpretation requires significant experience , which makes the search for optimizations hard to automate. Optimization efforts for real-world applications are thus often resource-intensive manual processes where the outcome strongly depends on the skill set of the individual performance engineer .
In this paper, we present a similarity-based approach to the automatic performance optimization of general loop nests, summarized in Figure . We develop a method for encoding both static and dynamic performance characteristics of loop nests and capturing them as performance embeddings -a latent, continuous space in which a multidimensional point represents a subprogram. Based on these embeddings, which are trained separately, optimizations derived from a variety of methods (such as brute force, manual tuning, or state-of-the-art auto-schedulers) are stored in an optimization database. This enables knowledge transfer of optimization between different programs with similar static or runtime characteristics, which we call transfer tuning.
During transfer tuning, loop nests are then optimized by fuzzy matching the optimizations of the k-nearest neighbors from the database according to their performance embeddings. We demonstrate the effectiveness of our approach on a series of polyhedral and non-polyhedral real-world applications, significantly reducing the search complexity for performance optimizations and outperforming state-of-the-art auto-schedulers by reaching up to 92% better runtime improvements.
In summary, this paper makes the following contributions:
• Methodology for encoding performance characteristics of general loop nests in performance embeddings; • Development of a general matching algorithm for loop nest optimizations; • Reduction of the optimization search space size by orders of magnitude through transfer tuning; • Demonstration of effectiveness compared with state-of-theart auto-optimizers and extension to tailored optimizations.