6 CONCLUSIONS
We proposed using additive modifications in Gaussian elimination for dense matrices to avoid the performance overheads associated with partial pivoting while retaining the numerical stability achieved in classic implementations of LU factorization. As a result, we recorded speedups reaching as high as 5√ó against the GEPP implementation on a GPU-accelerated supercomputer. Our method of additive modifications for dense matrices (unlike single-element modifications occasionally used for sparse matrices) factors the diagonal blocks with the SVD to reduce the number of modifications required. We experimentally established that BEAM provides better performance for comparable accuracy to GEPP and better accuracy for comparable performance to GENP for the majority of our test matrices. Furthermore, by testing this approach both with and without the Woodbury formula, we find that (when using iterative refinement) omitting the Woodbury correction often provides a better time-to-solution than when applying the correction.
6.1 Parameter Selection
The success of BEAM depends heavily on both the tolerance and whether to apply the Woodbury formula. The block size also matters but, based on Table , to a lesser extent; we suggest starting with the size of cache-blocking for non-pivoted LU or slightly larger. Below, we analyze in detail considerations for choosing the threshold and whether to use the Woodbury formula. But as a starting point, we suggest œÑ = min(0.5ùúÖ 2 (ùê¥), 10 ‚àí8 ) and no Woodbury formula. For most applications, however, various configurations should be tested on the linear systems produced by representative domain problems. For the Woodbury formula, recall that in Table and Figure , the corrected solver never outperformed the corresponding uncorrected one. However, a few ill-conditioned cases failed to converge without the Woodbury formula, while all cases converged when using the Woodbury formula (except for zielkeNS, which overflowed for both). Furthermore, as noted before, Table indicates that using the Woodbury formula can enable convergence when œÑùúÖ 2 (ùê¥) ‚â• 1. Thus, the Woodbury formula appears to be preferable for ill-conditioned matrices. And, iterative refinement already measures the quality of the factorization. So, we suggest initially skipping the Woodbury formula. Then if iterative refinement fails to converge within, e.g., five iterations, use the Woodbury formula in subsequent iterations.
For selecting œÑ, we first wish to draw attention to the importance of the inequality œÑùúÖ 2 (ùê¥) ‚â™ 1. For BEAM without a Woodbury correction, implies that this inequality is a prerequisite to proving that the solution has at least one digit of accuracy and that iterative refinement can converge to full backward accuracy. For BEAM with a Woodbury correction, this is necessary to show that ùê¥ is well conditioned using Theorem 4.1. Finally, our experimental results demonstrated that violating this inequality can lead to a failure of BEAM without the Woodbury formula. Although, our experimental results also failed to show a similar result when the Woodbury formula was applied. Thus, there may be a subtle interaction between the perturbations and the resulting Woodbury correction that leads to better stability than the existing analysis suggests.
Beyond ensuring œÑùúÖ 2 (ùê¥) ‚â™ 1, there are a few relative concerns in the selection of œÑ. First, consider the omission of the Woodbury correction. Recall that in Table , the smallest tolerance (i.e., 10 ‚àí10 ) outperformed the largest tolerance (i.e., 10 ‚àí6 ) in all but one case. Furthermore, the added perturbations become overwhelmed by the roundoff perturbations when 10 ‚àí10 ‚â≤ œÑ ‚â≤ 10 ‚àí8 (depending on the matrix). Thus, a small tolerance, such as the square root of unit roundoff, is preferable. Next, consider the inclusion of the Woodbury correction. Here the number of modifications becomes relevant in addition to their magnitude. Unfortunately, we know of no way to determine a priori the number of modifications that will result from a given tolerance. However, Table suggests that, like in the uncorrected case, smaller tolerances usually result in better performance. Thus, we recommend starting with a similar tolerance to the uncorrected case.
6.2 Future Research Directions
Because of the novelty of this approach, there are several areas in which it can be improved. First, the theoretical analysis could be significantly extended, particularly for growth factors and numerical errors in both the worst and the average cases. Second, the experimental accuracy results indicate that matrices with many modifications are sensitive to a small tolerance. Thus, it would be beneficial to explore using a more dynamic policy that increases the tolerance if too many modifications are made. Relatedly, the overhead of applying correction is only significant when many modifications are applied. Thus, a scheme where only a subset of modifications (e.g., the largest twenty) are corrected may better exploit the tradeoffs between BEAM with and without the Woodbury correction. Next, the SVD will be expensive for large block sizes. Replacing it with a cheaper factorization, such as pivoted QR, may allow for increasing the block size without impacting performance. Finally, some previous efforts involving element-wise modifications have included a permutation to help place large elements on the diagonal before starting the factorization . Such permutations may improve the reliability of BEAM (and other pivot-avoiding methods) since many of the troublesome matrices have leading blocks with small norms.
Another area of potential future work is in applying BEAM to matrices with exploitable structure since it provides numerical stability without destroying the block structure. First, solving symmetric-indefinite matrices requires either an LU factorization (which cannot exploit symmetry) or symmetric pivoting (which incurs complicated data-access patterns that are prohibitive on modern memory hierarchies). So, BEAM could be used to exploit the matrix's symmetry while avoiding the complexity of symmetric pivoting. This would likely take the form of a block LDLT factorization, with a symmetric-eigenvalue decomposition replacing BEAM's SVD. Second, while the approach of element-wise modifications has been previously explored for sparse matrices , our proposal of choosing modifications based on diagonal blocks has not. This would be particularly useful for block-sparse matrices or multi-frontal methods where the diagonal blocks are already dense and no extra fill-in would be incurred. Finally, there are several matrix formats designed to exploit low-rank structures within dense matrices . Unfortunately, pivoting is quite restricted in such formats, which limits the matrices to which they can be safely applied. Thus, BEAM may improve the numerical stability of factorizations using such bespoke formats.