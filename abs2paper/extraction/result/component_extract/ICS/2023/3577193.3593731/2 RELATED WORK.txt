2 RELATED WORK
Using element-wise modifications instead of pivoting was first suggested by Stewart for sparse matrices . However, preserving the sparsity limited the modifications to only a single diagonal element at a time. Additionally, the adjustments were combined into the factorization in a recursive manner that limits the available parallelism. Unfortunately, this approach has seen limited use outside the optimization community . There are a few areas where BEAM differs from past efforts. First, our new algorithm enhances dense LU factorization which is significantly more compute-intensive than the sparse equivalent. Furthermore, dense problems are unconstrained by sparsity issues. Next, the modifications are chosen based on entire diagonal blocks instead of individual diagonal elements, giving us a greater opportunity for exploiting non-local numerical properties. Finally, all modifications are corrected simultaneously at the end of the factorization. This increases the arithmetic intensity when applying the Woodbury formula, which improves the performance on modern hardware. An interesting variant of the additive approach is to correct the perturbations by adding extra rows and columns to the matrix . That is, the system 𝐴𝑥 = 𝑏 is replaced with
𝐴 + 𝐹 1 𝐹 2 𝐹 1 𝐹 2 𝐼 𝑥 𝑦 = 𝑏 0
where 𝐹 1 𝐹 2 modifies the appropriate diagonal elements. This variant is closely related to the derivation of the Woodbury formula via the Schur complement . Unfortunately, this idea has not been explored beyond avoiding fill-in for sparse, symmetric positive definite matrices with a few dense rows. We used the Woodbury formula to apply the correction instead of this approach to simplify matrix allocation. A final approach is to apply modifications without directly correcting them, relying instead on iterative refinement . While iterative refinement based on stationary schemes can recover minor errors, large errors can slow or even prevent convergence, especially for ill-conditioned matrices. Thus, there is a trade-off in the factorization's backward error between the direct perturbations to the matrix and the error induced by element growth. Using the Woodbury formula directly addresses the perturbations, which helps alleviate this conflict. Therefore, we tested the additive approach both with and without the Woodbury formula. As before, the prior work is related to ours, but our approach chooses modifications based on diagonal blocks and is applied to dense matrices unencumbered by sparsity considerations.
Beside additive modifications, there have been various other approaches to remove or reduce the cost of pivoting. One such strategy is to replace pivoting with randomized preprocessing, which allows using an optimized GENP code for the factorization at the cost of preprocessing the matrix . Recent work has demonstrated significant speedups for many types of matrices; unfortunately, these approaches can fail in some cases . Thus, it is valuable to investigate algorithms that may be successful for linear systems where randomization is insufficient. Furthermore, using BEAM in combination with randomized preprocessing is likely to be more robust than either of the methods separately.
Another pivoting-replacement strategy is a hybrid of the LU and QR factorizations . This algorithm attempts to factor each block column with GENP. It then tests the stability and, if unstable, re-factors the block column with the unequivocally stable QR factorization. Thus, in the best case, the factorization progresses as per GENP but provides more robust behavior when GENP struggles. Unfortunately, for task-parallel implementations testing each block column for stability and the occasional QR factorization results in similar parallel dependencies to GEPP, which reduces the available parallelism. This hybrid method and our algorithm are similar in their optimism about GENP, but they differ in the mechanism by which they recover stability in problematic cases.
Finally, there have also been efforts to reduce the cost of pivoting without completely removing it. The most well-known approach is tournament pivoting, which computes pivots block-wise to avoid synchronizing for each column . Another strategy is to relabel the rows without exchanging them in memory, although swapping rows may still be needed for load balancing . The recent CO𝑛𝑓 LUX algorithm goes further by combining these strategies . Finally, a recent proposal, threshold pivoting, tries to reduce data movement by allowing the selection of pivots smaller than the column's maximum . Unfortunately, these approaches still incur significant pivoting overheads and reduce the available parallelism since the exchanged rows are unknown until runtime.