5 EXPERIMENTAL RESULTS
To investigate the feasibility of this approach in terms of numerical stability, scalability, and performance, we implemented it in the Software for Linear Algebra Targeting Exascale (SLATE) library and evaluated it on the Summit supercomputer. SLATE is a dense linear algebra library that targets distributed-memory, GPU-accelerated systems . Our code and results are available at https://doi.org/ 10.6084/m9.figshare.21982472.
Our implementation follows Algorithm 1 and uses a high-level structure based on SLATE's GENP routine . However, we separated BEAM's algorithmic block size from the distribution tile size (the former being smaller than the latter). For simplicity, our code does not support an algorithmic block to be split across multiple tiles of SLATE and will truncate the last block in a tile to fit. But all our experiments align the block and tile sizes so that truncation only happens in the last tile. After the factorization is complete, the capacitance matrix is built and factored. While our theory defines ğœ in terms of âˆ¥ğ´âˆ¥ 2 , this is expensive to compute in practice. So, our experiments instead used the Frobenius norm, ğœ = Ï„ âˆ¥ğ´âˆ¥ ğ¹ , which is closely related.
For performance purposes, we implemented a GPU routine for batched, block-triangular solves, using a recursive formulation similar to the MAGMA and KBLAS libraries. Because the diagonal blocks come from the SVD, these inverses can be realized by a matrix multiplication and sometimes a diagonal scaling. While cuBLAS's batched GEMM routine was effective for the trailingmatrix updates, its performance was lacking for small block sizes due to the subsequent copy or scale operation. For such cases, we implemented a custom routine that combined the multiplication and the copy to improve cache reuse and avoid extra kernel launch overheads. To reduce the effort in performance tuning, we used part of MAGMA's matrix-multiplication routine in our kernel.
5.1 Experimental Setup
We ran our experiments on eight nodes of the Summit supercomputer at Oak Ridge National Laboratory. This machine is based on IBM Power System AC922 nodes, each containing two 22-core IBM POWER9 CPUs and six NVIDIA Volta V100 GPUs. One core of each socket is reserved for the OS (Red Hat Enterprise Linux version 8.2). Most of the computational power comes from the GPUs, each providing 7.8 TFLOP/s, 16 GiB HBM2, and 900 GB/s memory bandwidth. Each CPU provides 540 GFLOP/s, 256 GiB DDR4 memory, and 170 GB/s memory bandwidth. NVIDIA NVLink provides a bidirectional 50 GB/s between components in a socket. A dual-rail EDR InfiniBand network with a non-blocking fat-tree topology connects the nodes.
We used a modified version of SLATE's test harness for our experiments. The tester was compiled with GCC 9.1.0, CUDA 11.0.3, IBM Spectrum MPI 10.4.0.3, IBM ESSL 6.1.0, Netlib LAPACK 3.8.0, and Netlib ScaLAPACK 2.1.0. We set the smt1 flag and started MPI with jsrun -n 16 -a 1 -c 21 -g 3 -b packed:21 -d packed which allocates 16 processes, each bound to a single socket and its GPUs. For all experiments, we configured the tester with --origin h --target d --ref n --grid 4x4 --panel-threads 20 --seed 1 --seedB 2 --matrixB randn --nrhs 1. We also set the --matrix, --dim, and --check flags as appropriate for the experiment. For GEPP, we also set --nb 768 --ib 64 --lookahead 1. For GENP and BEAM, we set --nb 512 --lookahead 2 --ib 64, except for the experiment described in Section 5.3 which changed the last argument as appropriate for BEAM. This configuration gives a 2d block-cyclic distribution with a 4 Ã— 4 process grid and blocks of size 512 or 768, as indicated by --nb. Note that SLATE's ib parameter corresponds to the ğ‘› ğ‘ value discussed in this paper; SLATE's nb corresponds to the larger blocks used to distribute the matrix. All tests were preceded by extra tests of size ğ‘› = 5000 (with the otherwise identical configuration) to ensure that our results were not influenced by initialization costs. To measure the effects of system noise, we ran each performance test three times and computed the mean and 95% confidence interval. Except for svd_geo, the error and number of modifications were the same between the different runs. Due to minor non-determinism in SLATE's QR factorization, there is slight variability between runs in the entries of svd_geo. However, this variability is small and does not affect our conclusions or analysis, so we just present the error values and number of modifications from the first run.
To understand how our BEAM algorithm behaves across various linear systems, we used seven random and eight structured matrices in our tests. Table describes these matrices. We choose a righthand side with each element randomly taken from the normal distribution. The matrix generator was always seeded with 1 for the matrices and with 2 for the right-hand sides so that the test problems can be reproduced.
Accuracy was measured with the infinity-norm backward error:
ğœ‚ âˆ (ğ‘¥) = âˆ¥ğ‘ âˆ’ ğ´ğ‘¥ âˆ¥ âˆ âˆ¥ğ´âˆ¥ âˆ âˆ¥ğ‘¥ âˆ¥ âˆ + âˆ¥ğ‘ âˆ¥ âˆ . ( 8
)
Correspondingly, in experiments with iterative refinement, the refinement was terminated when this error was less than or equal to âˆš ğ‘› times the unit roundoff (âˆ¼3.5 Ã— 10 âˆ’14 when ğ‘› = 10 5 ) or after 30 iterations. We selected this criterion based on the accuracy of GEPP (see Table ).
5.2 Baseline Accuracy and Performance Experiments
First, Table compares the accuracy of BEAM against GEPP and GENP for varying values of tolerance Ï„. The matrices were of size 10 5 , with a blocking factor of 64 for BEAM. The reported error is the infinity-norm backward error of . Most importantly, the error of BEAM with the Woodbury correction is smaller than or approximately equal to that of GENP for all but one case (orthog with Ï„ = 10 âˆ’10 ). Furthermore, BEAM with Woodbury correction has a significantly smaller error than GENP for most matrices and only incurs NaN values for one matrix, zielkeNS. (Those NaN values resulting from growth-induced overflow.) These results demonstrate the ability of our approach to provide better numerical stability than GENP. Moreover, the error was smaller than 10 âˆ’10 for many of the matrices. This implies that the iterative refinement should often converge quickly to double-precision accuracy . While Ï„ = 10 âˆ’6 leads to modifications for most matrices, only five of the fifteen matrices required more than ten modifications when Ï„ â‰¤ 10 âˆ’8 (one of which was accurately solved without any modifications when Ï„ = 10 âˆ’10 ). This indicates that the proposed approach is likely effective for a large class of matrices. Additionally, many linear systems saw a significant improvement in accuracy compared to GENP, even without modification. Thus, even just applying an SVD factorization to invert the diagonal blocks increases the stability of a non-pivoted factorization.
The results become more nuanced when considering BEAM without Woodbury correction. For Ï„ = 10 âˆ’10 , the uncorrected solver behaved similarly to the corrected one. For larger tolerances, however, there is a significant gap between the two, particularly for Ï„ = 10 âˆ’6 . This indicates that the perturbation of the uncorrected modification becomes the dominant source of error when Ï„ â‰³ 10 âˆ’8 . This aligns with both the Ï„ term in the normwise backward error bound of ( ) and the recommended tolerance when applying scalar updates without correction . In contrast, when the Woodbury correction was applied, increasing the tolerances always saw similar or better accuracies. This suggests that the error in the corrected case comes from the presence of small diagonal singular values and the resulting growth and not from applying the modifications or the Woodbury correction process.
Table augments Table by showing the time to solve the linear systems of equations (again with ğ‘› = 10 5 ). Up to 30 steps of iterative refinement were used for BEAM but not for GEPP or GENP. To clarify where iterative refinement was unsuccessful, we marked the cases which failed to achieve convergence criterion for iterative refinement (ğœ‚ âˆ (ğ‘¥) â‰² 3.5 Ã— 10 âˆ’14 ). Furthermore, we provide the number of refinement iterations, with 30 being the limit.
First, note that by using iterative refinement, our approach achieved an error of less than 2 âˆ’53 âˆš ğ‘› for almost all cases. As above, zielkeNS's failure involved excessive growth generating NaN values. For the remaining failures, BEAM produced a non-NaN solution, but iterative refinement failed to converge to full accuracy. These cases included svd_geo, chebspec, fiedler, and riemann with larger tolerances (and many modifications) but without Woodbury correction. These matrices are all ill-conditioned, which limits the ability of iterative refinement to converge when the inner solution is only moderately accurate . For example, ğœ… âˆ (fiedler) = 2ğ‘›(ğ‘› âˆ’ 1) â‰ˆ 2 Ã— 10 10 [27, pg. 159], so iterative refinement can only be expected to converge to full accuracy when ğœ‚ âˆ (ğ‘¥) â‰² 5 Ã— 10 âˆ’11 . Furthermore, this further supports the implication of both (5) and Theorem 4.1 that Ï„ should be chosen such that Ï„ğœ… 2 (ğ´) â‰ª 1. Interestingly, these systems were successfully solved when using the Woodbury formula, despite the dire implication of Theorems 4.1 and 4.3 that Ï„ğœ… 2 (ğ´) â‰² 1 can lead to a large forward
5.3 Effect of tolerance
We next investigated the tradeoff in performance and accuracy for a larger variety of tolerance values and blocking sizes on select matrices without iterative refinement. Table shows the results. Matching Tables , the matrix sizes are all ğ‘› = 10 5 . Furthermore, the number of modifications and error in Table for Ï„ = 10 âˆ’6 , 10 âˆ’8 , 10 âˆ’10 correspond to the values in Table ; however, the run times differ from Table due to the omission of iterative refinement. While GEPP and GENP do not have the algorithmic blocking parameter ğ‘› ğ‘ of BEAM, they implement cache-blocking with a similar structure and a block size of 64.
Both rand_dominant and rand matrix types saw BEAM significantly outperform GEPP for all configurations. However, as mentioned earlier, BEAM performed worse than GENP, even when no corrections were applied. Moreover, smaller block sizes performed slightly better, likely due to increased arithmetic for the SVDs and block-triangular solves. For rand_dominant, all configurations resulted in no modifications and the same accuracy. For rand, on the other hand, increasing the tolerance above 10 âˆ’10 increased the accuracy when the Woodbury correction was applied but decreased the accuracy when it was not, with the number of modifications increasing in both cases. Given the number of modifications introduced when Ï„ â‰¤ 10 âˆ’6 , a tolerance of 10 âˆ’8 or 10 âˆ’10 is a better choice, particularly when not applying the Woodbury correction. Finally, increasing the block size reduced the number of modifications and increased the accuracy in all but one case.
The structured matrices provided more interesting results. As in the previous tables, BEAM applied numerous modifications to the orthog matrix for all of the tested configurations. Increasing the blocking factor helped the accuracy, although it also increased the number of modifications. The best tradeoff between performance and accuracy seems to be for tolerances of 10 âˆ’6 or 10 âˆ’8 (depending on the block size) without the Woodbury correction. Unexpectedly, a smaller block size led to fewer modifications; we suspect this is due to element growth in the later diagonals. For zielkeNS, only Ï„ = 10 âˆ’4 without the Woodbury formula produced a non-NaN solution. On the other hand, the block size had limited effect on the performance or the accuracy. For Ï„ = 10 âˆ’4 , all three block factor sizes resulted in the modification of about 95% of the diagonal singular values, whereas for smaller tolerance values, the number of modifications was just slightly larger than the number of blocks.
5.4 Scaling Results
Finally, we tested the performance of the different solvers as the size, ğ‘›, varies for the rand_dominant, rand, and orthog matrices. BEAM achieved speedups ranging from 4Ã— to almost 5Ã— for the three matrices compared to GEPP applied to rand. BEAM was configured with a blocking factor size of ğ‘› ğ‘ = 64 and a tolerance of Ï„ = 10 âˆ’8 . BEAM with iterative refinement ran out of GPU memory for ğ‘› = 250 000 due to the extra copy of the system matrix. Note that a diagonally dominant matrix, such as the rand_dominant, is the best-case scenario for the performance of GEPP because the selected pivots already reside on the diagonal and the memory traffic of exchanging rows is avoided (though we still perform the pivot search for each column). For the large matrices, BEAM reached 80% of GENP's performance for rand_dominant and rand, as the 0 50,000 100,000 150,000 200,000 250,000 300,000 0 former required no modifications and the latter required just a few modifications. Without the Woodbury correction, BEAM also performed similarly on orthog. However, with the correction, the performance dropped to approximately that of the best case for GEPP. Adding iterative refinement slightly reduced the overall performance, but BEAM still outperformed the best-case scenario of GEPP by 84% to 162% on the rand_dominant and rand matrices. Without the Woodbury formula, BEAM performed almost as well on orthog as on rand with speedups of 70% to 144%. With the Woodbury formula, BEAM performed in the range of GEPP, between 40% and 112% faster than the GEPP's performance on rand (which is close to its performance on orthog, as per Table ). These speedups for orthog are particularly promising because most approaches struggle to accurately outperform GEPP on this matrix, especially for large sizes .