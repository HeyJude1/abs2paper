3 ADDITIVE MODIFICATIONS ALGORITHM
The core idea of our approach is to apply additive modifications during the factorization when small entries occur on the diagonal instead of exchanging rows. A straightforward way to do this is to perform the classic non-pivoted LU factorization and modify diagonal entries whenever they dip below a preset tolerance. However, this results in a myopic view of the matrix; the issues with one diagonal element can often be fixed using just the next row, for example in a matrix where the leading 2-by-2 diagonal submatrix is 0 1 1 0 . Thus, we use a block LU factorization where the diagonal blocks are factored with the singular value decomposition (SVD). Then, we modify the singular values that are too small. However, the SVD requires significantly more computation than an LU decomposition: 21𝑛 3 operations instead of 2/3𝑛 3 -a 30× difference . This limits the block size, 𝑛 𝑏 , that can be used without introducing significant overhead. Other rank-revealing factorizations, such as QR with column pivoting, are cheaper; however, the SVD is a more robust factorization, which helps us focus on the effects of the overall block-wise factorization and the additive modifications. Note that by using the SVD in this way, the final decomposition is not a regular LU factorization (unless 𝑛 𝑏 = 1) but a decomposition into lower and upper block-triangular matrices.
Additive modifications commute naturally with the preceding Schur complement updates of the block LU factorization via the commutativity of matrix addition. Hence, the modified LU factors are equivalent to the factors produced by applying all modifications before beginning the factorization (if we ignore the effects of numerical round-off). This is analogous to representing row pivoting as pre-multiplication by a permutation matrix 𝑃: 𝐴 ≡ 𝑃𝐴. Thus, our proposed method factorizes 𝐴 into
𝐿 𝑅 = 𝐴 ≡ 𝐴 + 𝑀 𝑈 𝑀 Σ 𝑀 𝑇 𝑉 (1)
where 𝐿 and 𝑅 are lower and upper block-triangular matrices, respectively, while 𝑀 Σ is a diagonal matrix containing the modifications. Note that we denote the upper block-triangular factor as 𝑅 ("right") instead of the usual 𝑈 ("upper") to avoid confusion with the 𝑈 factor of the SVD. The columns of 𝑀 𝑈 and 𝑀 𝑉 are the left and right singular vectors corresponding to the modifications in 𝑀 Σ and padded with zeros to match the size of 𝐴. Thus, 𝑀
MV M T L MΣ MU A + ≡ × R
𝑈 and 𝑀
𝑉 are tall-and-narrow matrices whose columns are a subset of a block-diagonal matrix. Figure visualizes these sub-matrix structures.
Because of the perturbations, the factored matrix 𝐴 −1 often only provides the solution to a nearby system, so a correction is needed to obtain the solution to the original system. We considered two approaches for this correction: iterative refinement and the Woodbury formula. While the former has a well established formulation, the latter can take various forms. The most general form of the Woodbury formula is
(𝐴 − 𝐵𝐶𝐷) −1 = 𝐴 −1 + 𝐴 −1 𝐵(𝐶 −1 − 𝐷𝐴 −1 𝐵) −1 𝐷𝐴 −1 , (2)
although a simplified form is often used where 𝐶 ≡ 𝐼 . The term 𝐶 −1 − 𝐷𝐴 −1 𝐵 is called a capacitance matrix, and its inverse is the centerpiece of the Woodbury formula. Here, we formulate the correction as
(𝐴−𝑀 𝑈 𝑀 Σ 𝑀 𝑇 𝑉 ) −1 = 𝐴 −1 +𝐴 −1 𝑀 𝑈 (𝐼 −𝑀 Σ 𝑀 𝑇 𝑉 𝐴 −1 𝑀 𝑈 ) −1 𝑀 Σ 𝑀 𝑇 𝑉 𝐴 −1 instead of the more obvious (𝐴 − 𝑀 𝑈 𝑀 Σ 𝑀 𝑇 𝑉 ) −1 = 𝐴 −1 +𝐴 −1 𝑀 𝑈 (𝑀 −1 Σ − 𝑀 𝑇 𝑉 𝐴 −1 𝑀 𝑈 ) −1 𝑀 𝑇 𝑉 𝐴 −1
to avoid the need to invert the possibly ill-conditioned 𝑀 Σ and to improve the conditioning of the entire capacitance matrix. We discuss these numerical properties further in Section 4.
We outline our approach, abbreviated BEAM, in Algorithm 1. While we describe the algorithm with a fixed block size, 𝑛 𝑏 , it can easily be extended to a variable block size. In lines 6-17, we decompose the diagonal block and apply any necessary modifications. Then, lines 18-21 proceed as per a regular blocked, non-pivoted LU factorization. Finally, we compute and factor the capacitance matrix if the Woodbury formula is needed. While computing the capacitance matrix, we form and save the C 𝑅 and C 𝐿 matrices; this reduces memory accesses at the cost of a slight increase in storage unless there are numerous modifications. In spite of the factored capacitance matrix being denoted C −1 , the inverse should not be formed explicitly; instead, the factored form is preferable for numerical accuracy. We factor this second matrix with GEPP, but other methods could also work. The solve step simply applies the block-triangular factors and possibly the Woodbury formula.
A key advantage of Algorithm 1 comes from the fact that it has the high-level structure of a non-pivoted, block LU. Such structure provides more parallelism than partial pivoting because the panel of 𝐿 and panel of 𝑅 can be updated simultaneously . Furthermore, it allows the trailing matrix update from one iteration to overlap with the panel updates from a subsequent iteration.
To outperform GEPP, the overhead introduced by this method must be lower than that of pivoting. To that end, we count the Algorithm 1 BEAM algorithm's factor and solve steps. Subscripts for 𝐴, 𝐿, 𝑅 denote submatrices in terms of matrix blocks, and 𝑛 𝑏 denotes the block size.
𝐴 (0) ← 𝐴 5:
for 𝑘 = 1 : 𝑛 𝑡 do 6:
𝑈 𝑘 , Σ 𝑘 , 𝑉 𝑇 𝑘 ← SVD(𝐴
(𝑘 −1)
𝑘,𝑘 )
7:
for 𝑖 = 1 :
𝑛 𝑏 do 8: if Σ 𝑘 [𝑖] ≤ 𝜏 then ⊲ is 𝜎 𝑖 below tolerance 𝜏 9: 𝑚 ← 𝑚 + 1 ⊲ Record modification 10: 𝑀 Σ [𝑚, 𝑚] ← 𝜏 − Σ 𝑘 [𝑖] 11: 𝑀 𝑈 [:, 𝑚] ← [0, 𝑈 𝑘 [:, 𝑖] 𝑇 , 0] 𝑇 12: 𝑀 𝑉 [:, 𝑚] ← [0, 𝑉 𝑘 [:, 𝑖] 𝑇 , 0] 𝑇 13:
Σ 𝑘 [𝑖] ← 𝜏 ⊲ Apply modification  if 𝑚 > 0 and using Woodbury formula then 24:
𝐿 I,𝑘 ← 𝐴 I,𝑘 𝑅 −1 𝑘,𝑘 20: 𝑅 𝑘,I ← 𝐿 −1 𝑘,𝑘 𝐴 𝑘,I 21
C 𝑅 ← 𝑀 Σ 𝑀 𝑇 𝑉 𝑅 −1 25: C 𝐿 ← 𝐿 −1 𝑀 𝑈 26: C ← 𝐼 − C 𝑅 C 𝐿 27: C −1 ← FACTOR(C)
⊲ Using, e.g., GEPP
𝑥 ← 𝐿 −1 𝑏 32:
if 𝑚 > 0 and using Woodbury formula then 33:
𝑥 ← (𝐼 + C 𝐿 C −1 C 𝑅 )𝑥 34: end if 35:
𝑥 ← 𝑅 −1 𝑥 36: end procedure number of arithmetic operations used in the modifications and Woodbury formula. Let 𝑛 be the size of the system, 𝑚 be the rank of the Woodbury correction, 𝑛 𝑏 be the size of the diagonal blocks, and ℓ 𝑟ℎ𝑠 be the number of right-hand sides. (If the Woodbury formula is not applied, 𝑚 = 0.) Because the factors' diagonal blocks are full instead of triangular, computing the Schur complement takes an extra 𝑛 2 𝑛 𝑏 + O (𝑛𝑛 2 𝑏 ) FLOP. Thus, BEAM without the Woodbury correction takes
2 3 𝑛 3 + 2𝑛 2 ℓ 𝑟ℎ𝑠 + 𝑛 2 𝑛 𝑏 + O (𝑛𝑛 2
𝑏 + 𝑛𝑛 𝑏 ℓ 𝑟ℎ𝑠 ) FLOP. Next, building and factoring the capacitance matrix (via GEPP) takes 2𝑛 2 𝑚 + 2𝑛𝑚 2 + 2 3 𝑚 3 + O (𝑛𝑚) FLOP. Finally, the Woodbury formula requires two triangular solves and two matrix multiplies.
So, the Woodbury formula adds an extra
2𝑛 2 𝑚 + 2𝑛𝑚 2 + 2 3 𝑚 3 + 4𝑛𝑚ℓ 𝑟ℎ𝑠 + 2𝑚 2 ℓ 𝑟ℎ𝑠 + O (𝑛 2 + 𝑛ℓ 𝑟ℎ𝑠 ) FLOP.
Hence, if 𝑛 𝑏 , 𝑚 ≪ 𝑛, the arithmetic overhead compared to GENP should be negligible. While this does not measure the cost of data movement, most of the added computations have high data locality, especially compared to pivoting.