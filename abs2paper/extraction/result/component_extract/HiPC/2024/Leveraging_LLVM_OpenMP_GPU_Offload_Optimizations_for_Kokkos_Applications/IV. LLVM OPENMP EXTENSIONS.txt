IV. LLVM OPENMP EXTENSIONS
In this section we discuss extensions proposed to the LLVM/OpenMP ecosystem. We explain the motivation for these extensions and their use in Kokkos-OMP backend. We also show the performance impact on Kokkos applications using the newly optimized Kokkos-OMP backend.
A. Dynamic Shared Memory Extension
One limitation of current OpenMP API is the inability to expose dynamic memory as "shared" among threads in an OpenMP team. While OpenMP 6.0 has proposed extensions to address the issue for stack or compile time constant variables, support for sharing of dynamically allocated memory among members of a team is still unavailable. LLVM/OpenMP has introduced extensions to ad-  dress this issue. It provides a new target directive clause ompx_dyn_cgroup_mem(<N>) that allocates N bytes of data per team that can be shared among threads in that team. This feature corresponds to the shared memory that is allocated during a CUDA kernel invocation. The llvm_omp_target_dynamic_shared_alloc routine, called inside the target region, returns a pointer to the shared memory. The routine returns the same value to each thread of a target team and returns a NULL pointer on the host. The extension is available in upstream LLVM since release 18.
Kokkos provides a similar abstraction called the "scratchpad" to share data among threads in a team. Multiple levels of scratch-pad are provided in Kokkos. The first level (level-0) of the scratch-pad is typically mapped to the team specific allocatable local storage in the memory hierarchy, and hence is restricted to a few kilobytes. The native backends of Kokkos, those for CUDA and HIP, use the unified L1 / shared memory and local data share (LDS), on NVIDIA and AMD GPUs respectively. The second level (level-1) of scratch-pad is typically mapped to the high bandwidth memory (HBM). Until the availability of the ompx_dyn_cgroup_mem(<N >) clause in LLVM/OpenMP, Kokkos-OMP backend provided support for this feature by allocating the required scratch memory (for both level-0 and level-1) on the main memory of a GPU by using the omp_target_alloc routine. This led to negative performance implications in certain applications since the expectation that shared reads/writes within a team take place in the fast access local storage specific to each team did not match the implementation. We resolved this issue with the use of the new clause when LLVM/Clang is 18 or newer.
The basic behavior of Kokkos and its ability to pass functors to the backends and its subsequent handling of them is explained in prior work . Subsequent work provides a deeper discussion specific to the Kokkos-OMP backend.
Fig. shows an example of how the scratch-pad can be used in Kokkos. Elements in scratch memory are accessed through a custom version of Kokkos::View, the primary Kokkos data structure. The first line in Fig. view, ScratchViewType. The second template parameter indicates that the memory space where the data would reside is the scratch space of the default execution space, i.e., the default architecture on which a Kokkos parallel_pattern would be executed. Lines 4 and 5 determine how much scratch memory is needed per team. The Kokkos::team_policy specifies the dimensions of a work grid of thread teams, in this case N teams with 32 threads per team. To request scratch space shared among threads of a team we use a member function to the team_policy called set_scratch_size as shown in the Kokkos parallel pattern. The first parameter to set_scratch_size is the scratch level (0 or 1). The second parameter is the amount of scratch memory needed per team. To access scratch memory inside the parallel_for kernel, we create a view of ScratchViewType. Fig. is the CUDA equivalent implementation of Fig. .
In Fig. , we show two implementations for scratch memory in Kokkos-OMP. The choice of the implementation is based on LLVM versions. shmem_size_L0 and shmem_size_L1 refer to scratch memory requested by the user for level-0  and level-1 of a TeamPolicy. For LLVM 18 and higher, we use the LLVM extension ompx_dyn_cgroup_mem to allocate shmem_size_L0 in shared memory. Level-1 of scratch memory is then allocated on HBM using omp_target_alloc. For LLVM prior to 18, both level-0 and level-1 scratch memory is allocated on HBM and indexed accordingly when accessed in a scratch view. The implementation using LLVM extensions in Kokkos-OMP is our contribution and is already available since the 4.3 Kokkos release. The code inside the parallel region in Fig. leverages hierarchical parallelism, as explained in later sections.
Fig. shows how a scratch memory view is created in Kokkos. It is a common interface across all backends that uses a routine named scratch_memory_space. The first two parameters to the routine are pointers for level-0 and its corresponding size. The last two parameters follow the same pattern for level-1 of the scratch memory. scratch_ptr refers to the memory allocated using omp_target_alloc from Fig. . For LLVM 18 and above, level-0 scratch is accessed via the llvm_omp_dynamic_shared_alloc extension. For LLVM 17 or lower, we index into scratch_ptr according to the size of level-0 and level-1 scratch memory requested.
As shown in Fig. , using the extensions to allocate dynamic shared memory within a team enables Kokkos-OMP to match the implementation of level-0 scratch memory as intended by the framework.
To demonstrate the impact of dynamic shared memory in the Kokkos-OMP backend, consider TestSNAP proxy application that is modelled on Spectral Neighborhood Analysis Potential (SNAP) computations in the LAMMPS molecular dynamics simulator. It calculates the total energy of a configuration of atoms as the sum of energies of individual atoms, each of which is dependent on its neighbor atoms within a certain distance. We select a problem from the Exascale Computing Project, with 2000 atoms on one GPU and 26 neighbors per atom. Of the three main kernels in TestSNAP that consume more than 98% of the total execution time, two (ui and duarray) use level-0 scratch memory to store team specific intermediate information for fast access .
The ui kernel calculates expansion coefficients for each pair of neighbor atoms. It is implemented by generating the number of teams based on the outer loop while the inner loops are iterated in a 2-dimensional thread-block. The kernel uses scratch memory to store partial updates to coefficients for fast access. The duarray kernel computes the partial derivative Fig. : TestSNAP performance of native and OMP backend under scratch-levels 0 (shared memory) and 1 (HBM).
of the coefficients that impact the force on each atom and neighbor pair. The parallelism exposed in this kernel is similar to the ui kernel and hence implemented similarly. Fig. shows the impact of of using dynamic shared memory extension in the Kokkos implementation of TestSNAP. We compare the benefits gained by using level-0 scratch memory compared to level-1, i.e., implementation prior to the use of the LLVM extension. The Y axis shows the time for each kernel to run 100 timesteps while the X-axis indicates the two kernels using scratch memory under Kokkos-native and Kokkos-OMP backends. For the rest of the paper we follow the convention of prefixing the Kokkos implementations with "kk" in figures. For a fair evaluation of the benefits of dynamic shared memory we also show the performance using native backends under the two scratch levels. The comparison with the native backend illustrates the performance regression observed if the scratch memory is allocated on HBM rather than in the L1 cache.
SNAP and TestSNAP use all three levels of hierarchical parallelism available in Kokkos in ui and duarray, i.e., the kernels using scratch memory. Kokkos-OMP backend however has only two levels of effective hierarchical parallelism . Hence, for a fair comparison, in Fig. , we restrict the native backends to two levels of hierarchical parallelism.
Fig. shows that on NVIDIA A100, ui performance improves by 15% with Kokkos-OMP when data is stored in level-0 scratch versus level-1 scratch while Kokkos-native backend shows a 2× speedup in the same scenario. In duarray we see a ≈4.5× performance improvement with Kokkos-OMP backend and a 5× speedup with the native backend. For both Fig. : TestSNAP performance with optimized native and OMP backends under scratch-levels 0 and 1.
kernels, the speedup achieved by Kokkos-native backend is higher than the Kokkos-OpenMP backend when team-shared data is stored in fast access local storage rather than HBM. On AMD GPUs, going from level-1 to level-0 scratch memory gives the same rate of performance improvements on both Kokkos-native and Kokkos-OMP backends. For ui in both the backends, we do not see any observable performance improvement. However, in the case of duarray we observe an ≈15× performance improvement when using the LLVM extensions to store team shared data in LDS on AMD MI250X. This matches the observation from the Kokkos-native backend.
While we only show one application, our goal of this paper is to show a proof of concept of how the ompx_dyn_cgroup_mem extension in LLVM/OpenMP can bridge one of the gaps between OpenMP and native frameworks. There are several other applications written in Kokkos that can take advantage of the fast access local storage per team available through the level-0 scratch-pad memory interface. With the new extension in LLVM, Kokkos-OMP can provide the intended implementation to the user. The ability to allocate and access dynamic scratch memory that can be shared among threads in a team is a major capability previously missing in OpenMP but available in the native frameworks. Fig. might give an impression that the dynamic memory extension largely closes the performance gap between native and OpenMP backends. However, as mentioned earlier, we had restricted the Kokkos-native backends to use only two hierarchical parallelism levels for an equivalent comparison with the Kokkos-OMP backend. Fig. shows a comparison with the optimized native versions that use all three levels of hierarchical parallelism. The differences in kernel execution times between the Kokkos backends is still significantly dif-ferent. For ui on both NVIDIA A100 and AMD MI250X, the native backend benefits significantly with the addition of three levels of parallelism, making it ≈4.5× faster on Kokkosnative compared to Kokkos-OMP. Currently the Kokkos-OMP backend has no meaningful way to extract the 3rd parallelism level . The remainder of this paper addresses that need.
B. LLVM OpenMP Kernel Mode Extension
The second LLVM extension we leverage is perhaps an even more important addition to the LLVM/OpenMP ecosystem, enabling us to bridge the gap between native frameworks and OpenMP. While existing OpenMP offers a rich set of parallel semantics, which includes a fork-join model and automatic workload distribution, it needs the support of an extensive runtime library to manage execution. Runtime operations often limit performance and consume substantial resources, particularly on a GPU. Also this overhead has been generally regarded as unavoidable under the existing API semantics .
To overcome the runtime overhead, LLVM/OpenMP proposed a new set of extensions that allow OpenMP target regions to execute in a "bare metal" mode, also known as kernel mode . This feature enables OpenMP GPU code to be written in single instruction multiple threads (SIMT) style, facilitating an easy transition of an existing GPU code written in kernel languages such as CUDA and HIP to OpenMP thereby benefiting from the portability offered by OpenMP. Furthermore, the OpenMP kernel mode only requires a thin layer of runtime library support compared to existing OpenMP, eliminating runtime overhead to potentially improve performance. This work is built on the work of Tian et al. to implement features necessary for supporting the Kokkos programming model using a combination of the existing OpenMP and the extended OpenMP kernel mode.
We use the LLVM extension ompx_bare as an additional clause to the pragma omp target teams construct that directs the compiler to execute the associated code block in "bare metal" SIMT mode. As with the dynamic shared memory feature, the additional clause is an extension and not in the OpenMP standard. Therefore it is prefaced with ompx instead of the usual omp. Multiple extensions are needed to support CUDA/HIP style code generation in OpenMP, the first of which is the implementation of multi-dimensional grid and blocks. For that LLVM extends the num_teams and thread_limit clauses to accept a list of integers. For instance, a three-dimensional CUDA block size represented as dim3 blockSize(4, 32, 2) can be equivalently expressed using thread_limit(4, 32, 2). Similarly, a 3-D grid can be generated as num_teams(x,y,z), which would be equivalent to dim3 gridsize(x,y,z).
We can replicate the CUDA style kernel launch shown in Fig. in OpenMP by using the two kernel mode extensions: 1) the SIMT style code generation clause and 2) multidimensional grid and blocks. The OpenMP version using the extensions is shown in the first line of Fig. . In both programming models, it generates a grid of 128 thread-blocks 1 cu_kernel<<<dim3(128,1,1), dim3(4,32,2)>>>(a); Fig. : CUDA kernel launch.
1 #pragma omp target teams ompx_bare num_teams(128, 1, 1) thread_limit(4, 32, 2) firstprivate(a) 2 { 3 // number of teams in X-dimension 4 int blockDimx = ompx::block_dim(ompx::dim_x); 5 // team-id in X-dimension 6 int blockIdx = ompx::block_id(ompx::dim_x); 7 // thread-id in X-dimension 8 int threadIdx = ompx::thread_id(ompx::dim_x); 9 // thread-id in Y-dimension 10 int threadIdy = ompx::thread_id(ompx::dim_y); 11 ... 12 } Fig. : OpenMP target region (kernel launch). in X-dimension, in which each thread-block/team contains 4, 32 and 2 threads in X,Y and Z dimensions respectively. Fig. also shows the C++ specific access mechanisms for grid and thread dimensions and the corresponding team and thread ID inside the kernel mode.
The kernel mode extensions allow the Kokkos-OMP backend to have a meaningful implementation for three levels of hierarchical parallelism which include, 1) a league of teams, 2) each team comprising multiple threads and 3) each thread comprising of multiple vectors. Having a vector level allows optimizations on CPUs with hardware vector units and instructions. Fig. shows how multi-level parallelism can be exposed using the three levels of parallel hierarchy in Kokkos.
Fig. creates a 3-dimensional Kokkos::View that is initialized by traversing each dimension in each of the hierarchical levels. The current implementation of the Kokkos-OMP backend available in upstream Kokkos maps the outer league level to omp target teams, the intermediate thread level to omp for and the final vector level to omp simd. However, the simd clause inside a target region is serialized in most OpenMP implementations, including LLVM. Prior work Gayatri et al. discusses in detail the drawbacks of such a mapping and its impact on the performance of Kokkos applications that use the Kokkos-OMP backend.  The introduction of the ompx_bare extension allows SIMT style kernel generation such that the Kokkos-OMP backend can now exploit all three levels of hierarchical parallelism, similar to native Kokkos backends. The CU-DA/HIP backends map the outer level to teams (thread blocks). Within each team, the native backends generate a 2dimensional block of threads. Within a thread block, Kokkos ::TeamThreadRange is mapped to Y-dimension of threads and Kokkos::ThreadVectorRange is mapped to Xdimension of the threads. The innermost loop is mapped onto consecutive threads to improve memory coalescing.
Fig. shows our implementation of Fig. in Kokkos-OMP when kernel mode extensions are enabled. This implementation is currently in a separate fork . When the extensions are available in upstream LLVM, we will submit a pull request for the implementation to be adopted in the main Kokkos repository. For the rest of the paper, to differentiate between our implementation of Kokkos-OMP backend using the kernel mode extensions and the upstream Kokkos-OMP backend discussed in , we call our implementation the Kokkos-OMPX backend. Fig. shows the effective code and not the exact Kokkos code as we want to focus on the Kokkos-OMPX backend rather than the Kokkos API.
The functor from the league (outermost) level parallel_for pattern in Fig. is passed to the ParallelFor class shown in Fig. . The execution policy and its associated information is passed as a template parameter, shown in the second template parameter to the ParallelFor class. The execute member function in ParallelFor implements the parallel pattern. This style is consistent for all patterns in all Kokkos backends.
Requests for the two levels of scratch memory are represented by shmem_size_L0 and shmem_size_L1. The dynamic shared memory extension, discussed in the previous section, is used to request shmem_size_L0 amount of shared memory. Kernel mode is entered using the ompx_bare clause and the grid dimensions are generated using the num_teams and thread_limit extensions. Instead of generating a kernel with the same number of teams as requested by the user, the implementation calculates an optimized number of teams to maximize performance of the kernel. Using a larger number of teams can improve GPU occupancy for better code performance. However, it can also increase the memory footprint required by Kokkos to maintain team specific metadata, e.g., the level-1 scratch memory allocated on HBM. Conversely, generating fewer teams can limit the available parallelism. Kokkos backends try to optimize the number of teams generated based on the underlying architecture and scratch memory requested. In Fig. , this number is represented as max_teams. A loop inside the kernel mode handles cases where max_teams is smaller than the requested league_size (iteration space of the outermost loop in Kokkos hierarchical parallelism).  Below the ParallelFor class, we show the code snippet for the parallel_for implementation of the TeamThreadRange hierarchy. The Kokkos-OMPX backend iterates through the Y-dimension of the team threads rather than using omp for directives as in Kokkos-OMP.
At the end of Fig. , we demonstrate the parallel_for implementation for ThreadVectorRange. In Kokkos-OMPX we iterate through the X-dimension of threads in a team until we reach the loop boundaries. The same is implemented using simd in Kokkos-OMP as mentioned earlier. The ability to use parallelism in all 3-levels of parallel hierarchy is one of the main advantages of kernel mode extensions. Fig. shows the performance of TestSNAP when running with the Kokkos-OMPX backend compared to Kokkos-native and Kokkos-OMP backends. The team and vector sizes in the kernel mode are similar to the native backend, matching the best grid dimension for the backend, i.e., a vector_size of 32 on NVIDIA A100 and 64 on AMD MI250X. The vector size is a constant across TestSNAP and team sizes are chosen based on the amount of scratch memory that can be requested without oversubscribing the resource.
On NVIDIA A100, the impact of kernel mode is minimal for duarray, but using dynamic shared memory (also available in Kokkos-OMP) already improved the performance of the kernel, now only 3-4% slower than the native backend. For ui, Kokkos-OMPX is 4× faster than Kokkos-OMP, although 40% slower than the Kokkos-native backend. However on AMD MI250X, Kokkos-OMPX performs better on ui by 15% compared to the Kokkos-native (HIP) backend. Although in duarray Kokkos-OMPX is faster than Kokkos-OMP, it is still slower than the native backend. We are currently investigating the slowdowns. The slowdown might be due to the small but still existing LLVM/OpenMP runtime overhead, or differences in generating the optimal kernel parameters, i.e., max_teams. The overhead can also be caused by the presence of lambdas as explained in . Kokkos supports multi-dimensional parallelism in closely nested loops using MDRangePolicy, passing in the number of nested loops and iteration ranges for each of the loops. MDRangePolicy is semantically equivalent to the OpenMP collapse clause, and the Kokkos-OMP backend implements it using that clause.  A bonus of kernel mode is that kernel yi, which uses MDRangePolicy, now exposes more parallelism because the vector_size is now 32 and 64. The yi kernel performs a Clebsh-Gordon product on the coefficients calculated in ui. It is a 3 dimensional perfectly nested loop whose parallelism is exploited using the collapse clause from OpenMP where the iteration range of the innermost loop is equal to vector_size. Since there was no effective implementation for simd used for the ThreadVector loop in Kokkos-OMP, we had to restrict vector_size to "1" in Kokkos-OMP to maintain correctness. This restriction is eliminated in kernel mode, so the collapse clause can now extract more parallelism and performance of yi improves by 25% on both NVIDIA A100 and AMD MI250X as shown in Fig. .
C. Reductions in OpenMP Kernel Mode
The introduction of kernel mode also allows us to perform inter team reductions using an efficient implementation that can exploit dynamic shared memory and advanced techniques of shuffle instructions. Gayatri et al. used a conjugate gradient solver (CGSolve) application as a vehicle to understand performance differences between the Kokkos-OMP and Kokkos-native backends. It discussed how the performance of CGSolve is heavily dependent on the Sparse Matrix Vector (SPMV) computation which uses three levels of hierarchical parallelism to extract maximum parallelism out of the kernel. Fig. describes how all three hierarchical parallelism levels are used to implement SPMV. A group of rows is distributed among teams. Within each team, each thread is assigned a set of rows and a reduction over all non-zero elements in each row is performed by vectors in a thread.
The Kokkos-OMPX backend with LLVM/OpenMP extensions preallocates a dynamic shared buffer sized to the number of threads in a team to store partial results for nested reductions. Fig. shows the Kokkos-OMPX backend's allocation  in scratch memory of one element per thread of a team for team local reductions. Fig. shows how the reduction in Fig. is implemented in the Kokkos-OMPX backend. For simplicity, we omit the fallback code used for compilers that do not support kernel mode.
Fig. shows the implementation of reductions using dynamic shared memory in the native frameworks. Fig. shows the implementation of parallel_reduce in KK-OMPX, which has an additional parameter compared to parallel_for shown in Fig. to store the final result. The pointer to the shared memory buffer is acquired via the LLVM/OpenMP extension llvm_omp_target_dynamic_shared_alloc as shown in line 7. The right element within the buffer is indexed by advancing the pointer with the amount of level-0 scratch requested. The first elements in the buffer are used for level-0 scratch memory as shown in Fig. . In SPMV, scratch_0 would be zero, since it does not request any scratch memory. The type of the reduction result is abstracted in ValueType class. The indexed shared memory is then initialized to the default value and the partial update of each thread is accumulated into the thread specific index. The default constructor for the ValueType class can be used to assign an initial value. A sync operation is performed at the end of the partial updates for all threads in a team to finish their work. This would be equivalent to the __syncthreads operation available in both CUDA and HIP. The LLVM/OpenMP has been extended by the authors to provide a portable synchronization called ompx::sync_block_acq_rel. The partial results are then accumulated by a single thread, thread-0, to calculate the final result, which is stored in a location that is accessible to each thread in the team. Kokkos semantics do not require any specific thread to do the final write, and every thread in the ThreadVectorRange shall have the final result to maintain correctness. Ultimately, an optimized implementation would provide a common mask for each set of threads in the Xdimension corresponding to a single thread in the Y-dimension and broadcast to all threads with the same mask.
Fig. shows a performance comparison of SPMV using the Kokkos-OMPX backend against Kokkos-OMP and Kokkos-native backends. There is also a direct OpenMP version that uses LLVM extensions without Kokkos, which we Fig. : Kokkos-OMPX ThreadVector reduction. call "direct-ompx". The Y-axis of Fig. shows the bandwidth achieved by SPMV. There are three different blocks of bars in the figure to illustrate the impact of problem size. On NVIDIA A100, the Kokkos-OMPX backend performs at-least 15% better than the the upstream Kokkos-OMP backend. The difference between the two versions increases with the amount of data transferred. The Kokkos-OMPX backend is however always slightly less performant compared to the native backend. The best performance was achieved by the direct-ompx version. On AMD MI250X, Kokkos-OMPX backend is atleast 2× more performant than the Kokkos-OMP backend. However the Kokkos-OMPX backend is 2× slower than the Kokkos-native backend and the direct-ompx implementation.
D. Optimizing Occupancy and Shuffle Operations
Remaining performance gaps concern GPU occupancy and shuffle operations. Both native and OpenMP backends use multiple heuristics to improve the occupancy of a Kokkos kernel. One heuristic is based on the maximum number of possible teams that can simultaneously run on a given architecture, such as the number of thread blocks that can be simultaneously scheduled on a single streaming multiprocessor (SM) on NVIDIA GPUs. A kernel is generated using this heuristic to determine the number of teams, implying that a loop is needed to meet the user provided league_size. Even if the number of teams generated in the backend is similar to the requested league_size, the overhead of a loop is still inevitable.  To avoid this overhead, we modified both backends to create special instances of kernel generation in which the loop can be eliminated depending on the league_size requested.
Kokkos native backends use shuffle primitives from CUDA and HIP to implement optimized reductions. An equivalent interface (ompx::shfl_down_sync) has been added as an LLVM OpenMP extension. The parameters to this routine are similar to those in the native frameworks, i.e., mask, value and offset. Fig. shows how the accumulation of values from Fig. can be modified to use the shuffle primitive.
Fig. shows the performance of SPMV when both Kokkos-native and Kokkos-OMPX backends optimize the number of teams generated, i.e., elimination of the loop when league_size is at most max_teams. The Kokkos-OMPX backend additionally uses shuffle primitives to accumulate updates same as what the direct-ompx version does.
On NVIDIA A100, the Kokkos-OMPX backend sees 15% better performance when running in the optimized mode and reaches the equivalent performance as native backend in cases of higher data transfer. However, the version of SPMV using OpenMP extensions still achieves a 5% higher performance compared to the Kokkos versions. In this scenario we can attribute some overhead to using the Kokkos framework.
On AMD MI250X, the optimized Kokkos-OMPX versions are 50% faster for higher data transfers compared to nonoptimized versions. The optimized Kokkos-OMPX backend outperforms the direct-ompx version with LLVM/OpenMP extensions and the Kokkos-native backend. Profiling reveals several key differences between optimized the Kokkos-OMPX version and the direct-ompx version that contribute to their performance difference on AMD MI250X: Kokkos-OMPX uses fewer total registers (64 versus 80) for direct-ompx, resulting in slightly higher occupancy (29.42% vs 26.49%). Kokkos-OMPX also exhibits a higher L2 cache hit rate (65.07% versus 61.65%), indicating more efficient memory access patterns. Inspection of the intermediate representation Fig. : SPMV performance using kernel mode.
(IR) reveals differences in instruction sequence order rather than significant structural variations, suggesting that the performance gap stems from subtle optimizations rather than fundamental algorithmic differences. This target-dependent performance variation strongly indicates that the observed differences on the AMD platform are likely due to distinct backend optimization pipelines employed by the two targets.
In summary, we have shown how LLVM/OpenMP extensions can be used to optimize Kokkos parallel patterns on GPUs. The new Kokkos-OMPX backend can now match the native backends of Kokkos on NVIDIA and AMD GPUs.