IV. RESULTS
In this section, we present the results of each evaluated performance analysis tool based on the methodology presented in Section III. We evaluate the performance analysis tools based on the following key features: data collection, hotspot analysis, scalability, and performance variance.
A. Abundance and Overhead of Data Collection
For the data collection, we evaluate the abundance and overhead of the performance data collected by the evaluated performance analysis tools. The abundance of performance data indicates the types of performance data collected by the tools, including CPU performance counters, elapsed time, function parameters, MPI communication, and function traces, as mentioned in Section III-B. The results are shown in Table . Generally, the abundance of the data collection is tightly correlated to the basic data collection methods, regardless of the specific implementation details of each tool (e.g., both TAU and Scalasca are instrumentation-based tools). For sampling-based tools, the collected performance data is limited to CPU performance counters, elapsed time, and MPI communication, while instrumentation-based tools can collect more detailed performance data, including function parameters and function traces. Note that the sampling-based tools obtain function elapsed time via the distribution of the collected samples, which is statistically accurate based on the assumption that the samples are uniformly distributed along time and time-consuming functions will result in more samples. In contrast, by instrumenting the target program with probes, the instrumentation-based tools can collect the accurate function elapsed time and function parameters with notable accuracy.
The overhead of data collection indicates the time and storage overheads of the performance data collection. The time overhead is the additional time consumed by the performance analysis tools for data collection, and the storage overhead is the additional storage space consumed by the performance data collected by the tools. The time and storage overheads are measured in seconds and kilobytes (KB), respectively. The results are shown in Table . Specifically, for time overheads of all evaluated tools, the sampling-based tools (i.e., HPCToolkit) generally have lower overheads than the instrumentation-based tools (i.e., TAU-P and Scalasca-P) when collecting all kinds of function events without considering function parameters. For TAU-T and Scalasca-T, which are configured to collect only MPI communication events, the time overheads are significantly lower and even negotiable at a small scale. For TAU-P and Scalasca-P, which are configured to collect all kinds of function events, they both incur significant time overheads at both scales and are even not able to finish the data collection for real-world applications. Especially for Scalasca, LULESH at both 27 and 1000 ranks fails to collect for tracing due to job timeout, as shown in Table . According to the slurm output files, we figure out that the Scalasca is blocked by the tightly coupled implementation of postmortem analysis of tracing data, which can not be disabled or executed separately. Therefore, for collecting computational events for large-scale HPC programs, sampling-based approaches are more suitable than instrumentation-based approaches, while communication events are more suitable for instrumentation-based approaches due to the rich parameter information collected for further analysis.
For storage overheads, the instrumentation-based tools have higher overheads than the sampling-based tools due to the detailed performance data collected at higher event frequency. The storage overheads of the instrumentation-based tools are often proportional to the number of function calls, while the storage overheads of the sampling-based tools are often proportional to the number of samples collected by the tools. © Pitfall 1: Leveraging only one type of data collection method may not be sufficient for comprehensive performance analysis with acceptable overheads.
As demonstrated in the above results, the sampling-based tools generally have lower overheads than the instrumentationbased tools when collecting all kinds of function events without considering function parameters, while instrumentationbased tools can obtain runtime values of function parameters. For performance analysis of large-scale HPC programs, the parameters of MPI functions often play an important role in the detailed diagnosis of inefficient communications and scalability issues - , while function parameters are rarely exploited by the performance analysis techniques. Therefore, for future development cutting-edge performance analysis tools, it is essential to combine the advantages of both sampling-based and instrumentation-based tools to provide comprehensive performance data collection with abundant data types at low overheads.
B. Trace Analysis
The visualization of the collected performance traces of BT with 16 MPI ranks by HPCToolkit, TAU, and Scalasca are demonstrated in Figure . Specifically, for HPCToolkit as shown in Figure (a), its sampling-based trace can represent the distribution of the samples by leaf functions and corresponding calling contexts. For TAU as shown in Figure , its visualized traces additionally provide message-passing linkage between communication events that present the triangular communication patterns of BT to some extent. However, it is still hard to figure out the performance issues buried beneath the visualized traces. Generally, for both HPCToolkit and TAU, even though the scale of the collected traces is limited (only 16 processes), the trace visualization is already complex, and hard to interpret any actionable performance guidance from the messy visualized traces as shown in Figure (a) and (b). For Scalasca as shown in Figure (c), it does not provide such timeline visualization of collected traces. Instead, it extracts several useful metrics that can be representative of some kinds of inefficient communications, such as the late sender and receiver issues , . Such extracted profiles from the massive amounts of tracing data are more intuitive and easier for developers to understand the potential performance issues of the target program execution. § ¦ ¤ ¥ Pitfall 2: Direct trace visualization is too messy and meaningless for actionable performance guidance at scale.
In sum, for trace analysis of large-scale HPC programs, the existing performance analysis tools still lack effective visualization techniques to present the collected traces in an intuitive way. The visualized traces are often complex and hard to interpret, which requires developers to have a deep understanding of the target program execution to figure out the potential performance bottlenecks. Effective performance analysis tools can provide useful metrics extracted from the collected traces for better optimization guidance. For future development directions, trace analysis tools can also provide intuitive highlights of the abnormal performance patterns in the visualized traces to help developers quickly locate the potential performance bottlenecks.
C. Hotspot Analysis
The top 10 hotspots of the evaluated HPC programs with HPCToolkit, TAU, and Scalasca are demonstrated in Figure . Specifically, for HPCToolkit as shown in Figure (a), its bottom-up view of the hotspots can provide the tree view of the call stack with the leaf function call as root. For TAU as shown in Figure (b), its top 10 hotspots are presented in a flat view with detailed performance statistics, including elapsed times and accumulated number of function calls. For Scalasca as shown in Figure (c), it provides function elapsed times in the memory buffer size (a.k.a., relative to the number of function calls) order, which is not intuitive enough for identifying the hotspots. As demonstrated in Figure (c), such poor presentation can lead to difficulty in identifying the most significant code regions for further performance investigation as the function on the top apparently is not the most timeconsuming function. Generally, all evaluated performance analysis tools have the ability to identify the hotspots of the target program execution. However, the presentation of the hotspots is quite essential for intuitive and actionable optimization guidance. According to our opinion, the hotspot presentation of HPCToolkit (Figure ) is the most intuitive with rich optimization insights due to the tree view of the call stack. The flat view of the TAU is also acceptable for identifying the most time-consuming functions with detailed performance statistics. However, the presentation of the hotspots of Scalasca is not intuitive enough to identify the most significant code regions for further performance investigation. All evaluated performance analysis tools only provide the performance statistics of the hotspots, which requires further investigation to identify the performance opportunities. Although these tools provide basic hotspot analysis capabilities, they still reveal little actionable performance guidance for developers to optimize the target large-scale HPC program. § ¦ ¤ ¥ Pitfall 3: Presenting the performance statistics of hotspots is not sufficient for actionable optimization guidance.
D. Scalability
The scalability analysis results of the LULESH with HPC-Toolkit and TAU are demonstrated in Figure . Note that Scalasca does not provide any description of the scalability analysis capabilities in their user manuals . Specifically, for HPCToolkit as shown in Figure (a), its scalability analysis results are presented as the custom scalability loss metrics defined in its user manual in the form of top-down tree views of the target program execution. Such scalability loss metrics with calling context attributions can provide intuitive insights into which functions are suffering from significant scalability loss. However, as it has merged with all processes and threads in the tree view, it is hard to figure out the root causes of the specific scalability issues of the target program execution and still requires non-trivial efforts to figure out actionable optimization insights from the given profiles. For TAU as shown in Figure (b), its weak scalability analysis results are presented in the form of the measured and ideal speedup of the target program execution with different numbers of processes. Compared to HPCToolkit, TAU can provide visualized plots for an intuitive understanding of the gap between measured and ideal scalability of the target HPC programs, but it cannot provide any actionable guidance on how to achieve the ideal speedups. Although there are research works that can be tailored to accurately localize the root causes of specific scalability issues , , the existing performance analysis tools are still limited to small program binaries to provide guidance with acceptable overheads. © Pitfall 4: Large-scale performance analysis tools are lack of actionable optimization guidance with accurate localization of scalability root causes within acceptable overheads.
E. Performance Variance
The performance variance analysis results of the LULESH with HPCToolkit and TAU are demonstrated in Figure . Specifically, HPCToolkit does not actually provide specific analysis capabilities to identify the performance variance. As shown in Figure (a), when visualizing the function traces of the target program execution with and without injected disturbances by HPCToolkit, we cannot identify the injected performance variance from the visualized traces. For TAU, its performance variance analysis results are presented with ParaProf as the distribution of the elapsed times of the target program execution with different runs. As shown in Figure (b), we can intuitively identify the abnormal MPI Wait performance metrics of several MPI ranks (i.e., red peaks in Figure ), which are the exact nodes with memory noise injection. However, the significant performance anomaly is identified as MPI functions that are waiting for the asyn- According to the aforementioned evaluation results of the commonly adopted performance analysis tools at scale, we can obtain the following insights for future development of performance analysis tools for large-scale HPC programs:
1) Combining sampling-based and instrumentation-based data collection approaches. According to Pitfall 1, computational events are more suitable for sampling-based ap-proaches, while communication events are more suitable for instrumentation-based approaches. Therefore, for comprehensive performance analysis of large-scale HPC programs, it is essential to combine the advantages of both sampling-based and instrumentation-based tools to provide comprehensive performance data collection with abundant data types at low overheads.
2) Trace visualization of large-scale execution should highlight the problematic regions with human-friendly annotations. According to Pitfall 2, tools should provide a more intuitive focus on the abnormal or suspicious events or regions by highlighting or other human-friendly annotations of the massive amount of events and processes in the collected traces of large-scale HPC program execution. In addition, such a massive amount of tracing data brings a further exploration of combining machine-learning techniques to automatically identify the potential performance bottlenecks of the target program execution.
3) Multi-dimensional hotspot analysis with actionable optimization guidance. According to Pitfall 3, tools should provide multi-dimensional hotspot analysis with actionable optimization guidance for developers to quickly locate the potential performance bottlenecks of the target large-scale HPC program execution, including inefficient instruction or data structures, time or resource-consuming, and so on.
4) Accurate and fast root cause analysis of scalability loss. According to Pitfall 4, tools should provide accurate localization of scalability root causes with actionable optimization guidance for developers to achieve the ideal speedups of the target large-scale HPC program execution. We can combine several cutting-edge machine-learning techniques (e.g., graph neural networks ) to automatically identify the root causes of the scalability issues of the target program execution within acceptable overheads for large-scale HPC programs.
5) Leveraging rich attributions of function traces with deeplearning-based anomaly detection for performance variance analysis. According to Pitfall 5, tools should provide accurate localization of the root causes of the performance variance for developers to avoid or alleviate such unexpected fail-slows at scale via hardware re-configuration or software optimization. We can combine cutting-edge deep-learning-based anomaly detection techniques for time-series data with function event traces that are attributed to rich parameters and performance counter metrics to automatically identify the root causes of the performance variance of the target program execution within acceptable overheads for large-scale HPC programs.