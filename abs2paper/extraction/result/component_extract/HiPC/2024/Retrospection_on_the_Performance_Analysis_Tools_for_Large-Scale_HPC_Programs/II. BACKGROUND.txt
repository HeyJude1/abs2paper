II. BACKGROUND
In this section, we briefly summarize the key concepts and terminologies related to performance analysis tools for large-scale HPC systems. For analyzing the performance of large-scale HPC programs, developers first need to collect the performance data from the specific execution of the target program and then analyze the collected performance data to identify the performance bottlenecks for further optimization. The following subsections provide the background of performance data collection and large-scale performance analysis tools in detail.
For effective performance analysis, several performance data need to be collected from the target program, including CPU performance counters, elapsed time, MPI communication, function events, and function parameters. The performance data can be collected by two methods: sampling-based and instrumentation-based.
Sampling-based performance data collection -Generally, the sampling-based performance data collection method is based on the idea of sampling the target program at regular time intervals, where the sample distribution can provide a statistically accurate profile of the target program execution. The sampling rate is often represented as the number of samples per second (e.g., 1000Hz indicates 1000 samples per second). For each sample, tools can obtain the current state of the sampled program execution, including its call stack, timestamp, and the value of CPU performance counters. However, the sampling-based data collection approaches are non-invasive, and detailed value-related data (e.g., function parameters) is hard to collect. This leads to the incapability of providing detailed performance analysis for some specific performance issues, including the MPI communication patterns , late sender/receiver , , and fine-grained analysis of performance variance , . Some hardware platforms provide hardware precise event-based sampling (e.g., PEBS in Intel X86) that can sample the program states, including register files and control states, with low overheads at high accuracy , which can support some detailed value-related data collection and fine-grained performance analysis - . However, the hardware-based sampling methods are often limited by the hardware platforms and may not be available on all platforms.
Instrumentation-based performance data collection -Generally, instrumentation-based performance data collection methods are based on the idea of instrumenting the target program with probes to collect the performance data at the function entry and exit points, MPI communication points, and other specific code regions. Specifically, the probes can be inserted into the target program with library interception, binary instrumentation, compiler instrumentation, and source-code instrumentation. Library interception can intercept the library functions (e.g., PMPI profiling interface in MPI standard) by pre-loading specific library wrappers via LD PRELOAD , , , . Binary instrumentation can insert the probes into the binary code with static or dynamic binary rewriting - . Compiler instrumentation can insert the probes into the target program by integrated instrumentation pass at the compile time , , , while source-code instrumentation requires program developers to insert the corresponding probe API calls into the interested code regions , . Regardless of the specific instrumentation approaches, the instrumentation-based performance data collection methods can provide a wide range of performance data, including the function parameters, accurate function event traces, and even fine-grained instruction or operand values. However, the instrumentation-based performance data collection methods often exhibit higher overhead than sample-based tools when inserted probes are triggered at significantly high frequency.
For performance analysis of large-scale HPC programs, performance tools often need to balance the trade-off between the abundance and overhead of data collection to provide effective performance analysis - . However, collecting the entire trace including all computation and communication events with detailed performance data is often infeasible due to the high overheads (details in Section IV-A). To mitigate this issue, large-scale performance analysis tools either collect performance data via sampling , or selectively obtain the most significant traces (e.g., MPI communication traces) via instrumentation , for further performance analysis.
For performance analysis of large-scale HPC programs, the common concern of the program developers is how to better understand the performance bottlenecks of the target program and optimize the performance of the program through intuitive guidance from the performance analysis tools. Although there are varieties of performance issues that can be analyzed, in this paper, we focus on the most common performance issues at a large scale, including the hotspots, poor scalability, and performance variance.
Trace -The trace analysis is the common ability of performance analysis tools to provide detailed performance data of the target program execution, including the function events, function parameters, and MPI communication patterns. Several performance issues, such as late sender and receiver , require MPI communication traces for analysis. The trace analysis is often presented in a timeline view to help developers understand the performance bottlenecks of the target program execution. For more accurate trace analysis, the existing performance analysis tools already adopt timeline alignments to mitigate the potential time skewness of the collected traces , .
Hotspot -The hotspot analysis is a common performance analysis technique that identifies the functions that consume significant execution time or resources. The hotspot functions are often the performance bottlenecks of the target program, and optimizing the hotspot functions can significantly improve the performance of the program. For presenting the hotspots of the target program execution, some commonly adopted performance analysis tools, such as HPCToolkit , can provide top-down (i.e., tree view with the top of the call stack as root), bottom-up (i.e., tree view with the bottom of the call stack as root), and flat (i.e., functions) view of the hotspots with the detailed performance data, including elapsed times and collected metrics from hardware performance counters.
Scalability -The scalability loss is another common performance analysis goal to diagnose the performance bottlenecks of the target program when numbers of processes increase. The scalability analysis helps developers understand the performance loss of the target program compared to the ideal linear speedup. Almost all commonly adopted performance analysis tools, such as HPCToolkit , TAU , and Scalasca , can provide the scalability analysis of the target program with different numbers of processes. Some advanced performance analysis techniques , can further locate the root cause of specific scalability issues for better optimization guidance. The scalability analysis can help developers identify the performance bottlenecks of the target program and optimize the performance of the program for large-scale HPC systems.
Performance Variance -Performance variance indicates the significant performance difference between different runs of the target program. The performance variance can be caused by various factors, including system noise, hardware failure, and the misconfigured runtime environment . The performance variance analysis can help the developers figure out the sources of the specific performance variance and provide rich information to avoid or alleviate such unexpected fail-slows at scale via hardware re-configuration or software optimization , .