I. INTRODUCTION
Performance is critical for both scientific and industrial applications on various domains, including molecular dynamics , computational fluid dynamics , climate modeling , and large language models . However, due to the end of Moore's law, the performance improvements of general-purpose processer are bogged down, which achieves only 3% per year for single core performance . Software is becoming increasingly difficult to achieve notable performance improvements through hardware updates. To make matters worse, recent years of Golden Bell Prize winners have shown that the cutting-edge performance of application performance only achieves 1.78% of peak performance of Fugaku Supercomputer . The performance gap between the peak performance of the hardware and the actual performance of the software is widening.
As large-scale high-performance programs are becoming increasingly complex, it is unrealistic to manually analyze and understand the performance of target programs for largescale high-performance computing (HPC) systems. To better understand the gap between the attainable software and hardware performance, various performance tools are proposed for performance analysis, including HPCToolkit , TAU , and Scalasca . These tools can identify the performance hotspots of target programs and help in optimizing the program performance by providing insights of various common performance * Both authors contributed equally to this paper. issues, including poor scalability and performance variance. However, performance analysis tools have different features and capabilities, and the choice of a performance analysis tool depends on the specific analysis requirements of the user.
Different performance analysis tools have different data collection methodologies as well as analysis capabilities according to their attention to the functional requirements. Specifically, one common concern of the performance analysis tools is the overhead of data collection. The data collection overhead can significantly affect the performance of the target program, and thus it is essential to minimize it. Another concern is the accuracy and abundance of data collection. The performance analysis tools should collect accurate and abundant performance data to provide insights into the performance bottlenecks of the target program. The performance analysis tools should also provide the ability to analyze the collected performance data to identify the performance bottlenecks with intuitive presentations for developers to guide further program optimizations.
For data collection methods, the performance analysis tools can roughly be divided into two categories: sampling-based and instrumentation-based tools. Sampling-based tools, such as HPCToolkit , collect performance data by sampling the target program at regular time intervals. These tools are generally lightweight with acceptable overheads, but they may miss some performance data (e.g., function parameters). Instrumentation-based tools, such as TAU and Scalasca , collect performance data by instrumenting the target program with probes. These tools often exhibit higher overhead than sample-based tools when collected function calls are triggered at significantly high frequency, but they can collect more detailed performance data with notable accuracy.
For analyzing the performance of large-scale HPC programs, developers are commonly concerned about the bottlenecks of the target program, which are often represented as hotspot functions (i.e., the most time-consuming or resourceconsuming code regions), scalability issues (i.e., performance loss compared to the ideal linear speedup), and performance variance (i.e., the significant performance difference between different runs). The performance analysis tools should provide insights into these common performance issues to help developers optimize the performance of the target program. However, the performance analysis tools have different strengths and weaknesses in analyzing these performance issues. The choice of a performance analysis tool depends on the specific requirements of the user, including the type of application, the target platform, and the analysis criteria. However, there is no empirical study that evaluates the commonly adopted performance analysis tools on large-scale HPC systems to provide guidance on choices of performance analysis tools as well as discuss common shortbacks of existing tools on large-scale HPC systems for future direction.
In this paper, we present a comprehensive study of performance analysis tools for large-scale HPC systems. We identify the key features of performance analysis tools and then evaluate the tools based on these features. We provide a detailed comparison of the performance analysis tools regarding their features and capabilities. We also discuss the challenges and future directions in the field of performance analysis tools for large-scale HPC systems. Specifically, this paper makes the following contributions:
• We present a comprehensive study of performance analysis tools for large-scale HPC systems, including data collection and analysis capabilities in common concerns. • We identify the strengths and pitfalls of the existing performance analysis tools from the key feature aspects, including data collection, trace analysis, hotspot analysis, scalability, and performance variance. • We provide several future directions of performance analysis tools at scale according to the above comparison of the representative tools. The rest of this paper is organized as follows. Section II provides the background of performance analysis tools. Section III presents the comparison methodology used in this study. Section IV evaluates the performance analysis tools based on the methodology and discusses the comparison results. Section V concludes the paper and discusses future directions.