III. METHODOLOGY
Although there are widely adopted performance tools in large-scale HPC systems, there is no existing work that provides a comprehensive study of the existing performance analysis tools for performance analysis of large-scale HPC programs according to our knowledge. In this section, we describe our testbed of the performance analysis tools and the corresponding comparable metrics on the common concerns for performance analysis.
We evaluate a homebuilt cluster with hardware and software configuration as shown in Table . Specifically, the cluster consists of 32 computing nodes. Each node is equipped with one 36-core Intel Golden 6240 processor running at 2.60 GHz frequency. There is 384 GB of memory for each node. All nodes are connected with a 100 Gbps network. The storage has over 160 Gbps I/O bandwidth. All programs are compiled with GCC 9.4.0 with -O3 compiler optimizations. We use OpenMPI 4.0.7 for MPI communication.
For a comprehensive comparison of the existing large-scale performance tools, we evaluate the following performance tools that are well-known for providing rich performance guidance for large-scale HPC programs:
• HPCToolkit is a sampling-based performance analysis tool that provides insights into the performance bottlenecks of the target programs. HPCToolkit can collect traces and generate profiles from the collected data. In our evaluation, we leverage the default sampling rate at 300Hz per event type for HPCToolkit.
• TAU is an instrumentation-based performance analysis tool that provides the ability to collect wide ranges of performance data. TAU requires re-compilation with its own compilation toolchains (e.g., tau cc) to obtain the detailed function traces. For trading off the overhead and abundance of data collection, TAU provides both profile run (denoted as TAU-P) that do not collect detailed function traces and trace run (denoted as TAU-T) that collect detailed function traces. In our evaluation, we enable the auto event throttling with default settings (numcalls > 100, 000 && usecs/call < 10) for the TAU-P run and only collects MPI functions for TAU-T. • Scalasca is an instrumentation-based performance analysis tool that targets scalable performance tracing and analysis. Scalasca requires re-compilation with instrumentation compiler toolchains (e.g., scorep ) to obtain the detailed function traces. Scalasca requires one profile run (denoted as Scalasca-P) before collecting the full trace (denoted as Scalasca-T) for the target programs to obtain reasonable configurations as well as function filters for lower overhead. In our evaluation, we collect without any filters for Scalasca-P and tracing with the provided configuration as well as filters that only collect MPI communication traces. The aforementioned performance tools are representative of state-of-the-art performance analysis tools and are widely adopted in the HPC community. We evaluate these tools based on the following criteria: abundance and overhead of data collection, trace analysis, hotspots analysis, scalability, and performance variance. Note that although primitive diagnosing of the above performance issues does not require full event traces, MPI communication traces are still required for several advanced root cause analysis of specific performance issues in state-of-the-art research . We evaluate these tools with the NAS Parallel Benchmarks (NPB) and the real-world application LULESH .
Specifically, we use class B input for NPB-16 processors, class D for NPB-1,024 processors, and -s 40 -i 400 for LULESH in our evaluation according to the evaluation scale. For HPCToolkit, we use -t -e REALTIME -e PAPI TOT INS to enable the trace function and performance metric collection function. For TAU, we enable the auto throttling function, callpath, and communication matrix collection. For Scalasca, to generate the filter file and trace configuration (e.g., max buffer size), we first use scalasca -analyze and scalascaexamine to profile the target benchmarks and applications. We use the same input datasets for all the performance analysis tools to ensure a fair comparison.
The performance analysis tools can be evaluated with two aspects: data collection and analysis capabilities. We evaluate the performance analysis tools based on the following criteria:
1) Data Collection: For both primitive and advanced performance analysis, developers first need to collect enough performance data from the target program execution. In this paper, we call the ability to collect different types of performance data as abundance of data collection. The abundance of data collection is essential for supporting various useful and important performance analysis tasks. For large-scale homogeneous clusters, we investigate whether the performance analysis tools can collect the following performance data: • CPU Performance Counter: The ability to collect CPU performance counter data, often represented as PAPI or Linux perf events. It can provide deep insights into the hardware performance bottlenecks of the target program (e.g., top-down microarchitecture analysis ). Besides, the time and storage overhead of data collection is another important aspect for evaluating the performance analysis tools. Specifically, time overhead is measured as the execution time of the target program with and without the performance analysis tools. Storage overhead is measured as the storage space required to store the collected performance data. The overhead of data collection is essential for minimizing the impact of the performance analysis tools on the target program execution. The higher time overhead leads to significant time and commercial costs for performance analysis and limits the applicability of the performance analysis tools at a large scale. The higher storage overhead leads to the difficulty of storing and analyzing the collected performance data, which may further result in unexpected fails due to exceeding the storage capacity (e.g., maximum 1 TB storage budgets adopted in our evaluated homebuilt HPC cluster).
2) Analysis Capabilities: For performance analysis capabilities of the evaluated performance analysis tools, it is difficult to provide a quantitative metric for comparison. Instead, we provide the pros and cons of the evaluated performance analysis tools based on the following common performance analysis tasks for large-scale HPC programs: We evaluate the performance analysis tools with the trace analysis capabilities with their built-in visualization GUI interface for intuitive comparison. • Hotspot Analysis -Hotspots indicate functions or code regions that consume the most significant time or resources. For a fair comparison, we run the default hotspot analysis within each evaluated performance tool and provide the top few functions reported by these tools. Apparently, for the same program execution with the same input at the same scale, the analysis results of the top 10 hotspots should be similar, which gains the most attention for developers to further investigate the performance optimization opportunities. • Scalability Analysis -Scalability analysis aims to identify the causes of poor performance scalability of target program execution at different scales. Poor scalability can lead to low utilization of large-scale computation resources and even the inability to obtain higher performance even running with more nodes. We evaluate the performance analysis tools with 16 and 1024 processes to evaluate the tool's ability to analyze the scalability issues. • Performance Variance Analysis -Performance variance indicates the significant performance slowdown of the different execution instances of the same program. For a fair comparison, we run the program with and without injected disturbances to evaluate their ability to identify the performance variance. For each evaluated analysis capability, we qualitatively investigate the intuitiveness, accuracy, and completeness of the analysis results provided by the performance analysis tools. Specifically, the analysis results should be intuitive for developers to understand the performance issues of the target program. The analysis results should be accurate to provide reliable guidance for optimizing the performance of the target program. Besides, the analysis results should also be actionable to provide optimization guidance for the target program, such as providing problematic code locations, calling contexts, and comprehensive diagnosis of root causes.