VI. EXPERIMENTS
We first evaluated the impact of disaggregated memory allocation solution in simple cases where the memory is constant by phases (Section VI-A). Next, we evaluated the limits of the online strategy Priority when the memory pattern changes frequently. Finally, we evaluated the importance of Stochastic (Section VI-B) to alleviate the cost of reconfiguration. In this section we refer to the term baseline performance as the performance on an architecture where memory is not limited (in this case this is satisfied by 256GB per node).
Using the nomenclature proposed by Peng et al. we evaluated our algorithms on Phased patterns, that is patterns where memory is constant by segments. The number of segments can be equal to one which corresponds to the constant patterns. We first provided a general evaluation based on Marconi100 job profile, then we studied the limits when the architecture parameters vary. In this section, we slightly modified the trace generation by drawing the number of nodes per applications uniformly between 1 and 23. This provides more variability in the applications to observe more differences in the results. However our take-aways hold with the original Marconi100 node distribution. 1) General evaluation: In Figure , we measured the mean useful utilization (i.e. equivalent to the number of Flops) when the average memory per node varies. Since all applications use less than 256GB of memory per node, all allocation policies give similar results with 256GB (or any volume of memory larger, what we consider in the following baseline/unlimited memory case). The difference in behavior between Aggregated and disaggregated heuristics is that in our implementation, disaggregated heuristics still release unused memory and reallocate it later when needed.
When the memory becomes more limited, disaggregated heuristics behave better than the Aggregated that uses the same volume of memory. Our result showed that using 150% of the average occupied memory (150GB), then using disaggregated heuristics allow to have roughly the same performance to that of a machine where memory is not limited (loss of 0.2%). In this scenario, heuristics that do fewer reconfiguration (Oldest-First, Largest-First) perform slightly better than Priority. When the average memory per node gets closer to the average memory usage (125GB), then as expected by our theoretical results Priority outperforms other heuristics. The overhead compared to the baseline solution is still minimal with 125GB (i.e. the machine has 25% more memory than the average memory usage): there is a 0.7% utilization loss compared to the baseline on average. Finally, the Priority heuristic really shows its performance with very limited memory on the machine compared to the memory needed by applications. In these case it allows to gain up to 2% of utilization over Oldest-First and nearly 4% over Largest-First.
The response time is the average duration between a job submission and its completion. In Figure , we showed the response time of Priority for various job sizes, when the available memory varies compared to the baseline strategy. This allows to confirm that no jobs are arbitrarily hurt by our scheduling heuristic, even when the memory is small. Take-aways:
1) Disaggregated heuristics allow to considerably reduce the memory needed by the machine. 2) When the memory available is small, then a more precise heuristic like Priority is important, however otherwise, simpler heuristics like Oldest-First are sufficient and do not require precise memory requirements.
2) Impact of machine parameters: In this section we are interested by how architectural parameters impact the performance of disagregated algorithms. Specifically we evaluated the impact of α, the ratio between memory bandwidth and out of node storage bandwidth, and τ alloc , the reconfiguration time. In the following of this section, instead of an absolute value of the utilization, we study relative values, that is, the difference with the baseline utilization when memory is considered as unlimited (i.e., Aggregated with 256GB).
In Figure , we plotted the mean utilization as a function of the memory for various values of α (α = 0.1 means that memory bandwidth is 10 times faster than external storage).
We perform two evaluations: in Figure , we eval-  uated the relative performance between Priority and Oldest-First when α varies. This evaluation confirms our previous findings that when there is enough memory, a solution that minimizes the number of memory reallocation by giving priority to longer running jobs is better, however, as memory becomes limited, Priority improves this performance. It should be noted that in both cases, the relative gains are negligible (about 1%).
In Figure , we evaluated the relative performance of Priority compared to the baseline architecture (256GB memory per node). Naturally, the higher α is, the better performance disaggregated solutions obtain. From the results in Figure , we observed a robustness of disaggregated heuristics. Take-aways:
3) Even in limit cases where the cost of overflowing the memory is high, previous take-aways hold.
Our next step is to evaluate the impact of τ alloc . In practice, this characteristic time of the system is interesting compared to the characteristic time of applications (i.e., the average phase length). This is what we showed in Figure .
We increased the value of τ alloc gradually, and plotted as a function of the ratio r = E(L m )/τ alloc , for r ∈ {1, 2.5, 10, 25, 100, 1000} (hence, τ alloc ∈ {1000, 400, 100, 40, 10, 1}).
We observed that Priority stays efficient until E(L m )/τ alloc = 100. The difference between r = 100 and r = 1000 are negligible. As this ratio decreases we observed the following:
• When r = 25, the reconfiguration cost becomes so important that when memory is not limited (memory per node greater than 200GB). In this case, an aggregated strategy is better; • When the reconfiguration cost has the same order of magnitude than the phase length, even with small volumes of memory it is better to use an aggregated storage. These observations hint that an online reconfiguration solution may not be adapted to a scenario with dynamic memory pattern which is what we study in the next section.
In this section we are now interested by dynamic memory patterns, such as in Figure . In order to do so, we update the generation protocol as follows:
• The number of phases is drawn uniformly between 50 and 149, but all phases have length τ alloc ; • For each application, we draw uniformly at random one variable out of 4 to describe their memory patterns in the following truncations of the normal law of mean 105 and scale 30:
-A truncation between 30 and 80; -A truncation between 80 and 130; -A truncation between 130 and 180; -A truncation between 180 and 240. This statistical behavior is the only memory information available to the scheduler ahead of time.
• All phases memory from one application are drawn according to the selected distribution (this information is used for the evaluation, but not for the scheduling decision). 1) General evaluation: The results are presented in Figure . We compared Priority, Stochastic and Aggregated, and we normalized their performance to the baseline architecture with 256GB memory per node. As expected, Priority is outmatched by the two others policies. Indeed, as the length of a phase is equal to the reconfiguration time, it can no longer keep up with the fast-changing memory profile. Stochastic is able to correctly balance the memory allocation between the application which allows it to perform better than Aggregated when the memory is constrained.
For each quantity of memory per node, the Aggregated policy performed as well in this case as in the first experiment. This is not surprising as this policy is independent of the variations speed of memory consumption of the application. Unsurprisingly and due to the dynamicity of patterns, the best performance cannot reach the same level of utilization that the Priority algorithm was able to reach in the first experiment (80% of utilization for Stochastic with 105 GB of memory versus 85% for Priority in the first scenario). Yet, those performance are still impressive given the dynamicity and overhead dut to reallocation, where the Stochastic algorithm allows to reduce the memory per node by more than 40% (to 150GB) with only 2% loss in performance.
A small comments on the results: in very rare cases we can observe that Stochastic of Aggregated with less memory behaves better than the Baseline that uses 256GB (see for instance 200GB on Figure ). After checking the Gantt chart and the results, this is an artifact of the global scheduling strategy and a consequence of their heuristical nature: slowing down jobs can reorder the order in which the jobs are executed, which in turns can provide improvement on the global performance.
Finally, by studying the response time as a function of the size of the jobs, we confirm in Figure , that in this case as well, no jobs are arbitrarily hurt by our scheduling heuristic even when the memory is small. Take-aways: 4) Even with dynamic patterns, when the overhead of an online algorithm would be too high, the static algorithm Stochastic is able to provide important gains, simply by using the statistical memory behavior. For our traces, it brought memory consumption reductions by more than 40% while losing less than 2% of performance.   2) Impact of machine parameters: In Figure , we plot the results of the simulations for others values of α. In Figure , we evaluate the relative performance between Stochastic and Priority when α and M vary. A performance higher than 1 means that Stochastic performs better, lower than 1 than Priority performs better. In Figure we compared the performance of Stochastic compared to the baseline. Just like in the phased pattern use-case, these two experiments confirm that the performance of Stochastic are robust to others machine parameters. Take-aways:
5) The results for Stochastic are robust to various access cost to the second tier of memory.