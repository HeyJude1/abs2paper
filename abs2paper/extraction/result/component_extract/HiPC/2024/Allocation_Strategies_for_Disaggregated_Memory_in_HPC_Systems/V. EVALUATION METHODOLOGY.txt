V. EVALUATION METHODOLOGY
In order to generate traces for the evaluation, we rely on real behavior. We use application traces from the Marconi100 supercomputer . The Marconi100 supercomputer consisted of 980 computing nodes, each of which having 2x IBM POWER9 AC922 (32 cores), 4x NVIDIA Volta V100 GPUs, and 256GB of RAM. In Section V-A, we explain how we extract stochastic profiles. These profiles are then used to generate synthetic traces as described in Section V-B.
For the evaluation we designed an event-based simulator based on the model designed in Section III-A. This simulator is available freely at https://doi.org/10.5281/zenodo.13981594. This simulator takes in entry the memory profiles of the applications, the platform parameters (number of nodes and quantity of disaggregated memory) and a memory allocation strategy. It then performs the simulation and return some parameters of interests such as the quantity of memory used at each event or the completion time of each application.
To extract stochastic memory from real workload data, we sampled 400 jobs from dataset 8 (2022-05) of Marconi100 . The sampling method used mainly two criteria to select jobs: select jobs that (i) run for more than one minute, and (ii) had exclusive access to the computing nodes.
For each sampled job, its memory footprint consists of a time series of measurements of the nodes' memory consumption. From this time series, we define the phases frontiers in three steps. First, we calculate the distribution of memory consumption differences between subsequent measurements in the memory consumption time series. Second, we use this distribution to calculate the z-scores of the memory consumption differences. Third, from these z-scores, we define the phase frontiers as the timestamps where the z-scores of the memory consumption differences were larger than a threshold ∆, expressed as a ratio of the standard deviation.
Put another way, we set the phases frontiers where the memory consumption significantly went up or down, where "how much significantly" is determined by ∆. Finally, for each phase determined by two subsequent frontiers, we determine its memory consumption as being the maximum memory consumption of the initial memory consumption time series during the duration of the phase.
In practical terms, we reduce a fine-grained time series of memory consumption into a coarse-grained sequence of memory consumption phases, where we only store new information when the memory consumption significantly changed. Figures illustrate some outputs of the above procedure. In these figures, the points represent the data and the lines the output.
Based on the obtained phases, we generated four distributions of behaviors on Marconi100 that we use for synthetic trace generation:
• a node distribution C m (see Figure ). We have (c) Distribution of number of phases per applications on Marconi100.
E(C m ) = 7.
Figure : Job parameters on the Marconi100 dataset that we studied.
• a phase length distribution L m (see Figure which is showed truncated). We have E(L m ) = 1000s. • a distribution of number of phases N p (see Figure ).
We have E(N p ) = 17. • a memory usage distribution X m (see Figure ). We have E(X m ) = 105GB.
In all the experiments we consider that there are P = 54 nodes. Unless specified otherwise, we used as machine parameters: τ alloc = 1s and α = 0.03. We chose α = 0.03 as the ratio between the speed of a SSD at 300MB/s versus a RAM at 10GB/s. The available memory M depends on the experiment, but we call M = E(X m ) = 105GB the average memory usage of Marconi100's traces, and M mar = 256GB the memory per node on Marconi100.
Unless specified otherwise, the applications are generated as follows: there are 30 batches of N = 1000 applications. For each application A j :
• Its number of nodes is selected following C m • The number of phases is selected following N p (where we have bounded the number of phases at 45), and for each phase:
-Its memory is selected following X m -Its length is selected following L m
The release time of each job is 0 for the first 10% of jobs, then each job is released 0.9 • 10E(L m ) Cm P units of time after the previous one. This release ratio is to guarantee that there is always enough work to be executed. Intuitively, if there was no scheduling constraints, the jobs released after t = 0 could allow a theoretical utilization of 90%.
When measuring the performance of an algorithm, we consider an interval of time included in the workload generation interval. That is, if the last application is submitted at time T last , we measure the system utilization on the interval [0, T last ]. In practice this means that the executed workload is not the same for all solutions, but when we take T last to be large enough, we reach a steady state which makes the solution trustable .