III. PRELIMINARY CONCEPTS A. Disaggregated memory model
We consider an architecture with P compute nodes and a two tier memory. Typically, this corresponds to an archi-
B. Application model
We consider N parallel applications {A 1 , . . . , A N }. Each application can run using one of the three processing modes: with no access to the disaggregated memory at limited speed, (2) at full speed with complete access to disaggregated memory, or (3) a trade-off with limited access to disaggregated memory. All applications have access to the slower memory. For an application A i , we characterize its memory profile as a function of the requested memory: γ → m i (γ) with γ ∈ [0, 1] the proportion of the application completed. Note that in general this profile is an unknown variable that can only be traced after execution (see Section III-D).
To model the application memory consumption, we first consider the memory profile as piecewise constant. We divide the memory profile into successive phases, with constant memory profile during each phase. A memory profile is therefore denoted as a set {(γ j , m j ), 0 ≤ j < J} where J is the number of phases of the application, m j is the memory request on phase j, that start when a proportion γ j−1 of the application is executed, and finishes at ratio γ j of the application (γ 0 = 0 and γ J−1 = 1).
Performance model: The performance of application A i is computed based on the allocation M i of memory allocated to A i : If the application runs with requested memory at each phase, then the runtime of phase j will be T opt i (γ j − γ j−1 ) and the total runtime of the application will be T opt i . With a fixed memory M i ≤ M , a slowdown affects the runtime of each phase. In general this slowdown is very application dependent, but some data , show that a linear slowdown on memory accesses may be a good approximation. Particularly, Liu et al. ] have shown it using several memory swapping systems (Linux, Infiniswap, Fastswap). We can observe that the main difference in performance lies in an architecture-dependent growth factor. This is coherent with the roofline model , . Hence, we compute the slowdown on phase j as follows:
SL i (M i , j) = α + (1 − α) min 1, M i m j .
Discussion: the underlying hypothesis behind this model is that all memory accesses are accessed with the same frequency. This is the case for some applications such as HPL/SuperLU . For some applications where the inbalance between the memory blocks that are accessed is extremely important (NekRS, BFS, XSBench ), this model could also work by considering as main memory footprint, only the blocks that correspond to 90% of the memory accesses and considering a first order approximation.
We only model a slowdown due to not having enough memory available: in line with recent literature on disaggregated memory systems in HPC , we consider that access to this shared memory does not impact the bandwidth in general.
With this formula, an application that has all the memory it needs (or more) will have a slowdown of 1 (i.e., no slowdown). An application with no memory will have a slowdown of α (the bandwidth ratio between fast and slow storage). An application with βm j memory (β ∈ [0, 1]) will have a slowdown of α + (1 − α)β, which is the linear interpolation between the slowdown with no memory and the slowdown with all the memory.
We obtain a runtime for phase j:
r i (M i , j) = T opt i (γ j − γ j−1 ) SL i (M i , j) . (1)
More generally, an application can be run with successive memory allocations
{(γ ′ k , M k ), 1 ≤ k ≤ K} during phase j with M k ≤ M and γ ′ K = γ j and γ ′ 0 = γ j−1 ,
where memory M k is allocated to the application when the application is between γ ′ k−1 and γ ′ k of its execution. Then, the execution time of the phase will be
K k=1 T opt i (γ ′ k − γ ′ k−1 ) SL i (M k , j)
.
The execution time of the application will thus be the sum of execution of each phase. For better clarity, we formulate in the following the memory allocation as a function of the time.
C. Scheduling problem
Given a number of nodes P , a fast, disaggregated memory of size M , a relative bandwidth ratio α between fast and slow memory, and a set {A 1 , • • • , A N } of applications defined by: their memory profile m i (γ), their node request c i , a minimal execution time T opt i with unlimited memory. A schedule consists in allocating applications to the P nodes, and partitioning the memory between applications. The memory allocation of a schedule can be described as a time vector t → π(t) = (M 1 (t), . . . , M N (t)) s.t. for all t,
i≤N M i (t) ≤ M .
Optimization objective: In line with the literature , we first define the throughput or instantaneous utilization of the system. More specifically, given a set of applications
S t = {A 1 , • • • , A k }, if at time t applications A i uses c i cores, then the throughput ρ is: ρ(t) = Ai∈St c i
Due to low memory allocation, an application can be slowed down, hence we use the or 'useful' throughout. It consists in giving to each node a weight proportional to the quantity of work it is effectively processing. We use this modified version of the metric to account for the fact that if an application is executed on a node with a slowdown of 0.5, then the node run at half of its maximal capacity, hence, only half of the node counts as producing 'useful' work. If application A i is running phase j i with memory M i , then the useful throughput is:
ρ(t) = Ai∈St c i • SL i (M i (t), j i ).
Finally, we want to maximize the useful utilization of the system, defined as the mean useful throughput over time. The useful utilization U of the schedule S is therefore given by the formula:
U (S) = 1 T S t Ai∈St c i • SL i (M i (t))
where T S is the makespan of the schedule. For simplicity, in the rest of this work we use utilization when we talk about useful utilization and throughput when we talk about useful throughput.
D. Limits of Memory Models
While an application can be described by a memory profile t → M (t), it may not always be reasonable to consider that we can accurately obtain this profile in practice . There are too many variables that impact the accuracy of this profile: the performance of the system (the CPU may be slower at some time which translates the memory function in time), the shape of the data etc.
Storing an entire memory profile for applications running on a large scale machine may be extremely costly. For instance, if we collect the node memory consumption every second (stored on 4B), that is roughly 350KB/day/node. On a machine like Frontier with more than 9400 nodes, this comes to 3.2GB/day,  In comparison, over five years of Mira (Jan 2014 to Dec 2018), 330k jobs were executed . If we ran aggregated data per jobs (for instance the percentage of time spent using 0-8GB, 8-16GB, 16-32GB, 32-64GB, 64-128GB ), that would amount to 6.6GB. Of course, this comes with a loss of information.
In addition to the inaccuracy of the measurement, Peng et al. identified four types of memory temporal patterns to describe applications:
• Constant pattern: the memory utilization has minimal changes throughout the execution; • Phased pattern: the memory consumption is constant by segments; • Dynamic patterns: a pattern with frequent and substantial changes in memory usage over time; • Sporadic patterns: low memory utilization most of the time with spiked memory usage for short periods. In the evaluation we consider two flavors of input that the algorithm can use, that would represent various means of collecting memory information:
1) Constant memory profiles: Our observation from Mar-coni100's traces is that some applications have very structured behavior (see Figure ). In cases where phases with constant memory usage are quite long in front of the reconfiguration cost, this cost can usually be neglected.
2) Stochastic memory profiles: Finally, there are jobs whose memory behavior varies dynamically during its execution (see for instance Fig. ). This is typical of jobs relying on Deep-Learning phases. This variability can be random or deterministic but with variations that are too fast in front of the time to reconfigure the machine. For these jobs, we propose to use a stochastic model to describe the memory consumption. This model can be obtained based on historical data.
Definition 1 (Stochastic Memory Profile). We say that an application has a stochastic memory profile (m j , p j ) j on time interval [l, l + 1], with j p j = 1 and m j0−1 < m j0 for all j 0 > 0, if for t ∈ [l, l + 1], t → m(t) is equal to X where X is a discrete random variable of distribution (m j , p j ) j (i.e. P(X = m j ) = p j )).
Some comments: we focus on a simple definition, but it could be generalized trivially to a model where the random variable is time dependent (phase behavior). Similarly, the duration of the time interval where the memory is constant could be non-deterministic.
One can verify that, for application A with node request c and memory profile (m j , p j ) j , if m j0 ≤ M 0 < m j0+1 , then we have the following properties:
E (ρ A (M 0 )) = cα + c(1 − α) j≤j0 p j + c(1 − α)M 0 j>j0 p j m j E (ρ A (M 0 )) − E (ρ A (m j0 )) = c(1 − α)(M 0 − m j0 ) j>j0 p j m j = (1 − α) • (M 0 − m j0 ) • g(m j0 ) (2)
E (ρ A (m j0+1 )) − E (ρ A (M 0 )) = c(1 − α)(m j0+1 − M 0 ) j>j0 p j m j = (1 − α) • (m j0+1 − M 0 ) • g(m j0 ) (3)
where, for
m j0 ≤ M 0 < m j0+1 , g(M 0 ) = c j>j0 p j m j = cE 1 X X > M 0 P (X > M 0 ) (4)
IV. ALGORITHMS
In this section we present various algorithmic strategies to solve the problem described in Section III-C. We propose two algorithms: the Priority algorithm, which is suited for application scenarios with known memory behavior, and the Stochastic algorithm, which is suited for application scenarios with dynamic memory requirements. These algorithms are compared to baseline algorithms presented in Section IV-C. Finally, we discuss how these algorithms impact the batch scheduling mechanism in Section IV-D.
A. Priority
The first algorithm is priority-based: at all time t 0 , Priority sorts all running applications by decreasing c i /m i (t 0 ). Then while there is memory available, it allocates it (up to the memory requirement) to the first application of the list that needs additional memory.
Theorem 1. Given a set of running jobs {A 1 , • • • , A N }, with respective constant memory profile t → m i (t) = m i .
Priority is optimal with respect to the system throughput ρ(t).
Proof. We show the result by contradiction.
Assume an optimal solution π = (M 1 , . . . , M N ), such that there exists two jobs A 1 and A 2 , such that c 1 /m 1 > c 2 /m 2 , and such that M 1 < m 1 and 0 < M 2 (≤ m 2 ), then we show that there exists ε > 0 such that the allocation
π ′ with M ′ 1 = M 1 + ε, M ′ 2 = M 2 − ε and M ′ i = M i for i > 2 has better performance.
Because the throughput is additive, one can notice that
ρ(π ′ ) − ρ(π) = (1 − α) c 1 m 1 ε − (1 − α) c 2 m 2 ε > 0
which contradicts the optimality of π.
B. Stochastic
Based on Theorem 1, we expect Priority to work well when the memory profile of an application stays steady, close to the predicted peak usage as can be observed on traces on the Marconi100 supercomputer (see Section III-D1). For applications where the memory consumption is more dynamic within the execution , or where the predicted peak consumption can be far-off, there can be an important reconfiguration cost.
We provide another approach based on a stochastic memory profile (Definition 1). In the following we assume that the memory profile of applications are described by random variables X whose discrete distributions are known (m j , p j ) j (i.e. for all j, P (X = m j ) = p j ).
In this case, we derive Stochastic, a strategy with loglinear complexity that minimizes the expected throughput when applications follow a stochastic memory model. In general this can be obtained with historic data or known behavior for recurrent jobs (see Section III-D).
In the following and for simplicity when the job index i is omitted when it is obvious. Lemma 1. Let X a random variable that follows the discrete distribution (m j , p j ) j , then for all j:
g(m j ) ≥ g(m j+1 )
This lemma is trivial with Equation (4). We now prove the main theorem that we use to define Stochastic. Intuitively Theorem 2 along with Lemma 1 state that we can sort applications by decreasing values g (m j ), and allocate memory greedily up to the next value m j .
Theorem 2. Given a set of running applications {A 1 , . . . , A N } with stochastic memory profile. Given an optimal schedule π = (M 1 , • • • , M n ) for the problem of maximizing the expected throughput, then this schedule satisfies the following property. For all i 1 , i 2 , let j 1 (resp. j 2 ) s.t. m
(i1) j1 ≤ M i1 < m (i1) j1+1 (resp. m (i2) j2 ≤ M i2 < m (i2)
j2+1 ), then:
g i1 m (i1) j1 < g i2 m (i2) j2−1 .
Proof of Theorem 2. We show the result by contradiction. Given π = (M 1 , • • • , M n ) an optimal schedule, assume that there exists
m (1)
j1 such that m
(1)
j1 ≤ M 1 < m (1) j1+1 m (2)
j2 such that m
(2)
j2 ≤ M 2 < m (2) j2+1
and g 1 (m
(1)
j1 ) > g 2 (m (2) j2−1 ) ≥ g 2 (m (2)
j2 ). We show that for
0 < ε ≤    min m (1) j1+1 − M 1 , M 2 − m (2) j2 if M 2 > m (2) j2 min m (1) j1+1 − M 1 , M 2 − m (2) j2−1 if M 2 = m (2) j2 the schedule π ′ = (M 1 + ε, M 2 − ε, • • • , M n ) has a better expected throughput than π. E (ρ(π)) − E (ρ(π ′ )) = E (ρ A1 (M 1 )) − E (ρ A1 (M 1 + ε)) + E (ρ A2 (M 2 )) − E (ρ A2 (M 2 − ε))
Using Equation ( ) and ( ):
E (ρ A1 (M 1 )) − E (ρ A1 (M 1 + ε)) = −(1 − α) • ε • g 1 (m (1) j1 ) E (ρ A2 (M 2 )) − E (ρ A2 (M 2 − ε)) = (1 − α) • ε • g 2 (m (2) j2 ) if M 2 > m (2) j2 (1 − α) • ε • g 2 (m (2) j2−1 ) if M 2 = m (2) j2 ≤ (1 − α) • ε • g 2 (m (2) j2−1 ) (Lemma 1)
Hence
E (ρ(π)) − E (ρ(π ′ )) ≤ (1 − α) • ε g 2 (m (2) j2−1 ) − g 1 (m (1)
j1 ) < 0
Contradicting the optimality hypothesis.
Algorithm 1 is derived from Theorem 2. It initially allocates 0 memory to each application. Then, in increasing order of g i (m (i) j ), it increases the memory allocation of each application up to the next value of the memory distribution until the memory limit is reached or until all application met their maximum memory value.
Algorithm 1 Stochastic (A 1 , . . . , A N , M ) 1: Avail_M ← M 2: G table of size N , initialized with G[i] = c i • j≥1 p (i) j /m (i) j . 3: V table of
j ← V [i] 9: M t ← π[i] 10: π[i] ← min m (i) j+1 , M t + Avail_M 11: Avail_M ← Avail_M − (π[i] − M t ) 12: G[i] ← G[i] − c i • p (i) j+1 /m (i) j+1 13: V [i] ← V [i] + 1 14:
if G[i] > 0 then insert(H, i) end if 15: end while 16: return π
C. Others heuristics
We compared Priority and Stochastic with several heuristics:
• Aggregated, the baseline heuristic, behaves like a solution would on a machine with aggregated memory, i.e. the memory allocation is proportional to the number of compute resources used by the application. • We also compare to other priority-based heuristics (i.e., heuristics that sort jobs by a priority function and allocates the maximum between the available memory and what the job requires), namely:
-Oldest-First: sorts jobs by increasing arrival time (i.e. the date in which the job entered the system); -Largest-First: sorts jobs by decreasing size.
D. Batch Scheduler Integration
In our work we considered a batch-scheduler relying on EASY-BF . At submission time, users are expected to provide an expected walltime to the batch scheduler. The usual impact of this expected walltime is that if a job lasts longer than this value, it is killed by the resource manager. In our implementation, we chose to separate the memory partitioning from the node allocation, in order to simplify the overhead. This means that if the expected walltime is shorter than the worse case (T opt /α), the job is at the risk of being killed. Hence, a batch scheduler needs to use the worse possible walltime as the expected walltime.
The focus of this work was to compare different memory allocation strategies, and determining the impact of disaggregated memory. In future work, it may be interesting to study the co-design of Batch-Scheduling strategies with memory partitioning algorithms, this co-design could use better runtime estimates.