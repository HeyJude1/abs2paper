I. INTRODUCTION
To reach peak performance, node memory on High-Performance Computing systems (HPC) is usually high, designed to be able to cope with most applications. Yet, data shows that HPC systems generally underutilize memory. For instance, Peng et al. observed on a large scale study of four HPC clusters at Lawrence Livermore National Laboratory, that 90% of the time a node utilizes less than 35% of its memory capacity. By studying traces of the Marconi-100 supercomputer , we observed a higher memory utilization. Still, Marconi100 uses up to 50% of its total memory for half of its operation time.
This underutilization of resources has a high cost in terms of machine construction. To give orders of magnitude, today 256GB of RAM costs about 2000USD . The Marconi100 supercomputer had 920 nodes, each with 256GB of RAM. This leads to an estimated cost of RAM in the order of 2 million euros. Wahlgren et al. estimate the node memory cost (DDR4+HBM2e) on Frontier to $170 million out a global cost of $600 million. All these motivate memory size reduction.
Recent architectural advances have included the use of disaggregated memory (such as CXL ). The idea behind it is to semi-dynamically (provided various costs such as a reconfiguration cost) share and allocate memory for the compute nodes, adjusting the allocated memory to the actual needs of the applications. Disaggregated memory would therefore help to reduce the total volume of memory consumed by the HPC resource.
In this work we discuss algorithmic solutions for efficient disaggregated memory usage. We consider an architecture with multiple tiers of memory/storage, such that when an application does not have enough first tier memory available (which we call in the following node memory), it needs to access a much slower memory which slows down its performance. We aim to answer the following question: given an implementation of disaggregated memory on a system with multiple memorytiers, how do we allocate the first-tier of memory between competing applications? To be able to design these algorithmic solutions, we assume that we have access to some knowledge on application behavior. This knowledge can be precise (the memory footprint for the next x units of time), or statistic, based on historical data. For instance, we plot in Figure the distribution of memory utilization per node in May 2022 on Marconi100. Similar statistical studies have been performed on other systems such as Quartz and Lassen at LLNL .
We consider the problem of disaggregated memory in a much larger scheme of multidimensional HPC Resource Management, where the resource manager has to allocate the applications on the compute nodes, and partition the extra memory amongst the running applications.
More specifically, this work presents the following contributions:
• We present two novel algorithmic strategies with guaranteed performance when not considering the reconfiguration cost, one of which is using as input a statistical description of resource usage; • We show that these solutions can help reduce largely the memory needed by an HPC machine while keeping the Figure : Distribution of the per node memory utilization in Marconi100 dataset 8 (May 2022) . Marconi100 has 256GB of RAM available per node (242 usable) .
same performance, including in cases where the memory pattern of an application is unknown; • We demonstrate their performance via a thorough evaluation, including in limit cases. Our work focuses on the robustness and performance of the proposed algorithmic strategies and is complementary to those focusing on practical/technical implementations , . We observe that an implementation of disaggregated memory with our algorithms could theoretically halve the memory usage of HPC machines with insignificant losses of performances.
We organize the rest of this paper as follows: we provide some related work in Section II. In Section III, we propose a mathematical formulation of the scheduling problem, and propose algorithmic solutions along with proofs of their optimality for certain objectives in Section IV. We design an evaluation framework based on real traces in Section V and evaluate our solutions compared to baselines in Section VI. Finally, we conclude and open discussion on the next steps.