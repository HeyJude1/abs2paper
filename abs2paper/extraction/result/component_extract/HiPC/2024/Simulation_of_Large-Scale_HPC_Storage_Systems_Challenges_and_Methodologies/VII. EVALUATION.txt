VII. EVALUATION
In this section we quantify the simulation accuracy of the calibrated FIVES simulator. Direct comparison is not feasible between our results and that obtained with previously proposed simulators , , , , some of which have actually not been validated against real-world ground-truth data. For instance, neither nor offer a network and I/O bandwidth model which could be used as part of a feedback loop during the simulation, which make them fundamentally incompatible with FIVES. In , the authors present a simulator closer to FIVES, but which is able to model the execution of only a very limited number of applications at a time, and requires an unspecified, but consequent, cluster of nodes to execute. Due to these fundamental design and implementation differences (and others: supported input traces, simulated platform models, storage allocation algorithms, output metrics, . . . ), performing sound comparisons would require extensive modification of these previously proposed simulators to augment their existing capabilities.
For calibration and validation of FIVES we consider one year of Darshan traces from the Theta system at Argonne. We choose the year 2022 because it is recent and representative of peak machine usage. Out of 18,086 individual jobs, ≈7,000 do not contain actual I/O activity, and we filtered out another ≈6,000 jobs based on the criteria defined in Section VI-B, leaving us with ≈5,000 suitable jobs. Computing the calibration using the full year as ground-truth data is computationally intensive, and our experiments show that it brings only small accuracy gains when compared to using a single month. In all results below, the calibration was computed using data from the month of November, considering only jobs in the regular class. We picked this month because it has a significant number of regular jobs (366 jobs) and is representative of the job heterogeneity seen in the full dataset. We use Pearson's correlation between simulated and real cumulative I/O times in order to report on the quality of the calibration. While this metric doesn't fully validate accuracy, it helps make sure that FIVES captures a satisfactory trend of the time spent in I/O, which is relevant for a simulation of large periods of time and thousands of jobs.
Figure shows, for each job of the regular class from November 2022, the cumulative real I/O time (x-axis) extracted from Darshan traces and the FIVES-simulated cumulative I/O time (y-axis). This corresponds to the month that was used for calibrating the simulator, and we thus expect high accuracy. Most data points are close to the target diagonal, depicted as a red straight line, and the Pearson's correlation between simulated and real cumulative I/O times is 0.98.  For the year of 2022, excluding the month of November, Figure shows the I/O volumes (bytes) vs. the I/O time (seconds) for each real regular job (green) and its simulated counterpart (grey). We observe that simulated jobs almost all fall within the same job class as that of their real jobs counterparts (92% of jobs). Most of the simulated jobs falling outside of the class bounds form a horizontal pattern (dotted box in the figure). The same pattern, although contained within the class, is present in the real traces. Because these ≈ 200 jobs process almost exactly the same I/O volume and have mostly sequential job IDs, we conjecture that they come from the same application, executed multiple times under various platform conditions and/or with slightly different input parameters. However, the traces do not contain enough details to determine clear differences between these jobs, which is especially challenging for the calibration of FIVES (same, incomplete, input values for n jobs, but different expected outcomes). Another observation from this figure is that simulated jobs occupy a narrower diagonal than real jobs, which spread on the entire width of the 25-75 th percentile domain. This is because, as explained in Section V-B1, we do not have full information regarding the behavior of each individual job and make assumptions that make the job mix more homogeneous than in the real world. We could mitigate this by adding artificial noise to the models, but that would make the simulator non-deterministic, which is not desirable before reaching more confidence in the results. , but shows results for the whole year of 2022 and includes the three job classes (≈ 1, 250 slow jobs in blue, ≈ 2, 500 regular jobs in grey and ≈ 1, 250 fast jobs in red). The pattern first observed in Figure is once again shown inside a black-bordered box. This cluster causes the Pearson correlation on I/O time to drop to 0.43 for the regular jobs, while it would be 0.71 without it. An examination of the workload traces reveals that this cluster corresponds to a single job, which is executed more than 200 times and has a somewhat unique I/O behavior. The calibration procedure attempts to find a single configuration of the parameters that best fit all jobs on average, and the many invocations of this single job end up suffering from lower simulation accuracy. Because our simulator doesn't seem to be a good fit for this type of job, and it was repeated so many times over two months of our studied dataset, the impact is significant. Results also highlight a global high level of accuracy for slow jobs, with a Pearson correlation at 0.83. By contrast, fast jobs experience the worse correlation, at 0.52, although the overall trend is correct. This is because, as explained in Section V-B, to increase the scalability of FIVES we place artificial bounds on several parameters, such as the stripe count and the number of files. This can lead to the execution of jobs with highly optimized I/O implementations to be simulated with lower levels of performance than in the real world. Note that our calibration of FIVES is for a particular trade-off between accuracy and scalability, which has allowed us to run large numbers of simulations as necessary for performing this research. For a given production use case, scalability could be reduced to increase accuracy, either overall or for particular job classes.
The results presented above are a first valuable step towards informing architecture and configuration of storage systems. However, FIVES's job-level accuracy is limited by the level of detail of the traces, which in turn limits the accuracy of a generic job model. Addressing this issue would require access to trace datasets with more complete per-job information, so that more detailed job models can be developed.
The high degree of I/O behavior heterogeneity among jobs makes it impossible to design a simulator that is accurate for all jobs. But it is possible to define job classes and achieve desirable trade-offs between simulation scalability and simulation accuracy for each class.