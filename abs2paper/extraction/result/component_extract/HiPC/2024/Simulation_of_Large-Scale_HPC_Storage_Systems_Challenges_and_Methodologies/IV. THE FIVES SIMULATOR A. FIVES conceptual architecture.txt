IV. THE FIVES SIMULATOR A. FIVES conceptual architecture
We consider that an HPC system comprises three main conceptual elements as depicted in Figure : a Job Manager, an Orchestrator and an Infrastructure. The Job Manager receives user requests with compute resources and time demands, creates a job for each request, and submits jobs for execution to the Orchestrator. The Orchestrator implements scheduling policies by which jobs are assigned to particular hardware resources in the Infrastructure. We describe these conceptual elements in more details hereafter.
1) Infrastructure: The Infrastructure represents a simulated hardware platform, such as the one depicted in Figure . In this example the compute partition consists of homogeneous compute nodes interconnected via a Dragonfly network topology, and is defined by the number of compute nodes, their compute speed and number of cores, and the bandwidth and latency of the network links. Other network topologies are already available in SimGrid, and more can be manually implemented as needed. The storage partition comprises homogeneous storage nodes interconnected via a star topology, and is defined by the number of storage nodes and the bandwidth and latency  2) Job Manager: The Job Manager is in charge of interpreting the resource demands in each user request, creating jobs, and sending these jobs to the Orchestrator. We define a job as a set of compute and I/O operations caused by the execution of one or more applications. These are to be executed on a set of resources on the Infrastructure (specified as a desired number of nodes and cores) that are requested for a given time period. A reservation corresponds to a job for which the requested resources have been allocated. During a reservation one or more applications may run, each with a known start and end timestamp within the bounds of the reservation.
3) Orchestrator: The Orchestrator is responsible for scheduling submitted jobs and their I/O operations. It thus must employ both a job scheduling algorithm and a striping policy for allocating storage resources and distributing data to disks. Although users can provide their own implementations of these algorithms, we have already implemented several algorithms in FIVES, described in the next section, which correspond to a large spectrum of relevant use cases (i.e., classical job scheduling algorithms, the striping policy of the Lustre file system).
FIVES is implemented using the WRENCH and Sim-Grid state-of-the-art simulation frameworks. SimGrid provides foundational simulation abstractions for sequential concurrent processes that use compute, network, and storage hardware resources, using scalable and validated simulation models. WRENCH builds on top of SimGrid to implement high-level simulation abstractions of "Services" to which "Jobs" can be submitted and perform "Actions." Using these abstractions, simulators of complex workloads and systems can be implemented with minimal effort.
1) Main Simulation Components: Figure depicts the simulation components used to implement the conceptual architecture described in Section IV-A, and shows the workflow of the simulation. The figure indicates which components are native to WRENCH and which are developed in FIVES.
The entry point component of FIVES is the Controller, which manages job executions throughout the simulation. It takes as input a dataset that specifies the jobs whose executions are to be simulated on the HPC platform, with an arrival date for each job. At the onset of the simulation, the Controller creates a set of jobs to simulate based on the job dataset. It submits each job for execution to the Batch Compute Service configured to use a particular job scheduling algorithm. WRENCH already comes with several such algorithms, and in this work we use its (default) conservative backfilling implementation . The Controller acknowledges job completions until the execution of the entire workload has been simulated, and outputs timestamped job execution, I/O operation and I/O resource usage events. The Controller incurs no load during the simulation, but requires a few seconds before and after, while instantiating the jobs and computing the final metrics.
The jobs submitted by the Controller are executed on a simulated HPC system such as the one shown in Figure . SimGrid provides a powerful API for describing arbitrary hardware platforms to be simulated that includes compute, network, and storage resources. Using this API, the FIVES user can implement the Infrastructure conceptual element described in the previous section for any target hardware configuration. Building on top of SimGrid, WRENCH offers high-level services implemented in simulation that manages the simulated hardware resources: a Compute Service manages compute nodes (called Compute Hosts), while a Simple Storage Service manages storage nodes (called Storage Hosts). However, the Simple Storage Service implemented in WRENCH, which answers file read/write/delete/copy requests over the network, can only run on a single host and manage file systems on disks attached to that host. To get around this limitation, which makes the framework unsuitable for modeling distributed storage systems, we introduce a new type of storage service that we have contributed to WRENCH: the Compound Storage Service (CSS).
2) Compound Storage Service: A CSS (see Figure ) aggregates and provides a high-level interface to multiple Simple Storage Services on multiple hosts. Files stored on the CSS are transparently distributed and/or striped across any subset of the Simple Storage Services, and the CSS keeps track of the location of each file and file stripe. It intercepts requests for read, write, copy or delete operations in order to redirect them to the appropriate Simple Storage Services.
The CSS abstraction implements all core mechanisms for simulating a parallel file system. Custom policies, such as the striping policy, are provided through the Allocator component. This makes it possible to use the generic CSS abstraction and instantiate it to simulate a particular parallel file systems, as explained in Section V-B3. Once a simulator has been implemented, it must be instantiated to be representative of a real-world system of interest. For this study, we selected Theta at the Argonne National Laboratory, a 11.7-PetaFLOPS Cray XC40 HPC system that ran for almost 6 years until the end of 2023, producing a significant number of major scientific results. Theta featured 4,392 compute nodes (Intel KNL) interconnected via an Aries network with a Dragonfly topology. The choice of Theta was motivated by its hosting of a 10 PB Lustre file system, the most widespread storage system on Top500 machines to date. In addition, unless explicitly specified at compile time, applications running on Theta were monitored with the Darshan I/O monitoring tool, which has yielded a valuable dataset with years of I/O execution traces. An aggregated version of these traces is publicly available online .
A. Background 1) Darshan: Darshan is a monitoring tool for recording the I/O performed by an application with low overhead. Information is collected at a high level of detail, down to the granularity of the data chunks written by a process. Darshan is deployed on several top-tier supercomputers, such as those at Argonne National Laboratory or at NERSC, where the majority of applications are monitored. However, for reasons specific to these institutions (confidentiality, data control), raw data is not publicly distributed. Only aggregated data is available, providing a global view of job I/Os but omitting information such as the number of files written or the number of processes that took part in I/O. Overall, for a given job, the information made available is job start and end times, volumes of data read and written, and time spent performing I/O (plus other information not relevant to this work).
2) The Lustre Parallel File System: Lustre is an opensource parallel file system (PFS) that has been maintained and evolved for over twenty years. As depicted in Figure , a Lustre file system consists of I/O servers called OSS (Object Storage Servers) and disks called OST (Object Storage Targets). The number of OSTs managed by an OSS is variable ranging from one on the Theta supercomputer at Argonne National Laboratory, to three on Frontier at Oak Ridge National Laboratory, or higher on other systems. Following the same model, metadata is managed by metadata servers (MDS) and targets (MDT). All storage resources are accessed via so-called LNET nodes, which correspond to the I/O forwarding nodes found in many HPC systems. These LNET nodes are generally part of the interconnection network and access the Lustre file system via a dedicated network interface. Fig. : Lustre architecture Data written to a Lustre file system is striped across the OSTs, allowing performance gain by aggregating bandwidth from multiple OSTs and possibly OSSs. Striping a file is a two steps process. First, Lustre creates an ordered list of usable OSTs. Second, Lustre distributes file parts on a subset of this list according to a striping layout. Selecting and ordering which OST(s) will be used is achieved using one of two allocation strategies: round-robin or weighted. The roundrobin strategy is used in most cases. The weighted strategy implements a bias towards selecting the least used OSTs first and is triggered on rare occasions, when free-space imbalance between OSTs is above some threshold.
Lustre's default striping layout takes two main parameters: the stripe count which defines the number of OSTs on which the stripes will be distributed and the stripe size, which defines the granularity at which the data is divided into chunks of identical size inside the stripes. The stripe count and the stripe size have default values, although they can be changed by the user, on a per-file or per-directory basis. On Theta, the stripe count was set to 1 and the stripe size was set to 1MB. Section V-B3 describes our implementation of the Lustre's striping policy in the FIVES Allocator component.
Given the characteristics of the platform to be simulated and the information available regarding the jobs whose executions are to be reproduced on that platform, one must make various decisions regarding how the simulator should be instantiated. In what follows we discuss the three main decision axes when using FIVES to simulate the Theta platform and workload.
1) Job execution: When submitting a job to the Batch Compute Service, FIVES uses the real-world walltime and resource requirements of each reservation and respects the actual interval between subsequent job submissions, as available in the job trace. The start date of a job, however, is conditioned by the conservative backfilling job scheduling algorithm implemented in WRENCH. These start dates can thus be different from start dates that were driven by the specific batch scheduler configuration deployed on the real-world platform. Each job is defined by a runtime that we estimate based on the information present in the workload trace, and bound by the reservation walltime. Each I/O phase is subdivided in a variable number of individual read or write WRENCH actions, executed collectively from a subset of the job's compute nodes. Each action internally spawns multiple actual reads and writes to the storage system, depending on the job's I/O volume. The typical number of simulated reads or writes for a job can be approximated as F × H × S H , where F (1 ≤ F ≤ 10 2 ) is the number of files, H (1 ≤ H ≤ 10 1 ) is the number of participating compute nodes and S H (1 ≤ S H ≤ 10 2 ) is the number of stripes accessed by each host, similar for all hosts ±1. S H is bounded by the number of OSTs in use and the number of file chunks allowed on each OST for simulation scalability reasons (see Section V-B3). This approximation is an underestimation of the number of I/O accesses that jobs performed on the real system. Despite this underestimation, our results demonstrate that our simulations are in line with real-world behaviors.
We assume that all reads and writes from a job are on the PFS. Some might be local since the Theta compute nodes feature node-local SSD, but the workload trace does not distinguish between local and non-local I/O. Furthermore, for a given job, the trace does not specify the number of files (F ) and the number of compute nodes that are involved in I/O operations (H). We consider that F and H are parameters that must be calibrated based on observed real-world job executions. We assume that, for all jobs, the ratio of the total I/O volume to the total number of files, and the ratio of the number of compute nodes involved in I/O operations to the number of compute nodes allocated to the job, are based on fixed constants for reads and writes. In other words, we need to calibrate only 4 parameters (2 for reads, and 2 for writes), instead of 4n parameters where n is the number of jobs, which would render the calibration problem intractable due to high dimensionality. This choice leads to a smaller range of possible I/O behaviors among the simulated jobs, which at the moment we consider better than to allow more randomness, as our data doesn't currently provide us with details on the matter.
Making assumptions and abstracting away certain I/O behaviors is unavoidable because the information contained in I/O monitoring datasets is not comprehensive. Some of these assumptions, however, cause simulated workloads to be less heterogeneous than their real-world counterparts.
2) Disk contention model: SimGrid implements a default naive fair-share model for disk bandwidth, but users are advised to provide their own model for bandwidth degradation as a function of the number of concurrent I/O operations. This degradation depends on the mix of read and write operations and occurs both on HDDs and SSDs, with various behaviors that depend on many architecture-and workloadspecific characteristics , . Producing a general such model is outside the scope of this paper. Instead, we developed an empirical model derived from experiments we conducted on Seagate ST1000NX0443 SATA HDDs. The results (omitted due to space limitations) show that going from a single access to concurrent accesses causes an initial sharp decrease in bandwidth. But as concurrency increases, contention from each additional I/O operation has less impact and the bandwidth deterioration curve flattens out. It turns out that this behavior is modeled accurately (for our experimental results) using a model that includes a logarithmic component. Specifically, in FIVES, we model the instantaneous bandwidth (read or write) of a disk as:
bw = bw max * 1 C + log n ,
where bw max is the maximum achievable disk bandwidth, n is the number of ongoing concurrent I/O operations, and C is a constant. Difference choices for the C value makes the model more linear or more logarithmic. Values for bw max and C must then be calibrated based on observed real-world job executions.
3) Striping Policy: The Allocator component provides the Compound Storage Service with a striping policy. Since Theta's storage system is a Lustre PFS, we have implemented in FIVES the internal striping policy based on Lustre's open source code. It handles both round-robin and weighted allocation strategies, with recommended default parameters for choosing between strategies, and accepts stripe size and stripe count parameters. It is important to note that the Darshan data source used in this study does not contain the stripe count and stripe size parameters used for each file. In our simulations, we resort to calibrating these values based on the following rules:
• stripe count: All jobs with a mean I/O bandwidth (bw job ) below a calibrated threshold use the same static default value for all I/Os (set to 1 in this paper as it is the default stripe count on Theta). Jobs above the threshold use a dynamic value computed as alt stripe count × bw job /threshold. The alt stripe count and the threshold values are different for reads and writes. • stripe size: This value is manually configured in the range [50, 100] MB. For simulation scalability reasons, this is higher than Lustre's default value of (1MB) to limit the number of simulation objects being created. For the same reason, FIVES can be configured so that the number of file parts on each OST is bounded by a user-specified values (F OST ). If using the stripe size default value in conjunction with a large file size L ends up exceeding F OST , the stripe size is instead set to L / (F OST × stripe count) on a per-job basis. We pick F OST so that one invocation of FIVES in our experimental evaluations takes at most 20min on a 2.5GHz core.
Each simulation model must abstract away some of the details of the real-world component it simulates for two reasons that can apply simultaneously: (i) because some of the details of the real-world system and/or workload are unknown and (ii) because of simulation scalability concerns.