I. INTRODUCTION
As increasingly powerful HPC systems are being deployed the gap between compute and I/O performance keeps widening. This is seen plainly by the evolution of the ratio between compute power and I/O bandwidth of the top three machines in Top500 , which has decreased by ∼10x over the last 13 years (see Figure ). At the same time, the recent shift from compute-centric to data-centric applications and workflows has resulted in a so-called "data deluge", the effect of which has been observed in major supercomputing centers. For instance, at the National Energy Research Scientific Computing Center (NERSC) the volume of data stored by applications increased by ∼41x between 2010 and 2021, with an annual growth rate estimated at 30% . To alleviate the storage bottleneck, high-performance storage systems, vertically and/or horizontally scaled, have been installed alongside compute partitions. Vertical scaling consists in enhancing the storage infrastructure with additional fast storage layers based on flash memory or non-volatile memory . This is done in emerging systems like DAOS but also in mainstream parallel file systems like Lustre. Horizontal scaling consists in increasing the number of storage devices. For instance, this is the approach used by Orion , the 700PB parallel file system deployed on the Frontier supercomputer at Oak Ridge National Laboratory, which features 47,700 hard disks. The integration of I/O forwarding nodes into the interconnect also provides more gateways for I/O transit, reducing contention and increasing bandwidth. These developments have led to significant performance gains, at the cost of higher system complexity.
Date
The architecture, implementation, configuration and efficient use of these high-performance storage systems open up many research questions. However, providing sound answers requires conducting comprehensive experiments, which often proves arduous. This is in part because some challenges, such as the sizing of the infrastructure, need answers upstream of the storage system design, before an actual system is available. One approach is to analyze job execution historical data, collected on other previously available systems. Unfortunately, this data is strongly tied to the particular features of those systems and of their application workloads. Even when a system is accessible for experimental purposes, the limitations imposed by its implementation and/or its production use reduces the scope of experiments that can be carried out to answer questions in areas such as storage-aware job scheduling, resource allocation, file system configuration or storage system's energy consumption. For instance, consider Lustre , the most widely deployed file system on supercomputers to date. The provisioning of a Lustre-based storage system, the choice of a data striping policy, or the development of strategies for mitigating contention are well-known questions that can only be answered via extensive experimental campaigns . But running the necessary experiments at scale on a production system is a difficult proposition as these experiments would not only disrupt the system's production use, but could also lead to prohibitive resource and energy consumption.
Simulation is a promising approach to overcome the above experimental obstacles. It provides a way to 1) evaluate possible architecture designs and configurations before the system is deployed; 2) evaluate a wide range of I/O and storage resource management algorithms; and 3) perform post-mortem analysis of a decommissioned storage system and draw lessons from it. The simulation of parallel and distributed systems has long been an active field, enabling reproducible experiments for arbitrary scenarios in a way that is less time-, labor-, and resource-intensive than real-world experiments. However, there have been few attempts at simulating high-performance storage systems in a way that is both accurate and fast.
In this work, we first identify the main challenges for the design and validation of an accurate simulator of highperformance storage systems, ranging from the question of which data the simulator should take as input to the difficulty of achieving a desirable trade-off between simulation accuracy and simulation speed. We then propose solutions for these challenges, which we implement as part of FIVES, a "Simulator for Scheduling on Storage Systems at Scale" (five "S"). The goal of FIVES is to be sufficiently accurate for its output to inform file system configuration and design decisions. This goal is partly achieved via an automated simulation calibration method. Our contributions in this work are as follows:
• The identification of key challenges for the accurate and fast simulation of high-performance storage systems; • A simulation abstraction for a distributed storage system;
• The design and implementation of the FIVES simulator;
• A method for automatically calibrating FIVES using Bayesian optimization; • An experimental evaluation of FIVES for simulating a production Lustre deployment and workload.