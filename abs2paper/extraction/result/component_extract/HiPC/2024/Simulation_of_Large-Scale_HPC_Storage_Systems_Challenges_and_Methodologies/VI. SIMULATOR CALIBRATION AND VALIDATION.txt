VI. SIMULATOR CALIBRATION AND VALIDATION
Calibrating a simulator is essential for it to produce accurate results. The calibration process consists in determining simulation parameter values that make the simulated behavior as close as possible to the real-world We have identified several parameters to calibrate, as detailed in the next section. One way of determining values for these parameters is to vary each of them in successive simulation runs until a sufficient level of correlation between simulated and real performance is reached. However, this is a prohibitively long process due to the size of the parameter space and the non-zero simulation execution time. Instead, we use Bayesian Optimization (BO), a proven technique for parameter search that helps reduce the exploration space. In FIVES, we chose to work with the Ax framework , using 50-80 iterations.
FIVES is configured via dozens of parameters that pertain to the hardware platform, the jobs to be simulated, and the storage system. We have empirically determined which parameters play a significant role in the simulation output when simulating our production workload, leading us to identify 17 parameters that should be calibrated:
• Platform bandwidths (R/W on disks, network link between compute and storage partition) -3 parameters • Jobs file count (coefficient applied to job's mean bandwidth to determine read and write file counts for each I/O phase) -2 parameters (R/W) • Number of compute nodes participating in I/Os of each job -2 parameters (R/W) • Disk contention model coefficients -2 parameters (R/W)
• Striping model parameters (stripe size and count values, adjusted per jobs and for R/W operations and metadata access overhead) -7 parameters • Maximum file parts count on OSTs -1 parameter Ranges of possible values for these parameters are loosely defined based on approximate knowledge of the platform, workload, and storage system. But for some parameters, as explained in Section V-B, the range is constrained for simulation scalability reasons. Some of the parameters that are not calibrated are set to their known values (e.g., the total number of compute nodes, the number of disks per storage node). Others have been empirically determined to have little impact on the simulation output provided they are set to reasonable guesses. For instance, the network latency is set to the same value (24 µs) for all links, and only deviations from this value by orders of magnitude impact the simulation output.
1) Calibration loss function: The calibration process aims at minimizing a loss function that quantifies the simulator's accuracy. We define our loss function as the Mean Absolute Error of the percentage difference between the cumulative I/O time of each simulated job and its real-world counterpart. We use this percentage difference so that the impact of jobs with very long I/O on the loss function is not larger than that of jobs with shorter I/O. Formally, our loss function is defined as:
loss = 1 N * N i=1 |R IO i − S IO i | R IO i ,
where N is the number of jobs, R IO i (resp. S IO i ) is the real (resp. simulated) I/O duration for job i.
We find that our workload dataset includes either highly under-performing or over-performing jobs in terms of mean I/O bandwidth, e.g., ranging between 0.1MB/s and 3.5GB/s for reads. For our generic job model to be applicable, it is thus necessary to filter out outliers (mean read or write bandwidth ≥ 90 th percentile or ≤ 10 th percentile, I/O volume > 10TB), which corresponds to ≈ 29% of the jobs in the original dataset. The reason why we designed a generic job model, which cannot capture idiosyncratic behaviors of these outlier jobs, is that the workload dataset lacks the necessary information for attempting to model these behaviors. The remaining 82% of the jobs, however, can be simulated so that trends and correlations of simulated behaviors are in line with real-world behaviors. But still, we find that the calibration process is sensitive to the heterogeneity of the job dataset. Therefore, we sort jobs into bandwidth classes: fast jobs (mean bandwidth ≥ 75 th percentile), slow jobs (mean bandwidth ≤ 25 th percentile), and regular jobs in-between. Simulation parameters shared among all jobs (e.g., platform parameters) are calibrated using regular jobs only (and fixed for further calibrations). Jobspecific parameters are calibrated independently for each class.
We use the obtained calibration to run simulations on every single month of the dataset, and evaluate simulation accuracy using Pearson correlations between simulated and real job I/O times. We are particularly interested in minimum values and variance in correlations for read and write times. The month that was used for obtaining the calibration serves as a control, the expectation being to get near perfect Pearson correlations between real and simulated I/O times. The other simulation runs let us observe whether the calibration generalizes, since the workload exhibit variations throughout the year.
Defining the set of calibration parameters is not straightforward as one must determine which parameters have a significant influence on the simulation output to reduce the dimensionality of the calibration problem. Choices must also be made regarding which subset of the ground-truth data should be used for computing the calibration, requiring some method to define and exclude outliers.