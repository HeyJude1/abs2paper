III. PROBLEM FORMULATION
We assume an independently and identically distributed (i.i.d) rewards model, denoted as stochastic bandits. In our model, we assume a choice of K actions, which we refer to as arms, and which are to be executed over T rounds, where K and T are predefined. During every round, the algorithm selects one arm, leading to the accumulation of a reward specific to that arm. The primary objective of the algorithm is to optimize total reward accumulated throughout the T rounds. The model includes the following assumptions. First, we can only observe the reward associated with the action it chooses and no other information. Specifically, the model needs to be made aware of the potential rewards from other actions that were not selected (a.k.a., bandit feedback ). Second, the reward corresponding to each action is i.i.d. For any given action "a", we assume a reward distribution, D a , over the real numbers. Each time we choose an action, its reward is independently drawn from D a . Initially, these reward distributions are unknown to the algorithm. Third, we assume that the rewards received in each round are constrained within the range [0, 1].
We include user-defined priorities when selecting the optimal HPC configuration. To include users in the decision framework, we include two parameters -α for execution time and β for power consumption, both ranging within [0, 1]. The user can set these parameters to control the optimization balance, e.g., higher values in α and β indicate higher emphasis on execution time or power consumption, respectively. In our model, we define χ as the parameter space, where χ = {1, . . . , x} is a finite action space; i.e., we set every unique combination of the parameters (configuration) as an arm of the MAB setting.
We specify a distribution D over pairs (x, r), where x ∈ X denotes the parameter configuration and r ∈ [0, 1] A denotes a vector of rewards. In its basic stochastic form, our formulation involves a set of K probability distributions for each arm, denoted as {D 1 , . . . , D K }, each with associated expected values {µ 1 , . . . , µ K } and variances {σ 2 1 , . . . , σ 2 K }. Initially, these distributions are unknown to the algorithm. During each turn t = 1, 2, . . ., the algorithm chooses an arm, indexed by j(t), and receives a reward r(t) ∼ D j(t) . The objective is to determine the distribution with the highest expected value and to accumulate as much reward as possible in each iteration.
To model uncertainty, we employ an upper confidence bound (UCB) technique that employs "optimism under uncertainty". Based on current observations, this technique assumes that every arm represents the best possible outcome. Consequently, the selection of an arm is based on these optimistic estimations. The technique involves initially trying each arm once. Then, for each round, t = 1, . . . , T , the technique selects the arm x(t) that appears to be the most promising. The selection of configurations in each iteration is calculated as follows for a configuration x at iteration t:
U CB(x, t) = R x + 2 ln t N x , (2)
where R x = f reward (x) is the weighted reward for configuration x, and N x is the count of times configuration x has been selected up to iteration t. Eq. 2 dynamically balances the exploration of new configurations against exploiting those already known to be effective. The proposed model ensures that the reward is inversely proportional to the normalized metrics of execution time and power consumption, thereby aligning with the user's optimization goals.
After each iteration t, we identify the configuration x with the highest UCB value. The configuration, x * t , is determined as follows:
x * t = arg max x U CB(x, t). (3)
This iterative selection strategy ensures an adaptive balance between exploring untested configurations and exploiting known effective ones. We determine the most frequently selected configuration as follows: for each configuration x ∈ χ do
x opt = arg max x N x . (4)
5: Calculate weighted reward R x = w τ × 1 µ(τx) + w ρ × 1 µ(ρx) 6:
Calculate UCB values for each configuration using: end for 12: return The optimal configuration x opt = arg max x N x After T round of iterations, the algorithm outlined in Section IV outputs the most optimal configuration, x opt . The high-level block diagram of LASP is given in Fig. .