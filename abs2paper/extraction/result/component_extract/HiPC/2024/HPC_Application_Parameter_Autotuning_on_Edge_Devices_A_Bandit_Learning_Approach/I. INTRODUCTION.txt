I. INTRODUCTION
Motivation. Edge devices have been gaining popularity as a platform to execute computational workloads for widespread availability and increasing computational power . According to a recent report , the market of edge-to-process application data is expected to grow by 75% by 2026. Edge computing processes workload generated by end users nearby, thereby achieving low end-to-end latency and high bandwidth. High-performance computing (HPC) applications are characterized by their need for extensive computational resources and efficient performance. Edge devices can be used for scientific application execution due to their increasing processing capabilities. Recent U.S. DOE and Europe HPC reports outline the opportunities to solve scientific applications on the backdrop of emerging edge computing technologies. However, limited and heterogeneous distributed edge resources present unique challenges to HPC execution on edge devices.
HPC applications involve complex parameter configurations , which significantly affect their performance, contributing towards performance degradation and sometimes even causing non-execution faults . As such, it becomes challenging for the users to evaluate the impact of various tunable parameters on the execution time and understand their effects on each other . Application users must invest considerable effort in searching for the optimal values for all parameters to attain the least execution time . Because manual tuning is time-consuming and labor-intensive and prone to significant error, the automatic tuning of configuration parameters for HPC applications has been a significant subject of study for the past several years , . We propose an innovative approach where HPC applications are initially executed on edge devices to determine optimal applicationlevel parameters. The edge devices can efficiently identify the best parameters by running these applications at low fidelity (LF), which demands fewer computational resources. These parameters are then transferred to traditional HPC platforms for execution at high fidelity (HF). This method significantly reduces the time and energy typically spent on parameter tuning on traditional HPC systems, leading to more efficient overall execution of HPC applications. Our approach is illustrated in Fig. , where edge devices act as a preliminary stage for parameter optimization before the final execution on HPC clusters.
Notably, existing parameter autotuning techniques have been developed primarily for traditional HPC systems, which themselves demand significant computational resources. Our motivating experiments on four HPC applications on edge devices show the unique challenges HPC parameter autotuning presents on edge platforms. By leveraging edge devices, this paper aims to enhance the efficiency and performance of traditional HPC applications. Our method, based on stochastic techniques for application-level parameters, is portable across various edge and HPC platforms, though some tuning may be required for hardware-level parameters.
Limitations of state-of-the-art approaches. Traditional parameter tuning methods are either exhaustive, time-consuming, or based on heuristics that may not capture the nuances of different application scenarios and the resource-constrained nature and volatility of the edge devices. Existing knowledgebased tuning involves domain experts manually adjusting parameters based on experience and intuition. While this can be effective, it is time-consuming, not scalable, and heavily relies on expert availability, while heuristic approaches utilize rulebased methods , to select parameters. These methods are faster but often need more flexibility to adapt to different application needs or changes in the computing environment, and thus, usually get stuck at local optima. Both manual and heuristic methods do not scale well with the increasing complexity of HPC systems .
To tackle these challenges, state-of-the-art solutions have employed variants of learning-based approaches , . Recently, the effectiveness of configuration autotuning has been demonstrated by more advanced learning techniques such as utilizing machine learning (ML) techniques , . However, these models also come with their own overhead costs, making them non-ideal for edge devices. While numerous HPC applications may undergo multiple executions, the input type or size can vary over time. The optimal configuration evolves with changes in input type, input size, or the integration of incremental algorithmic improvements into the application code base . Consequently, the cumulative cost of autotuning increases over time, and autotuning efforts may demand substantial resources on large-scale systems, resulting in the dedication of millions of node hours for autotuning on expensive supercomputers . Simultaneously, the correlation with workload type and input dataset size in big data applications fluctuates, leading to the frequent initiation of time-consuming model retraining tasks .
Predictive models can provide quicker solutions but often require substantial training data and are usually limited by the accuracy of their underlying models. They also face challenges in generalizing across different HPC applications and may require retraining for different environments . More importantly, these models are generally static, often leading to suboptimal performance or excessive computational costs . HPC workloads and environments are highly dynamic; therefore, a tuning method that can adapt in real-time to changing conditions is required. However, existing predictive methods do not directly incorporate such dynamic workload in their learning .
Many search-based methods , achieve satisfactory configuration for many HPC applications. These methods consider the relationship between performance and configuration parameters as a black box technique and employ a specific exploration mechanism to search for the optimal configuration directly. One prominent technique is the Bayesian Optimiza-tion (BO). BO-based techniques and their variations can identify a near-optimal configuration with only a limited number of iterations for various HPC applications . However, the BO-based techniques have several limitations -(1) Bayesian optimization struggles with the intricate relationship in big data frameworks, requiring numerous iterations for an accurate model ; (2) Vanilla BO prioritizes quick convergence, risking time-consuming sub-optimal configurations due to overlooking evaluation times ; and (3) HPC workload characteristics change over time, necessitating configuration re-tuning, while, Vanilla BO lacks historical knowledge utilization and starts afresh for each task .
Key Insights and Contributions. To address the limitations of existing approaches, we propose a novel lightweight and online technique for determining the optimal HPC configuration on resource-constrained edge devices: Lightweight Autotuning of Scientific Application Parameters (LASP). We focus on the challenges of configuration selection in HPC for edge devices.
Our solution leverages the multi-arm bandit (MAB) technique, offering unique benefits for HPC applications. First, the flexibility of MAB models allows effective application across various HPC scenarios, adapting to specific needs and constraints. Second, to our knowledge, we are the first to apply this approach to autotuning on edge devices. We compare LASP's autotuning effectiveness with the default strategy, where applications run with their default settings, demonstrating LASP's lightweight nature and minimal overhead. Third, MAB models are adaptable, making them suitable for dynamic environments where reward distributions may change over time. This is particularly suitable to the volatile edge environment we are leveraging. Our performance evaluation shows that LASP can identify the best configuration, significantly enhancing HPC application performance on edge devices. Furthermore, our model dynamically adapts to user needs and changes in application behavior, determining the optimal configuration with minimal regret, thus fulfilling MAB properties.
Organization of the paper. The rest of this paper is organized as follows. In Section II, we present the background and discuss the challenges. In Section III, we formulate the problem and present the objective function. In Section IV, we introduce a lightweight technique for HPC application parameter selection. In Section V, we present results to show performance evaluation in dynamic workload scenarios. In Section VI, we conclude our paper and suggest future directions.