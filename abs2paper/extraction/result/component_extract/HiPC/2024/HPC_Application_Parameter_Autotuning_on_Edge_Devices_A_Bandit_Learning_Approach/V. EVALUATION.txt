V. EVALUATION
Here, we first discuss details of LASP's execution, followed by performance evaluation against other configuration selection strategies. We then present how different user-level parameters affect LASP, and finally show how LASP can adapt to sensitivity changes.
We collected experiment data on the NVIDIA Jetson Nano device, a widely-used edge device in research and industry. The device's compact size, combined with its robust processing capabilities and power efficiency, makes the Jetson Nano a suitable choice for edge computing applications .
The Jetson Nano features a 128-core Maxwell GPU and a Quad-core ARM A57 CPU running at 1.43 GHz. It is optimized for efficient parallel processing and computationintensive tasks. It runs on Ubuntu 20.04 OS and is equipped with 4 GB of 64-bit LPDDR4 RAM with a bandwidth of 25.6 GB/s. It uses a microSD card for storage. The device offers two power modes: MAXN and 5W. In Table , we provide a detailed description of each mode's specifications and operating characteristics. This operational mode mimics the typical power constraints encountered in edge computing scenarios . The highfidelity data used in this study was collected on a system featuring an Intel® Core™ i7-14700 vPro® processor, with 20 cores and 28 threads, and a maximum turbo frequency of 5.3 GHz. The system had 64 GB of DDR5 memory and ran on Ubuntu 24.04 LTS.
All the autotuning results and shown in the subsequent section are done on the Jetson Nano device to show the efficacy of our lightweight approach to autotuning. Furthermore, to mitigate potential performance interference, we ensured that no extraneous processes were running on the device, apart from the essential kernel processes and our target HPC applications.
Table II lists the HPC applications that we used to evaluate the effectiveness of our proposed techniques. To validate our results, we used applications with both small and larger parameter choices, excluding hardware-level parameters such as power and CPU capping. These applications cover a wideranging variety of science domains and have been used previously to capture the challenges in autotuning diverse HPC applications .
Hypre is a software library for scalable solutions of linear systems, leveraging parallel processing for highperformance computing. It includes the BLOPEX package for solving eigenvalue problems, making it a versatile tool for various scientific applications.
Clomp is a C-language benchmark that measures OpenMP overheads and performance impacts due to threading, simulating a typical scientific application inner loop workload under strong scaling conditions to assess the efficiency of various OpenMP scheduling algorithms.
Lulesh is a widely used proxy application that originated from the Shock Hydrodynamics Challenge Problem, designed to test the performance of high-performance computing systems and algorithms, and has since become a benchmark in DOE co-design efforts for exascale computing.
Kripke is a scalable, 3D deterministic particle transport code that researches the effects of data layout, programming paradigms, and architectures on Sn transport implementation and performance, aiming to optimize solver performance and parallelism.
Here, we show how LASP finds optimal configuration using efficient parameter exploration, where we change the  user's focus on controlling the optimization. We first show how LASP works when we have control over parameters in two dimensions for Lulesh. Next, we show the results for parameters in three dimensions for the application Kripke and Clomp. We also show the efficacy of LASP with multidimensional parameter application Hypre through our regret analysis and sampling efficiency to find the optimal configuration.
Efficient Configuration Allocation: In Fig. , we show how LASP achieves the optimal configuration. The figure presents a heatmap visualization of the configuration space for Lulesh, focusing on the application-level parameters "Materials in Region" and "Elements in Mesh."(The darker the cell, the more frequently LASP selected it as an optimal configuration.) The figure illustrates the frequency of the LASP's selection of specific configurations -the darker regions indicating a higher selection frequency. We evaluated LASP over 1000 and 500 iterations, observing that in both scenarios, the algorithm effectively converges towards the optimal configuration. It is important to note, however, that the optimal configuration identified by LASP may not always be the most optimal, but close to optimal. This is due to LASP's stochastic nature, which navigates the configuration space based on the reward distribution of configurations. We adapted LASP to optimize both execution time and power consumption simultaneously. Fig. shows that LASP effectively explores the configuration space, consistently identifying configurations that balance both objectives. To test its efficiency, we ran LASP for 500 and 1000 iterations in two representative scenarios. Fig. demonstrates that LASP converges to optimal configurations efficiently within 500 iterations when the parameter configuration dimensions are small (Lulesh, Kripke, Clomp). Whereas, running LASP for 1000 iterations helps it explore near-optimal configurations, which is beneficial for portability when deploying on traditional HPC clusters.
We performed a similar analysis for Kripke and Clomp,
The default values of parameters of these application have been shown in Table . We calculate the performance gain under the best configuration P G best as follows:
P G best = f default − f best f default • 100%, (8)
where performance under default configuration is denoted as f default and the performance under the best configuration is denoted as f best .
In Fig. , we do this performance gain analysis of the four applications by varying α. At lower α, LASP will work towards finding configurations with lower power consumption. When the user sets the power as the desired objective metric LASP achieves a 10% performance gain for Clomp, 14% for Lulesh, 9% for Hypre and 6% for Kripke. With increased α, LASP will search the configuration space that yields lower execution time.
LASP achieves significant performance gains performance gain in execution time (α = 0.8) and in power consumption with (α = 0.2). As expected, LASP performs better in smaller parameter spaces compared to bigger ones, as shown in Fig. . This is because smaller parameter spaces allow for more efficient exploration and convergence to the optimal configuration. However, LASP's fast convergence in finding the optimal configuration makes up for its performance in larger parameter spaces. We run LASP 100 times in order to see the mean distance from the oracle across different runs. The results are demonstrated in Fig. which shows that LASP can reach within 12% of the optimal configuration even in large parameter spaces, such as those of Hypre, when optimizing for execution time. When optimizing for power consumption, LASP's performance is less effective compared to when execution time as an objective metric. This is because power consumption is saturated by the edge device when running computationally intensive HPC applications, resulting in a less varied reward metric compared to execution time. As a result, LASP's ability to converge to the optimal configuration is impacted.
We compared our approach against BLISS ,a SOTA machine learning-based optimization method that leverages Bayesian Optimization (BO) to minimize tuning expenses. By creating a diverse pool of streamlined models, Bliss accelerates convergence and utilizes surrogate model predictions to streamline the evaluation of configurations, resulting in significant time savings. While we acknowledge our approach, did not do better in terms of efficiently finding the optimal parameters it is because we prioritized a lightweight approach for it to be applicable resource constrained edge devices. This is proved by our analysis of the CPU and memory footprint of using BLISS and LASP for autotuning on two modes (MAXN and 5W) to demonstrate the dynamic nature of our algorithm. A summary of our findings and a description of these two power modes are given in Fig .
We evaluate the efficiency of our proposed techniques by performing best-run(one time least regret run) regret analyses, as defined in Equation . The results, illustrated in Fig. , showcase the convergence of LASP from an initial trialand-error phase, characterized by suboptimal decision-making, to optimal configuration selection for four distinct applications. By observing the accumulated regret at each iteration, we notice that the regret saturates after a certain number of iterations for all applications. The number of iterations
Error in measurement data. We introduce synthetic errors to the measured data to observe the dynamic nature of LASP. To simulate real-world imperfections, we add random noise to our collected data within a range of 5%, 10%, and 15%. As can be seen in Fig. , despite the erroneous feedback to LASP, we are still able to achieve considerable performance gains. This resilience can be attributed to the fact that MAB algorithms are inherently adaptive to change due to their design.
In this context, the random noise introduced in our experiments also serves as a proxy for network fluctuation anomalies, such as varying latencies or packet loss, which can lead to inconsistent measurements. Despite these additional challenges, LASP's ability to adapt to changing conditions allows it to mitigate the impact of such errors and continue to perform well even in the presence of network irregularities.