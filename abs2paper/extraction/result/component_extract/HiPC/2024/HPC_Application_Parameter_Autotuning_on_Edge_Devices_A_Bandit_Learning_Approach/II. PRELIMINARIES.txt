II. PRELIMINARIES
A. Terminology
We first define essential terms that are used throughout the paper. Tunable parameters include the application-level parameters, which can take on various values or states, markedly affecting the execution time of an application.
The autotuning search space, or search space, comprises the extensive n-dimensional space created by the range of values that tunable parameters can take. The range of this search space is defined by the potential combinations of tunable parameter configurations, represented by the product of each parameter's possible values (a 1 a 2 ...a n , where n denotes the number of tunable parameters).
A configuration, or a sample, is a specific combination of parameter values selected within the search space. Sampling or sample evaluation involves running an application using a particular configuration and assessing its runtime. Oracle configuration describes the ideal configuration with minimal execution time or power consumption. While it is intuitive to aim for shorter execution times, we also consider parameter configurations that minimize power consumption of edge device. This is because power is often a limited resource for edge devices, and optimizing for power efficiency is crucial to ensure their effective operation. Identifying the Oracle configuration accurately involves examining all possible configurations in the search space, which is impractical in production settings. However, we conduct an exhaustive search to assess the effectiveness of any given configuration relative to the Oracle configuration. This assessment is quantified as the distance from the Oracle configuration and is defined as follows:
execution time of a configuration execution time of the Oracle configuration − 1 × 100%.
B. Multi-Arm Bandit
The multi-arm bandit (MAB) problem is fundamental in probability theory and decision-making under uncertainty. It involves a sequential decision-making framework where an agent must choose with limited information. Pure exploration bandit problems aim to minimize the simple regret, defined as the distance from the optimal solution, as quickly as possible in any given setting. The pure-exploration MAB problem has a long history in the stochastic setting , and was recently extended to the non-stochastic setting . Similarly, the stochastic pure-exploration infinite-armed bandit problem was studied by Carpentier et al. , where a pull of each arm i yields an i.i.d. sample in [0, 1] with expectation ν i , and ν i is a loss drawn from a distribution with cumulative distribution function F . Hyperband works by the best arm identification, i.e., selection of an arm with the highest average payoff in a non-stochastic setting.
The MAB technique has been applied in solving many real-life problems, including exploration and identification of efficient setting from a given distribution. Some application domains include healthcare, finance , recommender systems, etc. Naturally, due to their ability to continuously learn and adapt their strategies based on real-time feedback, these approaches have also seen widespread adoption in hyperparameter tuning solutions for neural Networks .
In its basic stochastic form, the bandit problem involves a set of K probability distributions, denoted as {D 1 , . . . , D K }, each with associated expected values {µ 1 , . . . , µ K } and variances {σ 2  1 , . . . , σ 2 K }. Initially, these distributions are unknown to the player. These distributions are often likened to the arms of a slot machine, with the agent acting as a gambler whose goal is to maximize rewards by pulling these arms over multiple turns. At each turn t = 1, 2, . . ., the player chooses an arm, indexed by j(t), and receives a reward r(t) ∼ D j(t) . The player's objective is to determine which distribution has the highest expected value and accumulate as much reward as possible. Bandit algorithms guide the player in choosing an arm j(t) at each turn. The primary metric for evaluating these algorithms is the total expected regret, defined for a given turn T as:
R T = T µ * − T t=1 µ j(t) ,
where µ * = max i=1,...,K µ i is the expected reward from the best arm. Alternatively, the total expected regret can also be expressed as:
R T = T µ * − µ j(t) K k=1 E[T k (T )], (1)
where T k (T ) is a random variable denoting the number of times arm k is played during the first T turns.
C. Edge Devices as a Surrogate for Autotuning
The use of edge devices for running HPC applications is increasingly gaining attention. The Waggle sensor platform is a key example of integrating HPC with edge computing, offering real-time data analysis and modular sensor network capabilities. An extension of this project, The Sage Continuum offers a distributed, software-defined sensor network that leverages machine learning and edge computing and provides a robust framework for real-time data analysis and sensor management. Bhupendra A. Raut et al. provides critical insights into optimizing algorithms for edge-computing sensor systems, particularly focusing on the stability and performance of the blockwise Phase Correlation method in estimating cloud motion vectors. Kim et al. introduces a two-layered scheduling model for edge computing and incorporates "science goals" to align user objectives with resource allocation, thereby offering a nuanced approach to HPC applications in edge systems. The Interconnected Science Ecosystem (INTERSECT) architecture open architecture is a federated instrument-to-edge-to-center framework that advocates autonomous data handling and processing in scientific research. This architecture aligns closely with the objectives of running HPC applications in edge systems and offers a system of systems and microservice architecture for enhanced scalability and adaptability.
By processing data close to the source, edge computing can significantly reduce latency and bandwidth requirements, crucial for time-sensitive HPC applications . Edge devices also enable real-time data processing, which is essential for applications requiring immediate analysis and decision-making. However, these unique advantages present their unique challenges as well. Unlike traditional supercomputing centers, edge devices suffer from limited computational power and memory, posing a challenge for resource-intensive HPC applications. The performance of edge devices can be inconsistent due to their varying specifications and the dynamic nature of edge environments.
Our proposed HF/LF approach is designed to overcome this challenge and accommodate the dynamic nature of HPC workload and edge environment on which we are performing this autotuning task. Notably, our algorithm, LASP, is applicationagnostic, meaning it can be employed with any application that associates distinct values with its parameters. In a multifidelity context, an application can be executed with varying levels of fidelity settings, such as adjusting the resolution in a numerical simulation or modifying the depth of a machine learning model. For example, the fidelity levels of Hypre is determined by the discretization using m 3 grid points, where m varies from m min = 10 to m max = 100. Due to the algebraic multigrid algorithm's computational complexity of O(m 3 ), the mapping from the fidelity parameter q to m is represented as a linear interpolation between [q min , m 3  min ] and [q max , m 3 max ]. It is to be noted that, there is a trade-off in accuracy due to the shift between low and high fidelity levels, as lower fidelity runs on edge devices are inherently less accurate than those at higher fidelities on traditional HPC systems. However, this trade-off is acceptable, as we are not concerned with the specific results from the low-fidelity runs. Our primary goal is to use these low-fidelity edge device runs to effectively tune the parameters of the model. Importantly, our analysis in Fig. shows that there is a significant overlap between the optimal parameters for both low and high fidelity settings, meaning that the parameters tuned at low fidelity are often effective at high fidelity as well.
We represent fidelity levels using q ∈ [q min , q max ], where q min and q max indicate the minimum and maximum fidelity values, respectively. The time required for function evaluation is assumed to increase linearly with fidelity q. To optimize efficiency and reduce tuning costs, we utilize lower fidelity settings on edge devices, leveraging their faster, lower-cost performance. These lower fidelity evaluations, g(y, q) where q < q max , serve as approximations of the high-fidelity objective function, g(y, q max ), which runs on traditional HPC systems. The overall goal is to determine the best tuning parameters y to optimize the high-fidelity function g(y, q max ) by using the lower fidelity, edge-based evaluations as proxies, thereby improving the efficiency of HPC tuning. By leveraging this property, LASP can dynamically navigate the parameter space to identify the optimal configuration, regardless of the specific application. To address the dynamic environment, LASP incorporates a reward feedback mechanism, enabling the algorithm to operate in real-time and adapt to changing environments. We simulate this dynamic behavior by tuning four HPC applications, and introducing error measurements into our readings, as described in Section V. Furthermore, in the same section, we demonstrate that our algorithm can yield satisfactory results under varying levels of power and CPU capping, underscoring its robustness and adaptability.
In this study, we run applications on varying fidelity settings, for example, Lulesh (mesh size = 50, 80), Kripke (Zone size = 32, 64), and Hypre (Grid size = 32, 64). In Fig. (b), we see a significant overlap with the most optimal configurations compared to running them in a low and highfidelity setting. As shown in Fig. , we observed that the top 20 configurations identified through low-fidelity simulations and then transferred to a high-fidelity setting achieved performance within 25% of the optimal configuration (oracle) on the target device.
D. Challenges in HPC Parameter Search
Numerous challenges are associated with attaining an efficient parameter search optimization. First, finding the optimal set of parameters necessitates exhaustive exploration within a vast multidimensional search space. For example, popular HPC applications, such as Kripke and AMG , have more than 1 million and 5 million tunable hardware and software parameters, respectively. Searching for the most optimal parameters from this vast set is infeasible without an efficient algorithm. Second, conventional parameter search approaches often yield suboptimal configurations, highlighting the importance of capturing the interplay between applicationand hardware-level tuning parameters to achieve maximum performance.
The significant impact of selecting the right configuration on the application's execution time is demonstrated in Fig. . This figure illustrates the variation in execution times that results from altering only two application-level parameters while keeping all other parameters constant. It is observed that the variance in execution time becomes much more pronounced when more parameters are modified. Fig. illustrates the varying execution times resulting from tuning each parameter individually, reinforcing this key point. Additionally, Fig. ) provides a distribution of execution times across all sets of configurations.
This clearly highlights the crucial role of proper configuration selection in achieving optimal runtime performance. Considering that most configurations deviate significantly from the absolute best-performing configuration, it is plausible to hypothesize that the challenge posed by a large search space can be mitigated by swiftly discarding the low-performing configurations, namely configurations with high runtimes. However, identifying these areas proves to be a formidable task, often fraught with the risk of overlooking the optimal configuration.