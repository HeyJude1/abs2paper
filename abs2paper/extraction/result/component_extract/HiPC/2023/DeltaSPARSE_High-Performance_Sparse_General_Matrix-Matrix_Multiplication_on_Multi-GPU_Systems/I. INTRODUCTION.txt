I. INTRODUCTION
Sparse General Matrix Multiplication (SpGEMM) plays a pivotal role as a fundamental sparse operator. For instance, in graph data processing , various operations such as loop detection , triangular counting , and multi-source breadth-first search can be formulated using SpGEMM. Additionally, when accelerating the resolution of sparse linear algebraic equations through the utilization of Algebraic Multigrid Preconditioners , sparse matrix multiplication emerges as a significant time-consuming operation. Therefore, the development of an efficient Sparse General Matrix Multiplication algorithm has become essential for enhancing the performance of these applications. Furthermore, with the rapid advancement of the Multi-GPU platform, which offers both high computational power and cost-effectiveness, the implementation of Sparse General Matrix algorithms on multi-GPU systems holds unique and crucial significance.
Currently, Sparse General Matrix Multiplication can be categorized into row-row-based and tile-based methods. In the former approach, rows of matrix C are computed in parallel by merging the corresponding non-zero elements of matrices A and B. To accommodate different sparsity characteristics of rows, efficient sparse accumulators such as ESC , Hash , , , , , Merge , , , , , Dense , , and heap , are designed. On the other hand, tile-based methods accumulate results and store non-zero elements using tiles as the basic unit. However, to maintain satisfactory performance across a diverse range of sparse matrix patterns, these methods often incur considerable costs during the pattern analysis or the definition of compressed memory layouts.
In contrast to Multi-GPU Dense Matrix Multiplication, SpGEMM on a Multi-GPU platform remains an unresolved challenge for several reasons. (1) Irregular sparse patterns. The difference in distribution and amount of non-zero elements in each row of the input and output matrices poses a significant design challenge for efficient GPU data compression storage formats and accumulators. (2) Multi-GPU platform load imbalance. Load balancing of multiple GPUs is pivotal to the efficiency of the algorithm and the effective use of hardware. However, the complex and unpredictable computation, memory access, and communication costs of SpGEMM make it harder to establish scheduling strategies.
In view of the inherent problems in SpGEMM and the challenges encountered in designing a multi-GPU framework, we propose the first Multi-GPU Sparse General Matrix Multiplication method called DeltaSPARSE. The main contributions of this paper are as follows:
• A hybrid accumulator is designed, which transitions between hash and dense accumulator strategies through lightweight analysis, facilitating adjustment to irregular sparsity in sparse matrices. • A hierarchical task scheduling strategy is developed, which accomplishes adaptive load balancing on multi-ple GPUs through low-cost partitioning steps, thereby enabling optimal parallelism. • DeltaSPARSE demonstrates near-linear scalability across diverse GPU configurations and shows significant speed enhancements relative to contemporary single-GPU SpGEMM techniques when addressing matrices with a range of sparseness characteristics. The remainder of this paper is structured as follows: Section II enunciates relevant SpGEMM methods and provides an insight into the current state of research. Section III focuses on detailing the design and implementation of DeltaSPARSE for multi-GPU computing. Section IV compares DeltaSPARSE with existing state-of-the-art single-GPU SpGEMM methodologies, scrutinizes the scalability of DeltaSPARSE, and decomposes its runtime. Finally, Section V draws the research to a close through a summary and outlines prospective avenues of exploration.