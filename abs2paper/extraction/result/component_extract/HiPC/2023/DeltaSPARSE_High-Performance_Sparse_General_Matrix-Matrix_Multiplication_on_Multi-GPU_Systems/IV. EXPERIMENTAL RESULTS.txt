IV. EXPERIMENTAL RESULTS
The test system uses six NVIDIA Tesla V100-SXM2-16GB with CUDA 11.7, GPU driver version of 530.30.02 and an 80core Intel(R) Xeon(R) Gold 6148 CPU with 572G RAM on Ubuntu 20.04. Direct peer-to-peer inter-GPU communication is enabled between the six GPUs. We are employing GCC version 9.4.0 and leveraging OpenMP to allocate a dedicated thread for each GPU. It is worth noting that our existing implementation is tailored specifically for single-node multi-GPU configurations. However, it is pertinent to highlight that this design can readily be extended to distributed environments through the integration of technologies such as MPI.
We compare DeltaSPARSE under a dual-GPU setup with state-of-the-art single GPU general sparse matrix algorithms, including cuSPARSE v11.7 , bhSPARSE , spECK , and NSPARSE . Additionally, we perform experiments to examine the scalability of the algorithm, investigate the balance of multiple GPU loads to illustrate the effectiveness of task scheduling strategies, and ultimately breakdown the running time of the DeltaSPARSE algorithm. For the data set, we selected 16 representative sparse square matrices from the SuiteSpare Matrix Collection for in-depth comparison and  analysis. These matrices have diverse sparse features and are classic datasets in many sparse matrix studies. Table shows detailed information. The compression ratio is defined as the ratio between the intermediate product of C = A 2 and the number of non-zero elements in matrix C.
In order to demonstrate the effectiveness of DeltaSPARSE, while considering the lack of comparable multi-GPU sparse general matrix multiplication methods, we performed the computation of C = A 2 using DeltaSPARSE with a 2-GPU configuration, as well as the state-of-the-art single-GPU sparse general matrix multiplication methods NSPARSE , spECK , bhSPARSE , and cuSPARSE v11.7 . We then plotted their performance bar chart. As shown in Figure , our method DeltaSPARSE outperforms the other four methods in terms of performance on most representative matrices, especially for matrices with high computational workload and memory requirements, such as 'm t1', 'nd3k', 'nd6k', 'pdb1HYS', 'smt' and 'pwtk'. While cuSPARSE v11.7 failed to execute due to memory constraints, DeltaSPARSE achieved acceleration factors of 1.97x, 1.77x, 1.68x, 2.15x, 2.06x and 2.20x compared to the second-best method, spECK, for these matrices. It is noteworthy that certain acceleration factors surpassed 2.0x, owing to the adaptability of the hybrid accumulators to rows with varying sparsity levels and the efficiency and cost-effectiveness of the task scheduling. With regards to the maximum acceleration factor, DeltaSPARSE exhibited speed improvements of up to 7.30x, 2.20x, 4.49x, and 19.46x in relation to NSPARSE, spECK, bhSPARSE, and cuSPARSE v11.7, respectively, while achieve on average 3.57x, 1.86x, 2.39x, and 9.83x speedups. We calculate the geometric mean to determine the average speedup. The subsequent average speedups mentioned in the following sections are calculated using the same method. It should be noted that these performance results are based on the 2-GPU configuration of DeltaSPARSE. As the number of GPUs increases, DeltaSPARSE will be able to effectively leverage the hardware to achieve performance gains. We will analyze the scalability of the method in the next section.
Figure illustrates the algorithm performance speedup of executing the C = A 2 operation on 16 representative matrices, comparing different GPU configurations to the single GPU setting. The examined matrices typically display near-linear scalability, represented by the average speedup factors of 1.64x, 2.02, 2.42x, 2.71x, and 3.02x for configurations ranging from 2 to 6 GPUs, respectively. Furthermore, peak speedup factors of 1.82x, 2.34x, 3.05x, 3.39x, and 3.91x have been realised. For matrices characterized by a higher computational complexity, embodied by 'm t1', 'nd3k', 'nd6k' and 'pwtk', the computation of results constitutes the majority of the processing time. The proposed methods successfully leverage the performance-enhancing prospects offered by stacked multi-GPU hardware. It is notable that the irregular characteristics inherent to the matrices serve as critical determining factors in the scalability of the algorithm. They can result in an imbalance of GPU load distribution, which further diminishes the degree of algorithmic parallelism.  To assess the efficacy of the algorithm's load balancing, we consider a scenario involving a 4-GPU configuration, meticulously measuring the execution times of both symbolic and numeric phase kernels on each GPU. Since we concurrently launch kernels on all GPUs, minor differences in execution times imply minimal GPU idleness. As illustrated in Figure , the upper subfigure represents the execution times of symbolic phase kernels on each GPU, while the lower subfigure displays the execution times of numeric phase kernels on each GPU. It is evident that, for the majority of matrices, a commendable load balancing performance is achieved. Furthermore, we employ the coefficient of variation (CV) to precisely quantify the disparities in execution times among GPUs. In terms of the symbolic stage, the maximum coefficient of variation for GPU execution time is 0.096, while the minimum is as low as 0.006. Regarding the numeric stage, the maximum CV for GPU execution time is 0.114, with a minimum of 0.001. Notably, even for matrices with highly irregular sparsity, such as 'smt', the CV for the symbolic stage is a mere 0.014, and for the numeric stage, it is merely 0.064.
Figure depicts the runtime breakdown of DeltaSPARSE in a configuration utilizing two GPUs. The bar chart sequentially presents the proportional runtime of various stages, including, in descending order: numeric phase kernel, numeric phase task scheduling, symbolic phase kernel, symbolic phase task scheduling, memory allocation, and memory copying. It is noted that the symbolic and numeric phase kernels, on average, account for 26.5% and 54.0% of the total runtime, respectively. Enhanced efficiency in memory allocation and copying is achieved through the application of asynchronous cudaStream launches. This enables the concurrent handling of memory allocation, computation, and communication workloads across multiple streams. Notably, the impact of scheduling overhead remains strikingly low. Despite the symbolic phase task scheduling requiring a marginally extended duration attributed to the calculation of the upper bound of nonzero elements per row, it constitutes merely 3.22% of the overall runtime. Similarly, the numeric phase task scheduling contributes a scant 0.46% to the total runtime. V. CONCLUSION This paper introduces DeltaSPARSE, a framework for multi-GPU Sparse Generalized Matrix Multiplication (SpGEMM). Our approach addresses the challenges faced by SpGEMM algorithms on multi-GPU platforms, including irregular sparse patterns in sparse matrices and load imbalance across multiple GPUs. To overcome these challenges, we propose an adaptive hybrid accumulator and a hierarchical scheduling strategy. The effectiveness of our method is demonstrated through near-linear scalability, reduced scheduling, storage allocation, and communication costs on representative matrices from the SuiteSparse Matrix Collection. In comparison to existing state-of-the-art single-GPU SpGEMM methods, DeltaSPARSE achieves significant acceleration. Our future work includes extending the application of SpGEMM to distributed clusters, enabling the method to be utilized in a wider range of scenarios.