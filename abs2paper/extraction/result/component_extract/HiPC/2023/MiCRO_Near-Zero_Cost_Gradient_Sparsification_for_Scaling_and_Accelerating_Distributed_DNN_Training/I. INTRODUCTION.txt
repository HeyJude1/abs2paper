I. INTRODUCTION
Over the past decade, overcoming the excessive communication traffic for gradient aggregation has been a major challenge to enhancing the distributed training performance of deep neural network (DNN) models. Gradient sparsification - is a widely-adopted solution for reducing the size of payloads in communication between workers. Gradient sparsification aims to select only large gradients from the entire gradient vector, and the number of sparsified gradients is quantified by the density . Therefore, gradient sparsification can alleviate the communication bottleneck when the communication bandwidth is insufficient to transmit all the gradients at every training iteration.
Gradient sparsification can be categorised into sorting-and threshold-based approaches. In sorting-based sparsifiers , , all gradients are sorted, and the k largest gradients are selected (top-k) for aggregation through communication. However, gradient vector sorting is an expensive operation because of its high computational complexity (e.g., O(n log k) ). Moreover, sorting operations cannot properly utilise the parallelism of streaming processors on GPUs . Figure Both types of sparsifiers cause gradient build-up. All experiments were conducted using d = 0.01 and n = {2, 4, 8, 16} with ResNet-18 on CIFAR-10.
shows the high computational cost for the gradient vector sorting of the Top-k sparsifier based on the breakdown of the training time of one iteration. The computational cost remains constant and consumes a significant portion of the training time, regardless of the scale-out degree. Therefore, sortingbased sparsifiers are inadequate for accelerating distributed DNN training. Threshold-based sparsifiers , select gradients using a conditional statement that indicates whether each gradient is larger than a threshold. Threshold-based sparsifiers are easier to parallelise and faster than sorting-based sparsifiers. Thus, threshold-based sparsifiers can significantly reduce the computational cost of gradient selection. However, threshold-based sparsifiers have difficulty effectively reducing communication traffic owing to inappropriate thresholds. Predicting a threshold that satisfies the density set by a user is challenging. Figure shows the excessively high actual density of the hard-threshold sparsifier where the user-set density was 0.01.
Additionally, most sparsifiers have difficulties scaling owing to gradient build-up , which causes that the number of aggregated gradients in the communication becomes larger than the number of gradients selected by each worker. This is because a lot of gradients selected by each worker do not overlap with those of the other workers, although all workers have the same search range for gradient selection. Consequently, the density increases at most n times the userset density, where n is the number of workers. As shown in Figure , the communication traffic increases as the number of workers increases because of the gradient build-up.
Therefore, the existing sparsifiers cannot effectively scale and accelerate distributed DNN training. Based on our observations, we address the following challenges:
• Gradient build-up. This hinders the scalability of the distributed training because the communication traffic increases as the cluster scales out. . With this partitioning approach, MiCRO can not only reduce the computational complexity of gradient selection but also prevent gradient build-up because each worker selects gradients from exclusively assigned partition.
To reduce the computational cost of the gradient selection, MiCRO adopts threshold-based sparsification instead of sorting-based sparsification , . Moreover, the gradient selection of MiCRO is faster than that of existing thresholdbased sparsifiers , because the gradient vector partitioning of MiCRO reduces the computational complexity of the gradient selection from O(n g ) to O( ng n ). In addition, the model accuracy of MiCRO can be maintained at the same level as that of other sparsifiers , . This is because the result of filtering elements (gradients) larger than the threshold in an array (gradient vector) is invariant, regardless of whether the array is partitioned or not.
Furthermore, MiCRO satisfies the communication traffic at the user-set level by estimating the threshold more accurately. MiCRO estimates the threshold by minimising the compression ratio error, which is defined as the difference between the actual and user-set densities. Therefore, MiCRO can maintain a low communication cost throughout the training period by estimating the accurate threshold and eliminating 2 MiCRO is an acronym for minimising compression ratio error on-the-fly.
gradient build-up. By addressing these challenges, MiCRO enables near-zero cost gradient sparsification for scalable and accelerated distributed DNN training.
This study makes the following contributions:
• Exclusive gradient selection. This eliminates the gradient build-up. In other words, communication efficiency can be improved because exclusive gradients are aggregated between workers. Moreover, computational cost can be reduced as the cluster scales out. This primarily contributes to the scalability of distributed DNN training. • Accurate threshold estimation. This prevents an excessively high density due to an inappropriate threshold. In other words, the communication traffic can be maintained as low as the user-set value. This mainly contributes to the acceleration of distributed DNN training. • Multidimensional evaluation. This study provides an extensive set of experiments and analyses for performance and efficiency comparisons between MiCRO and stateof-the-art sparsifiers. The remainder of this paper is organised as follows. Section II presents the preliminaries of the study. Section III clarifies the limitations of the state-of-the-art gradient sparsification methods. Section IV proposes MiCRO, which is designed to address the challenges stated in this study. Section V verifies our contributions through thorough empirical comparisons between MiCRO and state-of-the-art gradient sparsifiers. Finally, Section VI concludes the paper. II. PRELIMINARIES Gradient sparsification is a type of lossy algorithm because most of the computed gradients are discarded at every iteration. In terms of computational efficiency, discarding the majority of gradients is unproductive because backward propagation comprises a massive number of computational operations in DNNs. Moreover, discarded gradients cause a difference between sparsified and non-sparsified DNN models in distributed training. Thus, the fidelity loss of the sparsified model must be reduced to apply gradient sparsification to distributed training.
Error feedback is an auxiliary method for sparsifiers to reduce the fidelity loss caused by discarded gradients. Instead of discarding unselected gradients, the error feedback locally accumulates them into the n g -dimensional vector e i,t , where i and t are the iteration and worker numbers, respectively. In other words, each element of e i,t represents the accumulated gradient of one parameter. When each gradient is selected, the accumulated value is initialised to zero because the gradient contributes to the model update. Hereafter, we refer to the L2norm of e i,t as the local error denoted by e i,t . Accordingly, the error at iteration t is defined as follows:  In other words, minimising e t results in a reduction of difference between sparsified and non-sparsified models. However, it is challenging to maintain a high model training performance while minimising e t . To minimise e t , the sparsifier should initialise a lot of accumulated gradients to zero in e i,t . Because this implies a high density of sparsified gradients, the training performance slows down because of the huge communication traffic.
e t = 1 n n−1 i=0 e i,t . (1)
Figure shows the error variations in the two sparsifiers over the training iterations. The hard-threshold sparsifier selects all the gradients larger than the threshold, thus maintaining a consistent error level. However, the error of the hardthreshold sparsifier is much lower than that of Top-k in most iterations; thus, it is clear that the hard-threshold sparsifier can show a significantly higher density than Top-k. In other words, communication for the gradient aggregation of hard-threshold sparsifier is expensive. We verify how much runtime of hardthreshold sparsifier is occupied by the communication through experiments detailed in Section V.