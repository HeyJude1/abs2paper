V. EVALUATION A. Methodology
System configuration. All the experiments were conducted on a cluster equipped with 16 GPUs. A cluster comprises two nodes, each with eight NVIDIA Tesla V100 GPUs with NVLink, two 16-core Intel Xeon Gold 6226R @ 2.90 GHz CPUs, and 384 GB DDR4 memory. For all the experiments, mpirun of OpenMPI 4.0.5 was used for multiprocess execution to automatically assign an exclusive rank to each worker. Each worker was run on one GPU with CUDA 10.1 .
Models and datasets. We evaluated the performance of Mi-CRO and other sparsifiers (Top-k, CLT-k, and hard-threshold sparsifiers) on computer vision applications. For the DNN models, we used ResNet-18 , GoogLeNet , and SENet-18 . For datasets, CIFAR-10 and CIFAR-100 were used. To conduct an extensive set of experiments and analyses, our evaluation comprised multidimensional training settings, as listed in Table . By changing one factor among the model, dataset, and density, the impact of each factor on the performance of each sparsifier can be identified. Implementation. We implemented MiCRO and other approaches on top of the deep learning framework PyTorch 1.5 . The distributed communication package PyTorch was used to implement the communication routine for the distributed training. Moreover, NCCL 2.4 [25] was adopted as a backend to support multi-GPU and multi-node communication primitives such as broadcast, all-gather, and all-reduce, which are optimised for NVIDIA GPUs and networking. To fairly compare the appropriateness of the thresholds between the MiCRO and the hard-threshold sparsifier, the initial threshold δ 0 of the MiCRO was set to that of the hard-threshold sparsifier. The source code includes everything required to reproduce the results of this study, and is available publicly at https://github.com/kljp/micro.
Metrics. The metrics used for each type of performance evaluation are as follows:
• Convergence performance: The test accuracy by runtime was measured to evaluate how fast each sparsifier attained the final accuracy in 200 epochs. • Sparsification performance: The actual density was mea-sured to evaluate whether each sparsifier could satisfy the user-set density. sorting and the model fidelity is reduced by the delegated gradient selection policy. Top-k requires a long training time, similar to that of CLT-k. However, the convergence rate of Top-k was faster than that of CLT-k. This is because Top-k entails a gradient build-up, which makes Top-k select a lot more gradients than CLT-k (i.e., at most n times the userset density). Although the hard-threshold sparsifier has no computational cost for gradient sorting, its convergence rate is slower than that of MiCRO because it suffers from a large increase in communication traffic owing to the inappropriate threshold and gradient build-up. Sparsification performance. Fig shows the sparsification performance of each sparsifier in multidimensional training settings. In every experiment, MiCRO exhibited the actual density close to the user-set density owing to the threshold based on compression ratio error minimisation. Moreover, gradient build-up does not occur because of the exclusive gradient selection with gradient vector partitioning.
However, the actual densities of the Top-k and hardthreshold sparsifiers were not close to the user-set densities because of gradient build-up. In particular, hard-threshold sparsifier exhibited the excessively high actual density owing to inappropriate threshold in every experiment. Despite the increasing gradient accumulation during training iterations, the hard-threshold sparsifier used only a fixed threshold. In other words, the actual density increased as the iterations proceeded. In every experiment, the density of the hardthreshold sparsifier dropped suddenly after iteration 14,600. This is because we set the learning rate decay at epoch 150.
That is, the model almost converges after that epoch.
End-to-end training performance. Fig shows the breakdown of the training time for one iteration. For each sparsifier, the wall-clock time of one iteration was measured by the slowest worker, and the average time was calculated across all iterations. The experiment was repeated using four different seeds for each sparsifier. Finally, the average wall-clock time shown in Figure was obtained from the average value of the four executions.
In Figure , the training time comprises the forward propagation, backward propagation, gradient selection, communica- In contrast, MiCRO showed considerably faster training performance than the other sparsifiers in every experiment. This is because MiCRO reduces the computational cost to near-zero by using exclusive gradient selection with gradient vector partitioning and reduces the communication cost by eliminating gradient build-up and estimating the accurate threshold without overhead. As discussed in Section IV-C, the threshold estimation yields zero overhead because it only includes the condition statement for inspecting |k − k i,t | and the assignment of the adjusted value to a variable (i.e., the threshold). Moreover, the gradient vector partitioning of MiCRO does not yield any overhead because this process only determines the starting index of each partition. Therefore, when MiCRO is applied to distributed DNN training, the cost of gradient sparsification is near-zero, and this advantage contributes significantly to the scalability and acceleration of distributed training.
Scalability. MiCRO shows that every case consistently attains a similar convergence point, regardless of the number of workers. Moreover, the convergence rate was significantly accelerated by scale-out. These scalability and acceleration mainly result from communication cost reduction by eliminating gradient build-up and estimating the accurate threshold.
Threshold estimation performance. In this experiment, we evaluate the threshold estimation performance of the MiCRO. To maintain the accurate density, the threshold should be changed according to the error variation. That is, the threshold should increase when the error increases to prevent an increase in density. To identify whether MiCRO can satisfy this principle, we plotted the threshold and error trends. As the magnitude of the error is much larger than the threshold, we scaled the magnitude of the error to fit within a range of threshold changes. Thus, we multiplied the error of each iteration by the scaling factor, defined as the ratio of T −1 j=0 δ j to T −1 j=0 e j , where T is the number of iterations.