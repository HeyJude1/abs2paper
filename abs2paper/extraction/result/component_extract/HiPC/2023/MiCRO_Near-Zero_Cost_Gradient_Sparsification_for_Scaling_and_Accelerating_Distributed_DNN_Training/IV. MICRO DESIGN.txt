IV. MICRO DESIGN
We designed MiCRO as a threshold-based gradient sparsifier, in which each worker selects gradients from the exclusive partition of the entire gradient vector. Figure presents an overview of MiCRO. MiCRO comprises the following sequences: 1) coarse-grained gradient vector partitioning; 2) exclusive gradient selection by threshold; and 3) minimisation of compression ratio error by threshold scaling. These processes begin after backward propagation at each iteration. The following subsections present a detailed discussion of each process of MiCRO.
MiCRO equally divides the entire gradient vector into n partitions and assigns each partition to the corresponding worker. By gradient vector partitioning, each worker can obtain a search range that is exclusive to that particular worker. The partition assignment is not fixed for every iteration. At the beginning of each iteration, the partitioned vectors are assigned to the workers in a cyclic order. That is, each worker has the opportunity to search for gradients in the entire gradient vector in every n iterations, and it also has the chance to select its local gradients at every iteration.
As shown in Figure , the entire gradient vector is partitioned in a coarse-grained manner. Each partitioned vector has a subsequent range of gradient indices. We designed this coarse-grained gradient vector partitioning method by considering the GPU memory access pattern. Because each thread group of the GPU (i.e., warp in CUDA ) accesses the global memory simultaneously, the elements that the threads want to access should be in a cache line to utilise parallelism. Therefore, coarse-grained partitioning enables efficient GPU global memory access, unlike fine-grained partitioning, such as interleaved partitioning, which severely degrades performance owing to memory divergence .
To prevent gradient build-up, a sparsifier should provide workers with a non-overlapping search space. Because the gradient build-up results from overlapping search spaces between workers, MiCRO restricts the search space of each worker to one partitioned vector, which is divided by coarse-grained gradient vector partitioning. Accordingly, each worker in the MiCRO can select gradients in its exclusively partitioned gradient vector, and gradient build-up never occurs. As shown in Figure , exclusive gradient selection enables nonoverlapping selected gradients.
Notably, MiCRO prevents loss of model fidelity, unlike CLT-k . In CLT-k, each worker has the chance to select its local gradients once every n iterations. However, workers have no selection authority for the remaining n − 1 iterations. In other words, the locally computed and accumulated gradients of each worker will become stale. By contrast, MiCRO does not suffer from model fidelity loss because all workers can participate in the model update at every iteration. Moreover, the threshold-based gradient selection of MiCRO prevents model fidelity loss because all gradients are inspected to determine whether each of them is larger than the threshold. In other words, the significance of selecting the largest n−1 i=0 k i,t gradients is maintained.
In addition, exclusive gradient selection reduces the computational complexity of threshold-based gradient selection from O(n g ) to O( ng n ). Therefore, scalability is enhanced because of the reduction in computational cost as the number of workers increases.
To prevent a high density caused by an inappropriate threshold, a sparsifier should estimate the accurate threshold to achieve the user-set density. MiCRO focuses on minimising the compression ratio error at each iteration. Let k and k i,t be the number of gradients that should be selected and the number of gradients selected by worker i at iteration t, respectively. If |k − k i,t | is close to zero, it implies that the threshold is appropriate to satisfy the user-set value. Thus, minimising |k − k i,t | is crucial for adjusting the communication traffic of gradient aggregation.
As shown in Figure , k i,0 may be far from the user set k because it is difficult to predict the initial threshold δ 0 accurately at iteration 0. To minimise |k −k i,t |, MiCRO adjusts the threshold at each iteration by inspecting whether the number of selected gradients are larger than k. This threshold scaling has two advantages over statistical threshold estimation . First, it is robust to varying training settings such as models, datasets, and learning parameters because only the compression ratio error is considered when adjusting the threshold. Moreover, additional overhead does not occur because inspecting |k−k i,t | and adjusting the threshold are performed by inspecting the condition statement and merely assigning an adjusted value to the threshold, respectively. Therefore, the threshold scaling of MiCRO is generally applicable to various training settings and is faster than statistical model-based threshold estimation.
Algorithm 1 presents the pseudocode of the distributed SGD with gradient sparsification of MiCRO. In line 6, the gradients computed by backward propagation accumulate in local error. In line 7, the dedicated partition number is assigned to each worker in cyclic order. In line 8, the entire gradient vector is divided into n partitions, and each partition is assigned to a dedicated worker. In line 9, each worker selects gradients based on the threshold in its exclusive partition and returns the indices of the selected gradients. According to the partition and selection policies, gradient build-up never occurs because the selected indices of each worker do not overlap with those of the other workers. From lines 10 to 12, the globally selected indices are collected and the averages of the selected gradients are computed. In line 13, the threshold of the next iteration is derived based on the compression ratio error minimisation. As the iterations proceed, the threshold approaches a value that satisfies the user-set density. In line 14, the model is updated using averaged gradients. In lines 15 and 16, the accumulated value of each selected gradient is initialised to zero, and those of the unselected gradients become the local error of the next iteration.