VI. CONCLUSION
In this paper, we propose MiCRO, which partitions the gradient vector and assigns each partition to a corresponding worker. The design of MiCRO comprises coarse-grained gradient vector partitioning, exclusive gradient selection, and compression ratio error minimisation through threshold scaling. Using these components, MiCRO can achieve high performance in terms of convergence, sparsification, and threshold estimation. Consequently, it enables near-zero cost gradient sparsification by providing remarkable training efficiency owing to its reduced computational and communication costs. In our thorough empirical experiments, MiCRO outperformed state-of-the-art sparsifiers in terms of the scalability and acceleration of distributed DNN training.