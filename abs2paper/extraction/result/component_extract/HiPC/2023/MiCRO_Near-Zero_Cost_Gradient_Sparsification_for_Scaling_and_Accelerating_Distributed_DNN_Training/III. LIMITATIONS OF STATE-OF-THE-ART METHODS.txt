III. LIMITATIONS OF STATE-OF-THE-ART METHODS
In this section, we discuss the limitations of state-of-the-art gradient sparsifiers. Table I lists the strengths and weaknesses of the state-of-the-art sparsifiers.
A. Sorting-based sparsifiers
In gradient sparsification, most computationally inefficient results are obtained from the gradient vector sorting phase of sorting-based sparsifiers, such as Top-k and cyclic local top-k (CLT-k) . The extremely high sorting cost is the main hindrance to the scalability and acceleration of distributed training.
In terms of communication efficiency, CLT-k maintains the user-set density by eliminating the gradient build-up of the Top-k. In CLT-k, a worker becomes the leader worker at each iteration and is delegated to determine all the gradient indices to be aggregated. Consequently, the number of aggregated gradients is the same as that of the selected gradients. However, the delegation policy for gradient selection has two side effects. First, most of the computing resources used by all other workers cannot be utilised during the gradient selection of the leader worker. Second, the model fidelity may be reduced because only one worker determines the gradient indices that contribute to the model update at each iteration. Therefore, Top-k and CLT-k exhibit limitations in terms of scalability and training performance.
B. Threshold-based sparsifiers
The computational cost of gradient selection can be reduced considerably using threshold-based sparsifiers such as hardthreshold sparsifier and SIDCo . However, they have limitations in terms of communication inefficiency. In addition to the gradient build-up problem, the actual density is excessively high because of inaccurate threshold estimation. This is mainly because their threshold estimation methods are insufficiently generalised to fit various types of models and datasets well. In particular, hard-threshold sparsifier may not estimate the appropriate threshold with untested training settings because its threshold should be defined before training begins and remain constant for every iteration. Therefore, hard-threshold sparsifier requires hyperparameter tuning for each model and dataset.
In contrast, SIDCo derives a threshold using a statistical model at each iteration. Because the threshold changes over the training iterations, the density can be adjusted more flexibly than with a hard-threshold sparsifier. However, SIDCo is based on several predefined statistical models, and threshold estimation by statistical models requires a high computational overhead. In summary, both hard-threshold sparsifier and The limitations of state-of-the-art sparsifiers show that it is challenging to satisfy all the criteria listed in Table . We not only overcome the limitations of state-of-the-art methods using a novel sparsifier but also achieve it with near-zero cost.