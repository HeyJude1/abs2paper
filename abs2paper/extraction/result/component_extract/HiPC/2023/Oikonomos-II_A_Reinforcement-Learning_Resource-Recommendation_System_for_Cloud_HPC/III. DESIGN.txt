III. DESIGN
As the extensive related-work section demonstrates, there is a need for a middleware layer for resource recommendation in a heterogeneous HPC system that aids the user in selecting the hardware platform that is best-suited for the job they need to run. We avoid performance-model construction, as the complex interplay between the application parameters and the execution platform call for an application-agnostic approach. We also avoid running the same job more than once: we assume a stream of incoming jobs with different parameter values each time. Oikonomos-II gets to make only one decision regarding the instance type per job, and gets to observe the execution time and costs of only that particular job execution. In contrast to recommenders like Oikonomos, we assume the absence of any preexisting historical execution data. Therefore, the decisions that Oikonomos-II makes not only influence the costs and execution time of one particular job but also the available data to base future decisions on.
Because of the absence of preexisting historical data, Oikonomos-II is forced to take risks by recommending instance types it has not encountered before. At the same time, though, Oikonomos-II has to optimize performance for its users. This dilemma is known as the exploration-exploitation dilemma, which is a general problem to be found in datadriven, decision-making processes where a feedback loop exists between data gathering and decision making . This becomes most clear in a class of problems known a multiarmed bandit problems.
Lattimore and Szepesvári describe the bandit problem as a sequential game between a learner and an environment. Played over n rounds, for each round t ∈ [n], the learner picks an action a t from a set of actions A. After the action is chosen, the environment reveals a reward r t ∈ R. The learner does not get to see the rewards associated with the other actions.
The learner cannot see into the future, so in the classical multi-armed bandit problem, it has to rely on the history H t−1 = (a 1 , r 1 , . . . , a t−1 , r t−1 ) in order to make decisions. The learner is expected to adopt a policy π: a mapping from histories to actions. Most commonly, the goal is for the learner to find a policy that maximizes the cumulative reward over all rounds n t=1 r t . The regret of a policy π is defined as the difference between the cumulative expected reward using policy π and the cumulative reward of an optimal policy π * . Cumulative regret will often grow in a logarithmic fashion for good policies: cumulative regret will increase relatively fast in the beginning, when there is little historical data and a strong need for exploration, and will slow down with time, as the amount of historical data grows, allowing for more exploitation. Bandit algorithms are part of the wider class of reinforcement-learning algorithms.
The bandit problem has been studied since the 1930s , but interest has skyrocketed over the last two decades because of its applicability in online environments. Dynamic pricing of online airplane bookings is a good example of a bandit problem: when a visitor searches for a flight, the website picks a price to offer to the visitor. The reward is revealed when the customer either books the flight or leaves without booking. The goal of the algorithm is to maximize total cumulative profit over all visitors .
Contextual knowledge can be essential for adopting a policy to make decisions. For instance, in the airplane booking example, it might be useful to know the IP address of the visitor. After all, the visitor's location might be correlated to the price they are willing to pay. Context can also consist of similarity information regarding the actions in A. A visitor might be willing to book a flight on a different date, or to a different airport, and showing them such options could lead to a higher chance of booking. Multi-armed bandit problems where context plays a role are known as contextual bandits.
Two of the most widely used algorithms for solving the exploration-exploitation dilemma in multi-armed bandit problems are Upper Confidence Bound (UCB) and Thompson Sampling (TS). UCB was first proposed by Auer et al. , and is based on the principle of optimism in the face of uncertainty. This means that the algorithm estimates the expected reward, as well as a confidence bound for each action, and chooses the action that has the highest upper confidence bound. Whereas UCB is aimed at estimating the reward (see Figure ), TS builds a probability model based on previous rewards, and then samples from this model to choose an action . Both TS and UCB are widely used and have strong theoretical guarantees on the regret bound.
The original UCB and TS algorithms do not take contextual information into account. However, they have been used as bases for algorithms that do work with contextual information. One of the most popular contextual bandit algorithms is LinUCB, proposed by Li et al. . The algorithm assumes a linear relationship between the context parameters and the rewards. The relationship is represented by a vector θ, which is to be learned. LinUCB was presented in two versions: a disjoint version (where only one vector of context parameters in used) and a hybrid version (where two context vectors are used: one for parameters describing the context in round t, and one for parameters that describe the actions in A).
Li et al. applied the algorithm to personalized news-article recommendation and showed that it performs better than the original UCB algorithm.
The requirement of a linear relationship between context parameters and rewards in LinUCB is restrictive. For instance, in the case of cloud HPC, the relationship between application parameters, hardware, and execution time is potentially complex. This requirement, however, can be overcome using an artificial neural network (ANN). We will mention two relevant publications. Zhou et al. presented NeuralUCB, which feeds the context vector to a neural network ; NeuralUCB is a generalized version of LinUCB, achieving the regret bound of LinUCB without the aforementioned requirement. However, as the whole network is used for exploration, the algorithm is very complex and computationally expensive for large neural networks. Addressing this issue, Xu et al. presented an adaptation where representation is decoupled from exploration . Their algorithm, Neural-LinUCB, is based on the principle of deep representation and shallow exploration: it uses the entire ANN to learn the relationship between the context vectors and the rewards, but only uses the last layer for exploration. In this way, deeper and wider ANNs can be used, allowing for the representation more complex context-reward relationships. Additionally, the way in which the relationship vector θ is calculated after each round is highly parallelizable, allowing for better performance. The authors showed that Neural-LinUCB achieves similar performance to NeuralUCB, while being much less computationally expensive.
Betting et al. showed with Oikonomos that a deep MLP can be used to recommend an optimal cloud-instance type for HPC applications, based on the input-parameter values. However, as Oikonomos was purely prediction-based, it relied on a large amount of preexisting training data. The absence of this data creates a contextual multi-armed bandit problem.
A consists of all possible instance type recommendations, whereas the rewards are a function of execution time and/or usage costs. Each round t involves a decision to recommend an instance type to a specific job. We define 'job' as the (requested) execution of the application with specific parameter values. The context, therefore, consists of both roundspecific context (the input parameters of the job), as well as action-specific context (the hardware parameters of the instance types). The non-linear relationship between context and rewards rules out traditional LinUCB. Because of the complexity and computational costs of NeuralUCB for deeper neural networks, as well as the opportunities for parallelism that Neural-LinUCB offers, Neural-LinUCB was chosen to solve the multi-armed bandit problem that Oikonomos-II faces.
Figure shows the overall architecture of Oikonomos-II, and a detailed description of its workings can be found in Algorithms 1 and 2. We assume a sequential stream of jobs, with each round t corresponding to the recommendation of an instance type and subsequent execution of job j t . Job j t is defined by a vector p t of parameter values. Furthermore, we assume a set of S available instance types s; for every s, there is a vector h s containing hardware-parameter values of the instance type, such as the number of vCPU cores, memory size, GPU type, etc. A matrix A 0 , and vectors b 0 and θ 0 are initialized before any jobs are processed.
We make the assumption that each job j t can start only when job j t−1 has finished. Each action a ∈ A is the act of assigning a job to an instance type. Action a t,s signifies the act of assigning job j t to instance type s for execution. The context vectors x for Oikonomos-II consist of both the application parameters and the hardware parameters of the instance type. Here, we simply concatenate the vectors p t and h s to create x t,s . Theoretically, it is possible to implement a hybrid version of Neural-LinUCB to evaluate p t and h s separately, but this is to a large extent non-parallelizable and computationally much more expensive, to the extent that we consider it infeasible. Furthermore, combining p t and h s in a single context vector allows the MLP to learn possibly complex interplays between hardware and application parameters.
We use the combined context vector x t,s to calculate the Algorithm 1 Oikonomos-II adaptation of Neural-LinUCB 1: Input: regularization parameter λ > 0, number of jobs J, vector of retraining rounds k, exploration parameter α > 0, MLP φ(x, w), context scaler function σ x (x), reward scaler function σ r (r), custom reward function r(T ) 2: Initialization: A 0 = λI, b 0 = 0, and vector θ 0 of length d filled with values
A t = A t−1 + φ(x (σ) t,at ; w L )φ(x (σ) t,at ; w L ) , b t = b t−1 + r t φ(x (σ)
L min = L v 14:
end if 15: end for 16: w L = w min 17: Use φ(x, w L ) to recalculate A t and b t 18: Output: w L ; σ x (x); σ r (r); A t ; b t upper confidence bound for the reward of each job-instance type combination, as is done in the original Neural-LinUCB algorithm. The vector q t = φ(x t,s ; w L ) is obtained by passing x t,s through the MLP. q t is multiplied with vector θ t−1 to find the expected reward, and the inverse of A t−1 is used to calculate the confidence bound. Following the notation used by Xu et al., we use [k] to denote a set {1, . . . , k}, k ∈ N + . For a semi-definite matrix A ∈ R d×d and vector x ∈ R d , the Mahalanobis norm is denoted as x A = √ x Ax. The process is visualized in Figure . The action with the highest upper confidence bound is recommended to the user.
As shown in Figure , after the algorithm recommends an instance type, the job is executed there. The job output is then returned to the user. The execution time T t , as well as vectors p t and h s are stored in a database. The ANN is retrained periodically; it would be computationally expensive to retrain after every round. However, A t , b t , and θ t are calculated after every round, and are used for UCB calculation and instancetype recommendation in the next round. The Oikonomos-II algorithm is described in detail in Algorithms 1 and 2. For a more detailed explanation of Neural-LinUCB and proof of the regret bound, we refer the reader to the original paper. We made several adaptations to the Neural-LinUCB algorithm to make it suitable for our application. The original Neural-LinUCB algorithm retrains the ANN every k steps. We noticed that the network requires frequent retraining in the beginning, and requires less frequent retraining later, when there is more data available. Therefore, rather than defining k as an integer, we define k as a vector of positive integers. If t ∈ k, the ANN is retrained after round t. The size and content of k can be chosen by the user.
It was also noted that, when training the ANN, there exists a feedback loop between the weights of the ANN and the feature vector θ: after all, θ depends on A and b, and A, b are updated after every round using the MLP output vector q. This led to instability and reduced performance during backpropagation, as θ is used in the loss function (see Algorithm 2). We resolved the issue by applying soft updating, as described by Lillicrap et al. , where the target network is used to recalculate θ for each data point at the start of each epoch, and backpropagation is applied to the training network. A soft-update step is performed at the end of each epoch. This allowed us to use deeper neural networks, which makes it possible to represent more complex relationships between inputs and rewards. We also improved the MLP training process by applying best-practise techniques, such as data scaling, training with mini-batches, and early stopping with separate training and validation sets in Oikonomos-II.