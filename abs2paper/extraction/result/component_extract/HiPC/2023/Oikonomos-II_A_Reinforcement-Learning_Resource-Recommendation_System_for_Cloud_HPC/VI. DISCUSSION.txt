VI. DISCUSSION
So far, we made the assumption that cloud instances are up and running, and are readily available; start-up times were not taken into account. In a real-life scenario, it might be useful to keep instances running in some situations (for example, when there is a continuous stream of jobs), whereas in other situations, it would be better to shut them down between runs. Developing an algorithm that takes this into account would be useful, but requires additional information about usage patterns, which was outside the scope of Oikonomos-II. Still, this could be an interesting extension of the current work, when combined with a suitable scheduling algorithm.
Oikonomos-II uses a deep neural network which needs to be retrained regularly. Retraining can be a time-consuming task that is best done on specific hardware types, such as a GPU; this might incur extra costs. However, in our evaluation, we showed that Oikonomos-II can deliver outstanding results with a long retraining interval of 500 episodes. Furthermore, it is possible to reduce the training time by retraining on only a sample of the data. Xu et al. argued that this is possible for Neural-LinUCB without significant reduction in performance, and we expect the same for Oikonomos-II.
The current number of instance types offered by AWS is over 600. Oikonomos-II was tested on data from eight instance types, which is only a fraction of the number of instances offered. However, as there is currently no standard benchmark set for resource recommendation in cloud HPC, it was necessary to collect our own oracle datasets to evaluate performance. This required that we limit ourselves to a small selection of instance types. Even so, the small set of eight instance types contains sufficient diversity. The fact that oftentimes, there is not one overall 'best' instance type, attests to this.
Oikonomos-II was tested on two types of reward functions: cost-and time-optimized. A fixed reward function for all episodes was assumed. However, in a real-life situation, some users might want the instance type that delivers the fastest results, whereas other users want to have results at the lowest cost. Yet others might prefer a balance between these two or have additional requirements. The current implementation of Oikonomos-II does not support this diversity of user requirements but its design can be easily extended to accommodate a variety of custom reward functions in the future.
Finally, the assumption was made that jobs arrive and are dispatched in a sequential manner: a new job arrives when the previous job has completed. In reality, however, jobs may arrive simultaneously, and a new job may arrive before the previous ones have finished. This might affect recommender performance. However, the problem of delayed feedback in bandits is well-studied , and the structure of Oikonomos-II is suitable for expansion to incorporate solutions to challenges that may arise in practice. In addition, it would also be valuable to assess how Oikonomos-II would perform on other contextual bandit algorithms, such as Thompson Sampling. However, this falls beyond the scope of the current work. VII. CONCLUSION Oikonomos-II casts the problem of cloud instance-type selection for different HPC jobs as a contextual multi-armed bandit problem. It applies a variant of the Neural-LinUCB algorithm, balancing exploration and exploitation. The system starts off without knowledge of the application behavior, and is forced to explore when recommending instances for incoming jobs. However, as it gathers knowledge, Oikonomos-II starts exploiting and converges towards optimal choices. We evaluated Oikonomos-II on four diverse HPC applications, where it was shown to converge towards optimal choices, demonstrating its effectiveness and robustness. Oikonomos-II avoids the main issues of both prediction-based and searchbased recommenders. Combining the best elements of these two approaches into a reinforcement-learning recommender system, Oikonomos-II is both generalizable and accessible, making it a promising tool for researchers who want to harness the power of the cloud for their high-performance computing applications.