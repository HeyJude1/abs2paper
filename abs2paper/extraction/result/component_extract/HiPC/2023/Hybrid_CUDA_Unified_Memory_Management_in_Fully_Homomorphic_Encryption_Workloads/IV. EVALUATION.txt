IV. EVALUATION
We evaluated the performance of our three schemes on three different GPUs: One is a server-equipped A40 GPU with 48 GB of RAM, the second is a retail GeForce GTX 1660 Ti with 6GB of RAM, and lastly is the retail GeForce GTX 1050 with 2GB of RAM. We ran different workloads depending on the type of GPU used, and resorted to larger workloads for the more powerful GPU because it had more available VRAM. We run CUDA 12 to ensure that we are compatible with all the outlined features. All workloads are custom workloads built in C++ and interact with the HEaaN library. Our schemes make direct modifications to the HEaaN library, and depending on the scheme used the amount of invasiveness to library code varies. The dynamic scheme is the least invasive in that only modifications to the memory allocator are made, and the static is the most invasive in that actual library code is manually modified in the way that certain temporary objects should be allocated. The profiling allocator uses external resources like files and storage to store the raw CSV data, and Python scripts to process the data into a format that can be used by the allocator in the actual execution phase.
For the A40 GPU, we ran larger workloads which take up more memory like BM-bootstrap and ResNet, utilizing approximately 80GB and 200GB of memory respectively. We tested only the profiling allocation scheme of Section III-C on the A40 GPU because it has the best performance out of the three schemes. BM-bootstrap is a bootstrapping benchmark with different bootstrapping operations ordered after each other. After each operation, the parameters and corresponding keys required are switched to go to to the next operation. ResNet is a pre-trained homomorphic version of the DL ResNet model, and we test the inference operation in the homomorphic state. Figure shows the performance of each bootstrapping operation, and the peak asynchronous memory threshold (ratio of asynchronous allocations) that we set in the load run to allocate GPU objects. We noticed that for most of the bootstrapping operations, setting a threshold value in the mid-range of around 40-60% leads to the fastest results in bootstrapping time. In the case of ResNet, using a peak asynchronous memory threshold of 65% gives the best performance. The red line denotes manual swap performance, where the programmer is responsible for swapping out GPU Figure and 12 show the latency results for multiple HE functions for the FGa, and FGb, and FVb parameters on the GeForce 1660Ti and 1050 GPU cards. For the larger parameters, static seems to have the best performance. For the smaller parameters, the optimal approach is not so clear. Dynamic is highly advantageous because it does not require any modifications of inner library source code. Profiling was performed and applied with a 68% asynchronous allocation rate. Fully managed memory had the slowest performance with the greatest variance in all tests. For the 1050 in Figure , we compared static and dynamic performance. Both had similar performance, with static being slightly faster. This is due to being able to exactly pinpoint which objects in code we had to allocate asynchronously, with a straightforward benchmark tool testing only the homomorphic functions.