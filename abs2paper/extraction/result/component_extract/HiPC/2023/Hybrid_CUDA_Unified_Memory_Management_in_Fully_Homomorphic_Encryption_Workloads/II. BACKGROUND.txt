II. BACKGROUND
A. CUDA Unified Memory
NVIDIA's CUDA Unified Memory (UM) is a memory management feature introduced in CUDA 6.0 that simplifies memory management for GPU-accelerated applications. It provides a unified memory address space that can be accessed by both the CPU and GPU, enabling seamless automatic data movement between the host (CPU) and the device (GPU). This eliminates the need for explicit memory transfers and also simplifies the programming model, making it easier to develop and optimize GPU-accelerated applications. Developers allocate memory using cudaMallocManaged, which allocates memory regions that can be accessed by both the CPU and GPU using a single pointer. This makes it easier for the programmer to share data between devices.
Under the UM model, data is dynamically migrated between the CPU and GPU on-demand. When a CPU or GPU operation accesses data that resides in the other device's memory space, CUDA automatically handles the migration behind the scenes. This migration can also be performed asynchronously using functions like cudaMemPrefetchAsync, allowing the CPU and GPU to overlap computation and data transfers, thereby improving overall application performance.
The advantages of CUDA UM include simplified memory management, reduced code complexity, and improved productivity for GPU programming. While CUDA UM offers convenience and ease of use, it is important to consider its implications for performance. Data movement between the CPU and GPU can incur overhead, especially when frequent migrations are required. Careful consideration of data access patterns, memory usage, and synchronization points is crucial to optimize the performance of applications utilizing CUDA UM.
B. CUDA Stream-ordered Memory Allocation
Released in CUDA 11.2 , cudaMallocAsync is an extension of the cudaMalloc function, which is used to allocate memory on the GPU. The main difference is that cudaMallocAsync allows for asynchronous streamordered memory allocation, which means that the function call does not block the GPU until the memory allocation is complete. The asynchronous nature of cudaMallocAsync can potentially improve overall performance and resource utilization by allowing memory allocations to be performed in a stream-ordered manned. In this manner the GPU can continue processing using the allocated memory, without GPU device-wide synchronization that a cudaFree operation will invoke. Previously, custom memory allocators were needed to allocate large chunks of memory upfront in order for memory to be reused without lag. This could potentially cause bugs to occur, and increases complexity of the code. However, these new operations provided by CUDA eliminate that need with a programmer-friendly interface to increase productivity.
The cudaMallocAsync function takes as parameters the device pointer, the size of memory to be allocated and a CUDA stream. A CUDA stream is a sequence of commands that are executed in order on the GPU. By associating the memory allocation with a specific stream, developers can control the ordering and synchronization of GPU operations.
It's important to note that cudaMallocAsync does not guarantee immediate memory allocation. Instead, it initiates the memory allocation process asynchronously based on stream ordering. Internally, a memory pool is created by the CUDA driver. Memory from the pool is allocated from the OS based on a set threshold that the programmer decides. Synchronization mechanisms such as cudaStreamSynchronize or event-based synchronization calls automatically return unused memory back to the OS unless the specific threshold is set to maintain memory in the memory pool. Reusing memory by using cudaMallocAsync can result in much faster memory allocation performance.
C. Memory Usage in FHE
In FHE schemes, whether it is TFHE , CKKS , BGV , or BFV , memory usage can vary based on several factors:
• Security Parameters: The memory requirements can depend on the specific security parameters chosen for the FHE scheme, such as the modulus size, the ciphertext size, and the level of security desired. Larger security parameters generally result in increased memory usage.
Table shows a brief view of the parameter sizes in the HEaaN library . N denotes the number of slots in a ciphertext, Q is a set composed of q 1 , . . . , q n prime numbers and log(Q) = logΠq i , where i ∈ . | • | denotes the number of elements in a set. These parameters all have a security level over 128-bits. The larger the value of N , the more memory the given parameter consumes. Bootstrapping is an operation that is particularly memory-intensive, because of large numbers of keys that are generated. Each factor is independently related, and thus increasing all factors can greatly increase the memory footprint. In this paper, we primarily focus on the memory usage of the HEaaN library, which is an approximate arithmetic number implementation based on the CKKS scheme.
D. HEaaN Library Operations
As stated before, the HEaaN library is an approximate implementation of the CKKS scheme. We will not go into the mathematics in this paper as other work have already covered the details. Basically, CKKS has three types of data: a message, plaintext and ciphertext. Messages are stored as arrays of complex numbers. Plaintexts are messages converted to NTT polynomials in a process called encoding and decoding to be readily encrypted. Ciphertexts are encrypted plaintexts requiring encryption keys in order to be decrypted with the corresponding secret key. Addition is relatively straightforward in CKKS, where two ciphertexts are pairwisely added, resulting in an approximately correct result because added errors are insignificant. Multiplication between two ciphertexts requires a rescale operation to finalize the result, where the ciphertext modulus is reduced to a lesser number, and a number of least significant bits are truncated . This bounds the number of multiplications that can be performed without destroying the message. In order to recover the reduced modulus, bootstrapping must be performed. Rotation of ciphertexts involves shifting the location of elements in plaintext slots (a space containing an element in the message vector), which allows computation of different extracted elements .
Each parameter set determines the security level of the ciphertext, and number of ciphertext slots, and requires different sets of keys of different types, depending on the operations performed on ciphertexts. Therefore the amount of memory usage will largely vary depending on the parameter used. Additionally, faster variants will also require sets of constant values that are required to be transferred to GPU memory in order to perform faster computations. Such constants may also potentially consume more or less GPU memory, depending on the parameter sets used. The experiments in this paper are performed using different parameter sets on the above outlined operations.