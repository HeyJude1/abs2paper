III. DNA-TEQ
This section describes the proposed methodology for applying exponential quantization with pseudo-optimal parameters on a variety of DNNs. As discussed in previous sections, DNN quantization allows to reduce the numerical precision of activations and weights, which in turn lowers the memory footprint and the hardware complexity of the functional units. On the other hand, a proper quantization method should consider the distribution of activations and weights to reduce the impact in accuracy loss by minimizing the quantization error. The optimal quantization scheme must find the best trade-off between numerical precision and DNNs accuracy. In this work, we first observe and compare different distributions of activations and weights and show that an exponential quantization is the best fit. Then, we propose an offline search algorithm to find the optimal base of the exponential quantization along with other essential parameters. Finally, we take advantage of the exponential quantization to transform the traditional linear dotproduct operations of DNNs into the exponential domain, which simplifies the hardware required to perform DNN inference.
Previous works have used uniform quantization to compress the DNN parameters. However, we observed that activations and weights of most DNNs do not follow a uniform distribution, which causes a huge impact in terms of quantization error and accuracy loss when the precision is further reduced to lower bitwidths. Specially in recent DNNs that are extremely deep and can have hundreds of layers, the error is propagated and expanded among layers. To determine the best quantization function, we first perform a goodness-of-fit comparison of different distributions over the tensors of a variety of layers from multiple popular DNNs. The Residual Sum of Squares (RSS) metric is used to measure the discrepancy between the tensor distributions and an estimated model. RSS is computed by Equation , where y i is the i th value of the variable to be predicted, x i is the i th value of the explanatory variable, and f (x i ) is the predicted value of y i . In the analysis below, we use the absolute values of each tensor to measure the RSS.
= n i=1 (y i -f (x i )) 2 (1)
Table shows a comparison of the mean RSS of a set of common distributions for a trace of activations of all the FC and CONV layers of three different DNNs. For a given distribution and DNN, we report the mean RSS of all layers. As can be seen, the exponential distribution exhibits the lowest mean RSS, marked in red, for all the networks and, hence, an exponential curve is the closest fit for the majority of tensors of activations. As an example, Figure plots the histograms and exponential curve fittings of the empirical activations for the CONV2 layer of AlexNet and the FC4 layer of the Transformer DNN, which results in RSS values of 0.02 and 0.58, respectively.
Similarly, Table shows the mean RSS of the four distributions for the weights of all layers of each DNN model. Again, the exponential distribution exhibits the lowest RSS compared to the other distributions. To illustrate it, Figure depicts the histograms and plot fittings of the exponential curves of the empirical weights for the CONV2 layer of AlexNet and the FC4 layer of the Transformer DNN, which results in RSS values of 30.57 and 3.4, respectively. Compared to the activations, the RSS values of the weights are much larger. However, the exponential curve is still the best fit. In particular, the Transformer DNN exhibits the lowest RSS.
To summarize, we observe that the distributions of weights and activations, a.k.a. tensors, of a set of DNNs present a strong resemblance to exponential functions as demonstrated by its low RSS. Our aim is to quantize the tensors of all convolutional (CONV) and fully-connected (FC) layers with the nearest exponential representation that minimizes the quantization error, while achieving the best trade-off between numerical precision and hardware complexity. Next section describes how to obtain the pseudo-optimal parameters that define the closestfit exponential function for each DNN layer and tensor.
In this section, we describe our adaptive methodology for searching offline the pseudo-optimal exponential quantization parameters on a layer-by-layer basis for any DNN. Based on the observations from Section III-A, we propose a quantization scheme to represent the activations and weights of DNNs in the form of an exponential function x = Sign(x)(αb i + β), where x is the approximated value after the exponential quantization, x is the original floating-point value of the tensor element, b is the base of the exponential, i is an integer exponent, α is an scale factor, and β is an offset. The Sign(x) function returns 0 for zero values and 1/ -1 for positive and negative values. The integer exponent i of each element x in a tensor is computed with a logarithmic operation according to the following equations:
LogExpQuant(x, b, α, β, n) = Clip(Round(log b ( |x| -β α )), -(2 n-1 -1), (2 n-1 -1)), (2)
where
Clip(i, R min , R max ) = ⎧ ⎪ ⎨ ⎪ ⎩ R min i R min R max i R max i otherwise. (3)
In Equation , the Round function is defined as the rounding to the nearest integer, n is the number of bits required to represent the exponent i, and the clipping function in Equation forces the exponent values to be in the range of [R min , R max ], where R min = -(2 n-1 -1) and R max = (2 n-1 -1). Assuming an nbit exponential quantization, the number of unique intervals is 2 n -1. The approximated tensor values x are stored as exponents to reduce memory footprint since all other parameters are constants. We store an extra bit for the sign of the value, while the exponent -(2 n-1 ) is used as a special case to represent the zero value. On the other hand, the base b, bitwidth n, scale α, and offset β are constant parameters that are defined offline using an iterative algorithm that minimizes the quantization error and accuracy loss with the lowest possible number of intervals. For a given layer, we constrain n and b to be the same for both activations and weights in order to simplify the dot-product operations and the related hardware, by exploiting the property of exponentials: b i × b j = b i+j . Note that the quantization parameters, including n and b, can be different per layer and DNN to better fit the distributions of the corresponding tensors.
The flowchart in Figure summarizes the four main steps of our proposed quantization methodology, DNA-TEQ. Each  step is marked with a different color. Below, we describe in detail each of the steps to find the pseudo-optimal exponential quantization parameters that attain the best trade-off among accuracy, compression and hardware complexity. In the first step, we generate traces for the activations and weights of each layer to be quantized. For a given DNN, all the weights are obtained from the pre-trained model, while the trace of activations is generated from executing the inference of a small statistically representative subset of the training dataset.
The second step of DNA-TEQ, marked in green in Figure , performs the computation of the RSS metric of the tensors of each layer based on Equation . For a given layer, the tensor with the smaller RSS is selected to start the search of the base b. In all the layers of AlexNet and ResNet-50, the tensor of activations is chosen for computing the base, whereas in the Transformer DNN, 12 out of 96 FC layers choose the tensor of weights. The goal of this step is to start the search of the pseudo-optimal base from the tensor that has more similarity to the exponential distribution, reducing the induced error.
The third step, marked in yellow in Figure , starts the offline search for the pseudo-optimal base and the rest of the parameters of the exponential quantization for a specific layer l. As described above, the initial tensor t is selected from either weights or activations depending on the RSS metric. The base b l and scale α lt are initialized taking into account the maximum value of the tensor to cover the full scale range (FSR) with the exponential quantization as shown by Equation . Our empirical experiments demonstrated that by covering the FSR of the tensor, the quantization error is reduced due to the effect of large outliers. In addition, FSR helps to find the pseudo-optimal point of convergence of the search algorithm faster. On the other hand, the distribution analysis of activations and weights shows that most values are clogged in a range close to the minimum value of the tensor. Therefore, the offset β lt is initialized so that the smallest values of the tensor can be represented more precisely. In Equation , the first term shifts the quantization intervals close to the tensor minimum value, while the second term takes into account the rounding effect from Equation .
b l = max(t) 1 Rmax ; α lt = max(t) b Rmax (4)
β lt = min(t) -α lt b R min 1 + α lt b R min -α lt b R min -0.
Initialize(b, α, β);
3:
InitErr = RMAE(LogExpQuant(t, b, α, β, n), t); 4: ε = 0.01;
5:
IncBase = b + ε; DecBase = b -ε;
Alpha[], Beta[] = Update(α, β, IncBase, DecBase);
IncErr, DecErr = RMAE(t, n, IncBase, DecBase, Alpha[], Beta[]);
CurrentErr, ε, b = Direction(InitErr, IncErr, DecErr);
while Search = True do 10:
NewBase = b + ε;
11:
NewAlpha, NewBeta = Update(α, β, NewBase);
NewErr = RMAE(t, n, NewBase, NewAlpha, NewBeta);
if NewErr < CurrentErr then end while
For a given tensor, Algorithm 1 shows the pseudo-code for searching the optimal base and the related exponential quantization parameters that provide the lowest quantization error. First, we initialize the base b, scale α, and offset β with the equations described above (line 2). Then, we perform the initial quantization and compute InitErr (line 3) using the Relative Mean Absolute Error (RMAE) metric as defined in Equation . Next, we decide if the exploration of the base is going to be done by increasing or decreasing the initial base (lines 4-8) by a delta ε that is initialized to 0.01. The actual direction is selected by taking into account the lowest error among InitErr, IncErr and DecErr. Before computing the errors, the corresponding α and β are calculated for IncBase/DecBase. Finally, we continue to search for the pseudo-optimal base (lines 9-19) by repeatedly increasing/decreasing the base until the quantization error CurrentErr is no longer reduced. In the process, we update the corresponding scale α and offset β taking into account the NewBase according to Equations 4-5. Once the parameters of a tensor (activations or weights) of a layer are computed, for the other tensor of this layer the same base is used, and we simply compute the α and β parameters in the same manner as for the other tensor.
RMAE = | t -t| |t| (6)
In the fourth step, marked in red in Figure , and for a given layer, DNA-TEQ iterates over the quantization bitwidth n starting from 3 bits until a maximum of 7 bits, to find the lowest bitwidth that does not hurt accuracy. For all layers of a DNN, an error threshold is defined for each tensor, Thr act for activations and Thr w for weights, as the maximum quantization error (RMAE). Thr w is initialize to 1% while Thr act is scaled by Equation 7 to account for the difference in magnitude between the two distributions. Once the quantization parameters for a given bitwidth are obtained, the errors are compared against the thresholds, repeating the search algorithm until the condition is fulfilled.
Thr act = Thr w × log( mean(Act.) mean(W) )
After all layers have obtained their respective quantization parameters, we perform DNN inference to check accuracy loss, and iterate over Thr w in steps of 1%, that is, if the accuracy loss is negligible we increase the error that can be tolerated in each layer and re-run the search of parameters. This procedure is continued while the accuracy loss is lower than 1%. In the evaluation section, we show a sensitivity analysis of Thr w .
As previously discussed, our aim is to exploit the quantization not only to reduce memory footprint but also hardware complexity. In this section, we will first exploit that values are quantized with an exponential function to simplify the dotproduct operation, replacing the costly multiplications by simple additions. Then, we provide some hints on the requirements to implement and take advantage of our quantization scheme, including an initial hardware configuration of DNA-TEQ.
At a high-level, DNA-TEQ takes advantage of the property b a • b w = b a+w . After quantizing the values, all values are of the form S W (α W b int W + β W ) and S A (α A b int A + β A ) for weights W and activations A respectively, where S W , S A are signs, int W , int A are signed n-bit integer exponents, α W , α A are scale factors and β W , β A are offsets for the corresponding weight and activation tensors, respectively. As shown in Equation , each output activation is a sum of activations times weight products, which can be expanded into a sum of four terms:
m i=1 A i • W i = m i=1 S Ai (α A b intA i + β A ) • S Wi (α W b intW i + β W ) = α A α W m i=1 (S Ai S Wi )b intA i +intW i 1 + α W β A m i=1 (S Ai S Wi )b intW i 2 + α A β W m i=1 (S Ai S Wi )b intA i 3 + β A β W m i=1 S Ai S Wi 4 (8)
The first term is the sum of b int A i +int W i , which can be implemented with a table of 2 n+1 entries, where each entry stores the count of how many times each addition of exponents occurs. The sign bit of activation and weight are XORed and based on the result, the corresponding entry in the table is increased/decreased when S A i S W i becomes positive/negative. The second term is the summation of weights with respect to the sign of both weight and activation, while the third term is similar but the summation of activations. Both can be computed like the first term with a table that only requires 2 n entries for each one due to the limited range of the exponent. The last term is the accumulation of the sign products which can be obtained from any of the previous terms by adding the total number of occurrences. Note that the second and fourth terms depend exclusively on the weights, which are known, as well as the signs of the activations, which are mostly positive due to the ReLU activation function. Therefore, these two terms can be pre-computed offline for the majority of DNN layers. After filling the tables, we multiply each count with its corresponding value (b int ) and accumulate all products into a single value. The final values are multiplied by the constant coefficients and all terms are added together producing the final output activation.