II. RELATED WORK
Popular optimizations for reducing the memory footprint of DNNs include clustering and pruning. Clustering uses methods such as K-means to compress the number of different weights to K centroids. Each weight is then substituted by an index that corresponds to the closest centroid. Clustering alone does not reduce the amount of computations nor its cost, but storage requirements. On the other hand, pruning reduces the model size and the number of computations by removing unimportant parameters. The pruned model may loss accuracy but tends to regain it after retraining. Nevertheless, the pruned model becomes sparse, requiring specialized hardware to be efficiently executed. Pruning is orthogonal to DNN quantization methods such as DNA-TEQ.
As previously discussed, uniform quantization , has been used for DNNs in the past to reduce the numerical precision (i.e. 8b-16b) and computational cost with small impact in accuracy . However, the latest advances in machine learning have sparked the research of new quantization schemes to further reduce the memory footprint and computational complexity of modern DNN models . Some works , have attempted to use uniform quantization to reduce the numerical precision below 8 bits without paying attention to the tensor distributions, resulting in huge accuracy drops when performed on complex neural networks.
In order to further reduce the model size and computational cost of DNNs, recent studies , proposed to set different bitwidths for weights and/or activations based on their sensitivity to quantization error. As a result, multiple mixed precision quantization schemes have been developed. These schemes perform an analysis of each tensor distribution to use it as a criterion to determine the optimal numerical precision for each DNN layer. Mixed precision quantization schemes have the potential to represent the tensors with lower bitwidths (i.e. < 8b). Even so, most of these schemes still use uniform quantization once the precision of each layer has been determined, which may impact the accuracy of the model. Similarly, DNA-TEQ employs a mixed precision scheme for each layer but with a non-uniform quantization representation.
Several works , , , , proposed non-uniform quantization schemes based on the base-2 logarithmic representation of weights and/or activations of CNNs to reduce the numerical precision (i.e. < 8b) and HW complexity. The work in only quantizes the weights to avoid the overhead of the dynamic quantization of activations, but the logarithmic quantization of both can lead to better schemes to reduce the overall computational cost. In particular, the authors of exploit the base-2 quantization of activations and weights to perform operations in log-domain. However, the reduction in accuracy is non-negligible even in simple networks.
APoT is a non-uniform quantization scheme for the bellshaped and long-tailed distribution of weights and activations in DNNs. APoT constrains all quantization levels as the sum of powers-of-two terms. They can reduce the numerical precision of ResNet to 4 bits but require retraining to recover the accuracy. On the other hand, Mokey is a post-training compression method for Transformer models that does not require retraining. Mokey reduces the memory footprint by quantizing all values to 4-bit indexes into dictionaries of representative 16-bit fixed-point centroids. Mokey selects centroid values to also fit an exponential curve from a random normal distribution in an offline step. However, their method can not be adapted to any DNN model but Transformers. Moreover, they require to compute outliers in a costly post-processing phase.
In contrast to these works, DNA-TEQ reduces both storage and computational cost by quantizing activations and weights together, does not require retraining, and can be easily adapted to any DNN. The implementation details of DNA-TEQ are described in the following section.