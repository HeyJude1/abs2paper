V. HARDWARE IMPLEMENTATION
This section outlines the hardware support required for implementing DNA-TEQ. We begin by introducing the essential hardware components of the DNA-TEQ accelerator. Subsequently, we provide a detailed description of the execution process of DNN layers within the accelerator divided into three stages: 1) Pre-Processing, 2) Counting, and 3) Post-Processing. weights. Within the logic die, each tile comprises a single Processing Element (PE), a Memory Controller (MC), and a Router (R). The MC manages all memory operations within the associated vaults in the DRAM dies. Additionally, the MC incorporates two FIFOs to facilitate multi-domain frequency synchronization between the logic die and the DRAM dies. The Router enables local and remote accesses between PEs and vaults via a 2D mesh network. Finally, the PE is the core of the tile, and is responsible for accelerating the DNN operations by computing output activations based on Equation . Figure shows the main components of a PE divided into three main stages. Below is a detailed description of each stage.
A. Accelerator Architecture
B. Pre-Processing Stage
In the first stage, DNA-TEQ performs exponential quantization on the activations to extract their signs (S A ) and exponents (int A ) at runtime, in batches of eight. On the other hand, all weights are pre-quantized offline without requiring additional hardware. The Quantizer unit is in charge of mapping the activations from floating-point (FP16) to the nearest integer exponents according to the numerical precision of each layer.
During the quantization process, each activation is assigned to a CMP module (CMP 0 to CMP 7 ) to compare against all the boundaries of the exponential quantization intervals. These ranges are loaded during the execution of each layer and stored in a multi-banked memory buffer (L 0 to L 15 ) sized with a total of 256B for the worst-case scenario (i.e. 7-bit exponents). Comparisons are also performed in batches of 8 values to conduct the 3-bit quantization of multiple activations in a single cycle. To this end, each CMP module includes a set of 8 comparators that select the proper values from each bank of the memory buffer. Since the boundaries are sorted, the output of the comparators is an 8-bit vector of 0s followed by 1s. Based on the comparison results, the encoder detects the leading one and selects the corresponding int A value for the activation. Note that this quantization unit is less costly than implementing a complex log-base module, and banks of memory that are not needed for a given layer can be power-gated. The resulting exponents and their corresponding signs (S Ai , int Ai ) are stored in the Input Shift-Reg. This arrangement ensures that, in each cycle, a single quantized activation is fetched and broadcasted to all Counter-Sets (CS 0 to CS n-1 ) to be used in the subsequent stage. At the same time, the Weight Buffer receives N quantized weights and their signs (S wi , int wi ) from the Memory Controller.
C. Counting Stage
The counting stage is the core of our accelerator and consists of a controller to orchestrate all the execution, and N Counter-Sets (CS i ) to compute N different output neurons at the same time (e.g. . Each Counter-Set is composed of several components, including an adder to perform the sum of exponents, an accumulator to account for the occurrences of sign products of the fourth term when needed, and three Array Counters (AC 1 -AC 3 ) to calculate the other terms of Equation . Specifically, AC 1 is devoted to counting the occurrences of exponents required for computing the first term, while AC 2 and AC 3 are used for the second and third terms, respectively.
As shown in the left of Figure , each AC includes an adder, two multiplexers, and a small multi-banked SRAM buffer. Each SRAM has a total of 16 banks. All these buffers are sized for the worst-case scenario (i.e. 7-bit exponents). In particular, AC 2 and AC 3 have 8 entries per bank with a total of 128B, while AC 1 has 16 entries per bank with a total of 256B. For our set of DNN benchmarks, 8-bit per entry is enough to perform the count of each term without any numerical instability. To minimize the static energy of ACs among the multiple PEs, the unused banks will be power-gated according to the quantization bitwidth of each layer. For example, if the numerical precision of a layer is 3-bit, only one bank of each AC will be active.
D. Post-Processing Stage
The final stage of the accelerator is composed of two dequantization units, two multiplexers, and an Output buffer. Each Dequantizer consists of an FP16 multiplier, an accumulator, and two small buffers shared between de-quantization units: a Base-Lookup Table (BLUT) and a Scale Register.
During the post-processing, the CSs retain the occurrences of exponents that were computed in the counting stage, and send them to a Dequantizer. Next, each count of the ACs is multiplied by its corresponding b int value and accumulated to process each term in Equation . The FP16 base b powers are loaded and stored for every layer in a BLUT of 2 n+1 entries. Then, each dot-product result is multiplied by their related constant coefficients in the Scale-Register. Lastly, all terms are added together to provide the final output activation of a neuron.
The counting and post-processing stages are performed serially, while the pre-processing is done concurrently to hide the latency of the quantization. Serial post-processing avoids flushing the counters to a temporal buffer and additional data movements, while its latency is very small compared to the counting stage. Therefore, one de-quantization unit per Counter-Set does not significantly improve the overall speed. On the other hand, the area overhead associated with the FP16 multipliers is huge. Thus, we only employ two de-quantization units per PE to achieve the best trade-off. This configuration strikes a balance between performance and area, allowing for efficient processing of the output activations.