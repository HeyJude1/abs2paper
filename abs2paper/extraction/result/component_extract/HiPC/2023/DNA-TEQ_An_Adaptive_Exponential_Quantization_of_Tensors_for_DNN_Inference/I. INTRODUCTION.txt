I. INTRODUCTION
Deep Neural Networks (DNNs) have achieved human performance levels in cognitive computing applications such as speech recognition or machine translation . In order to achieve these levels of accuracy, DNNs have evolved and grown in complexity. From small Multilayer Perceptrons (MLPs) for simple tasks like recognizing written digits or characters, passing by large Convolutional Neural Networks (CNNs) , for recognizing objects in images, to complex Recurrent Neural Networks (RNNs) and Transformers to reach the aforementioned human parity in speech recognition and machine translation. Nowadays, the machine learning community has shifted the focus to Transformer models , , due to their extreme efficiency in terms of both accuracy and performance for multiple Natural Language Processing (NLP) applications. However, MLPs and CNNs still provide state-ofthe-art accuracy for applications such as acoustic scoring in speech recognition or self-driving cars respectively , . The dramatic growth of DNNs size has made their deployment on mobile and embedded devices extremely challenging .
Over the years, DNNs have grown steadily in computational complexity and memory footprint . For example, the Switch Transformer from Google has 1.6 trillion parameters, which requires 890 billion floating-point multiply-and-add operations per forward-pass and terabytes of memory to store the weights. The development trend of most DNN models shows that the more parameters, the more powerful the models become in terms of accuracy . However, this comes at the cost of more storage, data transfers, and computations. Hardware architectures must meet the memory storage and computational needs of modern DNNs. As these models are huge and the onchip memory of current chips is limited, it is off-chip memory accesses that account for most of the energy consumption . Therefore, techniques to compress or reduce the amount of parameters are required to perform efficient inference of DNNs in any given hardware architecture .
Researchers have proposed several DNN optimizations , , to ease the execution of real-time applications on mobile and embedded devices. Based on the observation that DNN models tend to be oversized, pruning removes redundant weights and/or activations to compress the DNN model size and reduce the amount of computations. However, pruning requires a costly re-training procedure of the DNNs to recover the full accuracy, as well as complex hardware to efficiently decode and run sparse models. On the other hand, linear uniform quantization is a popular technique to map a continuous set of values to a finite set. The main benefits of uniform quantization are the reduction in storage, due to a smaller representation of the tensor values, and the lower computational complexity, due to performing integer operations rather than floating point. Uniform quantization typically reduces the arithmetical precision of activations and weights to 8 bits with negligible accuracy loss for many networks, but lower bitwidths may result in high errors . Despite the advantages of pruning and uniform quantization, modern DNN models demand more aggressive optimizations to be suitable for current hardware architectures.
Recently, aggressive non-uniform quantization schemes have been proposed to further reduce the memory footprint by lowering the precision under 8 bits. For example, logarithmic quantization (LQ) , , has been previously proposed to represent weights and/or activations with only a fixed base and an integer exponent. LQ exploits the non-uniform distributions of tensors to achieve smaller bitwidths compared to the uniform quantization. Most works employ a base-2 LQ to not only compress the model size but also eliminate bulky digital multipliers by using simple add and shift operations. However, base-2 LQ is still far from being the quantization that best represents the distributions of tensors for modern DNNs, introducing large amounts of error that cannot be compensated even after re-training. Consequently, there is a need to further investigate in quantization methods that obtain the best tradeoff between accuracy and computational complexity.
In this paper, we propose a novel DNN Adaptive Tensor Exponential Quantization methodology, named DNA-TEQ, to quantize DNN tensors with an exponential representation by considering their distributions. DNA-TEQ includes an adaptive offline search algorithm to find the optimal parameters that provide the best trade-off between numerical precision and DNNs accuracy on a layer and DNN basis. DNA-TEQ also exploits the exponential quantization to simplify the dot-product operations by counting exponents. To demonstrate the cost-effectiveness of the exponential dot-product operations, we implement and evaluate DNA-TEQ both in software, using vector instructions (i.e. SIMD), and in hardware, on top of a baseline 3D-stacked DRAM-based DNN accelerator. Our experimental results show that specialized hardware is required to fully support the execution of different DNN models, providing higher performance gains and energy savings than the software counterpart.
To summarize, the goal of this work is to propose a quantization methodology that 1) does not require re-training, 2) quantizes weights and activations together, 3) generalizes for different DNNs, and 4) reduces hardware complexity improving performance and energy efficiency. The main contributions are:
• We analyze the distributions of activations and weights of different DNN models. We observe that most tensors follow a non-uniform distribution close to an exponential. • We propose DNA-TEQ, an adaptive quantization scheme that finds the optimal parameters for the exponential representation of tensors. DNA-TEQ minimizes the quantization error and achieves the best trade-off between numerical precision and model accuracy for several DNNs. On average, DNA-TEQ provides 40% compression over INT8 with negligible accuracy loss. Model parameters can be represented by only 3 bits in some cases. The rest of the paper is organized as follows. Section II provides a summary of works related to DNN quantization techniques. Section III describes our proposed quantization methodology including an analysis of the range and distribution of activations and weights for a set of popular DNNs. Sec-tions IV-V present the software and hardware implementation to exploit the benefits of our quantization scheme. Section VI provides the evaluation methodology and experimental results. Finally, Section VII sums up the main conclusions.