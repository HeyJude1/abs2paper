VII. CONCLUSIONS
In this paper, we propose DNA-TEQ, an approach to quantize weights and activations of DNNs in the exponential domain, which removes most of the bulky digital multipliers. This method is also motivated by the non-uniform distributions of tensors, making the exponential representation more robust and accurate compared to a linear uniform quantization. DNA-TEQ includes a search algorithm to find optimal parameters of the exponential quantization, achieving the best trade-off between numerical precision and quantization error. Results for a popular set of DNN models (Transformer, AlexNet and ResNet-50) show that, on average, DNA-TEQ provides 40% compression with negligible accuracy loss and without retraining the DNNs. Moreover, the DNA-TEQ accelerator achieves an average performance improvement of 1.5x and 2.5x energy savings with lower area than the baseline INT8 accelerator.