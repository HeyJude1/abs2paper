VI. EXPERIMENTAL RESULTS
This section evaluates our proposal in terms of accuracy, compression, performance and energy efficiency. First, we describe the methodology and tools employed to evaluate DNA-TEQ. Then, we introduce an analysis of the accuracy and compression of our quantization scheme across various DNNs. Next, we present the speedups and energy savings achieved by our hardware accelerator compared to the INT8 baseline. Furthermore, we discuss the accelerator overheads. Finally, we conduct a sensitivity analysis on the DNA-TEQ parameters.
We have developed a simulator that accurately models two different systems, our DNA-TEQ accelerator and a baseline with uniform INT8 quantization. The baseline architecture draws inspiration from 3D-stacked DRAM-based DNN accelerators such as Neurocube and Tetris . Both accelerators implement a similar architecture, including an output stationary dataflow, and hardware units to perform the quantization and de-quantization of input/output activations based on their respective schemes, that is, DNA-TEQ and INT8. Besides, weights are quantized offline in both accelerators. For a fair comparison, we set most of the configuration parameters to be the same for both the baseline and DNA-TEQ: a 3D-stacked memory of 4 GB with 4 DRAM dies partitioned into 4Ã—4 vaults and PEs, an internal bandwidth of 10 GB/s per vault, about 2.5 KB of SRAM per PE to store inputs/outputs/weights, 16 MAC or Counter-Set units per PE, and a frequency of 300 MHz in the logic die. DNA-TEQ requires 6 KB more of on-chip memory per PE due to the Counter-Sets that replace the MACs.
Regarding area and energy consumption evaluation, the logic components are implemented in Verilog, including all the additional components required by DNA-TEQ, and synthesized to obtain the delay, area, and power using the Synopsys Design Compiler, the modules of the DesignWare library, and the technology library of 28/32nm from Synopsys. On the other hand, we characterize the memory buffers of the accelerator by obtaining the delay, energy per access and area using CACTI-P. We use the configurations optimized for low power and a supply voltage of 0.78V. Finally, the energy consumption of the 3Dstacked memory is estimated by using DRAMSim3. The results  obtained with the aforementioned tools are combined with the activity factors and memory traces provided by our simulator to obtain the dynamic and static power of the accelerators. We evaluate our method on the classification task of Im-ageNet (ILSVRC-2012) using pre-trained models of popular DNNs including AlexNet and ResNet-50. In addition, we also evaluate a Transformer model on the machine translation task of Newtest2014 (English-German) which contains 3003 sentences. Python and Tensorflow are used to implement DNA-TEQ and the DNN models to assess the accuracy and measure the RSS.
The algorithm described in Section III-B finds the optimal bitwidth for each layer and their corresponding quantization error. Table shows a comparison between uniform quantization and DNA-TEQ in terms of accumulated RMAE of activations and weights among all layers. In addition, the table reports the accuracy loss of both quantization schemes compared to the FP32 baseline, assuming the same number of bits obtained from the search algorithm for both cases. As can be seen, DNA-TEQ provides the lowest error and loss in all the evaluated DNNs.
Table shows the average quantization bitwidth achieved by DNA-TEQ. The table also reports the accuracy of the baseline with 8-bit uniform quantization and DNA-TEQ without retraining. On average, DNNs are quantized to 4.83-bits, resulting in a compression ratio of 40% over the linear INT8 baseline. In all cases, the accuracy loss is less than 1% with respect to the FP32 baseline. These results are well correlated with the observations from Section III-A, proving that exponential quantization can reduce numerical precision below 8 bits.
As previously discussed, most current works on quantization either require re-training or target a specific type of DNN model. However, re-training is expensive in terms of time, energy, and system resources, hence our aim is to perform quantization without re-training and with negligible accuracy loss. Nevertheless, note that by doing re-training on top of DNA-TEQ, we could achieve additional benefits such as lower bitwidth per layer while recovering the accuracy completely. As a reference, Mokey quantized the tensors of Transformer networks to 4-bit precision plus outliers without retraining. They reduced memory footprint by 50% compared to their INT8 baseline model. In contrast, DNA-TEQ reduces the Transformer tensors' footprint by 61.86% with an average precision of about 3 bits. Furthermore, Mokey is developed exclusively for Transformers while DNA-TEQ is an adaptive methodology that covers a wide variety of DNNs.
Figure shows the speedups achieved by DNA-TEQ. Compared to the INT8 baseline accelerator, DNA-TEQ provides consistent speedups for our set of DNNs that range from 1.33x (ResNet) to 1.64x (Transformer), achieving an average performance improvement of 1.45x. The reduction in execution time is due to DNA-TEQ's efficient quantization scheme. The number of memory accesses is significantly reduced since DNA-TEQ provides lower numerical precision per layer. As shown in Table , Transformer exhibits the highest compression ratio, thus obtaining the largest performance improvement.
Figure reports normalized energy savings. On average, DNA-TEQ reduces the energy consumption of the accelerator by 2.5x over the baseline. The energy savings are well correlated with the hardware simplicity of the counting of exponents with variable bitwidth per layer. These energy savings are due to two main reasons. First, dynamic energy is reduced due to the savings in multiplications and memory accesses. Second, the performance improvements shown in Figure provide a reduction in static energy. Again, Transformer obtains the largest benefits, achieving a reduction of 3.3x in energy.
Energy: Figure shows a comparison of the dynamic energy consumed by the execution of a single counting step with different quantization bitwidths. We also show the baseline equivalent with the energy consumption of an INT8 MAC operation, that is, the product and accumulation of a single input and weight. As can be seen, DNA-TEQ delivers the lowest energy consumption per operation regardless of the numerical precision. However, the main overhead of DNA-TEQ lies in the post-processing stage, which may require several FP operations according to the precision of each layer, hindering the benefits of the quantization scheme. In particular, the layers quantized with 7 bits are more energy costly than those of the INT8 baseline. These overheads are taken into account for the energy of Figure . However, the number of layers quantized with 7-bit is lower than 3% in our set of DNNs. Therefore, the benefits due to the lower bitwidth of the remaining layers can easily overcome these modest overheads.
Area: DNA-TEQ replaces the MAC units by Counter-Sets to perform the exponential dot-product operations, requiring additional on-chip memory per PE. However, the extra memory does not incur in area nor energy overheads due to the MACs being more costly than the simple Counter-Sets. The area of DNA-TEQ in the logic die due to 16 PEs is 0.59mm 2 in 32nm. In comparison, the baseline at the same technology node occupies an area of 0.78mm 2 . In particular, the Counter-Sets of all PEs have a total area of 0.32mm 2 , while the MACs in the baseline occupy 0.67mm 2 . To ensure a fair comparison, the amount of FP multipliers required to perform de-quantization is the same in both accelerators. To summarize, despite the additional memory required by our proposal, DNA-TEQ achieves a smaller area compared to the baseline accelerator.
The proposed quantization scheme uses two thresholds to control the bitwidth of each layer and the loss of accuracy. Thresholds Thr w and Thr act are defined as the maximum quantization error that can be introduced in the weights and activations of a given layer, respectively. Our goal is to find values for these parameters that achieve high efficiency, i.e. a large amount of compression with negligible accuracy loss, for a wide range of DNNs. Therefore, the user does not have to manually tune these thresholds for each specific DNN.
As discussed in Section III-B, Thr w iterates in steps of 1% until the accuracy loss becomes higher than 1%. In addition, we consider the first layer of each DNN as an special case that may propagate higher quantization error. Consequently, the Thr w of the first layer is always ten times lower than the rest. Besides, Thr act is computed by scaling Thr w to take into account the difference in magnitude between distributions, as explained above. We performed a sensitivity analysis to understand the impact of these thresholds. Figure shows the accuracy loss and average bitwidth of three DNNs when iterating over different error thresholds. Most of the Transformer model parameters are quantized to the lowest possible bitwidth of our scheme (i.e. 3 bits) when Thr w = 30%, meaning it is highly fault tolerant. The remaining parameters are quantized to either 4 or 5 bits. ResNet-50 and AlexNet reach an average bitwidth of 5.65 and 5.78 when the Thr w is 5% and 4%, respectively. These thresholds provide the best combination of numerical precision and DNN model accuracy. In summary, DNA-TEQ iterates over different error thresholds to achieve the best compromise between accuracy and bitwidth per layer and tensor parameters.