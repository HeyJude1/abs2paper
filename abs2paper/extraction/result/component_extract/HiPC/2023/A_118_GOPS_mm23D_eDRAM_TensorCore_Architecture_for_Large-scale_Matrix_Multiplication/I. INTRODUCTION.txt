I. INTRODUCTION
In recent years, the surge in the adoption of large transformer-based language models and Neural Radiance Fields (NeRF) based rendering has been closely tied to their intensive computational demands. While these tools have spearheaded breakthroughs in fields like natural language processing (NLP) and 3D reconstruction, such as conversational AI and MR , , the computational challenges they introduce cannot be overlooked. For instance, to achieve high-quality responses, models necessitate more than 175 billion parameters and a staggering computational power in the ballpark of hundreds of petaflops . The sheer magnitude of these models stretches the limits of present-day accelerator architectures, underscoring the pressing need for extreme compute throughput.
Currently, most commercial products depend primarily on off-chip DRAM access and centralized matrix multiplication units. Further improvement of computation resources are limited by area and power density. Recent explorations involve implementing Single Instruction, Multiple Data (SIMD) units placed near High Bandwidth Memory (HBM) IO boundaries for machine learning workloads , , which extends the computation to off-chip. However, further exploration of increasing flexible on-chip computation units is necessary to address the rapidly changing requirements on the algorithmic side effectively.
In modern System-on-Chips (SoCs), a significant portion of the on-chip area is consumed by memory, which restricts the available area for computation, thereby limiting the chip's overall throughput potential. To address this, we propose repurposing memory components as high-density computational units that benefit from innovations in Back-End-Of-Line (BEOL) transistors and monolithic 3D integration methods. As Fig. shows, Our proposal is focused on the potential of ultrahigh density, monolithic-3D embedded DRAM (eDRAM) as a powerful compute engine for matrix multiplication. We aim to co-design a compact eDRAM integrated with analog compute circuits, thereby amplifying data reuse and curbing energy consumption. The emergence of monolithic 3D integration of BEOL transistors for high-density tasks further bolsters this approach. By facilitating matrix multiplication across dimensions that transcend 2D limitations, this vertical integration not only facilitates substantial data reuse benefits but also opens up new opportunities for architectural advancements.
In this paper, we first introduce the eDRAM tensor core circuit and its 3D BEOL integration. We then present a dataflow supporting INT8 and BF16 general-purpose matrix multiplication. Subsequently, we assess the advantages of this approach over the traditional processors in terms of critical metrics such as energy efficiency, performance, and compute intensity. In addition, we devise a comprehensive architecture, including cache, datapath, and DRAM access components. Moreover, we evaluate the effectiveness of the proposed ap- proach using state-of-the-art machine learning models such as NeRF and LLaMA-7B language model. II. 3D EDRAM TENSOR CORE CIRCUIT AND DATAFLOW Monolithic 3D memory technology offers higher bit-density and higher energy efficiency compared to tiled 2D memories and other 3D integration methods , thus meeting the density and energy efficiency needs of future ML workloads, exhibiting low write energy, and high write endurance due to many weight and activation updates (write operations) in each layer's computation. Monolithic 3D silicon SRAMs face low temperature processing challenges, incur higher transistor count and leakage current compared to other back-end integrated resistive, magnetic, and phase change memories . Although commercially available, these memories exhibit poor writeendurance (10 4 -10 6 ), and high write-energy (>10pJ), limiting their use in large scale accelerator designs requiring frequent, energy-efficient weight/activation updates.
A promising 3D memory technology suitable for extremescale designs is eDRAM, which employs back-end integrationcompatible Indium Gallium Zinc Oxide (IGZO) transistors . eDRAM features an intrinsic contention-free write mechanism, resulting in a low write energy (approximately 1pJ). Furthermore, IGZO eDRAM has shown an impressive 6-8 orders of magnitude higher write-endurance compared to other back-end integrated memories , . Consequently, IGZO eDRAM presents a distinct value proposition characterized by low write energy, high write endurance, and remarkable bit density.
This work introduces a monolithic 3D eDRAM approach using BEOL-integrated 2T only gain-cell bitcell topology to maximize memory density. Fig. illustrates our inner-product style dataflow that extends from traditional 2D Compute-inmemory (CIM) dataflow. Within this framework, 3D weights are retained in the columns of the 3D eDRAM array for inner product matrix multiplication. Inputs are fed into  the read-wordline (RWL) systematically. On the bit cell level, WBL and WWL are routed along the X and Y axes, while RBL extends in the Z direction. The write access transistor is utilized for writing one multiplier (b 22 in Fig. ) into the 3D array, while the complement of another multiplier (ƒÅ 12 in Fig. ) is fed through the RWL and performs multiplication through the read operation. The outcome of the multiplication process determines the discharging action of the readbitline (RBL), resulting in layer-wise accumulation similar to previous models. In this operational mode, the weight remains static within the array, reducing the requirement for high input bandwidth. However, unlike 3T cell designs, 2T cell designs exhibit RBL limited bitline swing due to the unselected cell sneak-path leakage current, which could impose computation accuracy issues. Additionally, a small voltage swing implies an insufficient read-sensing margin.
To surmount these challenges, we propose a higher performance outer-product dataflow which is shown in Fig. when reconfiguring the 3D eDRAM cache as TensorCore. This mode of operation feeds and broadcasts 2D multipliers through the array in the X and Y directions while results accumulate in the Z direction, enabling parallel outer product matrix multiplication. Instead of performing multiplication through read operation in inner-product dataflow, this proposed dataflow uses the memory write stage for multiplication to achieve a full RBL swing, leakage-free accumulation. Meanwhile, outer-production achieves the best parallelism among result matrix elements, maximizing the utilization of all 3D RBLs. In executing multiply and accumulate (MAC) operations, WBL is initially pre-discharged, and WWL is set high to discharge all storage nodes. Concurrently, RWLs are driven to the ground, and RBL is precharged to Vcc. Single-bit multiplier values are then delivered to WBL as pulses and WWL as steady voltages. A storage node charges only when both WBL and WWL are high, which completes a one-bit multiplication. The read access transistor discharges the RBL when the multiplication result is '1', facilitating accumulation in the Z direction. The partial sum is then converted into the digital domain using a 2bit ADC and sense amplifier. The accumulation phase can also be digitally designed using a set of digital adder trees to access the storage node, but this would demand using complementary P and N devices-based logic using IGZO BEOL, which is challenging to realize. Multi-bit functionality can be achieved by integrating a shift-and-add unit for each Z column. This bit-serial approach allows multipliers to be flexibly defined by digital control, easing mapping to the machine learning library.