III. METHOD
We introduce CAPTURE, a partitioner for hybrid-parallel DNN training. It partitions the DL model into pipeline stages, determines the parallelization method (data-or tensor-parallel) for each stage and assigns a number of workers (GPUs) to each stage. The recommended partitioning and parallelization plan minimizes the peak memory usage across GPUs.
CAPTURE is designed to be DL-framework agnostic and easily applicable to any hybrid-parallel training system. Therefore, a profiling-based approach and statistical modeling are favored over analytical modeling. CAPTURE uses profiling to capture the effects of memory optimizations present in DLand hybrid-parallel training frameworks on memory usage. It then applies statistical modeling to the profiling data to predict the memory usage for any data-or tensor-parallel pipeline stage. Fig. shows an overview of CAPTURE. This section discusses CAPTURE's design and components.
CAPTURE's profiling stage performs a number of short profiling runs with a specifically selected set of partitionings and parallelization plans for pipeline-and hybrid-parallel training. Through these runs, two metrics about the memory usage for each layer in the DNN are collected: M i (memory isolated) and M a (memory added). M i (l) is the peak memory usage observed for layer l when that layer is the only one placed on a GPU. M a (l) is the increase (or decrease) in memory usage observed when layer l is added to an existing set of layers on a GPU. These metrics are based on mCAP, which collects the same metrics for pipeline parallel-only training.
In CAPTURE, the metrics are collected multiple times, for three different training scenarios, namely: training with pipeline parallelism only, training with data-or tensor parallelism and training with different batch sizes. By collecting the memory statistics for different scenarios, we collect the data that is later needed in the prediction stage to accurately predict the memory usage for any partitioning and parallelization plan, as well as any training batch size.
The profiling runs are performed in the same setup as the target training job: the same software stack and hyperparameters are used, except for the batch size. The profiling runs perform two training iterations, in which all stages of the training process are performed (forward pass, backward pass and update step). Because profiling is performed in the same setup as the target run, the profiling data captures the effects of memory optimizations in the DL-and hybrid-parallel frameworks on peak memory usage.
1) The profiling partitionings: We demonstrate how the set of profiling partitionings for pipeline parallel-only training is generated for a DNN with L layers on G GPUs, in the idealized case where L is divisible by G. The aim is to collect M i and M a for each layer in the DNN. To collect M i for layer l, we need to perform a profiling run where layer l is the only one on a GPU. To collect M a , we need a run with some set of layers l − n, ..., l − 1 (called neighbors) on a GPU, and a run with layers {l − n, ..., l} on a GPU. The memory statistics are then calculated as follows:
M i (l) = P M(l) M a (l) = P M(l − n, l) − P M(l − n, l − 1)
where P M(l, l + n) indicates the profiled peak memory usage of a GPU with layers l to l + n placed on it. To limit the number of required profiling runs, M a (l) is collected once for each layer, for a single value of n (set of neighbors).
While CAPTURE records the same metrics for the model's layers as mCAP for this profiling scenario (pipelining only), the set of profiling partitionings that CAPTURE generates to record the statistics is different. The partitionings are summarized in Description 1, and the partitionings required for the first L/G layers are described in lines 1-4. These partitionings allow us to extract M i for layers 0 to L/G + 2, because all of those layers have been placed alone on a GPU at least once. We can extract M a for layers 1 to L/G. To extract M a also for layer L/G + 1, we add one extra partitioning, described by line 5. We then fix the number of layers for GPU 0 to L/G and repeat the same procedure for GPU 1 (lines 7-11, note that the partitioning on line 4 is duplicated on line 7 for clarity reasons), to add the runs for layers L/G + 1 to L/G * 2 + 1. The procedure repeats until the partitioning on line 14 is reached. A final partitioning is added to record M a for the last layer (line 15). Using this method of generating the profiling partitionings for pipeline parallelism, the maximum number of required profiling partitionings (and runs) is L + 1, where L is the number of layers in the DNN. We perform these runs with two different batch sizes (see Section III-A2), which brings the number of profiling runs to 2 * (L + 1).
The profiling partitionings for pipeline parallelism. The numbers in a partitioning denote the number of DNN layers placed on a GPU. eq(k, g) equally distributes k layers over g GPUs by number.
1 [1, 1, eq(L − 2, G − 2)] 2 [2, 1, eq(L − 3, G − 2)] 3 ... 4 [ L G , 1, 1, eq(L − ( L G + 2), G − 3)] 5 [ L G + 1, 1, 1, eq(L − ( L G + 2), G − 3)] 6 ... 7 [ L G , 1, 1, eq(L − ( L G + 2), G − 3)] 8 [ L G , 2, 1, eq(L − ( L G + 3), G − 3)] 9 ... 10 [ L G , L G , 1, 1, eq(L − (2 * L G + 2, G − 4))] 11 [ L G , L G + 1, 1, 1, eq(L − (2 * L G + 3, G − 4))] 12
...
...
14 [ L G , ..., L G , 1, 1] 15 [ L G , ..., L G − 1, 1, 2]
The set of profiling partitionings in Description 1 are the runs that collect the memory statistics for pipeline-parallelonly training. Additional runs are performed to collect profiling data for data-or tensor-parallel training of pipeline stages. Four sets of profiling runs are performed for those scenarios: sets for 2-way and 4-way data-parallelism, as well as for 2-and 4-way tensor parallelism. In those runs, the GPUs are grouped together into S sub-meshes of size k, where k is the degree of data-or tensor parallelism (2 or 4). The generation of the set of runs is similar to Description 1, but the L DNN layers are now divided over S sub-meshes instead of G individual GPUs. Each layer or set of layers is trained with k-way parallelism.
The profiling runs are only performed with 2-and 4-way data-and tensor parallelism to limit the time required for profiling. The predictor applies statistical modeling techniques to predict the memory usage for stages with parallel degrees higher than 4 (see Section III-B).
2) Scaling the batch size: The profiling runs cannot be performed with the same batch size as the target run, because some of the profiling runs have an imbalanced memory usage and could go out-of-memory with the full batch size. Thus, we perform the profiling runs with a smaller batch size. Because the total memory usage (and hence the collected memory statistics) grows linearly with the batch size, but with a different rate for each layer, we perform the profiling runs with two batch sizes (by default half and quarter of the target batch size). The predictor uses the statistics collected with both batch sizes to scale the memory statistics to the batch size (see Section III-B). The total number of required profiling runs for pipeline-parallel stages, 2-and 4-way dataand tensor-parallel stages, each for 2 batch sizes, is 10 * (L−1).
3) Reducing profiling time: CAPTURE reduces the time required for profiling in three ways. First, profiling runs are performed with a reduced batch size, as described previously. Scaling down the batch size reduces the time required for the profiling runs. The default batch sizes chosen for profiling can be decreased further to trade accuracy of the memory predictions for reduced profiling time. Second, instead of performing data-and tensor-parallel profiling runs with multiple batch sizes, the memory statistics of parallel stages can be scaled to the target batch size based on the pipeline-parallel profiling data. This eliminates the need to profile with two batch sizes for data-and tensor-parallelism, but could affect the accuracy of the memory predictions. Finally, CAPTURE includes a layer merger, which can be used to reduce the number of profiling runs for the data-and tensor-parallel scenarios.
Layer merger: After performing the profiling pass for pipeline parallel stages, the layer merger can link ("merge") layers together based on their combined memory usage. After merging, multiple merged layers are treated as a single DNN layer during the profiling phase for data-and tensor parallelism and during the prediction phase, reducing the number of profiling runs. Merging layers will also reduce the number of predictions made by the predictor.
The merger links layers in a DNN until a user-specified number of layers (L m ) remain. It merges layers based on their combined memory usage when processed without dataor tensor-parallelism. It iteratively chooses the two DNN layers that result in the lowest M i value for the merged layer. The metrics (M i and M a ) for the new layer are calculated as follows (merging layers l and l + 1):
M i (l, l + 1) = M i (l) + M a (l + 1) M a (l) = M a (l) + M a (l + 1)
The predictor uses the profiling data and statistical modeling to predict the per-GPU peak memory usage for any hybridparallel pipeline stage, hardware setup, and batch size. A pipeline stage is defined by three components: (1) the group of DNN layers in the stage, (2) the type of parallelism used to process the stage and (3) the degree of parallelism (number of GPUs assigned to the stage). We call a tuple of (layer group, type of parallelism, parallel degree) a stage config. Note that a stage with parallelism degree 1 does not use data-or tensorparallelism and is referred to as a "pipeline-parallel stage".
The predictor predicts the memory usage for pipeline-parallel stages as follows:
M p (l, l + n, 1) = M i (l) + l+n k=l+1 M a (k)
where M p (l, l+n, 1) is the predicted per-GPU peak memory usage of a pipeline stage containing layers l to l + n and parallel degree 1 (no parallelism, so a pipeline-parallel stage).
2) Predicting for the target batch size: Recall that CAP-TURE collects the memory statistics for two batch sizes, which are smaller than the target batch size (by default half and quarter of the target batch size). The predictor scales the predicted memory usage for a pipeline stage to match the target batch size. It fits a straight line through the predictions for the two profiled batch sizes and samples the fitted function at the target batch size, as depicted in Description 2. 3) Statistical modeling for hybrid parallelism: The profiling data contains M i and M a measurements for each layer for 2 and 4-way data-and tensor parallelism. For 2-and 4-way parallel stages, we can therefore predict the per-GPU memory usage for hybrid parallel stages with parallel degrees 2 and 4 in similar fashion as for pipeline-parallel stages, including the scaling to the target batch size.
For parallel degrees larger than 4, we predict the memory usage through statistical modeling: during prediction, we fit a logarithmic function through the predicted memory usage of a given stage for the two degrees of parallelism used during profiling and sample the function at the degree of parallelism that we are predicting the memory usage for.
The shape of the fitted function is motivated by high-level analytical models of the per-GPU memory usage for data and tensor parallelism. For data parallelism, the memory usage is determined by the following components:
Mtotal = Mweights +
M activations nGP U s +Buf f er stage comm +Buf f er allreduce +Buf f er passthrough All the components of the memory usage either scale linearly with the number of GPUs, or are fixed regardless of the degree of data parallelism. The passthrough buffers scale linearly with number of GPUs or are fixed (depending on the implementation in the hybrid-parallel framework). The memory for the activations also scales linearly. The other components use a fixed amount of memory. Given that the per-GPU memory usage has a component that is fixed in size and a component that scales linearly (is reduced) when the degree of parallelism increases, the sum of those components scales logarithmically with the degree of parallelism. Hence, we fit a logarithmic function through the collected datapoints.
For tensor parallelism, the various components of the (per-GPU) memory usage are expected to scale differently than for data parallelism, but all components also scale linearly with the number of GPUs, or are fixed. Hence, we also fit a logarithmic function through the collected datapoints for tensor parallelism. The analytical model for per-GPU memory usage of tensor parallel pipeline stages looks like:
Mtotal = Mweights + M activations nGP U s + buf f ers C. Recommender
The recommender finds a memory-efficient partitioning and parallelization plan based on the predicted peak memory usages of pipeline stages provided by the predictor. The recommender takes the number of DNN layers (L), the (target) hardware mesh (number of nodes N and GPUs G) as input. Given L, it generates the list of L * (L+1) 2 layer groups that can form a pipeline stage, which is as described in Description 3. Description 3: All possible layer groups.
1 , ,
...[1, 2, 3, ..., L] 2 [2], [2, 3], ...[2, 3...L] 3 ... 4 [L − 1][L − 1, L] 5 [L]
In the target run, each layer group can be processed on a submesh (set of GPUs) of size 1 to (maximum) k = G N , using either data-or tensor parallelism. Recall that each tuple of (layer group, type of parallelism, parallel degree) is called a stage config. The recommender determines the possible degrees of data-or tensor parallelism and enumerates all possible stage configs. CAPTURE limits parallelism degrees for data-and tensor parallelism to powers of 2. Thus, if k = 2 d , a single stage can be processed with 2 * d parallel configurations, where d = log 2 (k). The case of 1-way dataor tensor parallelism is a duplicate, because it corresponds to pure pipeline parallelism. After correction, the total number of possible parallel configurations is: 2 * log 2 (k) − 1. After generating all possible stage configs, the recommender obtains the predicted memory usage for each config from the predictor.
The recommender then enumerates all possible placements of stage configs onto the hardware mesh. While placing layer groups onto sub-meshes, it ensures that (1) all the layers of the DNN should be placed and no GPUs should be left unused and ( ) the layer groups should fit on the hardware mesh. Concretely, this means that (1) the layer groups should cover all the layers of the DNN, in-order and without duplicates and (2) the degree of data-or tensor parallelism with which a group can be processed is limited by the submesh size k (number of GPUs/node) and by the other layer groups placed on the same submesh: the sum of the degrees of parallelism for all groups on a submesh should be equal to k.
For each valid placement of groups onto the hardware (a plan), the recommender tracks the peak memory usage across all GPUs in the plan, using the predicted memory usage of each stage config in the plan. After enumerating all possible placements, it recommends the partitioning and parallelization plan with the lowest peak memory usage across all the GPUs. Algorithm 4 summarizes the algorithm for enumerating all possible partitioning and parallelization plans.
Algorithm 4: Algorithm for hybrid-parallel placement of layer groups on the hardware mesh. If multiple plans are found with the same peak memory usage across all GPUs, a tie-breaking rule is applied among the remaining plans: the GPU(s) with the highest predicted memory usage are excluded from the memory predictions, and the plan with the lowest memory usage across the remaining GPUs is chosen as the winner. This procedure is repeated until a winner is found or no GPUs remain. If no GPUs remain, the remaining plans are equal and a plan is chosen at random. This tie-breaking rule increases the chance that the best plan is elected as the winner if the memory usage of the excluded GPUs was mispredicted.
Reducing the time needed for recommendation: CAP-TURE's recommender applies a number of optimizations to reduce the time needed for the recommendation phase. Applying the layer merger reduces the number of layers in the DNN model and thus the number of possible plans for both pipelined and hybrid parallelism. Further, the recommender applies a pruning strategy that reduces the number of possible plans based on the best recommendation found so far (the one with the lowest memory usage): if the predicted memory usage for a given stage config is higher than the peak memory usage across all GPUs of the best plan found so far, that stage config is omitted from placement (so not considered in lines 12-13 in Algorithm 4).