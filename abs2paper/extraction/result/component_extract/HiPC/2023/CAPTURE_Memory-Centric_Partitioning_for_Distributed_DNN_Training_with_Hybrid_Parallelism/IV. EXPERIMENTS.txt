IV. EXPERIMENTS
We evaluate CAPTURE for Alpa, the current state-of-theart framework that supports all hybrid forms of parallelism (pipelining with per-stage data and tensor parallelism). We experiment with the GPipe and the 1F1B schedule, which are both supported by Alpa. We perform experiments with three large DL models, GPT-3 , GShard Mixture-of-experts and Wide-ResNet . Table shows the different model configurations used in our evaluation. Our hardware setup consists of 4 compute nodes, each containing 4 NVIDIA A100 GPUs with 40 GB HBM2 memory. The nodes are connected through 100Gbps InfiniBand interconnects. We use Alpa 0.2.3, JAX 0.3.22 and CUDA 11.3.
The statistical performance of a model is not dependent on the chosen partitioning and parallelization plan. Thus, we do not explicitly evaluate the statistical performance of the models for the plans chosen by Alpa and CAPTURE. The (profiling) runs in our experiments last for two training iterations and we use randomly generated input data, as provided in Alpa's benchmarking suite. The dimensions of the input samples correspond to those typically used in other research. The contents of the data has very limited influence on our results, as they only affect optimizations such as compression of communicated activations. We use randomly generated data, since it serves as a worst-case scenario for such optimizations in terms of performance and memory usage.
As CAPTURE is based on mCAP and extends its approach to hybrid parallelism, it does not fundamentally differ from mCAP for pipelining only and yields the same result. Hence, we do not explicitly compare the two systems for pipelining only. Section IV-A does compare the results for pipelining only and hybrid parallelism with CAPTURE. A. End-to-end results / Memory reduction Fig. compares the peak memory usage of all 16 GPUs in our hardware setup for 12 parallel training scenarios. The figure contains results for the 3 different models in 5 different configurations, both pipelining schedules and 2 types of parallelism (hybrid parallelism and pipelining only).
Alpa's recommended partitioning for hybrid parallelism (as used in Fig. to 6f) is obtained by using its autostaging functionality, which automatically clusters model layers into pipeline stages and recommends a parallelization plan to optimize throughput. In all of these scenarios Alpa is given complete freedom in choosing a plan, except for Wide-ResNet (2B). Since Alpa recommends a plan for that model configuration that fails to run on our hardware setup, we restrict the search space for the degrees of per-stage parallelism ("logical mesh shapes" in Alpa) to match the layout of the real hardware ("physical mesh shapes"). This setting provides the best performing and most memory-friendly alternative plan that successfully runs on our hardware setup.
Alpa's recommended partitioning for pipelining only (as used in Fig. ) is obtained by requesting 16 layers from Alpa's "autolayering" component, which splits the DL model in x equally compute intensive layers, where x is a programmer-defined number. We request 16 such layers and assign one layer to each pipeline stage/GPU. Note that the maximum possible value of x depends on the DL model.
Fig. to 6c show the results for hybrid parallelism using the 1F1B schedule. The observed reduction in peak memory usage is between 18.36% and 43.92% compared to Alpa. Not only does CAPTURE recommend a parallelization plan that has a lower peak memory across the GPUs than Alpa for GPT, MoE and Wide-ResNet, its plan also has a lower combined memory footprint. Since CAPTURE does not explicitly adjust the memory usage predictions for the 1F1B schedule, it underpredicts the memory usage for Wide-ResNet.
Fig. to 6f show the results for hybrid parallelism using the GPipe schedule. Compared to Alpa, CAPTURE reduces the peak memory usage with 5.64% to 21.38% and in most cases accurately predicts the peak memory usage. As expected, the observed memory usage is higher than for the 1F1B schedule for all model configurations.
Fig. to 6i show the results for pipeline parallel-only training, using the 1F1B schedule. As MoE has only 16 layers (the maximum value of x), only a single partitioning is possible with pipelining only on 16 GPUs. Reducing or better balancing the memory usage requires the use of hybrid parallelism (Fig. ). Also, scaling to more GPUs is not possible without hybrid parallelism, as there are not enough model layers to do so. CAPTURE establishes a memory gain of 19.38% to 25.26% for the other two models, despite some mispredictions related to the 1F1B schedule.
Fig. to 6l show the results for pipeline parallelism only, using the GPipe schedule. As for 1F1B, MoE can only be trained with a single partitioning and shows a large imbalance in memory usage between the GPUs, motivating the use of hybrid parallelism. CAPTURE chooses the same partitioning  as Alpa for GPT-3, leading to no memory gain. CAPTURE achieves 12.53% memory gain over Alpa for Wide-ResNet and accurately predicts the memory usage for all models. For all pipelining-only scenarios, the potential memory gain is limited, because further memory reduction and better balancing of the memory between workers requires the parallelization of layers to multiple GPUs (hybrid parallelism).
B. Prediction accuracy
To evaluate the accuracy of CAPTURE's peak memory predictions in more detail, we compare the predicted and real memory usage of 500 different partitioning and parallelization plans for Wide-ResNet (1B), using the GPipe schedule and hybrid parallelism. The plans are randomly sampled from the set of possible plans that the recommender generates. Plans that are predicted to run out of memory and pruned options are excluded from sampling. The memory usage is predicted for 500 plans, each using 16 GPUs, so the total number of per-GPU predictions made is 8000. Fig. shows a histogram of the error for the predictions for GPUs and that of the peak across all GPUs for a single plan (per-plan error). 44.8% of the per-GPU predictions is within the 2% error margin, 65.5% within 5% and 97.1% within 11%. For the per-plan predictions that is 45.4%, 69.6% and 97.8%, respectively.
There are relatively more predictions within the 9%-11% error range for per-plan predictions than for per-GPU predictions, because the per-plan prediction only considers the highest memory usage across all the GPUs/pipeline stages. It is likely that the stage with the highest memory usage consists of a relatively large number of layers compared to the other stages in the pipeline. Having more layers in a pipeline increases the chance of prediction errors, because CAPTURE takes multiple aspects into account during prediction for each layer, such as the level of data-or tensor parallelism and scaling of the batch size and then combines the prediction for a layer with the predictions for other layers in the same stage.
We identify two possible causes for occasional mispredictions of per-GPU memory usage. In the 1F1B schedule, the memory usage of a stage config can vary based on the location of the config in the pipeline, as described in Section II-B. CAPTURE does not actively adjust its memory predictions to this behavior, causing occasional over-or under-estimations of peak memory usage, such as in Fig. . Other mispredictions, such as for the GPipe schedule in Fig. are caused by the limited number of values for n (neighbors) used to extract
C. Profiling and planning time
Table shows the time needed by CAPTURE and Alpa to generate a hybrid partitioning and parallelization plan for the three largest models in our setup, for the 1F1B schedule. CAPTURE's runtime consists of profiling (profiler) and planning (predictor and recommender). Alpa's runtime consists of profiling various computations and communication operations, profiling various parallel stage configurations and applying its autostaging functionality. The majority of the runtime is spent on profiling for both systems.
CAPTUREs planning takes between 17 seconds and 32 minutes (when the layer merger is not applied). The planning time is kept low by the pruner for MoE and WResNet, but is less effective for GPT-3 because of the repetitive nature of its model architecture: many layers exhibit similar memory usage, reducing the pruner's effectiveness.
Without applying the layer merger, CAPTURE's total runtime is between 4 and 10 hours, while Alpa's runtime is between 3 and 4 hours. Applying the layer merger reduces CAPTUREs runtime and brings it closer to Alpa's: merging to 16 layers reduces the number of profiling runs and plans traversed by the recommender and halves the runtime for MoE and GPT-3. WResNet's runtime does not change, as it already consists of 16 layers without applying the layer merger.
The time needed to recommend a plan is negligible compared to the runtime of the target training run: it takes weeks to fully train these models on our hardware setup. Hence, we consider the additional overhead introduced by CAPTURE compared to Alpa's throughput-oriented planner negligible.
D. Throughput
Fig. summarizes the reduction in memory usage for all hybrid parallel scenarios in Fig. and shows the loss in throughput that results from partitioning and parallelizing for memory usage. For the configurations using the 1F1B schedule the throughput loss is between 11.5 and 42.4%, while the gain in memory usage is between 18.4 and 43.9%. For the GPipe schedule, the case of MoE (7.1B) stands out, with a loss in throughput of over 60% and a memory gain of 5.6%. In contrast, GPT-3 records the lowest loss in throughput for this schedule with 26.2%, while obtaining 20.3% memory gain.
As demonstrated next, the loss in throughput can be compensated by taking advantage of the gain in memory usage, by training on a smaller hardware setup. Alternatively, the extra memory headroom can be used to train a larger model or increase the batch size.
E. Smaller hardware setup
Fig. shows the achieved throughput when training Wide-ResNet (1B) on 6, 8 and 16 GPUs with CAPTURE's and Alpa's parallelization plans for hybrid parallelism, normalized to Alpa's throughput on 16 GPUs. Alpa runs out of memory on 8 GPUs, while CAPTURE is able to train the model. Moreover, CAPTURE can scale the hardware setup down further to 6 GPUs without running out of memory. Training on less GPUs increases hardware utilization and reduces communication overhead, which results in an increase in achieved throughput. This experiment demonstrates how CAPTURE can train a given model in a more cost-effective way. By using the reduction in memory usage to train on a smaller hardware setup, less resources are used and all of the loss in throughput is compensated. In this experiment, CAPTURE trains a DL model on less than half the hardware resources while increasing training throughput by 36.3%, making the training significantly more cost-efficient.