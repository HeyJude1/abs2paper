I. INTRODUCTION
Scaling up Deep Learning (DL) model sizes has proven to be an effective way to increase a model's statistical performance for many application domains. Deep Neural Networks (DNNs) for natural language processing (NLP) and image processing typically show increased accuracy when the model is scaled up. Training such networks is not only a computeintensive task, but also incurs significant memory requirements and financial cost. Training large DNNs requires an abundance of hardware resources and state-of-the-art distributed training techniques such as pipeline parallelism to meet the memory requirements. However, pipeline parallelism only provides a partial solution to the performance and memory bottleneck in large-scale DNN training. As DNN sizes keep increasing, new hybrid forms of parallelism are being developed that aim to further increase the trainable model size and training throughput. To facilitate the training of large DNNs, state-ofthe-art hybrid-parallel training systems aim to increase the trainable model size by enabling scaling to more hardware resources and (2) reduce the cost of training large DNNs by making effective use of the available resources and enabling training on cheap, preemptible cloud instances.
In this work, we introduce CAPTURE, a new approach to partition DNN models for hybrid parallelism, focusing specifically on memory usage. CAPTURE is inspired by mCAP , our existing partitioning approach for pipeline-parallel DNN training that balances peak memory usage between workers. Similar to mCAP, CAPTURE uses a profiling-based approach to collect memory statistics about the DL model. Unlike mCAP, CAPTURE uses those memory statistics and applies statistical modeling techniques to predict the memory usage for various hybrid-parallel partitioning plans and is thus not limited to pipeline parallelism only. Based on the predictions, it recommends a partitioning and parallelization plan that minimizes peak memory usage across workers (GPUs).
Hybrid parallelism is a combination of pipelining with stage-wise data-and/or tensor parallelism. Applying dataand/or tensor-parallelism within a pipeline stage can help to balance the memory usage across GPUs, but also has an effect on the total memory requirement: the memory required to process a DNN layer using data-or tensor parallelism depends, amongst others, on the layer's weight to activation ratio.
CAPTURE determines how to partition a DL model into pipeline stages and what form (data-and/or tensor) and degree of parallelism to apply to each pipeline stage. CAPTURE recommends a partitioning and parallelization plan with a low and well-balanced peak memory usage across all GPUs. Minimizing the peak memory usage enables training of larger DL models or training with a smaller hardware setup. Because CAPTURE is profiling-based, it can capture the effects of memory optimizations present in DL and pipeline-or hybridparallel frameworks on memory usage. By combining profiling with statistical modeling, it can predict the peak memory usage of any model partitioning and hybrid parallelization plan and recommend a memory-friendly plan for the target hardware configuration, but also for smaller or larger hardware setups.
Existing partitioners in hybrid-parallel systems typically suggest a partitioning and parallelization plan that optimizes the achieved computational throughput. However, optimizing the partitioning for throughput can be disadvantageous when the goal is to maximize trainable model size or minimize financial cost. CAPTURE finds a partitioning and paralleliza-tion plan with a low and well-balanced (peak) memory usage, which enables training of larger DNNs with the same hardware, or training a given model with fewer resources, which can reduce the financial cost of training large DNNs.
CAPTURE consists of three stages: a profiling stage that gathers per-layer memory statistics, a prediction stage that models/predicts per-GPU peak memory usage for different pipeline-parallel model partitionings as well as different perstage data-and tensor-parallel parallelization strategies, and a recommendation stage that finds a memory-efficient and well-balanced partitioning. The recommendation stage uses predictions of the memory usage for different partitioning plans made by the predictor. Various optimizations, such as a knowledge-guided layer grouping approach, limit the time required in the profiling and prediction stages.
A major advantage of our method is that it is DL-framework agnostic. We have implemented it for two state-of-the-art hybrid-parallel training systems with different underlying DL frameworks and software stacks: Alpa and Varuna , though in this paper we focus on Alpa. We evaluate CAP-TURE's effectiveness on a distributed scale.
We demonstrate that our approach reduces the peak memory usage by up to 43.9% compared to other partitioning approaches for hybrid parallelism. It can train larger DNNs on a given hardware setup and it can train a given model on more than two times smaller hardware setups.
Concretely, the contributions of this paper are:
• We introduce CAPTURE, the first memory-centric partitioning and parallelization approach for hybrid parallelism. • We provide a method based on profiling and statistical modeling that can predict the memory usage of hybridparallel training plans with per-stage data-or tensor parallelism for hardware setups of any scale. • We demonstrate that CAPTURE is DL-framework agnostic and is generically applicable to hybrid-parallel systems by implementing it for two state-of-the-art hybrid-parallel systems, Alpa and Varuna. • We evaluate the effectiveness of CAPTURE for pipelineand hybrid-parallel training on a distributed scale. • We evaluate the impact of using a memory-centric partitioning approach over a throughput-oriented partitioner on computational performance. The rest of the paper is structured as follows: Section II provides background information on hybrid-parallel DNN training, existing systems and partitioning/parallelization approaches. Section III describes CAPTURE's design and components, Section IV contains our evaluation and Section V contains conclusions and directions for future work.