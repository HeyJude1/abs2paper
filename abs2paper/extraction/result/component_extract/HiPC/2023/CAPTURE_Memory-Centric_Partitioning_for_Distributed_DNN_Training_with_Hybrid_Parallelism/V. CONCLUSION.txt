V. CONCLUSION
We introduced CAPTURE, a method to generate memoryefficient partitioning and parallelization plans for hybrid parallel DNN training. CAPTURE combines profiling and statistical modeling to make accurate predictions of peak memory usage and recommends a plan that minimizes peak memory usage across GPUs. Our method can recommend a memory-efficient plan for any target batch size and hardware setup size.
By reducing the peak memory usage CAPTURE enables training of larger models, training on a smaller hardware setup and training in a more cost-effective way than existing approaches. CAPTURE provides the user with the flexibility to choose how to make use of the extra memory headroom, even when a model does not fully occupy the GPUs' memories. We demonstrated that CAPTURE reduces memory usage by up to 43.9% and can train DNNs on more than two times smaller hardware setups.