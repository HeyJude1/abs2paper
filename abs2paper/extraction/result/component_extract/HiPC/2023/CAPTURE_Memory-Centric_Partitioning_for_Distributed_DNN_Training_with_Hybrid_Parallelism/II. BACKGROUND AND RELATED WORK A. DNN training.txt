II. BACKGROUND AND RELATED WORK A. DNN training
A DNN is a self-learning model that can be trained by feeding minibatches of training samples into the model. For each minibatch, a forward pass is performed that makes a prediction (e.g. a classification) for that input data. The forward pass is followed by a backward pass, which calculates updates (gradients) for each weight in the neural network that improve the predictions for the current inputs. After the backward pass the updates are applied to the weights in the neural network.
During the forward pass, activations are computed, which flow through the neural network from the first to the last layer.
During the backward pass, gradients flow from the last to the first layer in the model.
In data-parallel DNN training, the DL model is replicated on all workers. Each worker trains its model on a different part of the input data. Thus, each worker processes a separate batch of input data in each training iteration and updates its own weights. The weights are periodically exchanged and synchronized. Figure illustrates this principle.
In tensoror operator parallelism the operations that constitute the DL model (matrix multiplications) are partitioned over multiple workers. Multiple workers collectively process the same input data for the same operator. Conceptually, a single DNN layer or a group of layers is distributed over multiple workers, as illustrated in Figure .
In pipeline-parallel training [4]- , the DL model is partitioned over the workers. Each worker trains a group of DNN layers and the input data is fed into the model in a pipelined fashion. After the full batch has been processed, a weight update is applied, after which the next batch of input data is fed into the pipeline. In this paper, we focus on intrabatch or synchronous pipelining, in which an input batch is split into multiple microbatches that are fed into the pipeline.
The pipelining schedule determines the order in which the forward and backward passes of the microbatches are executed. The GPipe schedule (Fig. ) first processes all forward passes, followed by all backward passes, and then the update step. The 1F1B schedule (Fig. ) interleaves forward and backward passes to enable earlier release of memory for activations. As a result, the 1F1B schedule is more memory efficient for GPUs at the end of the pipeline, but does not improve pipeline latency or training throughput.
CAPTURE can be applied to both pipelining schedules, but motivated by its generic nature, does not explicitly adjust its memory predictions to the variations caused by early release of memory in the 1F1B schedule. We study CAPTURE's effectiveness in recommending a memory-efficient partitioning for both pipelining schedules in our evaluation. Hybrid parallelism is the combination of pipeline-, dataand/or tensor parallelism. Existing frameworks that implement hybrid parallelism partition a DL model into multiple pipeline stages. Each pipeline stage can then be processed by multiple workers using data-or tensor-parallelism, as illustrated in Figure . In this scenario the input data that is partitioned over the workers consists of the input to the layer (group) that is parallelized using data-and/or tensor parallelism, i.e. the input activations and gradients to the layer or layer group.
The partitioning of layers into pipeline stages, together with the type and degree of parallelism chosen for each pipeline stage is called the partitioning and parallelization plan. This plan determines the computational demand per worker, the amount of communication between the workers and the memory requirements for each worker within a stage. For example, processing a pipeline stage in a dataparallel fashion requires an allreduce operation after the update step that synchronizes the weights across the workers. When processing a pipeline stage in a tensor-parallel fashion, two allreduce operations per forward and backward pass are required. Each allreduce operation communicates the (output) activations/gradients generated for the layers in that pipeline stage. Hence, the partitioning of the model into pipeline stages combined with the parallelization strategy per stage directly impacts the latency of the full pipeline, the achieved training throughput and the overall (peak) memory usage.
Note that simply reducing the batch size is ineffective to solve the memory bottleneck in hybrid-parallel training, and often impossible. A sufficiently large batch size is required to form microbatches for pipelining and for data-parallel execution of pipeline stages. It is also needed to provide enough parallelism to achieve reasonable hardware utilization, because hybrid parallel systems rely on parallel and pipelined execution of training steps and on overlapping of computation and communication. Moreover, the batch size influences the number of activations and gradients generated during training, but not the size of the model's parameters. Thus, depending on the model, lowering the batch size may not sufficiently reduce memory usage to resolve the bottleneck. Finally, some models generate so many activations and gradients that they cannot be trained even with the minimum possible batch size without running out of memory.
A partitioner for hybrid-parallelism recommends a partitioning and parallelization plan, which consists of the following aspects: a mapping of layers to pipeline stages (the partitioning), the type of parallelism applied to each stage (data-and/or tensor parallelism) and the number of workers assigned to each stage (the degree of parallelism). Existing partitioning approaches for hybrid parallelism focus on achieving high training throughput. High throughput is achieved by considering: the latency of computing forward and backward passes for a pipeline stage, the communication required between stages and inside stages for data-and/or tensor parallelism and the available communication bandwidth between GPUs and nodes. Existing partitioners do not consider the imbalance in memory usage that may exist when a DL model is partitioned for achieving high throughput.
Different hybrid-parallel frameworks support different combinations of pipeline-, data-, and tensor parallelism. Varuna supports pipelining combined with data parallelism, but not tensor parallelism, a design decision motivated by their target usage scenario. The allreduces in tensor-parallel stages are communication-intensive, which requires a fast interconnect between GPUs and compute nodes. However, Varuna targets training on cheap spot-VMs on cloud resources, where such high-bandwidth, low-latency interconnects are typically not available. Varuna also processes each pipeline stage with the same degree of data parallelism and is limited in the types of DL models it supports. In this work, we focus on generic DNN training with all three forms of parallelism.
DeepSpeed and Megatron-LM , support hybrid pipeline-, data-and tensor parallelism and divide the DL model into pipeline stages based on simple metrics, such as the number of parameters or layers. Merak is a hybrid-parallel system that performs throughput-oriented model partitioning. In all three systems, the degrees of data-and tensor parallelism are fixed across stages (or the full pipeline is processed in data-parallel fashion, which results in a similar plan as with a fixed parallel degree across stages). Neither of the systems provide an automated approach to generate a full partitioning and parallelization plan for hybrid parallelism, as we do in this work. Moreover, Megatron-LM and Merak are specialized for Transformer models, while we focus on generic DNN training. Alpa supports the most elaborate form of hybrid parallelism with pipelining and per-stage data-and/or tensor parallelism, and can assign a different parallel degree to each stage. It thus has the largest number of possible partitioning and parallelization plans of all of today's training systems. Therefore, we perform our evaluation of CAPTURE with Alpa. All other (simpler) forms of hybrid parallelism found in today's hybrid-parallel systems can be considered a subset of Alpa's capabilities, and each of those forms will reduce the number of possible plans and simplify the decision making process to determine a suitable plan. Alpa focuses on throughput when determining a parallelization plan, while we focus on achieving low peak memory usage.
CAPTURE is inspired by mCAP. mCAP is our memorycentric partitioning approach for pipeline-parallel-only training. It partitions DL models over GPUs to balance peak memory usage. By balancing the peak memory usage across the GPUs, it enables the training of larger neural networks. mCAP consists of 3 stages (see Fig. ). mCAP's profiler performs a number of profiling runs to collect two metrics for each layer in the DL model: the (peak) memory usage of the layer when it is the only layer placed on a GPU, and the additional memory usage caused by the layer when it is placed on a GPU together with a set of preceding DNN layers. It is important to record both metrics, because memory optimizations implemented in DL and pipelining frameworks, as well as buffers for communication and passthrough variables, cause the memory usage to be significantly different in both scenarios.
The metrics collected in the profiling stage are used by the predictor, which can predict the memory usage for any given partitioning. The recommender navigates the search space of all the possible partitionings for a given DL model over a given number of GPUs. It supports two search strategies: mCAP-BF and mCAP-BO. mCAP-BF (brute force) iterates over all possible partitionings and requests predictions for them. It keeps track of the prediction with the lowest peak memory usage across the GPUs and recommends that as the best partitioning. The mCAP âˆ’ BO search strategy uses Bayesian optimization to navigate the search space and find the (near) optimal partitioning in a fixed number of iterations.
mCAP is limited to pipeline parallelism only and cannot generate partitioning and parallelization plans for hybrid parallelism. CAPTURE supports both pure pipelining and hybrid parallelism and, unlike mCAP, can recommend a memoryfriendly parallelization plan for any given target batch size.
Several recent papers propose methods to address memory limitations during DNN training. STR optimizes the scheduling of CPU-GPU memory swapping and recomputation operations to reduce memory usage, but focuses on single GPU training. Harmony addresses the scheduling and placement of compute and swapping operations for multi-GPU training, but focuses on pipeline-or data-parallelism. Our work is orthogonal to these optimizations, and focuses on recommending a partitioning and parallelization plan that minimizes peak memory usage for hybrid parallelism.