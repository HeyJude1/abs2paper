II. PRELIMINARIES
Following Kolda and Bader , we denote vectors using bold lowercase letter (e.g., a, b), and tensors using bold calligraphic letters (e.g., X). For a tensor X and a vector b we will also denote by X × j b the product of X and b along the j th -mode of X resulting in a order (N −1) tensor. Using this notation, we can define Tensor-Time-Same-Vector in all modes but 1 (TTSV1)
s = Xb N −1 = X = X × 2 b × 3 b . . . × N b (1)
which plays an important role in calculating generalized eigenvalues and eigenvectors associated with X. Alterna-tively, by expanding along indicies this may be rewritten as
s i 1 = Xb N −1 i 1 = n i 2 =1 • • • n i N =1 X i 1 ,...,i N N k=2 b i k .
(2) Formally, a hypergraph is a pair H = (V, E), where V is the set of vertices, and H is the set of hyperedges on those vertices; that is, E is some subset of the power set of V . We say H is uniform if all hyperedges have the same size; otherwise, it is nonuniform. The rank of H is the size of the largest hyperedge.
An order-N symmetric tensor X has N modes or dimensions, with the special property that the values, X (i 1 ,i 2 ,...i N ) remains unchanged under any permutation of its indices. Symmetric tensors arise naturally in many context including the representation of hypergraphs. For example, if H is an N -uniform hypergraph on n vertices, there is natural symmetric representation as an order-N tensor
X ∈ R n×n×•••×n . That is, for every hyperedge e = {v i 1 , v i 2 , . . . , v i N } ∈ E with weight w(e), X σ(i) = w(e)
N ! , where σ(i) denotes any of the N ! permutations of the index tuple i = (i 1 , . . . , i N ). In order to extend this representation to non-uniform hypergraphs, we follow the approach of Banerjee et al. and define for a rank-N edge-weighted hypergraph H = (V, E, w) the order-N blowup tensor, B, associated with H. To this end, for each edge e ∈ H define the set of ordered blowups of e as β(e) = {i 1 , i 2 , . . . i N : for each v ∈ e, ∃j i j = v}. Then for each i ∈ β(e), B i has value w (e)  |β(e)| . As we be working primarily with the blowup tensor, it will be convenient to define E(B) as the collection of edges which generated the blowup tensor B. In this paper, we take w(e) = |e| to ensure that B1 N −1 = d, the vector of node degrees. It is worth noting that for uniform hypergraphs, B is precisely the uniform adjacency tensor of the hypergraph discussed above.
The H-eigenvector centrality vector of H is a positive vector x satisfying Bx N −1 = λx [N −1] , where λ is the largest H-eigenvalue of B and the vector operation x [N −1] represents componentwise N − 1 power of x. By the Perron-Frobenius theorem for the hypergraph adjacency tensor , if H is connected, then x is guaranteed to exist, and is unique up to scaling. Intuitively, here a node's importance (to the power of N − 1, which guarantees dimensionality preservation) is proportional to a product of centralities over all blowups of hyperedges that contain it. A popular approach is to compute the eigenpair (λ, x) using the NQZ algorithm (Algorithm 1, where denotes componentwise division), an iterative power-like method that utilizes TTSV1 as its workhorse subroutine, which we employ here.
Algorithm 1 NQZ algorithm for computing HEC 1: Input: n-vertex, rank N hypergraph H, tolerance τ 2: Output: H-eigenvector centrality, x 3:
y = 1 n • 1 4: z = TTSV1(H, y) 5: repeat 6: x = z 1 N −1 /||z 1 N −1 || 1 7: z = TTSV1(H, x) 8: λ min = min (z x [N −1] ) 9: λ max = max (z x [N −1] ) 10: until (λ max − λ min )/λ min < τ 11: return x
Motivated by questions in hypergraph node ranking, we investigate the TTSV1 operation for the blowup tensor of a non-uniform hypergraph. To distinguish from the more general case, and emphasize the applicability to sparse symmetric tensors, we will refer to this problem as he Sparse Symmetric Tensor Times Same Vector (S 3 TTVC) operation on the blowup tensor. In many ways, the current work can be thought of as synthesis of the implicit S 3 TTVC algorithm on the blowup tensor proposed by Aksoy, Amburg and Young , with the CSS format for storing sparse symmetric adjacency tensors of uniform hypergraphs .
To that end, we summarize some of the key features of these two approaches in the next two subsections.
A. Generating Functions for S 3 TTV C Aksoy, Amburg and Young proposed the implicit AAY algorithm (Algorithm 2) to evaluate TTSV1 for the blowup tensor that relies on generating functions. The fundamental observation which drives their algorithm is that, in the blowup tensor, all entries corresponding to a single edge have the same coefficient. Thus, by using generating functions to aggregate over the contributions of all elements of β(e), the computational requirements can be significantly reduced. More concretely, they observed that for any edge e ∈ E and vertex v ∈ e, the contribution of e to [Bb N −1 ] v can be captured as a rescaling of the last entry in
E N (b v ) * * u∈e\v E r (b u ) , where E N (c) = 1, c, c 2 2! , . . . , c N −1 (N − 1)! and E N (c) = 0, c, c 2 2! , . . . , c N −1 (N − 1)! and (a * b) is a vector of length N + 1 representing the convolution operation with (a * b)[k] = k i=0 a i b k−i .
Alternatively, their approach can be viewed as extracting a specific coefficient of t N −1 from a particular exponential generating function . This approach yields Algorithm 2.
Algorithm 2 AAY algorithm for implicit TTSV1 using Banerjee adjacency tensor.
1: Input: rank N weighted hypergraph (V, E, w), vector b 2: Output: S 3 TTVC output, s = Bb N −1 3: for v ∈ V do 4: c ← 0 5: for e ∈ E(v) do 6: c += w(e) |β(e)| (N − 1)! E N (b v ) * ( * u∈e\v E N (b u ))[N − 1]
end for 8:
s v ← c 9: end for 10: return s While the aggregation over vertex-edge pairs given by the AAY approach results in significant computational speedups, the lack of structure imposed on the computation results in frequent repetition of the convolution calculations. For example, in the AAY approach the convolution E(b v ) * E(b u ) is computed |e|−2 times for every edge containing both v and u.
The CSS structure , is a compact storage format that enables efficient S 3 TTMC computation for sparse symmetric adjacency tensors X arising from uniform hypergraphs. In order to take advantage of the symmetry of X, CSS stores all information based on the collection of sorted edges of the associated hypergraph, E(X) If X has order N , then the CSS is a forest with N − 1 levels where every length k subsequence of an element of E(X) corresponds to a unique root to level k path in the CSS. Further, the leaves at level N − 1 are equipped with the "dropped" index in the subsequence and the value of X for the corresponding element of E(X). A key advantage of using the CSS format is its computation-aware nature -by storing all ordered subsequences of E(X), intermediate results in the S 3 TTMC computation can be easily memoized with minimal additional index information. In Shivakumar et al. , the S 3 TTMC-CSS algorithm is used to find the tensor decomposition of an adjacency tensor of a uniform hypergraph. The convergence of this method requires that the original hypergraph be connected. For a non-uniform hypergraph, it is likely that there exists an edge size such that the collection of hyperedges of that size is not connected. Thus, it is theoretically necessary to work with a single tensor representation of the hypergraph, such as the blowup tensor, in order to preserve the necessary convergence properties. This presents two primary challenges in applying S 3 TTMC-CSS that the current work addresses: the blowup tensor can have super-exponentially many non-zeros corresponding to a single edge and any data structure must explicitly account for the repetitions of the vertices induced by the blow-up. Naively, extending the index-ordered non-zeros approach of S 3 TTMC-CSS to incorporate repeated vertices will result in a significant increase in the memory footprint of the CSS structure to account for the repeated vertices, as well as the computational cost of computing S 3 TTVC itself. On the other hand, adopting the implicit construction approach and storing the adjacency tensor of each constituent uniform hypergraph using the CSS format is a suboptimal approach in terms of memory requirement compared to CCSS (described in the next section) and results in greater computation cost as we lose out on memoizing intermediate ĒN across IOU nonzeros i.e. hyperedges. Moreover, directly applying the S 3 TTMC-CSS algorithm to such a storage construction will not lead to correct results for the tensor-times-samevector operation.