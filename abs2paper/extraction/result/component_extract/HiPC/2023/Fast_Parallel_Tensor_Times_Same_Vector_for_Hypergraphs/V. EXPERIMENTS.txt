V. EXPERIMENTS
We compare the runtime performance and thread scalability of our shared-memory parallel S 3 TTVC algorithm CCSS-MEMO against our two baseline approaches -CCSS-DIRECT and CCSS-FFT -for a collection of real-world and synthetic datasets.
A. Platform and experimental configurations
Our experiments were conducted on a shared-memory machine with two 64-core AMD Epyc 7713 CPUs at 2.0 GHz and 512GB DDR4 DRAM. This work is implemented using C++ and multi-threading parallelized using OpenMP; all numerical operations are performed using double-precision floating point arithmetic and 64-bit unsigned integers. It is compiled using GCC 10.3.0 and Netlib LAPACK 3.8.0 for linear algebra routines.
The polynomial multiplication optimization is performed via full one-dimensional discrete convolution using Fast Fourier Transform (FFT) implementation from FFTW library .
B. Datasets
We tested with eight real-world datasets and four synthetic datasets for more analysis, shown in Table .
Real-world data. The real-world data come from diverse applications with different amount of nodes |V | (ranging from 10 3 to 10 6 ), hyperedges |E| (ranging from 10 4 to 10 6 ), and component adjacency tensors N k (ranging from 22-100). A brief introductory description and reference of the real-world datasets are given in the last two columns. Note that datasets which contain "filtered" in their description are filtered versions of the originals, using the less than or equal to filtering from Landry et al. to remove all hyperedges of size larger than N k .
Synthetic data. The synthetic datasets are random hypergraphs on the same set of nodes with the same total amount of hyperedges. But the hyperedges are of varying sizes, uniformly chosen at random over the vertex set. For each of S1, S2, S3 and S4, the component symmetric adjacency tensor orders were taken to be multiples of five until the hypergraph rank i.e. maximum component adjacency tensor order, with each component tensor containing approximately the same number of hyperedges.
C. Overall performance
We evaluate the performance of both implementations of Algorithm 3, i.e., CCSS-DIRECT that directly computes the polynomial multiplication and CCSS-FFT that substitutes the polynomial multiplication kernel with a FFT convolution approach, and Algorithm 4 (CCSS-MEMO), which memoizes the coefficients to compute S 3 TTVC using a single traversal of the CCSS representation on both categories of datasets.
Comparisons to SOTA methods. In Table , we summarize the speedups of CCSS-DIRECT, CCSS-FFT, and CCSS-MEMO on a single core as compared to the state-of-the-art single-core Python implementation of Algorithm 2 . 3 Our CCSS-MEMO outperforms considerably, providing speedups of 1.3 -18.6 times on the eight real-world datasets, while CCSS-FFT provides considerable improvements as well. CCSS-DIRECT largely underperforms, due to using direct convolution instead  of FFT, with the exception being walmart-trips, where it obtains a speedup of 1.4. Although the timing results in Aksoy et al. were obtained using a different language and system, the speedup observed is almost two orders of magnitude for the largest dataset, amazon-reviews, which suggests that the speedup is due to improvements in the algorithm as opposed to system and implementation differences. Further, as the dominate subroutine (the convolution operation) is implemented using optimized and compiled code (via a Python-wrapper in ) one would expect that much of the performance differences resulting from language choice are mitigated. Furthermore, the strong scaling results seen in Fig. suggest even greater speedups as we increase the number of cores.
Real-world datasets. Fig. presents the runtime performance on 128 cores of these three approaches for the real-world datasets. CCSS-MEMO performs the best on almost all the eight datasets, achieves 1.97 − 53.98× speedup over CCSS-DIRECT and 0.65−12.45× speedup over CCSS-FFT. The variation in speedups across different datasets for CCSS-MEMO is inherent to the hypergraph structure, and is due to the degree of overlap in the hyperedges present in the hypergraph. For the R3 dataset, we can see from Fig. that CCSS achieves very low compression compared to the coordinate format. Thus, there is no significant advantage in Fig. . Overall runtime performance of CCSS-MEMO, CCSS-DIRECT and CCSS-FFT for real-world datasets in Table using the memoization approach to compute S 3 TTVC, which is why we see that CCSS-FFT slightly outperforms CCSS-MEMO for higher thread configurations for this dataset.
Synthetic datasets. Since the synthetic datasets maintain the same number of IOU non-zeros across component uniform hypergraphs, the number of leaf nodes per level of the CCSS that contribute the S 3 TTVC computation is the same. This allows us to inspect the effect of the rank of the non-uniform hypergraph on the performance of all three algorithms. We see from Fig. that sub-coefficient memoization has a significant impact on performance for hypergraphs of increasing ranks. This is to be expected since with increasing tensor order, the reduction in the number of traversals of the CCSS, as well as sharing of sub-coefficients between overlapping hyperedges for a uniform hyperedge distribution, would result in improved performance compared to CCSS-DIRECT and CCSS-FFT. . Across all datasets, we see that the CCSS-MEMO approach is faster than both the baseline approaches. The dashed lines indicate the ideal speedup lines for each of the three algorithms. CCSS-DIRECT and CCSS-FFT do not use memoization for the intermediate ĒN (v) computations. The CCSS-DIRECT algorithm shows the best scalability of the three approaches, while both CCSS-FFT and CCSS-MEMO show decreasing scalability with increasing number of threads. For both of these approaches, as the work done per thread reduces (in terms of optimized FFT subroutines in FFTW for CCSS-FFT and in terms of coefficient W memoization for CCSS-MEMO), the overhead in the atomic operation becomes more significant, especially for the hypergraphs with smaller number of nodes, and this manifests as suboptimal scaling. Moreover, the scalability of CCSS-MEMO is also affected by the load imbalance between threads due to the varying number of IOU non-zeros across trees within the CCSS structure.
E. CCSS Construction
CCSS construction. While the CSS compressed only in terms of overlapping IOU non-zeros within a symmetric adjacency tensor, the CCSS adds another layer of compactness in terms of shared indices between IOU non-zeros across multiple symmetric tensors. Moreover, for computing S 3 TTVC on the blowup tensor using the CCSS, we can further prune paths if the leaf node of the path does not own any IOU non-zeros. Fig. shows the size of the constructed CCSS for representative synthetic and real-world datasets, while Fig. (b) examines the ratio of the amount of time spent in the construction of CCSS to the runtime of CCSS-MEMO for S 3 TTVC computation.
F. H-eigenvector centrality computation speedups
CCSS-MEMO provides a practical framework to compute centrality in large hypergraphs. We compute tensor H-eigenvector centrality using Algorithm 1 on the largest real-world dataset in Table -the amazon-reviews -on 128 cores. CCSS-MEMO obtains speedups of 6.49× and 3.53× over CCSS-DIRECT and CCSS-FFT respectively, which shows the applicability of this work in the analysis of real-world hypergraphs.