IV. S 3 TTVC COMPUTATION
This work adopts the generating function approach outlined in the AAY algorithm for computing S 3 TTVC in parallel using the CCSS data structure. We present two algorithms -a baseline approach given in Algorithm 3 which directly parallelizes the AAY approach given in Algorithm 2 and uses the CCSS to store the hypergraph and an optimized version in Algorithm 4 which rearranges the computations of the AAY approach in order to leverage the CCSS to reduce the overall computation by memoization of intermediate results.
For both of these approaches the focus is on calculating, for every pair e ∈ E and v ∈ e, the last entry of the convolution in the convolution of lists E N (v) and {E N (b u )} u∈e\v . For the baseline algorithms, we consider two different methods of computing this convolution; an in-place shift-and-multiply approach and a more efficient approach (but with a larger memory footprint) based on the Fast Fourier Transform (FFT) . In Algorithm 4, we only consider a variant of the shiftand-multiply convolution because of the memoization approach used.
Algorithm 3 S 3 TTVC using CCSS via generating function.
Input: Non-uniform hypergraph stored in CCSS, b Output: S 3 TTVC output, s = Bb N −1 1: for = N, . . . , 1 do 2:
parfor (e, v) ∈ L do //CCSS 3:
coefs = E N (b v ) 4: u = v 5: for = − 1, . . . , 1 do 6: u = parent(u) / / CCSS 7: coefs = E N (b u ) * coefs 8:
end for
9: AtomicAdd s v , (N −1)! |β(e)| coefs[N − 1] 10:
end parfor 11: end for
Note that Algorithm 3 iterates over the "special" leaves in the CCSS structure by level and for each of these leaves moves up through the CCSS forest to a root of one of the subtrees. This leaf-to-root traversal would make any memoization approach inefficient and challenging to implement as the repeated calculations occur at different locations in the CCSS. For example, in Fig. there are several repeated paths (for example, 4, 6 with a special leaf for vertex 1 appears in the trees rooted at 3 and at 4, or 7 with a special leaf for vertex 5 appears in trees rooted at 4 and at 7), however as these calculations occur at different nodes and different levels in the CCSS forest it is challenging to realize the benefits of memoization. This observation inspires our development of a root-to-leaf traversal of the CCSS structure, detailed in the next subsection.
Algorithm 4 Memoized S 3 TTVC using CCSS and generating function.
Input: Non-uniform hypergraph stored in CCSS, b Output: S 3 TTVC output, s = Bb N −1 1: For each processor allocate sub-coefficient memoization workspace W of size R (N −1)×(N −1) . 2: parfor v = 1, 2, . . . n do for i = 1, 2, . . . , j do 5:
W ij ← 1 (j−i+1)! b j−i+1
for u ∈ S(v) do 12: coefs = 1, b u , b 2 u 2! , . . . , b N −1− u (N −1− )! 13: AtomicAdd s u , (N −1)! β( ) coefs T W N −1− 14:
end for 15:
for u ∈ children of v do for q = 1, 2, . . . , p do 18:
Z pq ← q−p c=0 1 c! b c+1 u W p,q−c 19:
end for 20:
end for 21:
DFS(u, Z, + 1)
end for 23: end function
In this approach, we optimize the traversal of the CCSS forest by using a depth-first search to traverse each tree independently with a separate memoization space, W . After the algorithm has processed a node v on level which has path to the root
v 1 , v 2 , . . . , v = v, the k th column of W , denoted W k , stores the portion of the convolution of {E N (b v i )} k
i=1 with degree at most + k − 1. Furthermore, if S(v) is non-empty for a vertex v at level , for any u ∈ S(v) the contribution of to s u can be found by taking the dot product of column W N −1− with the terms from E N (b u ) of degree at most N − 1 − . The pseudocode for this approach is given in Algorithm 4.
To illustrate the memoization approach, consider the traversal of the subtree rooted at 1 in Figure . This tree will have 3 workspaces associated with it W 1 , W 2 , W 3 associated with each level of the tree. Workspace 1 will always contain the information necessary to construct the generating function ĒN (1), while W 2 and W 3 will contain the information necessary to construct the generating function for the convolutions of ĒN for v 1 , v 2 and v 1 , v 2 , v 3 , where v 1 , v 2 , v 3 is the path to the current node in the depth-first traversal of the tree. After the unique child of the path (1,2,8) has been computed, the next node in the depth-first traversal is vertex 3 as a child of the root (vertex 1). The updated W 2 can be computed directly from the information in W 1 while updated W 3 is delayed. Now, when traversing the children of vertex 2 (namely 4, 6, and 8) the appropriate W 3 can be computed directly from W 2 without recomputing the convolutions ĒN (1) * ĒN (2). This convolution has been effectively memoized for future computations in W 2 .
We note that for readability of Algorithm 4 we have suppressed the use of several easy optimizations. For instance, if all the edges associated with special leaves in a tree have size at least k, then the number of rows W can be reduced to N − k + 1 as the higher order terms in E N are irrelevant to the final output. Similarly, if the maximum size of an edge associated with a tree is m, then the number of columns of W can be reduced to m − 1 as the longest path to be tracked has size m. The memoization workspace is allocated per processor, which stores the W matrix of convolution operations. Moreover, note that W is a square uppertriangular Toeplitz matrix, which brings down memory costs to O(N ) 2 , since each vertex needs to update only a vector of length N . Finally, the CCSS forest can be trimmed to eliminate trees, or subtrees, which have no special vertices attached.
We compare the computational complexity of Algorithm 3 and Algorithm 4. For Algorithm 3, every special leaf corresponding to (e, v) requires one traversal from the special leaf to the root, and thus total number of convolutions computed is e∈E |e| 2 −|e|. However, in Algorithm 4 every edge of the CCSS forest corresponds to a convolution which is computed exactly once. In particular, the number of convolutions is one less than the number of nodes in the CCSS forest. While the precise speedup resulting from avoiding the extra convolutions is heavily dependent on the structure of the hypergraph, as we will show in Sec. V, in most real-world cases this results in an order of magnitude saving in runtime.
In spite of the difficulty in determining the exact number of convolutions saved by using the CCSS structure, we can still identify key substructures that lead to significant performance benefits; namely nested families Fig. . Hypergraph sub-structures in which the memoization of Algorithm 4 significantly improves the performance over that of Algorithm 3. On the left is the sunflower hypergraph where many edges share a common intersection, and on the right is the nested hypergraph where edges satisfy a containment relationship. of edges and "sunflowers" (collections of edges with a shared intersection), see Fig. . For example, with a nested series of edges e 1 ⊂ e 2 ⊂ • • • ⊂ e k , Algorithm 3 will require the computation of k i=1 |e i | 2 −|e i | convolutions without memoization. In contrast, the optimal CCSS tree will yield require only
|e k | 2 −|e k |−2 2 + k i=1 |e i | convolutions.
Similarly, if we consider a sunflower consisting of m edges of size k with a common intersection of size t, we can see that the non-memoized computation will require m(k 2 −k) convolution calculations while the memoized version requires
(k − t − 1) 2 + (k − t − 1) 2 + (t − 2)t + (k − t)m + km convolutions.
In fact, even for a single edge of size k, the non-memoized computation requires at k 2 − k convolutions while the memoized version will require
k 2 −k 2 .
Thus the memoization decreases by a factor of at least 2 times the number of convolution operations necessary to evaluate S TTVC.