1 Introduction
Large language models, including closed-source models like ChatGPT and , as well as open-source models such as LLaMA , have exhibited remarkable performance in a wide range of tasks . Such success is largely due to diverse techniques explored to enhance the pre-trained LLMs in downstream tasks. Among those methods, fine-tuning is a common strategy and has been thoroughly investigated in the literature. It mainly focuses on designing better fine-tuning methods from the algorithmic side , or constructing more effective datasets from the data side . For example, the Parameter-Efficient Fine-Tuning (PEFT) methods stand out among various fine-tuning methods due to their cost-efficient nature and impressive performance. Concurrently, a multitude of highquality, manually annotated datasets have been constructed, as an increasingly focal technique aimed at aligning the outputs of LLMs with human-like responses .
Despite their remarkable success, existing fine-tuning works rarely investigate the inherent knowledge acquisition of fine-tuned LLMs. Recent works indicate that the outputs of pre-trained LLMs do not always accurately reflect the knowledge they possess. Even if a model generates an incorrect output, it may still possess correct knowledge. For example, suggests that the intermediate representations in pre-trained LLMs might be correct, even if the final output is erroneous. finds that pretained LLMs can self-assess their correctness. Therefore, we pose a question: How can we leverage such inherent knowledge in the fine-tuned LLMs to enhance their performance in downstream tasks? Intuitively, the inherent knowledge within fine-tuned LLMs should be reflected by their token-predicting behavior alterations during the fine-tuning process. We argue that such token-predicting behavior alterations indicate an adaptive shift from common knowledge gained via pretraining to specific knowledge for downstream tasks. And we can improve the adaptation of LLMs on downstream tasks by manually mining and leveraging such inherent knowledge, regardless of the fine-tuning methods.
In this paper, we propose a novel Task-aware Decoding (TaD) method, which takes advantage of the differences in knowledge before and after fine-tuning to enhance the adaptation of LLMs on downstream tasks. Firstly, we formulate the knowledge difference as a knowledge vector to explicitly denote the direction of knowledge adaptation (or domain adaptation) learned by a pre-trained LLM during fine-tuning for a downstream task. As illustrated in Figure , though the fine-tuned LLM (i.e., after fine-tuning) outputs the
Training
Knowledge Vector
Task-Aware Decoding
+
--
+ + -- + Input Output
Could you provide a professional explanation of photosynthesis?
engage in necessary chemical reactions to produce food. This process is fundamental for plant growth and development.
engage more actively in transforming water and carbon dioxide. This process encompasses a series of complex biochemical reactions.
catalyze the conversion of water and carbon dioxide into oxygen and glucose. This involves chlorophyll absorbing photons efficiently.
Figure : An illustration of the proposed knowledge vector and task-aware decoding (TaD), where LLMs are asked for a professional explanation of photosynthesis. The underlined tokens are examples to illustrate the professional levels of the outputs of pre-trained, fine-tuned, and TaD-enhanced fine-tuned LLMs, and the corresponding probability distributions are displayed on the left. The proposed TaD enhances the predicted probability distribution of a token with the knowledge vector, and thus amplifies the knowledge learned during fine-tuning and achieves superior performance in downstream tasks.
same token "engage" as the pre-trained LLM (i.e., before fine-tuning) for the next token, the shift of the corresponding predicted probability distributions from the latter to the former implicitly reflects how the LLM adapts its knowledge to the downstream task during fine-tuning. Specifically, the predicted probability w.r.t the more professional token "catalyze" increases, while that w.r.t the less professional token "engage" actually decreases. Therefore, here we construct the knowledge vector based on the difference of output probability distributions w.r.t tokens from the pre-trained and the fine-tuned LLMs, which also reveals the output token preferences for downstream tasks. Given that tokens are inherently semantic, the knowledge vector naturally possesses semantic information. We then implement our proposed TaD by enhancing the fine-tuned LLM's output probability distribution with the knowledge vector, thereby reinforcing the model's knowledge adaptation to downstream tasks for better performance. Figure shows that, with TaD to enhance the output probability distribution with the knowledge vector, the fine-tuned LLM can then generate a more professional next token "catalyze", instead of "engage". In that manner, the enhanced LLM can finally yield a better answer with more professional words, including "catalyze","oxygen","glucose","chlorophyll", etc. The proposed TaD is a plug-and-play decoding method, and extensive experiments well demonstrate that it can enhance the performance of various fine-tuned LLMs in many downstream tasks. In summary, our contributions are as follows: 1. To enhance the adaptation of LLMs to downstream tasks, we propose a concept of knowledge vector, which explicitly denotes the knowledge adaptation learned by LLMs during fine-tuning. 2. We further develop TaD to enhance fine-tuned LLMs' output probability distribution w.r.t tokens with the knowledge vector, and enhance their performance in downstream tasks. 3. We conduct extensive experiments to validate the effectiveness of TaD across various tasks, models, and finetuning methods. Experimental results well demonstrate its superiority over baselines and promising potential in data-scarce scenarios.
2 Related Work , which incurs significant costs. In contrast, some studies construct high-quality training datasets by maximizing the utilization of existing datasets. For example, enriches the training instances by inverting inputoutput pairs of existing instances with specially crafted task descriptions. augments labeled datasets with human-written task descriptions to instruct LLMs to understand the tasks. Aside from those efforts, there are initiatives that enhance LLMs by focusing on their intrinsic features. employs post-pretraining activation editing to modulate LLM behaviors. demonstrate that steering vectors, whether trained or manually selected, effectively facilitate style transfer in LLMs. ITI enhances LLMs' performance in downstream tasks by discovering vectors for activations based on positive and negative instances. Our TaD follows a similar direction. But it opts for the differences in output probability distributions w.r.t tokens as the direction of knowledge adaptation, and derives a knowledge vector to enhance the performance of fine-tuned LLMs.