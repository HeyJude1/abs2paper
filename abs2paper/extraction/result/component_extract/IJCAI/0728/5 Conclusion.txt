5 Conclusion
In this paper, we introduce TaD, a plug-and-play method to enhance the performance of fine-tuned LLMs in downstream tasks. TaD leverages the differences in output probability distributions w.r.t tokens before and after fine-tuning, to construct the knowledge vector for downstream tasks. Then TaD refines the output probability distribution of the fine-tuned A Experimental Results with Full-Parameter Fine-Tuning
We conduct experiments with full-parameter fine-tuning (FPFT) on the multiple-choice tasks. As shown in Table , the experimental results demonstrate that TaD achieves comparable and even more significant improvement compared to being applied to PEFT models. This further demonstrates that TaD can achieve performance that cannot be obtained with the distribution of training data.
B The Difference in Improvements on the Multiple-Choice Tasks
To investigate the difference in improvements observed in Table 6 across other tasks, we conduct experiments using the LLaMa series models on the multiple-choice tasks with 50% and 100% of the training data, respectively. As shown in Table 8, we observe a consistent pattern with that in
C Standard Deviations of Experiments
Table shows the standard deviations of the performance of fine-tuned models, TaD-enhanced fine-tuned models, and the improvement after applying TaD over 5 runs, respectively. Figure correspondingly displays a bar graph of the mean value of the improvement with standard deviation error bars. It is noted that the standard deviation of the improvement is smaller than that of the single model's performance. Concurrently, we observe that although the performance of finetuned models and TaD-enhanced models varies across different runs, TaD guarantees the improvement upon the finetuned models' performance. The experimental results above well demonstrate the robustness of our proposed TaD.