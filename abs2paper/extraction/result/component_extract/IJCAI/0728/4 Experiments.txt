4 Experiments
4.1 Setup
Models and Datasets
For our experiments, we select three remarkable LLMs: LLaMA (including LLaMA-7b and LLaMA-13b) , GPT-J (6b) and BLOOMz (7b) , and focus on two main categories of downstream tasks: multiple-choice tasks and open-ended generation tasks.
For multiple-choice tasks, we employ the commonly used TruthfulQA benchmark, following previous works . In this benchmark, questions are concatenated with correct or incorrect answers and fed into the model. Then the model's performance is evaluated with MC (multiple-choice accuracy) based on the predicted probabilities for each question-answer pair. Specifically, TruthfulQA involves three specific MC scores, i.e., MC1, MC2, and MC3, for different experimental setups, and higher MC1/2/3 scores indicate better model performance.
For open-ended generation tasks, we focus on three subtasks: closed-book question answering, mathematical reasoning, and commonsense reasoning. For closed-book question answering, we use TruthfulQA, where the model's input and output are, respectively, given questions and generated answers. The answers are evaluated for Truthfulness and Informativeness using Fine-Tuning Methods and Implementation Details PEFT are widely adopted for LLM fine-tuning, offering computational efficiency and comparable performance to full-parameter fine-tuning. In our experiments, we employ four PEFT methods to incorporate the proposed TaD, i.e., LoRA , AdapterP , AdapterH and Parallel Adapter . Specifically, our fine-tuning settings are based on LLM-Adapters , which provides comprehensive insights on applying PEFT to LLMs. We align our fine-tuning hyper-parameters with the study.
For TruthfulQA, we adopt a 2-fold cross-validation strategy following ITI , ensuring no test data leakage, and we perform fine-tuning with only <Question, Best Answer> pairs. In mathematical and commonsense reasoning, we utilize Math10K and Commonsense170K datasets from LLM-Adapters for fine-tuning, followed by evaluations on GSM8K, MultiArith, BoolQ, and PIQA.  Moreover, we set α in Eq. 5 and λ in Eq. 7 by default to 0.1 and −∞, respectively, without additional tuning. For TruthfulQA, we set the weighting parameter µ in Eq. 9 to 0.8 empirically for all setups. For mathematical and commonsense reasoning tasks, we select the optimal µ based on the model's performance on the training sets of GSM8K and BoolQ, respectively, because we utilize Math10K and Com-monsense170K rather than them for fine-tuning. Then the selected µ on GSM8K is applied to MultiArith, and similarly, the selected µ on BoolQ is applied to PIQA, so as to see the transferability of µ from one dataset to another in the same task category. Unless explicitly stated, all experiments employ Greedy Search, which is a commonly used and time-efficient decoding strategy adopted in many previous works .
4.2 Main Results
Results on Multiple-Choice and Closed-Book Question Answering Tasks As shown in Table , TaD consistently improves fine-tuned LLMs' performance across different models and PEFT methods. The experimental results well demonstrate its general applicability. Moreover, given that the metric MC1 focuses on only Best Answer while MC2/MC3 also considers other Correct Answers, we can see that, despite our training data comprising only <Question, Best Answer> pairs, the alignment to the knowledge vector introduced by TaD also improves both MC2 and MC3.
Results on Mathematical Reasoning and Commonsense Reasoning Tasks
Table illustrates the performance of our method on more challenging reasoning tasks. Without loss of generality, we just present LoRA and AdapterP's results here. The results show that TaD exhibits substantial improvement across various mathematical reasoning datasets and models, and exceeds the performance of fine-tuned models in commonsense reasoning datasets.
Compared to commonsense reasoning tasks, TaD gets larger performance gains in mathematical reasoning tasks. It can be attributed to the characteristics of the latter's outputs, which are usually longer and semantically dense. That enhances TaD's benefits in iterative generation and reasoning. In contrast, the commonsense reasoning benchmarks typically require outputs formatted like "the correct answer is which generally have limited semantic information. And TaD's effectiveness primarily lies on the single token [N], which is limited and leads to smaller performance gains. Nonetheless, TaD still yields considerable improvements for commonsense reasoning tasks in nearly all cases, as shown in Table .
Comparison With Other Contrastive Decoding Strategies
We compare TaD with other baselines that do not consider the knowledge adaptation for downstream tasks during finetuning. Our experiments are conducted on TruthfulQA for multi-choice tasks and on mathematical reasoning datasets for open-ended generation tasks. Specifically, we compare TaD with CD and .
Following DoLa, we conduct comparisons on LLaMA-7b and LLaMA-13b. And for CD, we treat the fine-tuned LLaMA-7b as an amateur and the fine-tuned LLaMA-13b as an expert. For fairness, we apply both CD and DoLa on finetuned models, and we carefully tune their hyper-parameters to yield the best results for comparisons. Specifically, for TruthfulQA, considering the training data comprises only <Question, Best Answer> pairs, we select the optimal DoLa interval and hyper-parameters based on MC1 scores using a 2-fold cross-validation strategy.
The comparison results, as shown in Table , indicate that TaD outperforms the baselines in most cases. Specifically, TaD exhibits a significant advantage over CD and DoLa in maintaining or improving fine-tuned LLMs' performance in various tasks. Actually, CD or DoLa can even degrade the performance of the original fine-tuned LLMs. For example, on TruthfulQA, though CD's performance surpasses that of TaD in terms of MC1 under the "LLaMA-13b + LoRA" setup, Table : Ablation study results of the knowledge vector on LLaMa models. The column M and pE − → pS represent the models used for calculating p ϕ in Eq. 5, Eq. 9 and the logarithm of the distribution in Eq. 4, respectively. A / in column pE − → pS indicates the independent performance of model M without applying TaD. Models marked with an * signify those fine-tuned with LoRA on Math10K. G and M represent GSM8K and MultiArith, respectively. We highlight our method with a gray background.
M p S − → p E G / M 7b /
it leads to a significant decline in terms of MC2 and MC3. In contrast, TaD consistently maintains and mostly improves the fine-tuned LLMs' performance in all MC1/2/3 metrics. On GSM8K and MultiArith, the decrease in CD's performance is particularly pronounced, which well aligns with the results reported in DoLa. In contrast, TaD still gains performance improvement upon fine-tuned LLMs. Overall, TaD generates better results in various downstream tasks by considering the knowledge adaptation learned during fine-tuning and consistently maintains and improves the fine-tuned LLMs' performance.
4.3 Analysis Integrated With Different Basic Decoding Strategies
As previously mentioned, our derived p in Eq. 9 is compatible with various basic decoding strategies. fine-tuned LLMs' performance across different basic decoding strategies on both mathematical reasoning datasets.
Ablation Study of the Knowledge Vector
Table displays the ablation study results of the knowledge vector on the more challenging mathematical reasoning task. We conduct extensive experiments by defining different directions of the knowledge vector to demonstrate the effectiveness and reasonableness of the proposed one. Firstly, we present the pre-trained and fine-tuned LLMs' performance and showcase the effectiveness of the proposed TaD. Table shows that the performance of the finetuned LLaMA-7b* and LLaMA-13b* is significantly improved compared to the original pre-trained LLaMA-7b and LLaMA-13b. Table (b) further shows that the proposed TaD consistently enhances the performance of the fine-tuned LLaMA-7b* and LLaMA-13b* by setting the direction of the knowledge vector as 7b− →7b* or 13b− →13b*. To validate our motivation in setting the knowledge vector's direction, we further investigate the effect of reverting the original knowledge vector, i.e., from the fine-tuned LLM to the pre-trained LLM, as shown in Table (c). We can see that reverting the direction of the proposed knowledge vector significantly degrades the fine-tuned LLaMA-7b*'s performance, which is consistent with our illustration in Figure .
Furthermore, we find that the direction from the smaller LLMs to the larger LLMs can also improve the pre-trained or fine-tuned LLMs' performance, as can be seen in Table . Therefore, we further conduct experiments to compare the knowledge vector derived from knowledge adaptation (i.e., from the pre-trained LLMs to the fine-tuned LLMs) and that derived from model size increase (i.e., from the smaller LLMs to the larger LLMs). The results are reported in Table (e), and we can see that the former outperforms the latter. It suggests that the direction indicated by the knowledge adaptation learned during fine-tuning is more essential in deriving the proposed knowledge vector. Moreover, we investigate the cumulative effect of combining these two kinds of directions, by setting the direction of knowledge vector from the pre-trained LLaMA-7b to the fine-tuned LLaMA-13b*. Table (f) shows that combining both directions can gain slight or even negligible further improvement over the default direction from LLaMA-13b to LLaMA-13b*, which further demonstrates the reasonableness of how we define the direction of the proposed knowledge vector. displays the performance of models fine-tuned with LoRA using different ratios of training data and highlights TaD's improvement over the original fine-tuned models. For each setup, we select the optimal hyper-parameters and train the LLMs for sufficient epochs to ensure convergence. As shown in Table , we can see that, for smaller ratios of the training data, the proposed TaD yields larger performance gains.
Furthermore, we conduct an ablation study on the selection of µ, and present the results in Figure . It can be observed that the optimal μ incrementally increases as the ratio of training data decreases, meaning that the fine-tuned LLMs prefer stronger alignment with the knowledge vector for smaller ratios of training data.
Intuitively, with less training data, the LLMs' output probability distribution w.r.t tokens is far from reaching the final training objective in Figure . Therefore, starting from a more distant point, moving in the direction of the knowledge vector can strengthen the knowledge adaptation for the downstream tasks and lead to more significant improvements. Interestingly, the results above also indicate that even with limited training data, the model can be guided in the correct direction. Such a finding well highlights our method's effectiveness in data-limited scenarios.