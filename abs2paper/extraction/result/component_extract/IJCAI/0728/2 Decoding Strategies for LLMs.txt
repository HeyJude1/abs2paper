2 Decoding Strategies for LLMs
Decoding strategies are critical for LLMs, which significantly affect the generation quality. Basic decoding strategies include Greedy , , and Top-p (Nucleus) Sampling . Greedy Search always selects the most probable next token, one by one. While efficient, it tends to yield repetitive text. In contrast, Beam Search maintains and expands multiple hypotheses for superior sequences. Top-k Sampling, choosing from highly probable tokens, injects diversity but may compromise coherence. Building on that, Top-p Sampling dynamically adjusts the selection pool based on the cumulative probability, targeting at a balance between randomness and coherence for more engaging outputs.
Additionally, there exists a range of incremental works, which can be integrated with and effectively optimize those basic strategies. Within this scope, the contrastive decoding series serves as a prime example. CD utilizes the likelihood difference between a large language model and a small one to produce higher-quality texts. Furthermore, ACD improves upon CD by maximizing log-probability differences across different layers in a single model. DoLa ] extends ACD's principles, integrating Jensen-Shannon divergence to dynamically choose layers for contrastive decoding. Those decoding strategies, however, are primarily focused on the performance of pre-trained LLMs, neglecting the changes in models after fine-tuning. Consequently, they fail to utilize the knowledge adaptation learned during fine-tuning, resulting in limited performance improvement or even performance decline in downstream tasks when applied to fine-tuned LLMs. On the contrary, our proposed TaD focuses on fine-tuned LLMs, and we define a knowledge vector to denote knowledge adaptation for downstream tasks. TaD is a plug-andplay method that can be natively integrated with those basic decoding strategies above to enhance the performance of finetuned LLMs in downstream tasks.