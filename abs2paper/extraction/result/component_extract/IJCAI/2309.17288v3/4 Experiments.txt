4 Experiments
In order to demonstrate the capabilities and performance of AutoAgents in orchestrating autonomous agent groups to collaboratively accomplish tasks, we have performed extensive quantitative experiments on benchmark tasks and thorough case studies on more complex and realistic applications. In the quantitative analysis, we mainly present results for the Open-ended Question Answer task (detailed in Section 4.1) and the Trivia Creative Writing task (detailed in Section 4.2) to evaluate the framework effectiveness under distinct settings. The Case Studies, discussed in Section 4.4, illustrate the potential of a multi-agent group tackling intricate practical scenarios cooperatively.
We conduct all experiments using the GPT-4 API and set the temperature to 0 to ensure reproducibility. The rationale behind this selection is the exceptional performance these models offer, providing more accurate and standardized output. Additionally, their accessibility and ease of use through APIs enable us to directly call and interact with the models during our research, significantly simplifying the process. The maximum number of discussions during the drafting phase is 3, and the maximum number of self-refinement by a single agent and collaborative refinement by multiple agents during the execution phase is 5.
4.1 Open-ended Question Answer
Task Description. Open-ended Question Answering is a crucial and challenging task in the domain of NLP and generative AI. It requires an AI system to produce coherent, elaborate, and human-like responses to questions that have no predetermined or fixed set of possible answers. proposed MT-bench, a benchmark consisting of 80 high-quality collected open-ended questions from various categories such as common sense, counterfactual, coding, etc. We then utilize AutoAgents to produce collaborative answers based on multiple generated agents and compare them with the responses given by Vicuna-13B, ChatGPT, and GPT-4. Evaluation Metrics. To measure the quality of open-ended responses with minimal evaluation bias, we adopt FairEval and HumanEval as the evaluation metrics for both the single agent and AutoAgents. FairEval incorporates several methods to mitigate the impact of various sources of bias, resulting in a better alignment with human judgment. For HumanEval, we enlist several volunteers to rate the responses from different models based on their helpfulness, reliability, accuracy, and level of detail.
Results.
4.2 Trivia Creative Writing
Task Description. The Trivia Creative Writing task challenges the capabilities of large language models to retrieve and integrate diverse information from their internal self-compressed knowledge. This task requires a model to craft a coherent story around a given topic while incorporating the answers to N trivia questions. We evaluate the models under two settings, N = 5 and N = 10, where a higher N entails more trivia questions and thus demands the model to exhibit more extensive domain knowledge. We constructed a benchmark consisting of 100 instances for each N , encompassing a total of 1000 trivia questions. Evaluation Metrics. Drawing on the approach of , we adopt an automatic metric to identify factual errors and measure a model's capacity to integrate diverse domain knowledge. We conduct string matching with the veridical target answers for each question on the generated output. The target answers are supplied from the TriviaQA dataset , and each question can have a list of answer variants. A match to any of the answer variants of a question is regarded as a correct mention. The metric score is calculated as Trivia Creative Writing Metric Score = # correct answer mentions/# trivia questions.
Results. Table demonstrates the superior performance of AutoAgents in knowledge acquisition over the existing methods. Compared to the Standard method, which does not employ Agent Generation, AutoAgents achieves a remarkable 10% improvement across all experiments. Moreover, AutoAgents also surpasses SSP , which utilizes agent generation but with a different approach. The enhanced performance of AutoAgents can be attributed to its elaborate methods of agent generation discussions and task execution including collaborative refinement and self-refinement. More examples are given in the Appendix A.
4.3 Further Analysis
This section delves into the significance of key components within AutoAgents by separately analyzing the self-refinement action, collaborative refinement action, dynamic memory, and observers in the draft stage across 20 instances of the Trivia Creative Writing task and additional case studies. 66.0 -11.5% SPP-Profile 74.0 -0.01% SPP 84.4 +13.1% Collaborative discussion is crucial for rational agent generation and plan allocation. During the Drafting Stage, the Planner in AutoAgents engages in collaborative discussions with two Observers to determine the optimal list of agents and the execution plan. Figure illustrates the contrast between agent generation with and without collaborative discussion. In the absence of Observer feedback, the Planner tends to generate programmers exclusively to accomplish game development, neglecting the holistic process of game creation. With the input and coordination of the Observers, the Planner incorporates game design experts, UI design experts, and testing experts into the agent list. It is evident that the agent generation under collaborative discussions is more comprehensive and more aligned with the realistic scenarios of game development. This also corroborates the significance of collaborative discussions for agent generation and plan allocation, which will subsequently influence the execution outcomes. Concurrently, Table elucidates that in the absence of observers, there is a marked 3% reduction in the overall performance of AutoAgents. This substantiates the imperative role of collaborative discussions in agent generation. AutoAgent markedly enhances the caliber of agent generation via collaborative discussions, a facet notably overlooked by other generative frameworks in their consideration of agent generation quality. The empirical data presented in Table and 3 further accentuate the superiority of AutoAgents when juxtaposed against counterparts like AgentVerse and SPP.
Enhancing single-agent through self-refinement. Self-Refinement is a technique that enables LLMs to "converse" with themselves, evaluate their own generation, and iteratively improve their answers. Self-refinement has been shown to enhance the accuracy of LLMs' outputs in various domains . Although AutoAgents is a framework for multi-agent  collaboration, it also requires self-refinement agents to perform specialized roles for individual tasks.
As shown in the results in Table , the performance of AutoAgents decreases by 3% in the absence of the self-refinement action. This observation corroborates the assertion that self-refinement is instrumental in augmenting proficiency in trivia creative writing tasks. Furthermore, the enhancement of single agents via self-refinement plays a pivotal role in fortifying the integrity of the overarching multi-agent framework.
Enhancing multi-agent collaboration through collaborative refinement action. For collaborative refinement, the process resembles the collaborative dialogue mentioned above, which involves integrating knowledge from different domains to accomplish tasks that demand cross-domain knowledge fusion. The results in Table demonstrate the performance when collaborative refinement is absent. It's observable that compared to the scenario with AutoAgents, there is a decline of 2%. Since the necessity for multiple agents to collaborate on a single task is entirely dependent on the decision made by the agent in the drafting phase, not all problems necessarily involve tasks that require collaborative refinement. However, it's evident that when this principle is omitted from the prompt's design, there's a noticeable performance decrease.
Improve the effectiveness of actions by dynamic memory. Dynamic memory predominantly addresses the requisites of specialized agents. As shown in Figure , the Action Observer amalgamates pivotal data for forthcoming tasks, utilizing the historical action records archived in long-term memory. Table elucidates a 1% diminution in the efficacy of AutoAgents bereft of dynamic memory. Quintessential insights derived from dynamic memory are assimilated into the prompt, thereby augmenting the comprehension of critical information and bolstering the operational proficiency of actions.
4.4 Case Study
To demonstrate the applicability of AutoAgents to more sophisticated and realistic scenarios, we conduct a case study in the software engineering domain. Software engineering is a complex collaborative endeavor that involves diverse roles and responsibilities. From developers who create the underlying code, to UI designers who prioritize user experience, and software testers who ensure the software's quality, experts collaboratively work to enhance and refine the application, ensuring that it meets both functional and user-centric criteria.
As an illustration in Figure , a Tetris game has been developed by employing AutoAgents, which has generated various expert roles, such as game design expert, UI design expert, programmer, and debugging expert, to accomplish the game development task. The game design experts provide the game logic documents that specify the rules and mechanics of the game. The UI design experts design the UI components that create the visual interface of the game. The programmers implement the game design based on the aforementioned documents and use appropriate programming languages and tools. Finally, the debugging expert tests the game and debuges the program to ensure its functionality and quality. The game development process is based on the collaboration of multiple expert roles, with more elaborate documentation and programs, making it easier for users to comprehend.
4. Creation of Detailed Execution Plan:
• Develop a comprehensive plan with multiple steps addressing the problem.
• Assign at least one expert role to each step, detailing their contributions and collaborations.
• Provide detailed descriptions for each step, including expected outputs and inputs for subsequent steps.
• Include a final independent step for a language expert to provide a detailed response to the user's question.
• Present the execution plan as a numbered list, indicating the expert roles involved in each step.
This structured approach ensures a systematic and detailed resolution of tasks, leveraging the specialized expertise of various LLM agents.
[ Prompt]Planner Here is an example of a valid JSON blob: {{{{ "name": "ROLE NAME", "description": "ROLE DESCRIPTONS", "tools": ["ROLE TOOL"], "suggestions": "EXECUTION SUGGESTIONS", "prompt": "ROLE PROMPT", }}}} 4. Finally, based on the content of the problem/task and the expert roles, provide a detailed execution plan with the required steps to solve the problem. 4.1. The execution plan should consist of multiple steps that solve the problem progressively. Make the plan as detailed as possible to ensure the accuracy and completeness of the task. You need to make sure that the summary of all the steps can answer the question or complete the task. 4.2. Each step should assign at least one expert role to carry it out. If a step involves multiple expert roles, you need to specify the contributions of each expert role and how they collaborate to produce integrated results. 4.3. The description of each step should provide sufficient details and explain how the steps are connected to each other. 4.4. The description of each step must also include the expected output of that step and indicate what inputs are needed for the next step. The expected output of the current step and the required input for the next step must be consistent with each other. Sometimes, you may need to extract information or values before using them. Otherwise, the next step will lack the necessary input. 4.5. The final step should always be an independent step that says 'Language Expert: Based on the previous steps, please provide a helpful, relevant, accurate, and detailed response to the user's original question: XXX'. 4.6. Output the execution plan as a numbered list of steps. For each step, please begin with a list of the expert roles that are involved in performing it. the primary responsibility or objective that the expert role aims to achieve. 3.6. You should specify any limitations or principles that each new expert role must adhere to when performing actions. These are called constraints and they must be consistent with the problem requirements and the domain of expertise. 3.7. You should select the appropriate tools that each new expert role needs to use from the existing tool set. Each new expert role can have multiple tools or no tool at all, depending on their functions and needs. You should never create any new tool and only use the existing ones. 3.8. You should provide some helpful suggestions for each new expert role to execute the task effectively and efficiently. The suggestions should include but not limited to a clear output format, extraction of relevant information from previous steps, and guidance for execution steps. 3.9. You should create a prompt template for calling each new expert role according to its name, description, goal, constraints, tools and suggestions. A good prompt template should first explain the role it needs to play (name), its area of expertise (description), the primary responsibility or objective that it aims to achieve (goal), any limitations or principles that it must adhere to when performing actions (constraints), and some helpful suggestions for executing the task (suggestions). The prompt must follow this format: "You are [description], named [name]. Your goal is [goal], and your constraints are . You could follow these execution suggestions: [suggestions].". 3.10. You should always have a language expert role who does not require any tools and is responsible for summarizing the results of all steps in natural language. 3.11. You should follow the JSON blob format for creating new expert roles. Specifically, The JSON of new expert roles should have a 'name' key (the expert role name), a 'description' key (the description of the expert role's expertise domain), a 'tools' key (with the name of the tools used by the expert role), a 'suggestions' key (some suggestions for each agent to execute the task), and a 'prompt' key (the prompt template required to call the expert role). Each JSON blob should only contain one expert role, and do NOT return a list of multiple expert roles. Here is an example of a valid JSON blob: {{{{ "name": "ROLE NAME", "description": "ROLE DESCRIPTONS", "tools": ["ROLE TOOL"], "suggestions": "EXECUTION SUGGESTIONS",
The design principles for the Action Observer in this prompt focus on coordinating the efforts of various expert roles to address human questions or tasks effectively. Key aspects include:
1. Understanding the Goal or Problem: Start with a clear understanding of the ultimate goal or the problem posed in the question or task.