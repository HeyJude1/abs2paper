5 Conclusion
This paper introduces AutoAgents, an innovative framework for automatically synthesizing collaborative specialized agents. AutoAgents mimics the collaborative process of human teams by decomposing tasks into drafting and execution phases and delegating different subtasks to different agents. Our experimental and empirical evaluation validates the advantages of AutoAgents, as it surpasses single agents and other groupings in various tasks that demand diverse skills. Furthermore, our case study in software development illustrates the versatility and potential benefits of our proposed framework. AutoAgents opens up new possibilities for enhancing the interaction and cooperation among agents and transforms the landscape of complex problem-solving. We envisage that its principles can be further generalized and refined to deal with a broader range of tasks, paving the way towards more useful assistive AI. Figure : An example of the self-refinement process of programmers' coding make sure the story they produce makes sense and is consistently improved upon. This also supports the idea that working together in this way is helpful when dealing with complicated tasks.
Dynamic agents enhance the adaptability of complex tasks. The ability to generate dynamic agents for various tasks is crucial for enhancing their adaptability to diverse scenarios. Figure illustrates the contrast between GPT4 and AutoAgents' responses to open-ended questions. Unlike GPT-4, AutoAgents can produce agents from three distinct domains, which can provide more elaborate answers. For the trivia creative writing task in Figure and 11, AutoAgents employs a four-step approach for task decomposition. Initially, it sources the answer to the given question using a domainspecific agent, followed by the construction of a narrative. Concurrently, the Language Expert Agent plays a pivotal role, conducting multiple checks to verify the coherence between the narrative and the question, thus guaranteeing the narrative's accuracy.
Furthermore, as illustrated in Figure , the Action Observer orchestrates the interaction among multiple generative agents. It provides a concise summary of essential information, proving instrumental in fostering collaboration between various intelligent agents. This coordination is key to ensuring the seamless flow of the task execution process. Collectively, these instances vividly showcase the adaptability and efficiency of our dynamic agent generation framework in handling complex tasks.
Conversely, the prompt employed by AutoAgents exhibits a more universal nature, signifying its capacity to acclimate to diverse tasks without necessitating bespoke customization. As Table 5
Prompt Generalization
Social Simulacra Social Simulation Epidemic Modeling Social Simulation SSP General Autonomous Agents AgentVerse General Autonomous Agents AutoAgents General Autonomous Agents delineates, both AgentVerse and SSP have implemented task-specific enhancements for varied task evaluations. In contrast, our methodology leverages a singular, unified prompt format to accommodate an array of tasks. The commendable efficacy in open-ended question-answer and trivia creative writing tasks further corroborates the wide-ranging applicability and versatility of prompt design within the AutoAgents framework.
In this section, we present the criteria for human evaluation. We instructed the volunteers, who are responsible for assessing the quality of different feedback, to adhere to these standards.
[ Text]Human Evaluation
We would like to request your feedback on the response to the user question displayed above. Please rate the helpfulness, relevance, accuracy, level of details of their responses.
Each response receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.
Output with the following format: Evaluation evidence: <your evluation explanation here> Score: <score>
Limitations. AutoAgents exhibit remarkable knowledge acquisition and adaptability in tackling complex tasks, but they are not flawless. One of the limitations is that they may still produce erroneous outcomes even with dynamic role generation. This could be ascribed to the rationality of role generation and planning arrangements. Although this framework employs collaborative discussions to enhance the quality of role generation and planning arrangements, it still necessitates a  more effective method for recruiting teams and devising plans, and further ameliorates the quality of role generation and planning arrangements.
Furthermore, the differences between different roles in this framework mainly hinge on variations in prompt and tool usage, but this does not accentuate the distinctions between different expert roles.
In the future, it is imperative to explore how to incorporate more expert knowledge and create more professional role agents, in order to improve the adaptability of professional agents to professional problems.
Currently, AutoAgents rely heavily on the powerful logical and textual capabilities of GPT-4, and their adaptability to some earlier LLMs is poor. In the future, it is essential to explore more reasonable prompts to improve the adaptability of AutoAgents to different LLMs.
Future Work. Cooperation among multiple agents requires dynamic adaptation and communication.
The initial plan generated by LLMs may not suffice to achieve the desired outcomes , resulting in erroneous final output results. Hence, future multi-agent systems need to swiftly detect and rectify errors and adjust their plans dynamically to align with the desired outcomes.
The memory capacity of existing agents is limited by the number of tokens in LLMs. How to devise a high-quality memory mechanism that enables efficient retrieval and storage of memory by agents remains an open question.
The professional skills of the generated agents are effective, but they can be improved by retraining or other mechanisms. Alternatively, an Agent Bank can be established to enable the invocation of professional agents on demand. More professional agent construction is still worthy of exploration.
In this section, we present the prompts of five components in our framework: Planner, Plan Observer, Role Observer, Action Observer, and Custom Agent. These prompts are designed to elicit the desired behaviors and responses from the agents in different scenarios and tasks.
The prompt design principles outlined in the template focus on creating and utilizing specialized LLM-based agent roles to solve complex tasks and problems. Here's a summary of these principles:
1. Understanding and Breaking Down Tasks: The first step involves comprehensively understanding, analyzing, and deconstructing the given task or problem.