3 The Framework for Automatic Agent Generation
To enhance the effectiveness of autonomous multi-agent groups in accomplishing their goals, the process of AutoAgents consists of two critical stages: Drafting Stage and Execution Stage, as illustrated in Figure . The drafting stage synthesizes an agent team and an execution plan that are customized to the task by analyzing the input problem or task. The execution stage refines the plan by enabling inter-agent collaboration and feedback, and delivers the final result. The interagent collaboration is based on some principles of multi-agent cooperation, such as communication, coordination, and consensus. These principles help the agents to share information, align their actions, reach agreements, and adapt to the environment.
3.1 Drafting Stage
Empirical evidence suggests that diversity within human groups fosters diverse perspectives, which enhances the group's performance across various tasks. The drafting stage, which determines the composition of a multi-agent group, plays a crucial role in setting the upper limits of the group's capabilities. Therefore, it is imperative to generate the optimal agent team and execution plan that can maximize the group's potential.
Predominant methodologies for assigning role descriptions to autonomous agents rely heavily on human intuition and prior knowledge, requiring manual assignment based on task understanding. Consistent with several parallel findings , dynamically designing agents with different roles can significantly enhance their efficacy. However, the scalability and rationality of agent and plan generation are still unclear, especially in the face of various complex problem environments.
On the one hand, the generated agents should exhibit diversity to accommodate various tasks. On the other hand, the agent and the plan generation should adhere to certain principles, rendering their role allocation more rational. Therefore, we devise three artificially predefined agents to produce agent teams and execution plans, integrating artificial prior knowledge and the dynamic adaptation capability of LLMs to generate more sensible agent teams and execution plans. The three artificially predefined agents comprise Planner, Agent Observer, and Plan Observer:
• Planner P generates and refines an agent team and an execution plan based on the content of the task. • Agent Observer O agent provides suggestions on the rationality of the agent team members and their matching degree with the task. • Plan Observer O plan provides suggestions on the rationality of the execution plan and its matching degree with the task and the agent team.
The Planner generates initial agent team members and a specific plan, and improves the agent team and execution plan based on continuous communication with the Agent Observer and Plan Observer.
Agent Generation. The Planner generates the agent team and facilitates its continuous improvement through reciprocal communication with the Agent Observer. To enable Planner to produce rational agents, we have devised a standard format for the essential elements of a single agent. For each agent A = {P, D, T, S}, the Planner needs to specify its prompt P, description D, toolset T, and suggestions S.
• Prompt P provides a detailed and customized depiction of the expert identity for each specific agent, which comprises profile, goal, and constraints. Profile reflects the domain expertise of the role or job title. Goal indicates the primary responsibility or objective that the role aims to achieve. Constraints specify limitations or principles the role must adhere to when performing actions. • Description D gives additional concrete identity to help establish a more comprehensive role, develop an execution plan, and inspect problems. • Toolset T equips the Agent with tools that it can use, selected from a predefined set of tools.
The rationale for not using all the tools for each agent here is to prevent decision-making confusion caused by excessive tools. • Suggestions S supplies some suggestions for each agent to execute the current task, including but not limited to a clear output, extraction of historical information, and suggestions for execution steps.
Based on the agent list {A 1 , A 2 , • • • , A n } generated by Planner, the Agent Observer evaluates the quality and suitability of each agent. The Agent Observer first verifies whether every agent conforms to the aforementioned specifications and identifies any missing elements {P, description D, toolset T}. Secondly, the Agent Observer assesses the compatibility of each agent with the task, according to their description information and task content. Finally, the Agent Observer examines the agent list for any redundant or missing roles and eliminates or adds them accordingly.
After n rounds of bidirectional communication between the Planner and the Agent Observer, the optimal agent list for accomplishing the task is established. Given the vital role of the agent list in the  task execution, this framework employs a predefined agent and multiple rounds of iterative dialogue among multiple agents to finalize the agent list, thereby enhancing the stability and reliability of the execution phase.
Plan Generation. In parallel to agent generation, the Planner formulates the execution plan and promotes its progressive improvement through reciprocal communication with the Plan Observer. For a given task, the Planner delineates the specific steps {S 1 , S 2 , • • • S n } to accomplish it in the execution plan P . Each step S i entails a clear identification of the agent A j responsible for it, as well as the input information and expected output required for it.
The Plan Observer subsequently validates the execution plan
P = {S 1 , S 2 , • • • S n } according to the agent list {A 1 , A 2 , • • • , A n }
and the task content. It firstly ensures that each step has a corresponding agent and that the step content is coherent and concise. It secondly assesses whether all the steps are sufficient, whether the task can be accomplished, and whether there are any gaps that need to be filled. It finally provides feedback to the Planner, who further refines the execution plan accordingly. After n rounds of dialogue between the Planner and the Plan Observer, the ultimate execution plan for achieving the task is established.
Task Execution Actions. The Planner devises an execution plan that automatically assigns the requisite agents for diverse tasks. The execution plan comprises two actions of task execution: selfrefinement by a single agent and collaborative refinement by multiple agents, as shown in Figure . Self-refinement empowers an individual agent to augment its proficiency in accomplishing some specialized tasks. Collaborative refinement fosters knowledge sharing among multiple agents and achieves tasks requiring interdisciplinary expertise.
3.2 Execution Stage
In the drafting phase, the framework generates an agent list and an execution plan based on the task requirements. Then, the framework creates corresponding roles and executes the plans in the execution environment . The communication and cooperation among multi-agent systems are essential for accomplishing the tasks effectively. This section elaborates on the communication among multiple agents, the task execution strategies, and the knowledge-sharing mechanisms.
Communication of Multiple Agent. The communication structures among agents have been investigated by many studies to examine their impact on task performance. In this framework, we adopt the vertical communication paradigm, which assigns different tasks to agents according to their roles. To facilitate the specific division of labor among the agents in the generated team, we introduce a predefined Action Observer as the team leader to coordinate the execution plan. Specifically, This mechanism of refinement and communication recurs until the Action Observer attains a unanimous agreement on the execution responses, or the process reaches its maximum iteration limit. For scenarios that demand iterative decision-making towards specific objectives, such as software development, vertical communication would be a preferable option.
Self-refinement Agent. Besides the inter-agent communication, the performance of a single agent also exerts a significant impact on the overall quality of feedback results. Hence, drawing on mechanisms such as AutoGPT and ReAct , we have devised a self-refinement mechanism for an individual agent.
For a single agent A, the action at step t is at a t = l t ∪ p t ∪ o t , where l t denotes the thought or the reasoning trace in the language space, which does not alter the external environment, and thus yields no observational feedback, p t represents the execution plan for task completion, o t comprises the completion steps and execution output for this time.
As illustrated in Figure , various types of useful thoughts can assist in devising a refinement plan. The execution plan enables the agent to anticipate the steps they need to undertake in the future, and the observational content of the execution result construction allows the agent to reevaluate and enhance the plan arrangement, thereby constructing more refined and complete actions. Through a cycle of self-continuous thinking, planning, execution, and feedback, a single agent can effectively execute and accomplish task content.
Collaborative Refinement Action. In the collaborative refinement action, the agents collaboratively refine and execute the tasks in a sequential manner. Each round of the collaboration involves a fixed order of turn-taking among the agents, who generate their responses based on the current observation. The chat history slot of each agent is updated by concatenating the previous utterances of the other agents. The collaboration terminates automatically when the agents reach a consensus or the maximum number of discussions is reached.
Knowledge Sharing Mechanism. AutoAgents also facilitates the sharing of execution results among various agents for improved communication and feedback. However, when the number of agents is large and a single agent has more self-iterations, it will generate more historical information. Due to the token limitation of LLM models, they often cannot encompass all information. Hence, this framework provides short-term memory, long-term memory, and dynamic memory.
Short-term memory is chiefly concentrated on a singular action, encompassing the gamut of intermediary notions, strategies, and outcomes that emerge during the self-refinement or collaborative refinement phases of an individual action. It is salient to note that these actions frequently culminate in a distilled summary of critical information, epitomizing the final phase of the refinement trajectory.
Long-term memory principally focuses on chronicling the historical trajectory of multifarious actions, predominantly documenting the executed results of each task along with the synthesis of vital feedback information. This aspect is imperative for evaluating the comprehensive extent of task completion.
Dynamic memory predominantly serves actions necessitating specialized attention. The Action Observer, having access to long-term memory archives, adeptly extracts ancillary information, dynamically tailoring it to the specific requirements of the action for task execution. This process significantly augments the efficiency of a single action in task fulfillment. O agent provides feedback on agent team.
P refines agent team based on feedback.
O plan provides feedback on execution plan.
P refines execution plan based on feedback. 9: until No feedback or reached the maximum iteration limit.
for {A i , • • • , A j } do 18:
Agent A m analyzes S k , M S and M D .
Agent A m plans the current step and executes this step.
The execution result is stored in M S .
end for
until No step or reached the maximum iteration limit.
The execution results of task S k are stored in M L .
O action coordinates {S 1 , S 2 , • • • S n } and monitors execution. 25: end for 26: return Execution results of final step.
3. Creation of New Expert Roles:
• Avoid duplication of functions in new roles.
• New roles should have clear names, detailed descriptions, domain expertise, available tools, execution suggestions, and prompt templates. • Ensure each new expert role has a distinct responsibility and domain of expertise.
• Specify the goals, constraints, and toolset for each new role.
• Provide execution suggestions and develop prompt templates for each new role.
• Output details of new roles in JSON format, following a specific structure.
3. Extracting and Utilizing Historical Information:
• Extract relevant information from the history to assist in completing the next step.
• Ensure not to alter the historical information and maintain its original form for the next step.
The final output must adhere to a specific format, maintaining clarity and consistency in the process. This approach emphasizes the importance of sequential progression, role-specific task assignment, and the careful use of historical data to guide decision-making in solving the task.
[ Prompt]Action Observer PROMPT_TEMPLATE = """ You are an expert role manager who is in charge of collecting the results of expert