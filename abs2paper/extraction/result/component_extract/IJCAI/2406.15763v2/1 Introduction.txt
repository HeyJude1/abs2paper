1 Introduction
Semi-supervised learning (SSL) , a research topic that aims to boost the model's generalization performance by leveraging the potential of unlabeled data, has received extensive attention in recent years. Among the proposed techniques, the combination of pseudo-labeling and consistency regulation , as introduced by , has emerged as a predominant approach. Specifically, FixMatch first assigns a pseudo-label to each unlabeled sample based on the prediction of its weakly augmented view. Subsequently, pseudo- * Corresponding author Figure : Pilot study for pseudo-label quality on CIFAR-10 with 40 labels. Regarding SoftMatch, pseudo-labels with a confidence lower than µt − σt are assigned close-to-zero weights and thus considered dropped in the analysis. Here, µt/σt denotes the estimated mean/std of the overall confidence on unlabeled data, respectively. labels exceeding a predefined confidence threshold are used as supervision for corresponding strongly augmented views, while those below the threshold are discarded. To ensure high-quality pseudo-labels, FixMatch uses a high constant threshold throughout training. However, this strategy results in the underutilization of unlabeled data, presenting a central challenge in SSL: making efficient use of unlabeled data.
To address the trade-off between the quality and quantity of pseudo-labels in threshold-based pseudo-labeling, previous studies introduce dynamic threshold strategies that align with the evolving learning status of the model. For example, FlexMatch utilizes the number of confident pseudo-labels to estimate the learning difficulty for each class, subsequently mapping the predefined threshold to class-specific thresholds based on the determined difficulty levels. Additionally, FreeMatch leverages the average confidence of unlabeled data to establish a dynamic global threshold. Besides, SoftMatch employs a Gaussian function representing the disparity between sample-specific and global confidence to model sample weights, assigning a positive weight to each unlabeled sample. However, samples with confidence significantly lower than the global confidence receive close-tozero weights, essentially treated as if they were discarded. This characteristic positions SoftMatch as a variant of the threshold-based method. While the aforementioned algorithms effectively employ pseudo-labels to assess the learning status, the impact of biased data sampling or potential interclass similarities can significantly influence the predictions of arXiv:2406.15763v2 LG] 9 Jul 2024 unlabeled data. Consequently, a pivotal question arises: Can we incorporate additional evidence along with pseudo-labels to achieve a more accurate estimation of the learning status?
While an improved threshold scheme can enhance the utilization of unlabeled data, a portion of unlabeled samples still face exclusion. This raises another question: Can pseudolabels assigned lower confidence provide valuable semantic guidance? To address this concern, we examine the pseudolabel quality of previous algorithms. In the case of CIFAR-10 [ with 40 labeled samples, more than half of the dropped pseudo-labels prove to be correct, as depicted in Figure (a). Furthermore, Figure (b) illustrates that the top-5 accuracy of the pseudo-labels reaches 100% within just a few thousand iterations. Accordingly, pseudolabels with lower confidence can eliminate false options (e.g., bottom-5 classes) and provide effective supervision signals.
Motivated by the aforementioned questions, this paper introduces AllMatch, a novel SSL model designed to enhance learning status estimation and provide semantic guidance for all unlabeled data. Specifically, AllMatch proposes a classspecific adaptive threshold (CAT) strategy, comprising global estimation and local adjustment steps, to achieve an improved characterization of the model's learning status. The global estimation step, similar to FreeMatch, employs the average confidence of unlabeled data as the global threshold. The ensuing local adjustment step utilizes the classifier weights to estimate the learning status of each class, adaptively decreasing thresholds for classes facing challenges. As illustrated in Figure (b, c) and Figure (f, g), CAT outperforms previous approaches in terms of the utilization ratio and pseudolabel accuracy of unlabeled samples. Besides, in response to the underutilization of unlabeled data resulting from the exclusion of low-confidence pseudo-labels, AllMatch introduces a binary classification consistency (BCC) regulation strategy to exploit the latent potential within such pseudolabels. In essence, the BCC regulation divides the class space into candidate and negative classes, encouraging consistent candidate-negative division across diverse perturbed views of the same sample to eliminate negative options. The candidate class for each sample corresponds to its top-k predictions, considering the impressive top-k performance of various algorithms. Note that the parameter k is dynamically determined based on varying sample-specific learning status and evolving model performance. As depicted in Figure ) and Figure ), the BCC regulation effectively identifies candidate classes for unlabeled samples and achieves a 100% utilization ratio for the unlabeled data. Overall, our contributions can be summarized as follows:
(1) We revisit existing SSL algorithms and raise two questions: how to develop an effective threshold mechanism and how to utilize the low-confidence pseudo-labels.
(2) We propose the class-specific adaptive threshold mechanism, which employs pseudo-labels and classifier weights to estimate global and class-specific learning status respectively.
(3) We design the binary classification consistency regulation to provide supervision signals for all unlabeled samples.
(4) We conduct experiments on multiple benchmarks, considering both balanced and imbalanced settings. The results indicate that AllMatch achieves state-of-the-art performance.