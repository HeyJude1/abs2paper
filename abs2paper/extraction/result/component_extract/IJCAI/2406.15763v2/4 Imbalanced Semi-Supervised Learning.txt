4 Imbalanced Semi-Supervised Learning
Settings. We evaluate AllMatch in the context of imbalanced SSL, where both labeled and unlabeled data exhibit a long-tailed distribution. All experiments are conducted on the TorchSSL codebase. Following prior studies , we generate the labeled and unlabeled sets using the configurations of
N c = N 1 • γ − c−1 C−1 and M c = M 1 • γ − c−1 C−1 .
Specifically, for CIFAR-10-LT, we set N 1 to 1500, M 1 to 3000, and γ to range from 50 to 150. For CIFAR-100-LT, we set N 1 to 150, M 1 to 300, and γ to range from 20 to 100. In all experiments, we employ WRN-28-2 as the backbone and utilize the Adam optimizer with the weight decay of 4e-5. The batch sizes B L and B U are set to 64 and 128, respectively. The learning rate is initially set to 2e-3 and adjusted by a cosine decay scheduler during training. We repeat each Table : Performance (%) on CIFAR-10-LT and CIFAR-100-LT.
experiment three times and report the overall performance.
Detailed implementation is listed in Appendix C. Performance. In the context of imbalanced SSL, we compare AllMatch with several strong baselines, including Fix-Match, FlexMatch, SoftMatch, and FreeMatch. The results in Table demonstrate that AllMatch achieves state-of-theart performance on all benchmarks. It is particularly noteworthy that AllMatch outperforms the second-best approach by 1.69% and 1.65% at γ=100 and γ=150 on CIFAR-10-LT, respectively, highlighting its robustness in handling significant imbalances. Furthermore, as detailed in Appendix B, AllMatch is compatible with existing imbalanced SSL algorithms, and their combination can further enhance resilience against severe imbalances. The consistently impressive performance observed in imbalanced SSL suggests that All-Match can effectively address real-world challenges.
4.3 Ablation Study
In this part, we systematically evaluate each constituent component of AllMatch. Additionally, we provide the grid search of K (upper bound for the number of candidate classes) and λ b (weight for the BCC regulation) in Appendix A.1 and A.2.
Component analysis. We conduct an ablation study on Table : Threshold comparison study (%). SoftMatch assigns trivial weights to samples with confidence significantly lower than the global confidence, approximating itself as a threshold-based model.
four challenging datasets: CIFAR-10 with 10 labels, CIFAR-100 with 400 labels, STL-10 with 40 labels, and CIFAR-10-LT with an imbalance ratio of 150. For simplicity, we refer to the performance on the four benchmarks as (a, b, c, d) in subsequent analysis. As shown in Table , the global estimation step in CAT (line 2) promotes the performance by (7.35%, 8.00%, 16.45%, 1.20%) compared to the baseline model in line 1. The significant improvement highlights the crucial role of aligning the threshold with the model's global learning status. Furthermore, the local adjustment step in CAT (line 3) yields additional gains of (3.22%, 1.46%, 6.95%, 1.90%), suggesting that it effectively captures class-specific learning difficulties and facilitates the learning of classes facing challenges. Additionally, the BCC regulation enables a 100% utilization ratio of the unlabeled data and achieves the improvement of (2.25%, 0.52%, 0.71%, 0.77%). The substantial improvement observed on CIFAR-10 with 10 labels indicates the potential of the BCC regulation when dealing with extremely limited labeled data. Overall, the results in Table demonstrate the effectiveness of the proposed modules and the advantages of their combination in AllMatch.  The threshold for STL-10-40 is restricted within [0.9, 1.0] to mitigate the adverse effects of noisy pseudo-labels. In the analysis of SoftMatch, we employ µt − σt as its threshold. Samples with a confidence lower than µt − σt are assigned negligible weights, essentially treated as if they were discarded. Consequently, the class-average threshold of SoftMatch is µt − σt, and its selected pseudo-label acc and utilization ratio can be computed like other threshold-based models.
Here, µt/σt denotes the mean/std of the overall confidence on unlabeled data. Detailed analysis for SoftMatch is provided in Appendix A.3.
Comparative study on threshold strategies. We conduct a comparative analysis of existing threshold mechanisms in two aspects. Firstly, we directly compare the proposed CAT with the threshold strategies adopted in previous models. The results are presented in line 1-4 of Table . Secondly, we assess the threshold strategies within the AllMatch framework, i.e., combining existing threshold schemes with the BCC regulation, and the results are provided in line 5-8 of Table . From both perspectives, AllMatch outperforms previous models in most cases, indicating the effectiveness of the proposed CAT. Moreover, the BCC regulation further boosts the performance of prior methods, suggesting its impressive compatibility and contribution to eliminating false options.
4.4 Quantitative Analysis
To gain further insights into AllMatch, we present various training indicators on CIFAR-10 with 40 labels and STL-10 with 40 labels, as illustrated in Figure . Besides, the indicators on CIFAR-10 with 10 labels and CIFAR-100 with 400 labels are presented in Appendix A.3. From Figure and Figure (e), it can be observed that the threshold exhibits the expected behavior, starting with a small value and gradually increasing thereafter. Moreover, AllMatch demonstrates a smoother threshold evolution in contrast to other classspecific threshold-based models, implying a preferred learning status estimation. Additionally, in comparison to previous algorithms, Figure