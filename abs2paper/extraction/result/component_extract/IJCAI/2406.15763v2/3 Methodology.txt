3 Methodology
3.1 Preliminary
We begin by reviewing the widely adopted SSL framework.
Let D L = {(x i , y i )} N L i=1 and D U = {u i } N U i=1
represent the labeled and unlabeled datasets, respectively. Here, x i and u i denote the labeled and unlabeled training samples, and y i represents the one-hot label for the labeled sample x i . We denote the prediction of sample x as p(y|x). Given a batch of labeled and unlabeled data, the model is optimized with the objective L = L s + λ u L u . Here, L s represents the crossentropy loss (H) for the labeled batch of size B L .
L s = 1 B L B L i=1 H(y i , p(y|x i )) (1)
L u indicates the consistency regulation between the prediction of the strongly augmented view Ω(u) and the pseudolabel derived from the corresponding weakly augmented view ω(u). To filter out incorrect pseudo-labels, FixMatch introduces a predefined threshold τ . Specifically, L u is defined as follows.
L u = 1 B U B U i=1 λ(p i )H(p i , q i ) (2) λ(p) = 1 if max(p) ≥ τ 0 otherwise (3)
Here, pi is the abbreviation for DA(p(y|ω(u i ))), where DA indicates the distribution alignment strategy . pi represents the one-hot pseudo-label obtained from argmax(p i ). Moreover, q i is the abbreviation for p(y|Ω(u i )).
Lastly, B U corresponds to the batch size of unlabeled data.
3.2 Class-Specific Adapative Threshold
Previous studies have demonstrated that the threshold should be aligned with the evolving learning status of the model. To achieve this, these approaches leverage predictions on unlabeled data to establish the dynamic threshold. In this paper, we unveil the abil-ity of the classifier weights to differentiate the learning status of each class. By combining pseudo-labels and classifier weights, we introduce a class-specific adaptive threshold (CAT) mechanism. As depicted in Figure , CAT comprises the global estimation and local adjustment steps. The following parts provide a detailed description of these two steps.
Global Estimation. The global estimation step learns from FreeMatch and evaluates the overall learning status of the model. Given that deep neural networks tend to prioritize fitting easier samples before memorizing harder and noisier ones, a lower threshold is necessary during early training stages to incorporate more correct pseudolabels. Conversely, as training progresses, a higher threshold is required to filter out incorrect pseudo-labels. Given that the cross-entropy loss encourages confident predictions, the average confidence of the unlabeled set captures information from all unlabeled data and steadily increases throughout training, thereby reflecting the overall learning status. However, making predictions for the entire unlabeled set at each time step incurs significant computational costs. Accordingly, we employ the mean confidence of the current batch as an estimation and update it using exponential moving average (EMA). Specifically, the global learning status estimation at t-th iteration, denoted as τ t , can be computed as follows:
τ t =    1 C if t = 0 mτ t−1 + (1 − m) 1 B U B U i=1 max(p i ) otherwise (4)
Here, p i represents p(y|ω(u i )), m denotes the momentum decay, and C corresponds to the number of classes.
Local Adjustment. Due to the inherent variations in learning difficulty among different classes and the stochastic nature of parameter initialization, the model's learning status varies across categories. To address this issue, we introduce the local adjustment step, which makes the model pay more attention to the underfitting classes by decreasing their thresholds. Specifically, our study reveals that the L2 norm of the classifier weights provides insights into class-specific learning status. The reasons are explained as follows.
Firstly, let M = G • F denote the model, where F and G According to the above analysis, the L2 norm of classifier weights characterizes the learning status of each class. Consequently, the local adjustment step leverages this indicator to establish the mapping from the global threshold to classspecific thresholds. Specifically, we linearly scale the threshold for each class based on the deviation of its learning status from the optimal learning status. As such, the threshold for class c at t-th iteration, denoted as τ t (c), can be computed as follows.
τ t (c) = τ t • ||W c || max{||W c || : c ∈ [1, • • • , C]} (5)
Moreover, to ensure a stable estimation of the learning status, we employ the classifier weights obtained from the EMA model. Notably, in contrast to FlexMatch, which maintains an additional list for recording the selected pseudo-label of each sample, the proposed CAT refrains from storing any samplespecific information during training. This eliminates the indexing budget concerns on large-scale datasets. With CAT incorporated, the mask for unlabeled samples in L u can be expressed as follows.
λ(p) = 1 if max(p) ≥ τ t (argmax(p)) 0 otherwise (6)
3.3 Binary Classification Consistency Regulation
While the proposed class-specific adaptive threshold alleviates the underutilization of unlabeled data, a substantial number of pseudo-labels continue to be discarded. As illustrated in Figure , in the case of CIFAR-10 with 40 labeled samples, the top-5 accuracy of pseudo-labels effortlessly achieves 100% regardless of the adopted algorithm. In other words, pseudo-labels assigned lower confidence contribute to identifying candidate classes (e.g., top-k predictions) and excluding negative options (e.g., classes not included in top-k predictions). Motivated by these observations and the consistency regulation technique, we propose the binary classification consistency (BCC) regulation, whose overview is shown in Figure . In a nutshell, the strategy introduces semantic supervision for all unlabeled data by encouraging consistent candidate-negative division across diverse perturbed views of the same sample. The details are described as follows.
Given the impressive top-k pseudo-label accuracy obtained by numerous algorithms, the BCC regulation adopts the top-k predictions of each unlabeled sample as its candidate classes and the rest as the negative options. Thus, the candidatenegative division is simplified to the selection of the parameter k. Moreover, considering the variations in learning difficulty among different samples and the evolving performance of the model, the candidate-negative division for each sample should be determined based on both individual and global learning status. To achieve this, the BCC regulation first computes sample-specific top-k confidence and the global top-k confidence of the entire unlabeled set. Specifically, let p k i denote the top-k probability of sample u i , and µ k t represent the global top-k probability at t-th iteration. The global top-k confidence can be estimated by the exponential moving aver-age (EMA) of the average top-k confidence at each time step.
p k i = k j=1 p i,cj (p i,c1 ≥ p i,c2 ≥ • • • ) (7) µ k t = k C if t = 0 mµ k t−1 + (1 − m) 1 B U B U i=1 p k i otherwise (8)
Here, c 1 , . . . , c k represent the k classes assigned the highest probability in p i . With the global top-k confidence determined, the number of candidate classes for each unlabeled sample is defined as the minimum value that makes individual top-k confidence higher than global top-k confidence. Particularly, the candidate class for confident unlabeled samples is defined as the pseudo-label. Accordingly, the number of candidate classes k i for sample u i can be expressed as follows.
k i = 1 if λ(p i ) = 1 min(min{k : pk i ≥ µ k t }, K) otherwise ( 9
)
where K is the upper bound for the number of candidate classes to prevent trivial candidate-negative division. With the division obtained, the candidate and negative probabilities for the weakly (b ω i ) and strongly (b Ω i ) perturbed views of the unlabeled sample u i can be calculated as follows.
b ω i = [ ki j=1 pi,cj , C j=ki+1 pi,cj ] (p i,c1 ≥ pi,c2 ≥ • • • ) (10) b Ω i = [ ki j=1 q i,cj , C j=ki+1 q i,cj ] (11)
Here, c 1 , . . . , c ki represents the k i classes assigned the highest probability in pi . Finally, the BCC regulation for a batch of unlabeled data can be calculated as follows:
L b = 1 B U B U i=1 H(b ω i , b Ω i ) (12)
3.4 Overall Objective
The overall objective of AllMatch is defined as the weighted sum of all semantic-level supervision.  AllMatch is trained using the SGD optimizer with an initial learning rate of 0.03 and a momentum decay of 0.9. The learning rate is adjusted by a cosine decay scheduler over a total of 2 20 iterations. We set m to 0.999 and generate the EMA model with a momentum decay of 0.999 for inference.
L = L s + λ u L u + λ b L b (13)
The upper bound K is set to 20 for ImageNet and 10 for the other datasets. For SVHN, CIFAR-10 with 10 labels, and STL-10 with 40 labels, we constrain the threshold within the range of [0.9, 1.0] to prevent overfitting noisy pseudo-labels in the early training stages. To account for randomness, we repeat each experiment three times and report the mean and standard deviation of the top-1 accuracy. Detailed implementation and data processing are listed in Appendix C.
Performance. Table presents the top-1 accuracy on CIFAR-10/100, SVHN, and STL-10 with various numbers of labeled samples. The performance on ImageNet is reported in Table . The experimental results demonstrate that AllMatch achieves state-of-the-art performance on almost all datasets. For CIFAR-10, AllMatch outperforms FullMatch with only 40 available labels, and performs comparably to FullMatch when there are 250 or 4000 labels. Moreover, regarding CIFAR-100, AllMatch outperforms ReMixMatch when only 400 or 2500 labels are available, while the latter achieves better performance when 10000 labels are available. The competitive results obtained by ReMixMatch mainly stem from the Mixup technique and the additional self-supervised learning part. Furthermore, AllMatch exhibits substantial advantages over previous algorithms when dealing with extremely limited labeled samples. Specifically, the approach surpasses the second-best counterpart by 1.87% on CIFAR-10 with 10 labels, 0.66% on CIFAR-100 with 400 labels, and 2.86% on STL-10 with 40 labels. Particularly, STL-10 poses significant challenges due to its large unlabeled set that comprises 100k images. Accordingly, the impressive improvement obtained on STL-10 highlights the potential of AllMatch to be deployed in real-world applications.