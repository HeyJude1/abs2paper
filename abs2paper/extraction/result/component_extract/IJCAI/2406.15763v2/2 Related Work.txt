2 Related Work
Consistency regulation and pseudo labelling are fundamental approaches in SSL. The former encourages consistent predictions across different perturbed views of the same sample while the latter assigns pseudo-labels to unlabeled samples. Among established techniques, perfectly combines both techniques, establishing an effective SSL paradigm. Specifically, FixMatch uses the predictions of weakly augmented samples as pseudo-labels and minimize their divergence from the predictions of the corresponding strongly augmented views.
To ensure high-quality pseudo-labels, FixMatch utilizes a high constant threshold throughout training to filter out potentially incorrect pseudo-labels. However, this strategy results in the underutilization of unlabeled data. To address this issue, FlexMatch draws inspiration from curriculum learning , mapping the predefined threshold to class-specific thresholds based on the learning status of each category. Dash defines the threshold based on the loss of labeled data, eliminating the empirical threshold parameter. FreeMatch employs the average confidence on unlabeled data as the adaptive global threshold. SoftMatch estimates sample weights by a dynamic Gaussian function, maintaining soft margins between unlabeled samples of different confidence levels. In addition to the threshold-based algorithms, CoMatch and SimMatch leverage contrastive loss to impose sample-level constraints on all unlabeled data. In contrast, AllMatch combines the advantages of both threshold-based and contrastivebased methods by introducing CAT, a learning-status-aware threshold strategy, and BCC regulation, the semantic-level supervision for the entire unlabeled set.
Concurrent with our work, FullMatch [Chen et al., 2023b] also introduces semantic guidance for all unlabeled samples. Specifically, FullMatch compares the predictions of weakly and strongly augmented samples to identify negative classes. In contrast, AllMatch identifies candidate classes by comparing sample and global top-k confidence, thus considering the learning state of both individual sample and model in this process. Additionally, FullMatch assigns low probability (similar to label smoothing) for the negative classes in the optimization objective, while AllMatch directly encourages consistent candidate-negative division for all unlabeled samples, thus exhibiting better consistency with the unsupervised loss.
In addition to consistency regulation and pseudo-labeling, entropy-based regulation is another widely adopted strategy. Entropy minimization [Grandvalet and promotes high-confidence predictions during training. Maximizing the entropy of the expectation over all samples introduces the concept of fairness, which encourages the model to predict each class with equal frequency. Specifically, distribution alignment (DA) and uniform alignment (UA) are prevailing strategies for achieving fairness in SSL, which adjusts the pseudo-label based on the overall predictions on the unlabeled data.