2 Regret Minimization
A regret minimization algorithm repeatedly plays against an unknown environment and starts with a decision x 1 ∈ D ⊆ R n , where D is the decision space. In each iteration t, it observes a loss ℓ t ∈ R n from the environment and computes its next decision x t+1 based on past decisions x 1 , . . . , x t and previous losses ℓ 1 , . . . , ℓ t . The objective of the algorithm is to minimize the total regret
R T = max x ′ ∈D T t=1 ℓ t , x t − x ′ .
Online Mirror Descent (OMD) stands out as a famous regret minimization algorithm compatible with arbitrary decision spaces. It updates the decision according to x 1 = argmin x ′ ∈D ψ(x ′ ), and
x t+1 = argmin x ′ ∈D ℓ t , x ′ + 1 η B ψ (x ′ || x t ) , where ψ : D → R is a regularizer, B ψ (x ′ || x) = ψ(x ′ ) − ψ(x) − ⟨∇ψ(x),
x ′ − x⟩ is the Bregman divergence associated with ψ, and η > 0 is an arbitrary step size. Optimistic OMD seeks to benefit from the predictability of slowly-changing loss ℓ t . In iteration t, it predicts the next iteration's loss ℓ t+1 as m t+1 and updates the
Algorithms Cumulative Regret R t j New Strategy x t+1 j Cumulative Strategy X t CFR R t−1 j + r t j R t j + / R t j + 1 X t−1 + ẋt CFR+ R t−1 j + r t j + R t j / R t j 1 X t−1 + t * ẋt Linear CFR R t−1 j + t * r t j R t j + / R t j + 1 X t−1 + t * ẋt DCFR R t j = R t−1 j ⊙ d t−1 j + r t j , where d t j [a] = t α t α +1 if R t j [a] > 0 t β t β +1 otherwise R t j + / R t j + 1 X t−1 ( t−1 t ) γ + ẋt DCFR+ R t−1 j (t−1) α (t−1) α +1 + r t j + R t j / R t j 1 X t−1 ( t−1 t ) γ + ẋt PCFR+ R t−1 j + r t j + Rt+1 j / Rt+1 j 1
, where
Rt+1 j = R t j + v t+1 j + X t−1 + t 2 ẋt PDCFR+ R t−1 j (t−1) α (t−1) α +1 + r t j + Rt+1 j / Rt+1 j 1
, where decision according to x 1 = z 0 = argmin x ′ ∈D ψ(x ′ ), and
Rt+1 j = R t j t α t α + 1 + v t+1 j + X t−1 ( t−1 t ) γ + ẋt
z t = argmin z ′ ∈D ℓ t , z ′ + 1 η B ψ (z ′ || z t−1 ), x t+1 = argmin x ′ ∈D m t+1 , x ′ + 1 η B ψ (x ′ || z t ) .
Regret Matching (RM) ] and Regret Matching+ (RM+) are two regret minimization algorithms operating on the simplex ∆ n . RM typically starts with a uniform random strategy x 1 . On each iteration t, RM calculates the instantaneous regret r t = ⟨x t , ℓ t ⟩ 1 − ℓ t and then accumulates it to obtain the cumulative regret R t = t k=1 r k . The next strategy is proportional to the positive cumulative regret, i.e.,
x t+1 = [R t ] + / [R t ] + 1
, where
[•] + = max {•, 0}.
For convenience, we define 0/0 as the uniform distribution. RM+ is a straightforward variant of RM. It sets any action with negative cumulative regret to zero in each iteration so that it promptly reuses an action showing promise of performing well. Formally, R t = [R t−1 + r t ] + and x t+1 = R t / ∥R t ∥ 1 .
2.3 Counterfactual Regret Minimization
Counterfactual Regret Minimization (CFR) is one of the most popular equilibrium-finding algorithms for solving IIGs. It operates as a regret minimization algorithm within the sequence-form strategy space. Given a sequence-form strategy ẋt ∈ X and a loss lt = A ẏt , the key concept involves constructing a counterfactual loss ℓ t j ∈ R nj for each decision node j ∈ J :
ℓ t j [a] = lt [j, a] + j ′ ∈Cj,a ℓ t j ′ , x t j ′ .
CFR then employs RM to minimize the total counterfactual regret R T j = max x ′ j ∈∆ n j T t=1 ℓ t j , x t j − x ′ j at each decision node with respect to counterfactual loss. It is guaranteed that the total regret R T is bounded by the sum of the total counterfactual regrets under each decision node, i.e., R T ≤ j∈J R T j + .
The computational procedure of CFR on each iteration t is summarized as follows: (1) decomposes the sequence-form strategy ẋt into local strategies x t j for each decision node j ∈ J ; (2) recursively traverses the game tree to calculate the counterfactual loss ℓ t j ;
(3) accumulates the instantaneous counterfactual regret r t j = x t j , ℓ t j 1 − ℓ t j to obtain the cumulative counterfactual regret R t j = R
2.4 CFR Variants
Since the birth of CFR, researchers have proposed many novel CFR variants, greatly improving the convergence rate of the vanilla CFR. CFR+ incorporates three small yet effective modifications, resulting in an order of magnitude faster convergence compared to CFR. (1) CFR+ employs RM+ instead of RM, i.e., R t j = R t−1 j + r t j + . (2) CFR+ adopts a linearly weighted average strategy where iteration t is weighted by t, i.e., X t = X t−1 +t ẋt . (3) CFR+ uses the alternating-updates technique. DCFR ] is a family of algorithms which discounts prior iterations' cumulative regrets and dramatically accelerates convergence especially in games where some actions are very costly mistakes. Specifically,
R t j = R t−1 j ⊙ d t−1 j + r t j ,where d t j [a] = t α t α +1 if R t j [a] > 0 t β t β +1 otherwise. X t = X t−1 t − 1 t γ + ẋt .
Linear CFR ] is a special case of DCFR where iteration t's contribution to cumulative regrets and cumulative strategy is proportional to t.
2.5 The Predictive CFR Variant
Recently, demonstrates that a general regret minimization algorithm can be adapted for constructing a regret minimization algorithm tailored to a simplex. To illustrate, let us focus on the objective of minimizing the total counterfactual regret R T j at a decision node j. Additionally, we have a general regret minimization algorithm A, which we apply to a decision space R nj ≥0 . At each iteration t, upon obtaining the counterfactual loss ℓ t j , we initially compute a modified loss lt j = −r t j = ℓ t j − ℓ t j , x t j 1. It is then fed into A, and we receive its next decision xt+1 j ∈ R nj ≥0 . Finally, we obtain the next strategy as x t+1 j = xt+1 j / xt+1 j 1 . Based on the construction, illustrates that employing OMD with ψ = 1 2 ∥•∥ 2 2 as the algorithm A yields the same strategy as RM+ in each iteration. Additionally, by using Optimistic OMD with ψ = 1 2 ∥•∥ 2 2 and m t+1 = −v t+1 j , where v t+1 j = m t+1 j , x t j 1 − m t+1 j is the prediction of r t+1 j and m t+1 j = ℓ t j is the prediction of ℓ t+1 j , Predictive RM+ (PRM+) is obtained, i.e.,
R t j = [R t−1 j + r t j ] + , Rt+1 j = [R t j + v t+1 j ] + , x t+1 j = Rt+1 j / Rt+1 j 1 .
Moreover, introduces Predictive CFR+ (PCFR+), which uses PRM+ as the regret minimization algorithm in each decision node and incorporates a quadratic weighted average strategy, defined as X t = X t−1 + t 2 ẋt . We summarized CFR and typical variants in Table .