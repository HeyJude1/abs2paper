1 Introduction
Imperfect-information games (IIGs) model strategic interactions between players with hidden information. Solving such games is challenging since players must reason under uncertainty about opponents' private information. The hidden information plays an essential role in real-world situations such as medical treatment , negotiation , and security , making the research on IIGs theoretically and practically crucial.
In this work, we focus on solving two-player zero-sum (2p0s) IIGs. The typical goal in these games is to find an (approximate) Nash equilibrium (NE) in which no player can benefit from deviating unilaterally from the equilibrium. The common iterative approach minimizes total regrets of both players so that their average strategies over time converge to a NE. The family of counterfactual regret minimization (CFR) algorithms decomposes the total regret into the sum of counterfactual regrets associated with decision nodes. Then it employs a local regret minimization algorithm, such as Regret Matching (RM) or its variant RM+ , at each decision node to effectively minimize counterfactual regret. Due to the sound theoretical guarantee and strong empirical performance, CFR and its variants have enabled several major breakthroughs in this field .
Besides CFR, Online Mirror Descent (OMD) stands out as a prominent and general regret minimization algorithm. It has promising theoretical results but remains less competitive than CFRs when directly applied to solving IIGs. Recently, researchers have tried to build connections between CFR and OMD . The study by demonstrates that RM+ is essentially a specialized form of OMD when employed to minimize counterfactual regret at each decision node. This connection inspires an optimistic variant of RM+, named Predictive RM+ (PRM+), seeking to benefit from the predictability of slowly-changing counterfactual losses over time. Its extension Predictive CFR+ (PCFR+) exhibits extremely fast convergence on non-poker IIGs.
Despite the notable success of PCFR+, a limitation arises from its practice of assigning uniform weights to each iteration when determining regrets. The algorithm becomes particularly challenging when dealing with dominated ac-tions, leading to high regrets for other actions and requiring a substantial number of iterations to mitigate this negative impact 1 . Assigning more weights to recent iterations has shown to be important for fast convergence . Notably, Discounted CFR (DCFR) ] is a family of algorithms that discounts prior iterations when determining both regrets and average strategy. Thus, it is desirable to design a CFR variant that not only allocates more weights to recent iteration, thereby alleviating high regrets from earlier iterations, but also leverages predictions to accelerate convergence.
To this end, we delve into minimizing weighted counterfactual regret with OMD and optimistic variant:
• We demonstrate that by directly employing OMD to minimize weighted counterfactual regret, we obtain a CFR variant, DCFR+, which incorporates regret discounting similar to DCFR and clips negative regrets akin to CFR+. Remarkably, the CFR variant previously discovered through evolutionary search is a special case of DCFR+, showcasing faster convergence than DCFR.
• Furthermore, by applying optimistic OMD, we derive a novel CFR variant, PDCFR+, which leverages predicted regrets to compute new strategy like PCFR+, while updating regrets similar to DCFR+. It swiftly mitigates negative effects of dominated actions and consistently leverages predictions to speed up convergence. j ∈ J , the player selects an action based on a local strategy x j ∈ ∆ nj , where ∆ nj is a simplex over the action set A j ⊆ A of size n j . After taking the action a, the player proceeds to an observation node k = ρ(j, a). At each observation node k ∈ K, the player receives a signal s ∈ S k and then reaches another decision node j ′ = ρ(k, s). The set C j,a = ρ(ρ(j, a), s) : s ∈ S ρ(j,a) denotes all decision nodes that are earliest reachable after taking action a at j. We introduce a dummy root decision node o with one action to ensure a unique root. An illustration is given in Appendix A. For a local strategy set {x j } j∈J , we can construct a sequence-form strategy ẋ represented as a vector indexed over {(j, a) : j ∈ J , a ∈ A j }. Each entry in the vector corresponds to a pair (j, a), with the value representing the product of probabilities of all actions along the path from the root to (j, a). The sequence-form strategy space is denoted by X .
In a 2p0s IIG, where player 1 and player 2 have sequencefrom strategy space X and Y, and the set of decision nodes are J x and J y , the problem of finding a NE can be formulated as a bilinear saddle point problem
min ẋ∈X max ẏ∈Y ẋ⊤ A ẏ = max ẏ∈Y min ẋ∈X ẋ⊤ A ẏ,
where A is a sparse payoff matrix encoding the losses for player 1. For a strategy profile ( ẋ, ẏ), let δ 1 ( ẋ, ẏ) denote the incentive for player 1 to unilaterally choose another strategy:
δ 1 ( ẋ, ẏ) = ẋ⊤ A ẏ − min ẋ′ ∈X ẋ′⊤ A ẏ. Similarly, δ 2 ( ẋ, ẏ) = max ẏ′ ∈Y ẋ⊤ A ẏ′ − ẋ⊤ A ẏ. An ϵ-NE satisfies δ i ( ẋ, ẏ) ≤ ϵ, ∀i ∈ {1, 2}.
The exploitability of ( ẋ, ẏ) measures its distance from equilibrium and is defined by e( ẋ, ẏ) = i∈{1,2} δ i ( ẋ, ẏ)/2.