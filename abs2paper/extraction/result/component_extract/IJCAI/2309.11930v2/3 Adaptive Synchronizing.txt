3 Adaptive Synchronizing
To start with, we describe the proposed adaptive marginal loss which regularizes the learning pace of seen classes to synchronize the learning pace of the model. Conventionally, the margin is defined as the minimum distance of the data to the classification boundary. For a sample (x, y), we have:
∆(x, y) = f (x) y − max j̸ =y f (x) j (1)
Instead of employing a fixed margin, LDAM introduces a class-specific margin, where the margin between rare classes and other classes is larger than the margin for frequent classes, for tackling class-imbalanced data. Specifically, it sets the margin of class j as:
∆ j = C n 1/4 j (2)
The constant C controls the intensity and n j denotes the frequency of class j in the training data. Motivated by this, we
propose a new variant of the margin loss to synchronize the learning pace of seen and novel classes. We apply adaptive margin loss to demand a larger margin between the novel and other classes, so that scores for seen classes, towards which the model highly biased, do not overwhelm the novel classes.
For each sample (x, y), the adaptive margin loss is defined as follows:
ℓ AM (x, y) = − log exp(z y − ∆ y ) exp(z y − ∆ y ) + j̸ =y exp(z j ) where ∆ j = −KL π π π j max( π) C, j ∈ [K] (3)
In this formulation, K represents the total number of classes, z j signifies the model output for the j-th class, z = f (x; θ), and π denotes the estimated class distribution by the model. Additionally, we introduce an approximation of the true class distribution π, which is naturally inaccessible during training.
In line with prior studies, we assume a uniform distribution for π, leaving the exploration of arbitrary distributions for future investigations. The hyper-parameter C is introduced to control the maximum margin, and we empirically set C = 10 across all experiments. We conduct a series of studies on the value of C in the supplementary material.
For the sake of simplicity, we assume that the mini-batch is comprised of labeled data B l and unlabeled data B u . Given that the computation of Eq. ( ) relies on the class distribution, we proceed to estimate the complete class distribution through labeled data and unlabeled data exhibiting high predictive confidence. Specifically, we endeavour to achieve this estimation through:
π = Normalize   xi∈B l y i + xj ∈Bu I max( y j ) ≥ λ y j   (4)
Here, y = softmax(z). In view of the tendency for novel class samples to exhibit underconfidence, we empirically introduce a progressively evolving confidence threshold λ novel = 0.4 + 0.4 × t T , where t and T signify the current training iteration and the total training iterations, respectively. For seen classes, a fixed confidence threshold λ seen = 0.95 is employed.
Given that the estimated class distribution π mirrors the model's confidence in class predictions, we harness this insight to regulate the learning pace of both seen and novel classes. Notably, in the early training phases, the model is inclined towards seen classes, with the logit adjustment term ∆ j assuming a larger negative margin for seen classes, thereby attenuating their learning pace. As training progresses, the model attains a more balanced capability across both seen and novel classes, as reflected by a diminishing Kullback-Leibler (KL) divergence between π and π.
In summary, the adaptive margin loss L AM for both labeled and pseudo-labeled data is defined as follows:
L AM = 1 |B l | xi∈B l ℓ AM (z w i , y)+ 1 |B u | xj ∈Bu I max( y j ) ≥ λ ℓ AM (z s j , p j ) (5)
In this context, z w and z s correspond to the output logits stemming from the weak and strong augmented versions of sample x, respectively. The symbol | • | denotes the set cardinality operation. Additionally, we utilize p = arg max(softmax(z w )) to represent the pseudo-label associated with the sample.
Distinctions and Connections with Alternatives. It is worth noting that the concept of adaptive margin has been used in prior literature . Different from LPS, leverages the semantic similarities between classes to generate adaptive margins with the motivation to separate similar classes in the embedding space, and [Ha and Blanz, 2021] utilizes the ground-truth distance between different samples to generate adaptive margins with the motivation to adapt to rating datasets. In OpenSSL, ORCA also integrates an adaptive margin mechanism based on the model's predictive uncertainty, which can only equally suppress the learning pace of seen classes. However, there are still differences in the learning paces of different classes among seen classes. Our proposed adaptive margin is based on the current estimated distribution to reflect the learning pace of different classes, which offers increased flexibility for regulating the learning pace across classes by generating the class-specific negative margin. Furthermore, the inclusion of the KL divergence term effectively guards against the model converging to a trivial solution where all samples are arbitrarily assigned to a single class.
3.2 Pseudo-Label Contrastive Clustering
The basic idea of discovering novel classes is to explore the correlations between different samples and cluster them into several groups. Prior OpenSSL approaches often transform the clustering task into a pairwise similarity prediction task, wherein a modified form of binary cross-entropy loss is optimized. Different from existing works, we introduce a new clustering method to fully exploit reliable model predictions as supervisory signals.
Our approach involves the construction of a multi-viewed mini-batch by using weak and strong augmentations. Within each mini-batch, we group the labeled and confident unlabeled samples, which is denoted as B l ′ . Concurrently, unlabeled samples exhibiting predicted confidence levels failing below the threshold λ are denoted as B u ′ . Pseudo-label contrastive clustering only takes B l ′ as inputs. For each sample in B l ′ , the set of the positive pairs contains samples with the same given label or pseudo-label. Conversely, the set of negative pairs contains samples of other classes. Formally, the objective of pseudo-label contrastive clustering is defined as follows:
L PC = − 1 |B l ′ | xi∈B l ′ log 1 |P (x i )| xp∈P (xi) exp (z i •z p /τ ) xa∈A(xi) exp (z i •z a /τ ) , (6)
where z i denotes the output logits, P (x i ) denotes the set of positives of x i and A(x i ) ≡ B l ′ \{x i }. τ is a tunable temperature parameter and we set τ = 0.4 in all experiments.
In contrast to existing methods such as ORCA and NACH , which establish a single positive pair for each sample by identifying its nearest neighbour, the objective in Eq. ( ) adopts pseudo-labels to form multiple positive pairs. This approach offers dual advantages: firstly, the alignment of samples within the same class is more effectively harnessed through the utilization of multiple positive sample pairs; secondly, it leverages the consistency of distinct views of the same sample to mitigate the negative impact of erroneous positive pairs, while concurrently imparting a repulsion effect to samples from different classes through negative pairs.
Since Eq. ( ) augments the labeled dataset by unlabeled samples of high predictive confidence, we ask whether unlabeled samples of low confidence can be used to enrich representation learning. In pursuit of this, we incorporate unsupervised contrastive learning [Wang and to encourage similar predictions rather than embeddings between a given sample and its augmented counterpart. This helps to signify the uniformity among unlabeled samples, ultimately leading to clearer separations. In detail, for each sample in the low-confidence set B u ′ , the unsupervised contrastive learning couples it with its augmented view to constitute a positive pair. Simultaneously, a set of negative pairs is formulated, containing all the samples within the mini-batch except the sample itself. The unsupervised contrastive learning loss L UC is formulated as follows:
L UC = − 1 |B u ′ | xj ∈B u ′ log exp (z j • z p /τ ) xa∈ A(xj ) exp (z j • z a /τ ) (7)
Here, x p is the positive sample of x j and A(x j ) ≡ B u ′ ∪ B l ′ \{x j } for the sample x j .
In essence, unsupervised contrastive learning complements the pseudo-label contrastive clustering by fully exploiting the unlabeled samples. In the experiments, the ablation studies underscore the pivotal role played by both types of contrastive losses in our approach.
Lastly, we incorporate a maximum entropy regularizer to address the challenge of converging during the initial training phases, when the predictions are mostly wrong (e.g., the model tends to assign all samples to the same class) . Specifically, we leverage the KL divergence between the class distribution predicted by the model and a uniform prior distribution. It is worth noting that the integration of an entropy regularizer is a widespread practice in dealing with the OpenSSL problem, including approaches such as ORCA, NACH, and OpenNCD. The final objective function of LPS is articulated as follows:
L total = L AM + η 1 L PC + η 2 L UC + R Entropy (8)
where R Entropy denotes the entropy regularizer, η 1 and η 2 are hyper-parameters set to 1 in all our experiments. We provide detailed analyses on the sensitivity of hyperparameters in the supplementary material.