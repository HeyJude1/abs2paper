2 Related Work
Semi-Supervised Learning. Within the realm of SSL, pseudo-labeling and consistency regularization are two widely used techniques. Pseudolabeling converts model predictions on unlabeled samples into either soft labels or hard labels, subsequently employed as target labels. Consistency regularization strives to ensure model outputs exhibit a high degree of consistency when applied to perturbed samples. Recent advancements combine pseudo-labeling with consistency regularization to yield further performance enhancements. In addition to the above techniques, the application of contrastive learning into SSL has also received substantial interest. For example, TCL introduces contrastive learning as a tool to enhance representation learning. TCL maximizes agreement between different views of the same sample while minimizing agreement for distinct samples. In consonance with this paradigm, we design a new complementary contrastive loss to explore the consistency of all samples effectively.
Novel Class Discovery. The setting of NCD aligns closely with the scenario investigated in this paper. NCD assumes a scenario where the labeled data consists of samples of seen classes, while the unlabeled data exclusively comprises samples of novel classes. initially raised the NCD problem. Subsequent research such as predominantly adopted multi-stage training strategies. The underlying principle is capturing comprehensive high-level semantic information from labeled data, subsequently propagated to unlabeled counterparts. The majority of NCD methods involve preliminary model pre-training, wherein several objective functions are invoked to minimize inter-sample distances for each class. However, in real-world scenarios, the assumption of unlabeled data solely comprising novel classes is unrealistic, as seen classes also significantly populate the unlabeled dataset. Our experimentation reveals that NCD algorithms struggle to match the performance of other leading methods in the context of OpenSSL.
Open-World Semi-Supervised Learning. While conventional SSL methods operate under the assumption of labeled and unlabeled data being associated with a predefined set of classes, recent advancements ; 3 The Proposed Method
Notations. The training dataset is composed of the labeled data D l = {(x i , y i )} n i=1 and the unlabeled data D u = {x i } n+m i=n+1 . Within the context of OpenSSL, the classes in D l are designated as seen classes, constituting the set denoted as C s . The scenario of interest acknowledges a distribution mismatch, leading to D u comprising instances from both seen and novel classes. The collection of these novel classes is represented as C n . Additionally, we adopt the premise of a known number of novel classes akin to prior OpenSSL methodologies . The goal of OpenSSL is to classify samples originating from the C s and cluster samples emanating from the C n .
Overview. The fundamental challenge in OpenSSL arises from the pronounced discrepancy in learning paces between seen and novel classes, primarily due to the precise supervisory guidance for seen classes. This discrepancy results in a bias towards seen classes in the model's predictions, adversely impacting both the accurate classification of seen class samples and the effective clustering of novel class samples. To circumvent this challenge, we introduce Learning Pace Synchronization (LPS), a methodology with adaptive synchronizing loss and pseudo-label contrastive clustering as in Figure . The adaptive synchronizing loss aims to achieve a balance between the learning pace of seen and novel classes, and the pseudo-label contrastive clustering exploits pseudolabels to group unlabeled data from the same class together in the output space.