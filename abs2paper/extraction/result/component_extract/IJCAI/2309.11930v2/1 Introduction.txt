1 Introduction
Over the past decade, Semi-Supervised Learning (SSL) algorithms have demonstrated remarkable performance across multiple tasks, even when presented with a meagre number of labeled training samples. These algorithms delve into the underlying data distribution by harnessing numerous unlabeled samples. Among the representative methods employed for this purpose are pseudo-labeling and consistency regularization . Pseudo-labeling involves utilizing † Corresponding author model predictions as target labels, while consistency regularization encourages similar predictions for distinct views of an unlabeled sample. However, the majority of current approaches operate under the assumption that unlabeled data exclusively comprises samples belonging to seen classes, as observed within the labeled data . In contrast, the presence of samples from novel classes in the unlabeled data is common, as it is challenging for human annotators to discern such instances amidst an extensive pool of unlabeled samples .
To aid this challenge, Open-World Semi-Supervised Learning, denoted as OpenSSL, has gained recent attention, leading to the proposition of several effective methodologies . Figure demonstrates the problem setting of OpenSSL as an intuitive example. To tackle this issue, extant methods adopt a two-pronged strategy. On one front, they endeavour to identify unlabeled samples pertaining to seen classes and allocate pseudo-labels accordingly. On the other front, they automatically cluster unlabeled samples belonging to novel categories. Notably, OpenSSL shares an affinity with Novel Class Discovery (NCD) , particularly concerning the clustering of novel class samples. However, NCD methodologies presuppose that unlabeled samples originate exclusively from novel classes. OpenSSL relaxes this assumption to mirror real-world scenarios more accurately. Evidently, the central challenge of effectively clus-arXiv: LG] 17 Apr 2024  tering novel class samples hinges upon the acquisition of discriminative feature representations, given the absence of supervisory information. To mitigate this quandary, existing methods harness self-supervised learning paradigms (e.g., ) which circumvent the need for labeled data during the training of feature extractors within deep neural networks. Subsequently, a linear classifier is cultivated by optimizing the cross-entropy loss for labeled data, in conjunction with specifically tailored unsupervised objectives for the unlabeled counterpart. Widely employed unsupervised objectives include entropy regularization and pairwise loss, both of which effectively enhance performance.
This paper introduces a novel OpenSSL algorithm. An initial observation reveals that the model exhibits faster learning of seen classes compared to novel classes. This discrepancy is intuitive because of accurate supervision within labeled data for seen classes, whereas novel classes are learned through unsupervised means. Figure This unsupervised contrastive objective operates as a complement to the pseudo-label contrastive clustering. Combining the aforementioned modules, we present, LPS, to address the OpenSSL challenge. Figure showcases the efficacy of LPS compared with existing state-of-the-art approaches. Notably, we reveal that the conventional practice of freezing the feature extractor, previously trained via self-supervised learning in prior research, falls short of optimal.
In summary, our main contributions are:
• We propose a novel and simple method, LPS, to effec-tively synchronize the learning pace of seen and novel classes for open-world semi-supervised learning.
• We conduct extensive experiments to verify the effectiveness of the proposed method against the previous state-of-the-art. Particularly, LPS achieves over 3% average increase of accuracy on the ImageNet dataset.
• We examine the effectiveness of the key components of the proposed method. Different from previous works, we discover that fine-tuning the pre-trained backbone allows the model to learn more useful features, which can significantly improve the performance.