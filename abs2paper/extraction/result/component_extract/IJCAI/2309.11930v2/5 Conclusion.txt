5 Conclusion
In this study, we present Learning Pace Synchronization (LPS), a potent solution tailored to the OpenSSL problem. LPS introduces an adaptive margin loss to effectively narrow the learning pace gap that exists between seen and novel classes. Moreover, we formulate a pseudo-label contrastive clustering loss to augment the process of novel class discovery. Extensive evaluation is conducted across three benchmark datasets with distinct quantities of labeled data. We also discover that the conventional practice of freezing the self-supervised pre-trained backbone hinders the generalization performance. We hope our work can inspire more efforts towards this realistic setting.
Parameters η 1 and η 2 define the weight of the pseudo-label contrastive clustering loss and the unsupervised contrastive learning loss, respectively. We conduct several experiments on the CIFAR datasets with various values of η 1 and η 2 to assess the performance of our method LPS, and the results are shown in Table . By further adjusting the parameters η 1 and η 2 , our method LPS displays great robustness and promising results. We also provide detailed analysis for λ novel . In particular, we set λ novel = 0.4 + {0.3, 0.35, 0.4, 0.45, 0.5} × t T and the results are reported in Table . We find that higher values of λ novel achieve better performance on seen classes. Intuitively, higher values of λ novel will pseudo-label less novel unlabeled samples and further let L AM give more importance to seen classes.
Recall that C defined in L AM is a tunable parameter to control the maximum margin. Table displays the results of LPS under varying C conditions. Intuitively, increasing the C will lead to a faster alignment between the predicted distribution and the prior distribution. From the results of C = 20 in the Table , if the alignment is too fast, the model may balance the learning pace between seen and novel classes by assigning incorrect pseudo-labels. Meanwhile, if the alignment is too slow, the margin mechanism does not effectively bias the model towards novel classes, which is reflected in the results of C = 1 and C = 5 in the Table .
CIFAR-10
CIFAR-100 C Seen Novel All Seen Novel All The temperature parameter τ within L PC and L UC is used to adjust the measurement of similarity between samples. Increasing the temperature will lead to a flatter representation space for samples. Conversely, decreasing it results in a sharper space. We conduct a series of experiments by setting various temperature values. Table depicts that alterations in τ do not exert a pronounced impact on performance and τ = 0.4 yields the best performance.