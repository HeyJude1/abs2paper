2 Transition Latency
Transition latency is the time required to transition from an idle state (e.g., C6) to an active state (e.g., C0) and vice versa and includes steps like saving/restoring the context of the core in an external SRAM, switching on/off the VR or PLL, and flushing L1D/L2. All C-states incur a transition latency; the shallower the state, the shorter its transition latency. Usually, the latency reported by the vendors (see Table ) along with each C-state represents the hardware transition round trip time and includes the time to transition to and from a C-state. Transition latency varies based on architecture and workload. Architecture choices, such as type of VR (i.e., Switching Voltage Regulator, Low Dropout Voltage Regulator ) impacts core frequency changes (C1E, sets V/F of core to minimum), as the VR type affects the power conversion efficiency and thus the time it will take to ramp up the voltage and frequency to a target V/F. Workload also matters; cache "dirtiness" of a workload affects cache flush time (C6). We analyze the C6 entry/exit latency for a Skylake-based server architecture. The step that takes the longest to execute when entering C6 is flushing of the L1D/L2 cache. This duration is variable, determined by factors such as clock frequency and the count of dirty blocks in the cache. We conservatively choose a clock frequency of 800MHz and "cache dirtiness" of 50% to estimate the time of this step, which is around ∼75 μs. In comparison, the latency for transferring the core context (e.g., control registers, patch RAM) is ∼9 μs for frequency of 800MHz and core context of 8KB. As a result, entry time of C6 including the flow overhead and the power-gate controller overhead is ∼87 μs. The exit time is dominated by the time required to restore the content of the core ( ∼20 μs). Power ungating, PLL relock, and fuse propagation take ∼10 μs, making the overall exit time of C6 ∼30 μs.
2.2 Cold-Start Latency
Power gating is a technique used during deep core sleep states (C6). When a core is power gated, it is essentially turned off, causing the loss of an application's architectural and microarchitectural state (e.g., cache and branch predictor state). When reactivated to re-execute the same application on the core, it faces longer execution times, as the core needs to retrain its cache and branch predictor state, resulting in additional cache misses and branch mispredictions. The magnitude of this effect, which we call cold-start latency , depends on the amount of state that is lost when power gating and the performance benefit that the running application is able to extract from this state.
Previous work found cold-start latency to be in the order of microseconds , with negligible impact on applications that execute for several milliseconds. However, as discussed earlier, microservice-based applications have per service active times of 30-250 μs , making the impact of cold-start latency non-negligible.
2.3 Interplay of Core C-State Overhead with Core Components
In this section, we present how different core components affect the transition latency and coldstart latency of deep C-states. Specifically, we present how the state loss caused by power gating each core component can have consequences either on the performance or the correct functionality of the core and how these consequences correlate with the attributes (i.e., transition latency and cold-start latency) of C-states. The taxonomy plays a vital role in shaping the development of the new Agile C-state later on. In the taxonomy presented in Table , architectural resources represent the resources for which state loss (after power gating) can lead to incorrect core behavior. Patch RAM is a representative example of this group. Patch RAM is memory within the core that holds updates to software drivers and firmware with the aim to protect the core against vulnerabilities that are identified after the core release. The state of the Patch RAM needs to be present once the core exits power gating for the core to have reliable behavior. Non-architectural resources represent resources for which state loss cannot lead to incorrect core behavior but rather affect the performance of the core. Branch predictor state is not necessary for the core to execute but can significantly optimize the execution time of a predictable program.
We observe that all of the resources that impact the transition latency are also architectural and all of the resources that affect cold-start latency are non-architectural. Finally, there is a subset of resources that affect both categories. L1D, for example, affects both transition and cold-start latency. Before power gating, the cache content needs to be saved, and after power ungating, the cache needs time to warm up. We include PDN and clock resources to complete the taxonomy.
The division of the resources into the groups we described previously allowed the division of the core structures into structures of which the state must be preserved by the proposed C-state (i.e., architectural) and structures of which the state must be preserved only if it significantly affects the performance of the core (i.e., non-architectural). It also offered an initial latency breakdown since the size of the state of the structure positively correlates with the overhead the structure causes.
2.4 Microservice Interplay with Core C-States
A microservice-based application is structured as a composition of multiple services that can be deployed independently. These services interact using well-defined APIs and communication protocols, such as HTTP, Message Queues, and RPC. Notably, a significant portion of the overall response time is attributed to communication overhead. Consequently, the computational aspects of microservices must adhere to stricter QoS constraints to meet the same requirements as compared to traditional monolithic applications. Usually, the QoS constraints per service are in the order of 30-250 μs , making both the transition and cold-start latency notable.
Besides having short execution times, microservices operate at low utilizations (5%-25% ) to keep the tail latency under control. However, as requests are characterized with irregular arrival times , the system's idle time is unpredictable. To avoid the transition and cold-start latency overheads, datacenter operators eventually disable the deep idle core states , even though a system that runs microservices is idle most of the time . Even if the request arrival time was predictable, and energy could be reduced by entering a deep idle core state, the cold-start latency problem remains. As a result, legacy core C-states are unattractive for modern servers running latency-critical microservice-based applications.
Finally, throughout its lifetime, a quer y executes on multiple ser vices, each of which could run on different cores. Each core has its own idle governor who decides which C-state the core should enter. The primary heuristic used in the C-state selection algorithm is the local core history.  Consequently, due to the independent nature of the decision-making process for entering core C-states, a query may encounter cold-start latency multiple times during its execution.
2.5 Quantifying Cold-Start Latency
To estimate the worst-case impact of cold-start latency, we execute several microservice-based benchmarks. Memcached is a lightweight key-value store that is widely deployed as a distributed caching service to accelerate latency-critical applications . MicroSuite is a microservice-based benchmark suite that contains information-retrieval services (HDSearch, Router, SetAlgebra, Recommend). HDSearch is an image similarity search service. Router is a Memcached protocol router. SetAlgebra performs posting list set intersection. Finally, Recommend is a recommendation service. Figure presents the architecture of the MicroSuite benchmarks. To achieve the effect of cold and warm resources, we configure the C-states of the system to enabled or disabled. C-states enabled represents the scenario where a query is served on cold resources, since when the system enters C6, it power gates the core, and C-states disabled represents the scenario where a query is served with warm resources. To quantify the worst-case cold-start overhead, which represents a scenario where ever y quer y sent by the client executes on a cold core that exits C6 in response to the arrival of the query, we develop a new client. We configure the client to run on the same node as the benchmarks and have the following characteristics: (1) contains a warm-up phase to allow the system to enter C6 (system does not enter C6 for the first queries), and (2) sends queries with big enough interval that allows the system to enter C6 between successive queries.
We use Top-Down analysis to examine the components of the core that contribute to the cold-start latency. Figure presents the Level 1 Top-Down analysis. The analysis reveals that the worst-case impact of cold-start latency on the CPI of each tier of an application is significant and it ranges from 25% to 126% . A possible reason for this variation is the execution time at each tier.
Additionally, the analysis shown in Figure reveals that the categories with the highest overhead are frontend bound and bad speculation. This is because, for both the frontend bound category and the bad speculation category, the contribution to the CPI increases when the application is running on cold resources versus when it is running on warm resources. The performance degradation caused by the frontend resources and bad speculation is 27% , 66% / 55% , 21% / 106% / 126% , 92% / 98% , and 21% / 96% for Memcached, HDSearch, Router, SetAlgebra, and Recommend, respectively. In most of the cases examined, the midtier experiences higher cold-start overhead (on average ∼93 . 75% ) than bucket (on average ∼71 . 25% ). This is because midtier enters two times more the core C-state C6 during the lifespan of a query compared to a bucket: (1) when it waits for the client to send a query and (2) when it waits for bucket response.   Figure narrows down the sources of performance degradation to frontend bound-branch resteers, frontend bound-others (ITLB, L1I, etc.) and bad speculation. Specifically, it depicts the slowdown in terms of CPI for frontend bound and bad speculation over the baseline scenario (cold vs warm). We include branch resteers as it is the frontend category (Top-Down level 3) that contributes the most to the overhead caused by cold-start latency followed by ITLB misses and L1I misses. Branch resteers represent the number of slots lost between resolving the branch instruction until renaming an instruction from the correct path. Along with bad speculation, they represent the penalty caused by branch mispredicts. With cold resources, the penalty caused by branch mispredicts as compared to warm resources increases by 46%-233% ( ∼118% on average). The overhead caused by the rest of the frontend resources increases by 12%-166% ( ∼50% on average).
Figure presents the MPKI instruction analysis for all benchmarks and tiers evaluated. It is interesting to notice that the MPKI figure correlates with the results of the Top-Down analysis since the instruction cache and the branch predictor are two of the structures with the highest number of misses. Both L1D and L1I cache have high MPKI for both the cold and warm scenarios, indicating a large instruction footprint and working set for all tested benchmarks.
We have shown that the cold-start overhead affects the performance of a microservice . The categories with the highest impact are frontend bound and bad speculation. Out of all resources of the frontend, branch predictors cause the highest overhead followed by L1I cache and iTLB. We conclude that all data arrays of the core (both architectural and non-architectural) contribute significantly to the cold-start overhead and need to be kept warm in a C-state architecture targeted for microservice-based latency-critical applications.
2.6 Real-World Impact of Cold-Start Latency
The dynamic workload behavior of latency-critical applications based on microservices prevents cores from entering deep sleep states even during periods of low utilization . As a result, in a real-world scenario, a workload would rarely experience cold-start latency. Thus, in this section, we analyze whether or not it is important to consider cold-start latency in the design of the new Agile C-state architecture, and what the impact of cold-start latency on the performance of a microservice would be if we only optimized the transition latency of C6. We do this by estimating the worst-case impact of cold-start latency on the average response time for a C-state with transition latency the same as C1 and cold-start latency the same as C6. Since microservices 66:9  operate at low utilization to keep the tail latency under control, we consider utilization between 5%-25% normal conditions, but we also examine higher utilization.
Figure (a) presents an upper limit of the percentage of Memcached queries that will enter C1 and thus experience cold-start latency. This is an upper limit because we do not take into account how the execution time increase caused by cold-start overheads affects the idle time of the system and thus the number of C1 transitions. Figure (b) presents how much the average response time of Memcached will increase due to cold-start latency based on the number of C1 transitions. Our analysis shows that for query rates between 10K and 100K QPS (queries per second) (5%-15%) all queries of Memcached can experience cold-start overheads and so the average response time increases by 27% . For the higher QPS of 200-500K (20%-60%), 90% to 40% of queries experience cold-start latency-in other words, the average response time increases from 24% to 10 . 8% . The rest of the article illustrates how C6Awarm/C6AwarmE can eliminate both transition and coldstart latency while maintaining most of the power savings of deep idle states (i.e., C6).