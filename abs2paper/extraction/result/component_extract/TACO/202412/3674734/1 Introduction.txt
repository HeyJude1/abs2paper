1 Introduction
Servers in today's datacenters are utilized inefficiently when running latency-critical applications based on microservices. The load unpredictability characterizing latency-critical applications makes it unfeasible for the system to use energy-saving techniques such as CPU idle sleep states . Even if the application load could be predicted and energy-saving techniques could be utilized, the substantial performance overhead introduced by CPU idle sleep states (Table ) poses a serious threat to the performance of latency-critical applications (30-250 μs ). Consequently, datacenter operators opt to deactivate energy-saving features such as idle CPU sleep states, thereby removing any potential performance overheads at the cost of higher power. The gravity of this issue becomes more pronounced when considering that servers running latency-critical applications frequently operate at low utilization levels to keep tail latency under control . As a result, even though a system is mostly idle, it consumes energy at a rate similar to when it is fully active.
Core C-states are a popular power management method whereby an idle core enters a power state to reduce or even eliminate its power consumption. Table presents the core C-states of an Intel Skylake-based server processor, along with their power consumption and transition latency. Transition latency represents the time the core requires to transition from one state to another. During the transition, the core is unable to perform any work. For example, it is estimated that C6 requires 133 µs transition time. As a result, entry/exit latencies can degrade the performance of services that have microsecond latency requirements, such as user-facing applications . Additionally, for a transition to result in power savings, the core needs to reside in the idle state for a minimum amount of time, referred to as target residency time . Target residency time represents the minimum time required by the system to amortize the transition energy overhead.
The bursty dynamic behavior of latency-critical applications based on microservices makes the idle time unpredictable and prevents a core from entering deep sleep states. Even if the Agile C-States: A Core C-State Architecture for Latency Critical Applications 66:3 133 µs 600 µs ∼0 . 1 W power management system could predict the idle time, deep core sleep states would not be utilized because their transition latency is close to the QoS constraints for latency-critical applications. For example, Memcached is a microservice with typical QoS requirements of 30-250 μs. An entry/exit from a deep sleep state (i.e., C6, 133 μs) consumes at least 50% of the latency budget, increasing the risk for QoS violations. Moreover, even if QoS violations could be avoided, the target residency for deep sleep states is often longer than the typical core idle period for Memcached .
Besides unpredictability of idleness and transition latency, the time required to warm up the core resources after power gating them further increases the performance overhead and reduces the opportunity to benefit from entering deep core sleep states. We refer to the time needed to warm up the data arrays of the core as cold-start latency .
The operating system (e.g., Linux) decides which C-state to enter based on an idle governor . Current idle governors only consider C-state transition latency and target residency and disregard cold-start latency. This is because idle governors were designed for client devices and applications and assume that workloads typically exhibit relatively long and predictable active and idle periods. This behavior is not representative for microservices because the computation time per microservice invocation often falls in the 30-250 μs range, and (2) servers that run microservices typically operate at low utilization to harness tail latency. These characteristics underline the significance of the cold-start latency due to deep core idle states.
Prior work proposes solutions to enable the effective use of existing power management techniques like C-states and Dynamic Voltage Frequency Scaling (DVFS) . The solutions include fine-grained latency-aware DVFS or/and core C-states management and core consolidation . Another set of works tries to quantify the cold-start latency either through power gating or process migration with the aim to model it or eliminate it through prefetching or improve idle governor decisions . Finally, another group of works focuses on improving idle time predictability through the use of machine learning. Our work differs from the previous ones because (1) it optimizes power consumption, the analysis focuses on both transition and cold-start latency of core C-states, and (3) the proposed solution is a hardware-based solution incorporated into the current core C-state architecture.
We propose C6Awarm, a new deep core C-state for processors in datacenters running latencycritical applications. C6Awarm goal is to reduce both transition and cold-start latency while achieving significant energy savings. C6Awarm achieves this goal by using the following techniques. First, instead of switching off the entire core in deep sleep states, C6Awarm uses mediumgrained power gates to maintain the core context (e.g., Control Registers, Patch RAM, microcode firmware) in place, avoiding the time to save/restore from an external SRAM. Second, instead of shutting down the structures of the core containing a program's architectural and microarchitectural state (e.g., branch predictors, TLBs, prefetchers, L1D, L1I, L2), C6Awarm keeps them power