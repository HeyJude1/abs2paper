5 Experimental Methodology
We evaluate AW using Memcached and MicroSuite (see Section 2.5 ).
G. Antoniou et al.
System . We use the CloudLab infrastructure for the experimental evaluation. Our baseline server is equipped with two Intel Xeon Silver 4114 Skylake-based processors running at a nominal frequency of 2.2GHz (minimum frequency of 0.8GHz and maximum Turbo Boost frequency of 3GHz), with 10 physical cores for a total of 20 hyper-threads, and with 192GB DDR4 DRAM. We choose to disable turbo in all cases to eliminate variations among runs.
Workload Setup . For evaluating Memcached, we run a single Memcached server process on one of the server machines and run a modified version of the Mutilate load generator on the rest of the machines. We configure the load generator to recreate the ETC workload from Facebook , using one master and four workload generator clients, each running on a separate server machine. We use three nodes to evaluate MicroSuite benchmarks (client, midtier, bucket). Our benchmark configuration follows the configuration of the MicroSuite work . Finally, we pin the processes onto specific cores to eliminate process migration.
5.1 Power and Performance Model
We model the average power of a core using the fraction of time spent at each unique C-state and its corresponding power consumption. Similar to prior works , we focus on CPU power, which is the single largest contributor to server power . Next, we describe our models for the baseline and AW.
Baseline CPU Core Power . Our analytical power model estimates the average CPU core power for a workload, assuming that P-states/Turbo are disabled, as
Avд P = i ∈{ 0 , 1 , 1 E, 6 } P C i × R C i . P C i
denotes the core power in state Ci (reported in Table ). R C i denotes the residency at Ci-that is, the percentage of the total time the system spends at state Ci. We obtain C-state residency and number of transitions using the processor's residency reporting counters . When executing our workloads, we use the RAPL interface to measure power consumption. The power model used for the baseline scenario has been validated and has an accuracy of 95% .
AW CPU Core Power . We model the power consumption of the CPU core enhanced with the two new C-states of AW (i.e., C 6 Aw arm and C 6 Aw armE) using (1) measured data from our baseline power model (C-state residency is scaled using our performance model; more details are presented in the following), and (2) estimated power of the C 6 Aw arm and C 6 Aw armE, as summarized in Table . We compute the average power in a similar way as for the baseline core, summing the power of each C-state weighted by its residency fraction.
To estimate power for the new C-states, we perform the following steps. First, we obtain the power and residency of each core C-state from the baseline. We scale the C-state residency taking into account (1) how the small core frequency degradation incurred due to the power gates (Section 4.1.1 ) affects performance by considering a workload's frequency scalability and (2) the higher C 6 Aw arm/ C 6 Aw armE transition latency (i.e., 90ns; Section 4.2 ) compared to ). We plug in the new values to estimate the average AW CPU core power.
C 1 / C 1 E. Sec- ond, we replace C 1 / C 1 E C-state residency (i.e., R C 1 / R C 1 E ) with C 6 Aw arm/ C 6
We compute AW's average power for workloads with Turbo enabled (and P-state disabled) as Naming Conventions . We use a triple-based naming scheme for concisely referring to different metrics and scenarios: Metric_Process_Scenario , where Metric can be T : Time (ms, us), P : Power (W), Process can be M : Midtier, B : Bucket, C : Client, S : Server, and Scenario can be B : Baseline C6Aw : C6Awarm.
AvдP s avinд s = R C 1 × (P C 1 − P C 6 Aw ar m ) + R C 1 E × (P C 1 E − P C 6
Performance Model . We model the average response time for each benchmark on each scenario (Baseline, C6Awarm) using a combination of real measurements and equations. Specifically, we use the fraction of time each query spends on each tier (i.e., midtier, bucket, server), the number of C-state transitions, and the scalability analysis of each benchmark. We assume that the network/queueing time and the decisions of the idle governor remain the same.
Baseline CPU Core Performance . To estimate the performance impact of C6Awarm, we first calculate the performance of the baseline scenario experimentally (we use counters to extract the timing regions we are interested in). In our baseline run, we extract three performance values: the midtier processing time ( T _ M _ B), the bucket processing time ( T _ B _ B), and the end-to-end average response time ( T _ C _ B). Since the tiers of the benchmarks evaluated execute sequentially, we are able to estimate the impact of C6Awarm on each tier and then use addition to evaluate the impact on the end-to-end response time.
AW CPU Core Performance . Once we have the baseline performance, we estimate the overhead of C6Awarm on each tier (midtier, bucket). We scale the midtier/bucket processing time by taking into account the frequency degradation due to power gating (see Section 4.1.1 ) using frequency scalability analysis and the higher transition latency (i.e., 90ns; Section 4.2 ) of C6Awarm/C6AwarmE. Then we add up the overheads of each tier and estimate the impact on the end-to-end average response time.