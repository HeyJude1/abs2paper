4 AW Implementation and Hardware Cost
As discussed in Section 3 , AW requires the following in each CPU core: (1) the UFPG subsystem, (2) the CCSM subsystem, and (3) the C6Awarm controller. This section discusses the implementation of each component, its power-performance-area cost, and the resulting transition latency for the new C6Awarm and C6AwarmE states.
4.1 Power-Performance-Area Modeling Methodology
In this section, we describe each of the modeling components in detail. Table summarizes the total area overhead and power consumption of AW's C6Awarm/C6AwarmE C-states. Our power and performance model (described in Section 5.1 ) uses C6Awarm/C6AwarmE power and AW performance overheads to estimate the average power consumption and performance impact of AW for a given workload. Our discussion focuses on the Power, Performance, and Area overhead of C6Awarm and points out the differences of C6AwarmE when relevant.
4.1.1 Units' Fast Power Gating.
As discussed in Section 3.1 , AW's UFPG places the majority of the core units behind power gates that are similar to the ones used for the AVX units in recent Intel cores. AW uses power gates for ∼55% of the core area (measured on the die photo in Figure ).  Power Overhead . The AVX power gates and the new UFPG shut off all units in the core execution domains; however, power gates only eliminate 95%-97% of the leakage power , thus the UFPG domain has residual idle power while in C6Awarm. Using the Intel core-powerbreakdown tool , we derive the leakage power contribution of the power-gated units starting from the leakage power of the entire core. Our estimation shows that the C6Awarm power-gated area contributes to approximately 55% of the core leakage (i.e., ∼55% of C1 power). Hence, the power overhead of UFPG (i.e., 3%-5% of the gated leakage power) is ∼24-40mW at base frequency (P1, C6Awarm) or ∼15-24mW at minimum frequency (Pn, C6AwarmE).
The three UFPG techniques combined retain ∼8KB context , which consume ∼0.2mW at retention voltage . To estimate the retention power at base (P1) and minimum (Pn) frequencies, we conservatively multiply the retention-level power by 10 × and 5 ×, respectively. Our estimate for context retention power is ∼2 mW (P1, C6Awarm) to ∼1 mW (Pn, C6AwarmE).
Performance Overhead . In an active CPU core, simultaneous operations in memory or/and logic circuits demand high current flow, which creates fast transient voltage droops . One power-gating design challenge is the resistive voltage (IR) drop across a power gate, which exacerbates voltage droops . The worst-case voltage droop can limit the maximum attainable frequency at a given voltage since it requires additional voltage (droop) margin above the nominal voltage to enable the CPU core to run at the target frequency . An x86 implementation of a CPU core power gate leads to < 1% frequency loss . Our AW analytical model (Section 5.1 ) assumes 1% frequency degradation due to UFPG. Area Overhead . A power gate adds 2%-6% extra area to the gated logic (i.e., UFPG , covering ∼55% of core area). We conservatively use the wide overhead range (i.e., 2%-6%) since the exact overhead depends on the implementation, the size of the gated area, the number of required isolation cells, and technology . The area overhead for the in-place context retention techniques 66:14 G. Antoniou et al.
of the ∼8 kB core context is as follows. First, moving the context of a unit to ungated power typically requires < 1% of the context area, mainly due to the isolation cells . Second, the use of SRPGs for components with too large or distributed context is a mature technique already used in productssuch as the Intel Skylake . Efficient SRPG designs, which use selective context retention, require less than 1% additional area relative to the power-gated area they control . Finally, including an SRAM into the ungated domain requires isolation cells that add overhead < 1% of the SRAM area .
4.1.2 Cache Coherence and Sleep
Mode. AW implements sleep mode for private caches similarly to the sleep mode used for L3 caches in multiple server products . Cache sleep mode implements P-type sleep transistors with seven programmable settings and local bitline float to reduce SRAM cell leakage in the data array; it also employs word-line sleep to reduce leakage further.
Power Overhead . CCSM implements sleep mode using sleep transistors in the L1I/L1D/L2 SRAM data array and branch predictors; this technique is already used for efficient design of the L3 cache in multiple server processors on the market . Based on a comparable CCSM implementation , we extrapolate a retention factor (0.25), representing how much the leakage power of a core array will reduce once we apply sleep transistors on it. We do not use the same methodology as in prior work for estimating the power consumption of CCSM, since characteristics of structures included in the CCSM domain of C6Awarm, such as size of the branch predictor, are unknown. However, we try to validate the results of the methodology described previously using the same methodology as in prior work and pessimistic structure sizes. The resulting leakage power estimation for L1/L2 and branch predictors is 162mW for C6Awarm and 111mW for C6AwarmE, respectively.
Performance Overhead . In AW, only the data array (accounts for more than 90% of L1/L2 cache size) is placed in sleep mode, whereas the other control arrays (e.g., tag, state) operate at nominal voltage. Doing so allows hiding the data array wake-up latency during the control array access time, thereby eliminating any performance degradation compared to operation without the sleep mode.
Area Overhead . Implementing sleep mode using sleep transistors for the SRAM data array of the private caches and branch predictors requires additional area similar to power gates (i.e., 2% -6% of the SRAM area) ); a recent implementation reports a 2% area overhead .
4.1.3 AW Power Management Control Flow.
The main implementation consideration to realize the C6Awarm flow in Figure is a mechanism to control in-rush current . This needs to support staggered wake-up, so as to ensure PDN stability . The remaining capabilities, such as clock gating and event detection (interrupts, snoops), are all commonly supported in state-of-the-art SoCs. The C6Awarm controller is implemented using a simple finite-state machine within the core's PMA, which resides in the uncore and controls clock gating/ungating, save/restore signals, and L1I/L1D/L2/branch predictor entry-to/exit-from sleep mode. The C 6 Aw arm snoop flow reuses the existing snoop handling mechanisms of the C1 state (shown in Figure ).
Power Overhead . AW implements the C6Awarm controller as finite-state machine within the PMA. Based on a comparable power management flow implemented in prior work , we estimate that the C6Awarm controller adds approximately 5mW to the PMA power.
Performance Overhead . The additional control circuit we add to the PMA has no direct impact on the CPU core's performance. The performance overhead is mainly due to the new features the PMA controls (described in Section 4.1.1 and Section 4.1.2 ).
Agile C-States: A Core C-State Architecture for Latency Critical Applications 66:15 Area Overhead . Based on a comparable power management flow implemented in prior work , we estimate the additional area to implement the C6Awarm controller to be up to 5% of the core PMA area.
Core PLL and FIVR. AW keeps the PLL and FIVR powered on in C6Awarm/C6AwarmE states. Next we describe only the power overhead of the PLL and FIVR as there is no extra performance or area overhead compared to baseline.
Power Overhead . We estimate the C6Awarm idle power needed by AW to keep the PLL on and locked while accounting for voltage regulator inefficiencies. The Skylake core uses an ADPLL and a FIVR . The ADPLL consumes 7mW (fixed across core voltage/frequency levels ). The FIVR presents dynamic efficiency loss due to conduction and switching inefficiency , and static efficiency loss due to power consumption of the control and feedback circuits . The static loss still applies when the FIVR output is 0V. The FIVR static loss accounts for ∼100 mW per core . The FIVR efficiency at light load is about 80% (excluding the static power losses) .
4.2 C6Awarm and C6AwarmE Latency
We estimate the transition time (i.e., entry followed by direct exit) required by the extra actions taken by C6Awarm/C6AwarmE of AW compared to C1/C1E to be < 90 ns. This is three orders of magnitude faster than the > 100 µs latency of C6. Thus, the overall transition time of C6Awarm/C6AwarmE is 90ns plus the transition time of C1. Next, we explain in detail this estimation using Figure .
C6Awarm and C6AwarmE Entry Latency . Clock gating all domains and keeping the PLL ON ( in Figure ) typically takes 1-2 cycles in an optimized clock distribution system . Transitioning to Pn (required for C 6 Aw armE) happens with a non-blocking parallel DVFS (i.e., P-state) flow that can take few tens of microseconds, depending on the power management architecture . Since AW keeps the context in-place, saving the context to power gate the core units only requires asserting the Ret signal followed by de-asserting the Pwr signal . We estimate this process to take 3-4 cycles. Finally, placing the L1I/L1D/L2 caches and branch predictors in sleep mode and clock gating them takes 1-3 cycles. Hence, the overall entry flow takes < 10 cycles-that is, < 20ns with a power management controller clocked at 500MHz. C6Awarm and C6AwarmE Exit Latency . Clock ungating the L1I/L1D/L2 caches and branch predictors and exiting sleep mode takes 2 cycles . Power ungating the core units takes < 60ns, and restoring the core context (i.e., de-asserting the Ret signal after restoring power) takes 1 cycle. Finally, clock ungating all domains typically takes 1-2 cycles. Hence, the overall exit flow takes ∼5 clock cycles + < 60ns, equivalent to < 70ns when using a 500MHz clock for C6Awarm/C6AwarmE.
C6Awarm and C6AwarmE Snoop Handling . Snoop handling latency in C6Awarm (C6AwarmE) is similar to that in C1 (C1E). Specifically, clock ungating the L1I/L1D/L2 caches and exiting sleep mode takes 2 cycles. In the first cycle, the flow ungates the clock; in the second cycle, the snoop requests simultaneously (1) access the cache tags (power ungated) and (2) wake up the cache data array . Placing the L1/L2 in sleep mode and clock gating L1/L2 caches after servicing the snoop traffic takes 1-3 cycles.