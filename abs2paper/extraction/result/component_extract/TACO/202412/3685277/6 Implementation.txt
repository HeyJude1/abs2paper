6 Implementation
In this section, we introduce the implementation details of sparse neural networks inference, including SpMM kernel and end-to-end DNN inference.
6.1 SpMM Kernel
The implementation of thread tile in an SpMM kernel is shown as Algorithm 2. During the code generation process, we entirely unroll lines 7-18 and fill in the corresponding sparse matrix elements, A[a, k], which we can know at compile time, according to the constant propagation technique . This approach enables the utilization of the constant cache for accessing A values, thereby minimizing random memory accesses caused by sparse indices. The accumulator, Acc, is stored in registers to enable rapid access. To further improve the performance of an SpMM implementation, we incorporate a prefetching technique for the matrix B data. Specifically, we allocate additional registers, enabling simultaneous data prefetching from matrix B while computing the current column of the sparse matrix A (lines 14-18). Besides, to avoid unnecessary memory access to the dense matrix, when
nnz of A[ROW list , b] is empty, we do not cache B[b, N thr ead ] in registers.
Based on the programming model for GPUs, the implementation of thread tile can be naturally extended to warp tile and block tile, respectively.
6.2 End-to-end DNN Inference
We implement end-to-end inference for sparse neural networks by combining LO-SpMM with SparTA and Rammer. First, we use SparTA to propagate sparse properties of sparse neural networks and obtain networks with higher sparsity. For each SpMM kernel, we obtain the Top-1 SpMM implementation using tiling search reduction, sparse matrix reordering, and proxy-based evaluation. After all SpMM implementations are generated, we optimize the neural network using Rammer. Rammer can improve the DNN performance through operator fusion and operator parallelism. Then, we replace all the GEMM kernels in the neural network with the SpMM kernel implementations from LO-SpMM .
LO-SpMM also supports the implementation of the sparse Conv2D operator by converting it to a combination of im2col and SpMM. For sparse Conv2D, LO-SpMM does not need to explicitly perform the im2col operator. When implementing Conv2D using SpMM, each reference to an element in the B matrix is replaced with the corresponding element in the original input activation tensor.