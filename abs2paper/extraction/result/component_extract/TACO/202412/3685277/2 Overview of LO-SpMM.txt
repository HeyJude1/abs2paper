2 Overview of LO-SpMM
The overall framework of LO-SpMM is illustrated in Figure , which consists of two stages: tiling space reduction, and proxy-based evaluation. Given a sparse matrix A of size M × K, and a dense matrix B of size K × N , LO-SpMM aims at quickly generating a high-performance SpMM implementation on a GPU for calculating C = A×B.
In the first stage, LO-SpMM utilizes a hierarchical 2-dimensional tiling strategy to tile the computations in the SpMM kernel. Different tile sizes form the search space for constructing SpMM implementations. LO-SpMM prunes search space by several constraints, based on criteria such as register resource, hardware utilization, and load balancing. A sparse matrix reordering operation is performed to reduce the memory load in the SpMM kernel before the constraints. The detailed motivation and design of sparse matrix reordering are introduced in Section 4. In addition to the constraints, LO-SpMM can employ a learning-based rank model to sort the remaining candidates and select the top-K candidates for evaluation, thereby further reducing the number of evaluations.
In the second stage, LO-SpMM generates proxies for the remaining tile sizes, thereafter selecting the corresponding proxy for each tile size and configuring runtime arguments of the proxy for performance evaluation. A proxy is a simplified CUDA code of the original SpMM implementations. It has a lower evaluation cost while keeping a similar relative performance rank as the original SpMM implementations. LO-SpMM selects the Top-1 tile size with optimal proxy performance and subsequently generates code for the SpMM kernel. The details of SpMM implementations are discussed in Section 6.