8 Related Work
This section reviews and discusses related existing studies along three categories: SpMM kernel for DNN inference, auto-tuning for SpMM kernel, and proxy program.
8.1 SpMM Kernel for DNN Inference
The SpMM kernel is crucial for the efficient inference of pruned DNNs, attracting significant research attention. The performance optimizations of SpMM mainly include three aspects.
(1) Tiling. Tiling partitions the computation and memory access of SpMM for better data reuse and load balancing. Yang et al. extend two primary tiling strategies in SpMV for the SpMM problem, row-splitting and merge-based , and implement them on GPUs. AspT uses adaptive sparse tiling to improve memory reuse. Sputnik modifies row-splitting by using multiple thread blocks to process a row of the output matrix for higher parallelism. SparseRT optimizes load balancing by assigning multiple rows per thread block during tiling.
(2) Reordering. Reordering the sparse matrix improves the data locality and load balancing of SpMM. Bandwidth-reducing reordering algorithms can reduce the matrix bandwidth of symmetric sparse matrices. For asymmetrical sparse matrices, which are common in sparse neural networks, graph partitioning can be used to enhance data locality. (3) Extracting regular components. Extracting regular components from a sparse matrix can reduce the storage overhead and enhance data reuse. The polyhedral framework is utilized to mine regular sub-regions, which does not need any indirection array to recover the nonzero coordinates . A sparse matrix can be decomposed into submatrices, which are then categorized into multiple categories, each stored and processed independently .
In this study, the proposed LO-SpMM employs a hierarchical 2-dimensional tiling method, which is a modification of Sputnik , to obtain better data reuse. Additionally, LO-SpMM integrates a novel row reordering algorithm that minimizes kernel memory access while maintaining load balancing. Given that LO-SpMM enhances data reuse by reordering and eliminates the storage of sparse matrices through full loop unrolling, LO-SpMM does not involve techniques of extracting regular components.
8.2 Auto-tuning for SpMM Kernel
Algorithms and applications typically have optional configurations that profoundly affect their performance. Auto-tuning aims at automatically searching for the optimal configuration with the best performance, under a certain search budget. General-purpose techniques regard auto-tuning as a global optimization of an expensive black-box function. This type of technique includes OpenTuner , KernelTuner , KTT , ATF , GPTune , BaCO , and so on. These techniques also use hardware information or provide interfaces for user-defined constraints to refine the search space for faster auto-tuning. Meanwhile, performance models are also used to improve the quality of the searched candidates.
In the field of deep learning, tensor compilers have been proposed for auto-tuning kernels in DNN inference. TVM and Ansor abstract dense operators into a tensor IR form and auto-tune their implementations. TACO , SparTA , and SparseTIR abstract the sparse operator and then use the abstracted representation to construct the search space. However, these tensor compilers fail to effectively leverage prior knowledge of hardware and algorithms, leading to unnecessary search costs. Besides, they require complete code compilation for measuring performance, which takes a long time for optimized SpMM kernels.
In this study, the proposed LO-SpMM utilizes GPU hardware features and SpMM algorithm features to constrain the search space and construct a rank model, thereby reducing the number of evaluations. Additionally, LO-SpMM employs proxies to decrease the cost of measurement. These strategies significantly reduce the search overhead for optimizing SpMM.
8.3 Proxy Program
Evaluating parallel programs can be labor-intensive and time-consuming, due to the prolonged runtime, the complicated software stack, and system dependencies. Proxy programs are proposed to mimic the performance characteristics of the target applications and enable convenient performance evaluation . Low-cost proxy programs can be constructed via partial execution , sampled simulation , and shrunk datasets . PerfProx generates proxies by leveraging hardware performance counters to monitor and extrapolate database performance.
In this study, we focus on designing specific proxies tailored for quickly evaluating SpMM on GPUs, rather than developing a proxy synthesis method for general parallel programs. By minimizing the code size of proxies based on the SpMM kernel implementation, we significantly enhance the efficiency of the auto-tuning process.