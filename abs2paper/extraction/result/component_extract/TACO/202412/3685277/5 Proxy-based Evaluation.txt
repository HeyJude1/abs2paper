5 Proxy-based Evaluation
The evaluation of an SpMM implementation mainly consists of two parts: compilation and execution. The usage of full loop unrolling and constant propagation for accelerating an SpMM kernel often results in significantly huge code files and a long compilation time. For example, in the SpMM kernel with the sparsity of 0.7 and shape (3,072, 4,096, 768), with different tile sizes, the compilation time ranges from several hundred to over a thousand seconds, while the execution usually takes a few milliseconds.
To mitigate the issue of long compilation time, we employ a proxy-based method to evaluate SpMM implementations. A proxy is a simplified program that behaves similarly to the original SpMM implementations but has much shorter code. Meanwhile, one proxy can represent a set of SpMM implementations via setting different runtime arguments. It does not output correct computation results and does not have the same absolute execution time as the original program. It only needs to keep a similar relative performance rank as the original program, so that it can be used to estimate which SpMM implementation has higher performance.
In the proxy-based evaluation for an SpMM kernel, the generation of the optimal SpMM implementation consists of three stages. In the first stage, LO-SpMM generates a set of proxies to replace the actual performance evaluation. In the second stage, for each tile size, LO-SpMM selects an appropriate proxy and determines the runtime arguments to evaluate proxy performance on the hardware. These arguments, including grid size, block size, and shared memory usage, are tailored to the specific tile size under evaluation. In the final stage, LO-SpMM selects the tile size with the optimal proxy performance and implements the SpMM kernel based on it. The details of proxy generation and runtime arguments configuration are introduced in the following.   implementations share one proxy, thus the number of compilations can be reduced. Thread block functions within an implementation are clustered and represented by a smaller number of proxy thread block functions (PBs), thus the code length can be reduced. By loop block reduction, each PB only reserves a few proxy iterations (PIs), so the code length can be further reduced. Detailed steps are introduced in the following sections.
5.1 Proxy Generation
5.1.1 Kernel Function Reduction.
In SpMM implementations, hardcoding the tile size as constants in code can reduce redundant computation. However, this approach produces a different code variant for each tile size, requiring repetitive compilations.
Since our purpose is to compare the relative performance rank of different implementations rather than assessing their absolute performance, we can skip the step of hardcoding tile sizes. This allows us to maintain a single proxy, which can accept different tile sizes as input arguments, to account for multiple SpMM implementations.
More specifically, with the same vertical tile size M1, SpMM implementations share the same code structure. Hardcoding the horizontal tile size N 1 will generate an SpMM implementation with a specific number of threads and thread blocks. By defining N 1 and related constants as variables, we can create a single proxy to represent multiple implementations. We can then estimate the performance of a given tile size by setting tile-related arguments at runtime. This method enables us to evaluate the performance of SpMM implementations with a variety of tile sizes after just one compilation.
5.1.2 Thread Block Function
Reduction. An SpMM implementation consists of M/M1 thread block functions. Each thread block function is a code snippet for processing a tile of the sparse matrix A. If we can reduce the number of thread block functions to be compiled, the code will be shortened, thereby the compilation cost will also be reduced. We achieve this by partitioning thread block functions into several disjoint clusters. Each cluster contains a set of thread block functions with similar features. According to the feature centroid (namely, the average values of features) of each cluster, a proxy thread block function is constructed to represent all thread block functions within the cluster. Figure (c) illustrates the usage of proxy thread block functions. In the proxy SpMM implementation, it first gets the cluster id of a block and then calls the corresponding proxy thread block function. Compared with the original implementation in Figure (a), the proxy reduces the number of thread block functions from N to N P .
Since we have conducted sparse matrix reordering, as introduced in Section 4, the operations in different thread block functions are relatively similar. The similarity of two thread block functions can be measured by the Euclidean distance of two features: the number of memory accesses and the number of floating-point operations within a thread block function. These features can be quickly obtained by static code analysis. We then use an agglomerative method to cluster thread block functions and obtain the feature centroid of each cluster. Given the feature centroid, we construct the corresponding proxy thread block function by repeating a certain number of global memory loads and multiply-add operations until achieving close feature values to the feature centroid.
The number of clusters, which equals the number of proxy thread block functions, has a crucial impact on the performance accuracy of proxy. On the one hand, if we set an extremely small number of clusters, the proxy will have a too-short code length. This can lead to discrepant instruction fetch miss rates of proxy thread block functions, compared with the original implementation. On the other hand, the number of clusters should not be greater than the product of the number of thread block functions concurrently executed by a stream multiprocessor and the number of stream multiprocessors in a GPU. Although an SpMM implementation can have many thread block functions, only part of them can be concurrently executed by the active thread blocks in a stream multiprocessor. Since one or multiple active thread blocks execute one thread block function, the maximum number of concurrently executed thread block functions can be equal to or fewer than the number of active thread blocks. Based on our experience, we set the number of clusters to 3 × max(NU MS AT B ), where NU MS AT B is an array consisting of the number of active thread blocks in each stream multiprocessor of all SpMM implementations corresponding to a proxy.
5.1.3 Loop Block Reduction.
As shown in Figure (b), a thread block function contains an entirely unrolled loop block followed by a result storage step. Since we only need to identify the performance rank across different tile sizes, we do not need to compile and execute all operations in the loop block. Truncating the loop block by the same proportion across different tile sizes has a negligible impact on their performance rank. Therefore, we can replace the original loop block with a truncated version that has shorter code. Specifically, a proxy loop block is constructed by repeating a proxy iteration N NC P times. The proxy iteration includes one global memory load and multiple multiply-add operations. The number of multiply-add operations is determined by the average number of computations among all global memory loads.
If N NC P is excessively small, it can cause a large gap in the instruction fetch miss rates between the proxy and original implementations, leading to inaccurate performance rank. Fortunately, within a thread block, the instruction fetch miss rates tend to stabilize after reaching a certain threshold of code length. This fact is depicted in Figure for SpMM kernels with different rows of sparse matrix M and iteration counts N NC P , where the column of dense matrix N is 256,000, sparsity is 0.5, and tile size is (M, 256). N NC P * (1 − sparsity) is positively correlated with code length. Based on our experience, we set N NC P to min(N NC, 300 (1−spar sity) ), where N NC denotes the number of non-empty columns in the A tile .
5.2 Runtime Arguments Configuration
A proxy can accept different runtime arguments, including grid size, block size, and shared memory usage, so that one proxy can flexibly represent the implementations resulted from different tile sizes. The grid size and block size arguments can be directly calculated according to the tile size and SpMM shape. The usage of shared memory needs a careful configuration since it is a key factor that affects the performance similarity between the proxy and the original implementation.
5.2.1 Relation between Occupancy and Shared Memory.
To ensure the proxy can keep a similar performance rank as the original SpMM implementation, we try to align their performance in terms of a dimensionless metric (e.g., a ratio between some counts). We select GPU occupancy as the metric since it provides an underlying measurement of GPU hardware utilization . Occupancy refers to the ratio of active thread blocks on a stream multiprocessor (NU M AT B for short) to its maximum thread block capacity. NU M AT B is determined by available hardware resources, including registers, shared memory, and the number of thread blocks. Due to the LO-SpMM: Low-cost Search for High-performance SpMM Kernels on GPUs 73:15 loop block reduction, the proxy uses fewer registers than the original implementation, leading to higher occupancy. To align the occupancy, we need to adjust register usage, shared memory, and the number of thread blocks. However, the number of thread blocks is determined by the tiling step and cannot be changed during constructing proxy. Moreover, the usage of registers is difficult to precisely control, as it is affected by a complicated mechanism of compiler and runtime. Therefore, we control the occupancy of the proxy by adjusting the allocation of shared memory.
5.2.2 Shared Memory Usage
Configuration. Specifically, the configuration process involves two steps. First, we calculate NU M AT B of the original implementation based on its register usage and the number of thread blocks, which can be formulated as follows:
NU M AT B = min( NU M T B /SMs , reдCapacity/reдU saдe), ( 5
)
where SMs is the number of stream multiprocessors in GPU, NU M T B is the number of thread blocks, reдCapacity is the capacity of registers in a stream multiprocessor, and reдU saдe is the usage of registers in a thread block. Next, we calculate the shared memory allocation needed (SharedMemorySize) for each thread block in the proxy to achieve the same occupancy, which is formulated as follows:
SharedMemorySize = maxSmemU saдe/NU M AT B − Constant, (6)
where maxSmemU saдe is the max shared memory usage in a stream multiprocessor. Since CUDA runtime allocates some shared memory for each thread block for management and tracking, we subtract the shared memory usage by a constant that is determined by hardware specification and runtime library version .