1 Introduction
Along with the great accomplishments of deep learning across various domains, the scale of deep neural networks (DNNs) has grown significantly. While larger models typically exhibit better performance, they also incur higher computation and storage costs. For example, GPT-3 , with its 175 billion parameters, requires 350 GB memory to store (using FP16 precision) and thus requires five A100 (80 GB) GPUs for inference.
Given the over-parameterization of large DNNs , neural network pruning methods have been developed to reduce model size by eliminating redundant parameters with minimal impact on model accuracy. Pruning techniques can be categorized as structured pruning and unstructured pruning . Structured pruning can accelerate inference through off-the-shelf GPU tensor computing libraries, but it often results in a significant accuracy drop due to its strict constraints on the position of pruned elements. Conversely, unstructured pruning can preserve higher accuracy, but the resulting sparse DNNs pose challenges in efficiently leveraging current GPU architectures, as they exhibit irregular memory accesses during inference. To execute the inference of sparse DNNs on GPUs, Sparse-dense Matrix Multiplication (SpMM) is the most critical kernel . It is the multiplication of a sparse matrix and a dense matrix. The sparse matrix is the weights of a pruned linear/convolutional/recurrent/attention layer, while the dense matrix is the input data.
An SpMM kernel can have various implementations based on different tuning configurations. Recent studies of advanced tensor compilers show that automatically generated SpMM implementations can achieve excellent performance and outperform hand-crafted libraries (e.g., cuSPARSE). The typical workflow of tensor compilers consists of defining a large search space for tuning configurations, iteratively selecting candidates from this space by a cost model, generating kernel implementations for evaluation, and adjusting the cost model by the evaluation results. It continues until obtaining a satisfactory kernel implementation. However, this workflow does not effectively leverage prior knowledge of the hardware architecture, SpMM algorithm, and input data layout, resulting in unnecessary search costs. Tensor compilers can take several days or weeks to tune a DNN model . Such a long time slows down the cycle of exploring better DNN architectures or pruning solutions in the AutoML pipeline .
The search cost of tensor compilers is primarily caused by two factors. First, the implementation of an SpMM kernel constitutes a large search space. For example, an SpMM kernel with shape 73:3 (M, N , K) has M × N possible tiling configurations to explore. Second, evaluating each candidate in the space takes a long time. State-of-the-art implementations of SpMM kernels adopt full loop unrolling and constant propagation , which write all nonzero elements of the sparse matrix into the code as constants. This optimization can effectively accelerate SpMM, but it also brings huge code files and a long compilation time.
In this article, we propose a method named LO-SpMM to generate high-performance SpMM implementations and minimize the search cost. It optimizes the processes of searching for optimal SpMM implementations from three aspects:
(1) LO-SpMM conducts a few-shot search for optimal implementations. According to our observation, a large portion of candidates in the search space do not properly leverage GPU hardware, leading to suboptimal performance or even execution failure. LO-SpMM automatically identifies and eliminates these ineffective candidates, using several constraints based on the characteristics of the GPU architecture and the SpMM algorithm. In addition, a learning-based rank model is built to further reduce the remaining candidates.
Consequently, the search space can be effectively shrunk.
(2) To avoid the costly compilation and execution for searched SpMM implementations, LO-SpMM constructs proxies as their approximations. A proxy is a simplified version of the original SpMM implementations, exhibiting a similar performance rank as the original. Therefore, by compiling and measuring the proxies, LO-SpMM can quickly estimate the performance rank of searched SpMM implementations.
(3) Besides focusing on the issue of long search time, LO-SpMM also tries to improve the performance of generated SpMM implementations. Existing tensor compilers primarily focus on loop reordering/tiling/unrolling optimization techniques, which are critical for GEMM (General Matrix Multiplication). In addition to these loop optimizations, LO-SpMM takes the layout of nonzero elements in sparse matrices into account. It adopts a heuristic method to reorder the sparse matrix in SpMM, enabling better memory access performance and load balancing. It also adopts a prefetching technique to reduce the latency caused by cache misses. We evaluate LO-SpMM on typical DNN models, including ResNet, ShuffleNet, MobileNet, and BERT, on NVIDIA RTX 2080Ti and Tesla V100. Compared with the state-of-the-art tensor compilers, our approach can reduce the search time by 281× at most. Compared with cuSPARSE, TVM-S , Sputnik , SparTA , and EC-SpMM , the performance of SpMM from LO-SpMM achieves 34.70×, 29.32×, 3.00×, 1.10×, and 1.03× average speedup, respectively. We further combine LO-SpMM with SparTA and Rammer to implement end-to-end sparse DNN inference, and achieve 1.01 × − 3.38× speedup over baseline solutions .
In summary, the main contributions of our article are as follows:
-We propose a method to automatically prune the search space of tiling for SpMM, based on constraints from prior knowledge of the GPU architecture and programming experience. -We propose a method to construct proxies to replace the evaluation of candidates. The proxies have a similar performance rank as the original SpMM implementations and enable faster evaluations. -We propose a matrix reordering method to adjust the layout of nonzero elements of the sparse matrix in SpMM. It can reduce memory access operations while keeping computational load balancing. -We conduct extensive experiments for sparse DNNs on both consumer-level and high-end GPUs. We validate that our method can achieve comparable or better SpMM kernel performance and significantly reduce the search time, compared with existing tensor compilers.
73:4
J. Lin et al.