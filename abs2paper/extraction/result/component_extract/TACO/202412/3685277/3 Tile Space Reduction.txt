3 Tile Space Reduction
To take full advantage of GPU parallelism, LO-SpMM uses a hierarchical 2-dimensional tiling strategy to divide an SpMM kernel into tiles for processing. The selection of tile size has a critical impact on the performance of the generated SpMM implementation. Since the search space of tile size is vast, an exhaustive search to determine the optimal implementation among all possible sizes incurs significant overhead.
The implementations of an SpMM kernel with some tile sizes cannot effectively utilize GPUs, leading to suboptimal performance compared to other tile sizes. Therefore, we can reduce the search time by eliminating these underperforming tile sizes. Specifically, LO-SpMM can accurately eliminate low-performance candidates in the search space based on certain constraints, including register resource constraint, hardware utilization constraint, and load balancing constraint. Then, optionally, for the remaining tile sizes, a model is built to estimate their relative performance rank. The estimated top K tile sizes are selected for evaluation. In this way, we can obtain a LO-SpMM: Low-cost Search for High-performance SpMM Kernels on GPUs 73:5 Fig. . An example of hierarchical 2-dimensional tiling for A * B = C. A is an 8 × 8 sparse matrix and the white cells within A are pruned elements. B and C are 8 × 8 dense matrices. In this example, we assume the device has 2 warps in a thread block, 2 threads in a warp, and tile size (M1, N1) = (4, 4), for simplicity.
high-performance implementation of an SpMM kernel with few actual evaluations, which greatly reduces the search cost.
3.1 Hierarchical 2-Dimensional Tiling
Hierarchical 2-dimensional tiling follows hierarchical 1-dimensional tiling with modifications. Instead of processing only part of a row of the output matrix, each thread block in our tiling strategy processes a submatrix of the output matrix. In the tiling process, first, the sparse matrix is divided into multiple A tile , each of which corresponds to M1 rows (could be discontinuous), of the sparse matrix. Then the dense matrix is divided into multiple B tile , each of which corresponds to continuous N 1 columns of the dense matrix. The output of A tile × B tile is a submatrix C tile of size (M1, N 1). Specifically, on GPUs, an SpMM kernel is divided into three hierarchies: thread, warp, and thread block, as shown in Figure . Each thread processes an output submatrix C thr ead of size (M1, 1), and each warp processes an output submatrix C warp of size (M1, warp_size). Each thread block processes a tile, namely, an output submatrix C block of size (M1, N 1).
The hierarchical 2-dimensional tiling strategy has the following benefits:
-Memory accesses within a warp can be merged to improve the efficiency of SpMM.
-The workload distribution among threads in a thread block is balanced due to processing the same A tile . -Threads are designed to handle multiple output elements, as opposed to just one, thereby reducing memory access to the dense matrix. This also increases the computational work performed per memory access about the dense matrix, which is advantageous for masking the latency inherent in memory accesses.
3.2 Register Resource Constraint
Due to the constraint of GPU register resource, the register usage for both threads and thread blocks within an SpMM implementation is limited. For GPUs, each variable in a thread typically corresponds to a register. When the number of variables in a thread exceeds the maximum number of registers, the exceeded variables are stored in the local memory. The overhead of accessing the local memory is comparable to that of accessing global memory and much higher than that of accessing registers. If a tile size requires exceeding registers and consequently causes local memory access, it is unlikely to exhibit optimal performance. Moreover, the number of thread blocks and threads in a kernel is determined at runtime, thereby the required register resource is also determined at runtime, which is unknown during compilation. As a result, despite successful compilation, an SpMM implementation may fail at runtime with a "too many resources requested for launch" error if the required register resource exceeds the limitation for thread blocks, leading to unnecessary compilation overhead. Therefore, LO-SpMM counts the register usage of the SpMM kernel across different tile sizes, excluding configurations that necessitate an excessive number of registers. Due to the generated SpMM implementations for each tile size being known, we can get the number of variables used in each thread and forecast the register usage. To avoid over-optimization by the compiler, which may increase the register usage and decrease GPU occupancy, we apply the "--maxrregcount" compiler option in NVCC. It limits the register usage per thread to a threshold that is slightly larger than the variable count, without causing local memory accesses. In other words, we can assess the register usage for different tile sizes without any compilation and execution. The required registers for a thread block can be calculated as the product of the register usage per thread and the total number of threads in a thread block. According to the GPU's register capacity, tile sizes of which register demand exceeds the capacity are excluded from the search space. As shown in Figure , assume that the maximum number of registers is 6 in a thread and 24 in a thread block. The search space size is reduced from 64 to 38 after the register resource constraint.
3.3 Hardware Utilization Constraint
The threads within a thread block are organized into warps, which are used as Single Instruction Multiple Threads (SIMT) execution units. If the number of threads in a thread block is not a multiple of the warp size, some warps are only partially utilized, leading to inefficiencies. Meanwhile, a GPU consists of an array of stream multiprocessors, where each thread block is allocated to a stream multiprocessor for execution. When the number of thread blocks is less than the number of stream multiprocessors in a GPU, some stream multiprocessors will be idle, and the hardware resources may not be fully utilized. Therefore, we can further reduce the search space by eliminating tile sizes that would result in insufficient thread blocks.
LO-SpMM sets the number of threads per block (N 1) to be a multiple of warp size to ensure sufficient warp utilization. Additionally, LO-SpMM calculates the number of thread blocks corresponding to each tile size by Equation ( ):
block_num = M/M1 × N /N 1 , (1)
where (M, N) is the shape of the sparse matrix and (M1, N1) is the tile size. Considering the influence of data reuse and cache access patterns in SpMM implementations, LO-SpMM adopts a relaxed constraint on the number of thread blocks and eliminates tile sizes that result in fewer thread blocks than half of the stream multiprocessor count, rather than the full count. As shown in Figure , assume the warp size is 2 and the number of stream multiprocessors is 8. The search space size is reduced from 38 to 12 after the hardware utilization constraint.
3.4 Load Balancing Constraint
In hierarchical 2-dimensional tiling, load balancing within a thread block is inherently ensured, but load balancing across thread blocks is not guaranteed. A tile size influences the load balancing among different thread blocks from two aspects: (1) the number of non-zero elements in A tile that each thread block processes, and (2) the number of columns in B tile that each thread block processes.
In the search space, a tile size with the same number of tiles but a more balanced load can be found, which can achieve better performance. Therefore, we should remove tile sizes that do not satisfy a certain load-balancing condition from the search space. For example, in Figure , if M1 is set to 6, the sparse matrix A is divided into two A tile . The first A tile has 6 rows with 22 nonzero elements, while the second A tile only has 2 rows with 8 nonzero elements. In this case, the load of different tiles is imbalanced. However, setting M1 to 4 can keep the same number of tiles and make the load of different A tile more balanced, since each A tile equally has 15 nonzero elements. Likewise, setting N 1 to 6 results in load imbalance among different tiles, where the first B tile has 6 columns, and the second B tile has 2 columns only. Setting N 1 to 4 makes a load of different tiles more balanced while maintaining the same number of tiles since each B tile equally has 4 columns.
LO-SpMM measures the A tile load balancing using COV row , calculated by the coefficient of variation of number of non-zero elements across different A tile . The B tile load balancing is measured using W AST E col , calculated by the padding ratio of columns in the dense matrix. LO-SpMM sets two thresholds, T H row for COV row and T H col for W AST E col , to exclude tile sizes that cause COV row or W AST E col exceeding the thresholds. We set both T H row and T H col to 0.25. The search space in Figure is reduced from 12 to 6 after the load balancing constraint.
3.5 Rank Model
Numerous factors influence the performance of an SpMM implementation, some of which can be explicitly interpreted and analyzed, such as the aforementioned constraints. However, beyond those constraints, a significant number of candidates remain in the search space that cannot be effectively distinguished using explainable rules. Consequently, we propose constructing a model to estimate the performance of the remaining candidates through machine learning techniques. We select the top K candidates based on these estimates for compilation and execution and identify the optimal candidate as the final output.
To search for the optimal tile size, we do not care about the absolute performance of each candidate but only need to know the relative performance order of different candidates. Therefore, we 73:8 J. Lin et al. use a rank model to sort tile sizes. We construct a listwise rank model using LambdaMART as the model. The features used in the rank model are shown in Table . An SpMM implementation is represented by schedule information of an SpMM kernel, sparse attribute in the sparse matrix, and tile size. The training dataset is constructed by benchmarking the performance of synthetic SpMM kernels under various tile sizes and sparsity degrees.