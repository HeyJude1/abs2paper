7 Evaluation
In this section, we first compare LO-SpMM with state-of-the-art SpMM implementations, evaluating the performance of generated SpMM kernels and their corresponding generation compilation time cost. Then we also evaluate the effectiveness of the rank model and proxy. Finally, we integrate LO-SpMM into SparTA and Rammer to evaluate the usefulness of LO-SpMM in end-to-end inference tasks.
7.1 Performance of Generated SpMM Kernel
We compare the performance of SpMM implementations in LO-SpMM with existing solutions, including cuSPARSE, TVM-S, Sputnik, SparTA, and EC-SpMM. Besides, we also take cuBLAS for comparison.
-cuSPARSE [2] is a vendor library for sparse computing on GPUs; -TVM-S is the sparse version of TVM with 2,000 searches for each SpMM kernel; -Sputnik carries out a lot of optimization techniques for SpMM in sparse neural networks; -SparTA is the state-of-the-art solution to sparse DNN computing; -EC-SpMM can rapidly generate a high-performance SpMM kernel; -cuBLAS is a vendor library for dense linear algebra computing on GPUs. We build a benchmark dataset to evaluate the performance of SpMM implementations. We first extract GEMM operators from a variety of neural networks, including BERT , ResNet50 , ShufflenetV2 , and MobilenetV2 . We convert linear layers to GEMM operators and Conv2D layers to combinations of im2col and GEMM, yielding 41 distinct GEMM shapes. The GEMM kernels are randomly pruned with five sparsity degrees, generating a total of 205 SpMM kernels. The shape of an SpMM kernel is denoted as (M, N , K). In our constructed dataset, the values of M range from 64 to 3,072; N ranges from 512 to 50,176; K ranges from 27 to 3,072.
The experimental results on 2080Ti and V100 are shown in Figure (a) and (b), respectively. In general, LO-SpMM is 37.75×, 35.24×, 3.32×, 3.43×, 1.10×, 1.06× faster than cuSPARSE, TVM-S, cuBLAS, Sputnik, SparTA, EC-SpMM on 2080Ti, respectively. While on V100, LO-SpMM is 31.65×, 23.40×, 3.30×, 2.56×, 1.09×, 1.00× faster than cuSPARSE, TVM-S, cuBLAS, Sputnik, SparTA, EC-SpMM, respectively.
Compared to SparTA, LO-SpMM achieves at least 95% relative performance in 93.66% of kernels, and performs better in 79.02% of kernels on 2080Ti. On V100, LO-SpMM achieves at least 95% relative performance in 82.43% of kernels and performs better in 69.27% of kernels. These results indicate that, in most cases, LO-SpMM can achieve comparable or better performance than SparTA.   Due to the usage of the prefetching technique and the exploration of more tiling configurations enabled by proxies, the performance of SpMM implementations in LO-SpMM is higher than EC-SpMM on the 2080Ti and comparable to EC-SpMM on the V100. LO-SpMM performs better at higher sparsity in general. This is due to the fact that reordering is more effective in reducing unnecessary memory accesses when the matrix has higher sparsity. Moreover, LO-SpMM outperforms cuBLAS in all cases, indicating that LO-SpMM can bring practical speedups for sparse DNNs.
7.2 Compilation Time Cost
Figure shows the compilation time of four methods. By removing poorly performing candidates by constraints and using the proxies to evaluate the performance of each tile size, LO-SpMM can save a significant portion of the search cost. LO-SpMM -Rank uses the rank model to sort the candidates in the search space and then choose the Top-10 candidates for evaluation, further reducing the search cost.
The average search time normalized by LO-SpMM -Rank is shown in Table . Compared with SparTA, LO-SpMM saves search cost by more than an order of magnitude. LO-SpMM also achieves lower cost than EC-SpMM, which is the most effective method for quickly generating SpMM implementations in existing works. LO-SpMM -Rank combines a rank model and only evaluates a subset of candidates, so it can achieve more than 18× improvement on the V100 compared to EC-SpMM.
Because LO-SpMM , SparTA, and EC-SpMM use a full loop unrolling and constant propagation method to generate SpMM implementations, the code size of an SpMM implementation has a positive correlation to the number of nonzero elements. With the increase in the number of nonzero elements, the SpMM implementation generated by EC-SpMM and SparTA needs more compilation time. Due to the usage of the proxies, the increase in search cost for LO-SpMM is less significant. Meanwhile, the search cost reduction of LO-SpMM on V100 is more than that on 2080Ti, because LO-SpMM: Low-cost Search for High-performance SpMM Kernels on GPUs 73:19
7.3 Rank Model and Proxy Performance
We generate synthetic SpMM implementations for benchmarking performance under various tile sizes and sparsity degrees. The value range of shapes and tile sizes is shown in Table . Based on the benchmarking results, we can build a rank model to predict the performance of an SpMM implementation, as introduced in Section 3.5.
To evaluate the effectiveness of the rank model and constructed proxies, we measure the performance loss (mean absolute percentage error, MAPE) of the searched optimal SpMM implementation. MAPE is calculated by the relative difference between the performance of the candidate selected by LO-SpMM-Rank/LO-SpMM and the real Top-1 performance. The results are shown in Table . The performance of the SpMM implementation selected by the proxy-based method is close to that of the optimal kernel implementation. The performance loss is lower than 2.1% on 2080Ti and V100. That is, the proxy-based method greatly reduces the search cost of SpMM implementations on the premise of acceptable performance loss. LO-SpMM -Rank can further reduce the search cost with a slight increase in performance loss.
7.4 End-to-end Performance
To illustrate the usefulness of LO-SpMM in a complete inference procedure, we also evaluate the inference time cost of different solutions. We compare LO-SpMM with existing solutions, including Pytorch with jit, TensorRT, SparTA, Sputnik, and EC-SpMM. The value of K in EC-SpMM is set to 10 in the end-to-end experiment. We take two neural network models for evaluation: BERT and ResNet50. In the BERT model, we use movement pruning to obtain a sparse model with 90.99% sparsity, and its accuracy on the QQP task is reduced from 89.1% to 86.3%. In ResNet50, we use DLTH pruning without pruning the first Conv2D layer and the last linear layer, resulting in a 90% sparse model with accuracy reduced from 75.7% to 66.0% on ImageNet tasks.
The batch size for end-to-end inference is 32. In ResNet50, we convert the Conv2D layers to SpMM. Going one step further, we also implement an SpMM version of fused Conv2D+relu for better end-to-end performance. W The experimental results are shown in Figure . Since most of the kernels in pruned BERT can be directly implemented using SpMM, LO-SpMM can achieve a large improvement on BERT. LO-SpMM has 1.84× speedup over TensorRT, 1.34× speedup over SparTA, and 1.16× speedup over EC-SpMM. For the ResNet50 model, LO-SpMM has 1.88× speedup over TensorRT, and is comparable to SparTA and EC-SpMM. The SpMM kernels in ResNet50 have fewer non-empty rows, which lowers the benefit from sparse matrix reordering. It is worth noting that this article mainly focuses on the optimization of compilation time and latency of SpMM kernels. The end-to-end inference experiments validate the feasibility of integrating LO-SpMM into a complete inference procedure. However, the optimization of end-to-end inference involves many other techniques that are beyond the scope of this article. More elaborate optimizations for maximizing the end-to-end performance deserve further investigation in future work.