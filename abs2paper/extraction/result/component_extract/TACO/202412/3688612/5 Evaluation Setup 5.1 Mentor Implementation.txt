5 Evaluation Setup 5.1 Mentor Implementation
Mentor is implemented on the xczu15eg-ffvb-2-i board through Xilinx Vitis v2022.2 with the High-Level Synthesis tool. Its maximum off-chip bandwidth is 9600MB/s. The achieved postimplementation frequency is 214 MHz according to the post-implementation timing report from Xilinx Vivado v2022.2. We configure the capacity of each scratchpad and FIFO as 16KB and 128B, respectively. The number of PE is set to 32. Table shows the resource utilization. For the Read A, Read B, and Write C modules, we have assigned AXI slave interfaces with data widths of 64-bit, 128-bit, and 128-bit, respectively. Note that Mentor adopts a 7-cycle delayed feeding.
5.2 Benchmarks
Table summarizes that we randomly selected 125 sparse matrices with varying size, sparsity, and structures from the SuiteSparse Matrix Collection . We construct ten corresponding dense matrices for each sparse matrix by setting N from 32 to 1024 and perform ten iterations of singleprecision floating-point SpMM operations. Note that α = 1.0 and β = 0.0.
5.3 Baselines
We use two state-of-the-art SpMM-dedicated accelerators, i.e., Sextans and AWB-GCN .
Sextans (row-wise) : This is a software-hardware co-design system based on the row-wise approach. We leverage the non-zero scheduling algorithm on the host and then implement Sextans on the same FPGA, where the number of multipliers is scaled to 32. AWB-GCN (column-wise) : This accelerator is designed for consecutive SpMMs based on the column-wise approach. We model its performance on a single SpMM operation with the same hardware resources (i.e., 32 multipliers, 0.5MB on-chip memory for accumulation buffer array, and 0.5MB for scratchpad memory) as other baselines and matrix blocking optimization, where t is set to 4. Since the density of benchmarks is less than 25%, we use TDQ-2 (task-distribution-queue) to distribute tasks to PEs and 1-hop distribution smoothing.
Given that there are very few SpMM-dedicated accelerators, we modify and fine-tune three SpGEMM accelerators (i.e., SIGMA, Spaghetti, and GAMMA) to support SpMM. To compare these architectures regardless of their underlying platforms, we build cycle-accurate simulators to evaluate their performance. SIGMA (inner) : This accelerator uses MAC arrays with distribution networks and reduction networks to process bitmap-compressed sparse matrices. SIGMA is a variant of the inner-product approach, which uses the B-stationary, A-streaming dataflow (referred to as inner(N) in Table ). Due to B's dense nature, generating source-destination pairs in SIGMA can be simplified to store non-zeros' positions in the fibers of A. For the hardware configuration, we use a 4×8 MAC array with an accumulator buffer of 1MB and the same off-chip bandwidth as that of Mentor. Following the original work, the total latency is obtained by Loading+Streaming+Add. Spaghetti (outer) : We reproduce the pattern-aware scheduling to tile and segment the multiplicand A. Due to the Pareto-optimal configuration of N m (the number of multipliers) being 16 in Spaghetti, we use 32 multipliers and double the bandwidth of the original work to maintain a fair comparison while leveraging the advantages of outer products. We configure the sort-merger depth to be 8K to ensure the same on-chip SRAM budget. GAMMA (row-wise) : This design features a FiberCache to capture irregular data reuse in a row-wise approach. We simplify its merge component since the elements in the rows of B are inherently ordered. We implement the affinity-based row reordering to preprocess A. The Fiber-Cache is scaled to 1MB for hardware, and the size of the cache line is fixed to 32. We use the original 128GB/s bandwidth to support 32 64-radix PEs in our model.