7 Related Work
Hardware accelerators for Sparse Matrix Multiplication. In addition to the sparse accelerators listed in Table , there are also sparse algebra accelerators focused on accelerating sparse matrix multiplication with ASIC or FPGA implementations. For example, ALRESCHA and AS-CELLA are accelerators based on inner-product, both of which propose custom storage formats to address index-matching and support input streaming. OuterSPACE is the first architecture based on outer-product, which serializes the multiply and merge phases and uses single-programmultiple-data processing units for SpGEMM and SpMV. Following OuterSPACE, SpArch provides a more efficient design with condensed matrix representation and a Huffman tree scheduler to improve output reuse. Apart from these, MatRaptor and InnerSP also recognize the advantages of row-wise dataflow and provide feasible solutions through either custom storage format for enabling streaming fashion (MatRaptor) or on-chip hash table for addressing memory bloating (InnerSP). However, these designs do not exploit the advantages of the column-wise product.
Additionally, some works attempt to accelerate various computation problems using a single dataflow. For example, Tensaurus is a versatile architecture for various sparse-dense tensor computations, proposing the SF 3 pattern based on the inner product and performing well in several computation problems. Recent work has emerged that integrates inner, outer, and row-wise dataflows within a framework to accelerate a single computation problem, dynamically matching the most suitable dataflow according to input data features and demonstrating the suitability of each dataflow varies across different problems. However, these works do not focus on the drastically different sparsity levels of the two operands and fail to deliver better performance on SpMMs.
Furthermore, designs from many other domains have also contributed to the problem of sparse matrix computation. In addition to the mentioned GCN accelerators, there are also many designs specifically tailored towards neural networks , where the computations can be permuted as matrix operations. GoSPA globally optimizes sparse convolutional neural networks (CNNs), especially the convolutional computation. SpAtten leverages the sparsity opportunities for improving the performance of the attention mechanism, which involves both sparse and dense computations. Software techniques for acceleration. Introducing pre-processing at the software level to further enhance the performance of accelerators is a critical technique widely adopted in the domainspecific architecture field. The affinity-based reordering algorithm in GAMMA improves the data locality and has been proven to boost the performance by 18%. The pre-processing algorithm in Sextans , a significant reference for our work, avoids RAW conflict and balances loads with the help of non-zero scheduling. Additionally, many works utilize matrix partitioning and customized storage formats, including ALRESCHA, ASECLLA, and MatRaptor. Refs. carefully study the limitations of traditional tiling approaches in sparse tensor algebra and propose new tiling strategies and apply them to existing accelerators for improving the performance. Software algorithms are also widely applied in other workloads involving SpMM, such as sparse attention. Sanger utilizes software pruning to predict the attention mask and rearrange the unstructured sparse patterns and proposes a custom architecture supporting SpMMs. DOTA introduces low-rank linear transformations to learn a mask that detects weak connections in attention maps. ViTCoD separates sparse and dense computation patterns from sparse attention maps through pruning and polarization, enhancing hardware performance. The success of these designs demonstrates that hardware performance can be further enhanced through software innovation.
To summarize, Mentor stands out as a software-hardware co-design approach based on columnwise to accelerate SpMM. Mentor implements streaming at the software level to enable streaming 79:22 X. Lu et al.
memory accesses and designs a fully-pipelined hardware architecture at the hardware level to adapt to the created data streams for near-optimal performance.