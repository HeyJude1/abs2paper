4 Mentor Architecture
This section introduces the Mentor architecture and our analytical model for design space exploration to provide an optimized data path and achieve near-optimal performance.
4.1 Column-Wise Pipeline Design
Our Mentor architecture design has four stages: read, multiplication, accumulation, and write.
4.1.1 Phase I-Read.
The Read B module streams the constructed matrix B into the architecture. Following the delayed feeding strategy discussed in Section 3.1, the number of traversed B elements in each cycle is less than #PE. Thus, the Read B module will distribute the B elements to each PE in a round-robin manner. Simultaneously, the Read A module streams both the idx and val arrays, and broadcasts the non-zeros of A to all PEs, as explained in the second advantage of the column-wise approach in Section 2.3. Thanks to the column-wise approach, each PE will independently access several columns of B rather than the entire matrix. And we avoid expensive bus control and memory interface by streaming access pattern. Thus, the Read B module employs two dependent crossbars for transferring data to four PEs with two channels, reducing the hardware overhead (Figure ).  A[m, rid] ⋅ B[rid, i] and row-id of m. The row-ids will be sent to the FIFO in the accumulation unit and a comparator, which triggers a completion signal when the consumed element is either Rest or Blockinд, indicating the completion of the scalar-vector product. This signal is connected to the FIFO storing B elements and the multiplier to control the starting of the following scalar-vector product and the output of the multiplier. Whenever the multiplier receives the completion signal, it produces a result with a value of 0 and a row-id of -1, indicating an invalid output. In addition, the Paddinд element triggers a skipping signal for the multiplier to produce a result with a value of 0, avoiding RAW conflicts.
4.1.3 Phase III-Accumulation. The accumulation phase consumes a row-id and a value computed in Phase II. The row-id m is used to fetch the C[m, i] from the scratchpad memory, and the fully-pipelined accumulator then performs C[m, i]+ = A[m, rid] ⋅ B[rid, i]. After the completion of an entire column B[∶, i], the C[∶, i]
stored in the scratchpad memory will be streamed out to the Write C module. This process will stall the following accumulation operations. Inspired by the double-buffer technique, we avoid the pipeline stall by employing a double-scratchpad structure to alternately store the partial columns and stream out a final column of C to DRAM. This onchip structure is controlled by an arbitration signal, which flips (XOR with the comparator output) when the arbitration unit detects a Rest or Blockinд element. The accumulation unit employs two multiplexers to determine the destination scratchpad for fetching and writing back and the source scratchpad for streaming out final rows. the Write C module is equipped with the same bandwidth as the Read B module and also follows the streaming fashion. Thus, we also adopt a c-cycle delayed strategy for writing the final C. Specifically, the Write C module concurrently collects #PE/(c + 1) elements from #PE FIFOs in a round-robin manner and streams the elements to the off-chip memory. In every c + 1 clock cycles, the Write C module can stream #PE elements, which forms a segment of a row of C. Therefore, the final C is also stored in blocks with row-major storage, and the dimension of each block is M ×#PE. By adopting this strategy, we ensure a synchronization between the consumption of B and the production of C. Figure shows a design resembling Phase I, where two crossbars are employed to write the results of four FIFOs back to DRAM.
4.2 Analytical Model
Our analytical model is motivated by the delayed feeding strategy adopted in streaming construction. First, given the definition of the delay parameter c,
c = P E b − 1, (6)
where P represents #PE, and E b is the number of transferred B elements per cycle. A larger c indicates a longer interval between two consecutive feeds for each PE, reducing the required bandwidth but lowering PE utilization. Thus, c presents a tradeoff between resource consumption and performance in Mentor. To explore the design space and determine the optimal value of c, we introduce an analytical model before configuring Mentor. Section 3.1 has introduced that the execution time of A[∶, rid] ⋅ B[rid, i] varies with the number of non-zeros in A[∶, rid]. Thus, it is necessary to obtain a reliable estimate of the execution time, denoted as T , based on features of the input matrix. The most conservative estimate considers the minimum number of non-zeros in rows of A, maximizing the PE utilization in Mentor but significantly limiting throughput. As a compromise, we calculate the average number of non-zeros per row, denoted as npr , and utilize the function f to make a more conservative estimate:
T = f (npr ) = 2 ⌊log 2 npr ⌋ , npr = nnz m , (7)
f (x) is defined with the consideration that the preferred #PE is a power of 2, which can reduce the sensitivity of #PE to data features, enabling excellent performance across input data with varying features. The optimal value of the delay parameter c can be determined as T − 1. By referring to Equation ( ), we can draw the following two conclusions:
P = f (npr ) ⋅ E b , E b = P f (npr ) . (8)
Finally, we employ the following two equations to estimate #PE configured on Mentor and the off-chip bandwidth of the entire system:
P = f ( nnz m ) ⋅ E b bandwidth = (E a + E b + E c ) ⋅W = (2 + 2 ⋅ P f ( nnz m ) ) ⋅W , ( 9
)
where W is the memory width. The entire system allocates bandwidth for the following three modules: Read A module, which only reads one non-zero of A per cycle, consisting of an index and a value, Read B, and Write C modules, which transfer E b and E c elements per cycle. The balanced 79:14 X. Lu et al. throughput makes E b = E c . Overall, the analytical model utilizes npr to determine an optimal value for c, thereby achieving a good tradeoff between hardware resources and throughput.