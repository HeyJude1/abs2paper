6 Experimental Results
This section compares Mentor to state-of-the-art SpMM approaches and shows the efficacy of our analytical model. It also discusses how Mentor performs in GCN and its overhead.
6.1 Comparing to State-of-the-Art
6.1.1 Overall Performance. We partition the 1250 SpMMs into 6 groups according to problem size (i.e., the number of floating-point operations, defined as 2 ⋅ nnz ⋅ N ). Figure shows the results normalized to SIGMA regarding speedup, traffic reduction, and bandwidth utilization. We see that Mentor achieves a geomean of 2.05× speedup, 2.92× memory traffic reduction, and 1.38× bandwidth utilization improvement against the state-of-the-art. This is because of three factors.
Firstly, the fully-pipelined design of Mentor avoids the potential pipeline stalls and achieves a near-optimal performance. Mentor performs 3.06× and 1.94×, 1.31×, 3.98× and 1.18× better than SIGMA, Spaghetti, GAMMA, Sextans and AWB-GCN. Mentor achieves an impressive measured peak throughput of 13.63 GFlops/s, close to its theoretical peak of 13.65 GFlops/s. SIGMA employs the bitmap storage for index matching, which has a non-negligible impact when processing sparse matrices. On 125 matrices, this operation accounts for 33% of the total latency. Spaghetti is limited by the merge phase for partial sums. Although it attempts to pipeline the computation and merge phase by matrix tiling, the sort-merger latency averages 3× that of the computation phase. GAMMA and Sextans use on-chip storage structures to overcome the irregular data reuse of the row-wise approach. GAMMA suffers from the increasing cache miss rate on large-scale problems. Sextans fails to hide the latency of memory accessing with computation, and the FPGA board's low bandwidth significantly limits its performance. AWB-GCN demonstrates the benefits of columnwise by hardware auto-tuning and employs a scratchpad to cache A as much as possible. These strategies deliver good performance on small-scale computations. However, the workload needs to be re-balanced after each tile is calculated, limiting the PE utilization on large-scale computations.  Secondly, column-wise's inherent advantages result in a substantial reduction in memory traffic. Mentor exhibits geomean reductions of 4.76×, 4.93×, 1.23×, 3.68× and 1.99× over state-of-the-art. On problems larger than 10 8 , the baselines suffer heavy memory traffic, but Mentor achieves reductions ranging from 1.84× to 9.98×. SIGMA uses the Inner(N) dataflow, but the bitmap is spaceinefficient on sparse matrices and accounts for an average of 66% of the total traffic. Spaghetti limits the merge of partial sums on-chip, which sacrifices good input reuse of multiplicand A. GAMMA can hold the entire small matrix with the FiberCache and perform well on problems smaller than 10 8 . But it struggles with large matrices, and exhibits 2.82× more memory traffic over Mentor on problems larger than 10 10 . Sextans sacrifices the reuse of A, resulting in 5.34× more traffic over Mentor on large problems. AWB-GCN can cache the entire A on problems smaller than 10 7 and achieves 1.27× reduction against our method. But this advantage diminishes as the problem scale increases, leading to 0.2× traffic reduction against Mentor on large problems.
Thirdly, the streaming access pattern reduces the average memory access latency and improves bandwidth utilization of 1.0×, 1.33×, 2.89×, 1.08×, and 1.21×. SIGMA shows comparable utilization because 33% of the total latency is spent searching for source-destination pairs by accessing bitmaps sequentially. The critical path of Spaghetti is sort-mergers, which leads to low utilization on small-scale problems. For GAMMA, most memory accesses occur when there are cache misses, and it can exhibit better utilization on small-scale computations. Sextans can exploit an enhanced data locality and demonstrate improved bandwidth utilization when the problem size exceeds 10 8 . Unfortunately, it cannot overlap memory access and computation, resulting in minimal efficiency SpMM Accelerator Based on Column-Wise Product 79:17 when dealing with small-scale problems. The AWB-GCN trend resembles that of Sextans, with access to A dominating memory traffic in large-scale problems.
6.1.2
The Impact of Matrix Structure. We further discuss the impact of special structure on the overall performance in Figure ). We include five types of matrices, where large matrices (M > 10K) and hyper-sparse matrices (density < 0.01 ) represent the matrix scales, and powerlaw, block-sparse, and band matrices represent the distribution of non-zeros in A. For power-law matrices, the non-zeros are usually clustered in some fibers. For block-sparse matrices, the nonzeros are usually clustered in some matrix blocks and form many dense blocks. For band matrices, the non-zeros are primarily distributed along the diagonals, and adjacency fibers exhibit similar access patterns.
Firstly, Mentor achieves a 2.06×-2.31× geomean speedup across the five structures, with a 2.31× on block-sparse matrices and a 2.28× on large matrices. Mentor can benefit from the dense blocks on block-sparse matrices, and introduce fewer control elements in streaming construction. As the baselines are all dedicated to sparse computation, they struggle to leverage the hardware performance on dense blocks. In particular, AWB-GCN performs worst on this structure due to the dense blocks alleviating the workload imbalance. Only GAMMA performs well because the similar access pattern exhibited by the dense blocks can avoid cache misses. For large matrices, as previously discussed, all baselines suffer from significant memory access overhead. In contrast, our method mitigates this issue by optimizing the access pattern.
Secondly, Mentor reduces memory traffic by 3.11×-4.77×. The better performance can be observed on block and large matrices. On hyper-sparse and band matrices, Mentor's traffic reduction is comparable to GAMMA, which is 1.13× and 1.12×, respectively. On the two structures, the streaming construction scheme introduces more padding elements to avoid RAW conflicts, leading to more redundant computations. The FiberCache in GAMMA can benefit from these features, resulting in a low miss rate.
Thirdly, Mentor improves bandwidth utilization by 0.94×-1.45×. On hyper-sparse and powerlaw matrices, Mentor improves utilization by 1.45× and 1.39× compared with the baselines. This is because both matrices exhibit poor spatial locality of non-zero, and Mentor can effectively mitigate this issue by streaming construction. Only on block-sparse matrices, Mentor exhibits poor bandwidth utilization, which can be attributed to the good locality of dense matrix blocks. Spaghetti performs well because the adopted matrix tiling enhances locality when accessing dense matrix blocks.
6.2 Comparing to CPU and GPU
We then compare Mentor with state-of-the-art SpMM implementations on CPUs and GPUs. Specifically, we compare Mentor against MKL on CPUs and cuSPARSE on GPUs regarding energy efficiency, defined as the throughput divided by the total power consumption. We utilize an Intel Core-i5 12600K CPU with 10 cores and 16 threads to execute mkl_sparse_s_mm for SpMM, measuring power using the RAPL interface . We use an NVIDIA RTX 3060 GPU with a 360GB/s bandwidth of GDDR6 and measure the execution time of cusparseSpMM in cuSPARSE. Additionally, we monitor power consumption using nvidia-smi. Note that we measure the real-time power consumption of CPU and GPU for comparison rather than thermal design power (TDP), with geomean values of 61.7W and 134.8W, respectively. In contrast, we follow the common practice of reporting the total power of Mentor in the power analysis from the implemented netlist, with a value of 6.4W, including static and dynamic power consumption. Figure shows that the geomean energy efficiency of MKL, cuSPARSE, and Mentor are 5.68 × 10 8 , 9.38 × 10 8 and 1.53 × 10 9 FLOP/J, respectively. When dealing with large-scale problems, general-purpose processors are limited by 79:18 X. Lu et al.  substantial memory access overhead. By optimizing memory access, Mentor demonstrates 2.49× and 1.86× better energy efficiency than MKL and cuSPARSE for problems larger than 10 7 .
6.3 Efficiency of Analytical Model
We perform SpMM operations on 20 sparse matrices with varying features to observe the performance variations with different values of c and demonstrate the efficiency of the analytical model. Note that the off-chip bandwidth remains constant at 9600MB/s and the interface width of Read B is set to 128-bit. Thus, we calculate that
E b = E c = 4.
Each subfigure of Figure shows the throughput (lines) of five matrices and the geomean speedup (bars) after doubling #PE with an upper limit of 2. The x-axis denotes c ′ (=#PE/E b ). Taking Figure (a) as an example, for the five matrices with f (npr ) larger than 16, our model predicts that c ′ = 64. We see that Mentor exhibits promising speedups, ranging from 1.85× to 1.98× when doubling c ′ , and it is smaller than 64. We observe the same trend in Figure , where the predicted estimate of c ′ is 16. Here, the speedups are 1.96× and 1.87× when c ′ does not exceed 16. But when c ′ increases from 16 to 32, the speedup declines from 1.87× to 1.58×, indicating that using more hardware resources leads to diminishing performance improvements. Similarly, Figure shows that the speedup decreases to 1.01× when c ′ ranges from 8 to 16. But the accelerator can still achieve 1.98× speedup with c ′ = 2. To summarize, when c ′ is smaller than our estimate, doubling #PE can yield an average speedup of 1.92×. When using excessive PEs and if c ′ is larger than our estimate, the speedup will decrease to 1.3×. This demonstrates that our analytical model can provide significant guidance for optimizing the architecture to better balance hardware resources and performance across different problems.
6.4 Case Study: GCN Layer
Since SpMM is a fundamental kernel, Mentor is equally applicable in SpMM-based applications such as graph convolutional networks (GCN). Thus, we evaluate Mentor with the GCN workloads  with a chain of SpMM, i.e., X (l +1) = σ (A(X (l ) W (l ) )), where A denotes the sparse adjacency matrix, X (l ) and W (l ) represent the sparse feature map and the dense weight matrix of the lth layer. Table shows the structure and sparsity of the three benchmarks. In the SpMM chain, the dense result calculated by X (l ) W (l ) can be directly streamed into the Read A module without being written back to the off-chip memory. We reconfigure the architecture as Mentor-GCN with 16 parallel PEs, and compare Mentor-GCN against three state-of-the-art GCN accelerators: HyGCN , AWB-GCN , and GCNAX . Following the configuration in Reference , all baselines use a 1×16 MAC array with a data width of 64-bit and a memory bandwidth of 128GB/s.
We first present speedup with normalized execution cycles in Figure (a). On three datasets, Mentor achieves 5-14.7×, 1.28-1.8×, and 0.96-1.15× speedup over HyGCN, AWB-GCN, and GC-NAX, respectively. On Citeseer, Mentor only achieves 0.96× speedup over GCNAX. In Mentor-GCN, #PE=16 and E b = 4, thus Mentor-GCN runs with a 3-cycle delayed feeding according to Equation . But the estimate of c is 1 because npr = 2.74 in the adjacency matrix A of Citeseer. In other words, the 3-cycle delayed feeding is slightly aggressive, leading to decreased PE utilization and slight performance degradation on Citeseer.
In Figure (b) and (c), Mentor exhibits 9.75-15.28×, 1.76-5.36×, and 1.05-1.93× traffic reduction, and 2.38-2.88×, 0.76-1.81×, and 1.35-2.20× higher bandwidth utilization. HyGCN and AWB-GCN employ inefficient execution orders and involve redundant computations and DRAM traffic. GC-NAX utilizes an optimized outer product dataflow similar to Spaghetti, which also suffers from a large overhead of partial matrices and exhibits poor performance on Pubmed.
6.5 Overhead Analysis
6.5.1 Power and Hardware Resource. We present the dynamic power and LUT breakdown of Mentor in Figure , extracted from the place-and-route results of Vivado 2022.2. The dynamic power consumption is 5.56W, which accounts for 87% of the total power consumption, with a majority (57%) attributed to the Processing System (PS) side, particularly the DDR.  For a fully-pipelined architecture, the accumulation phase involves a tailored on-chip structure, resulting in 31% dynamic power and 78% LUT count requirement. Both phases I and IV exhibit minimal power and resource overhead, indicating that the crossbar does not impose a heavy burden. 6.5.2 Hardware Resource. We further conduct experiments to evaluate Mentor with different on-chip resource support. Figure shows the speedups as FIFO and scratchpad capacities vary. Each line represents the benchmark with various levels of sparsity. We observe that the FIFO capacity has minimal impact on performance, with a maximum speedup of 1.018×, demonstrating that each phase's throughput is approximately equal, thus proving the efficiency of our full-pipelined design on the hardware level. In contrast, we can observe a significant impact of scratchpad capacity on performance from Figure . For matrices with a density below 0.2%, configuring the scratchpad capacity to 32KB yields a speedup of 4.48×. This is because the stream construction scheme partitions the matrix A into smaller blocks to fit the decreased scratchpad capacity, introducing more Block elements. But for matrices with a density exceeding 1.6%, the impact of Block elements is minimal, and increasing the scratchpad capacity can not yield considerable performance gains.
6.5.3 Overhead of Streaming Construction.
The streaming construction introduces three control elements, increasing the memory footprint of A. Our experiments on 125 matrices show that stream construction incurs an average increase of 11% (with a minimum of 0.2%) compared with CSC/CSR. We implement this algorithm in Python and compare its running time with the non-zero scheduling algorithm in Sextans and the affinity-based row reordering in GAMMA. Our approach is 2.47× and 11.51× faster than Sextans and GAMMA, respectively. The construction accounts for 35% of the end-to-end time taken for 10 SpMM iterations.
6.6 Limitations
Mentor has demonstrated the advantages of column-wise-based SpMM accelerators through software-hardware co-design. Here, we discuss two limitations of our approach. (1) Significant on-chip memory requirements. Both Mentor and AWB-GCN require substantial on-chip memory to buffer the partial results. On large sparse matrices, the size of the partial results can even exceed 1M, making it challenging to implement an accelerator with tight-budgeted on-chip memories. Matrix partitioning can help alleviate this issue, but this approach involves a tradeoff between hardware resources and data reuse. (2) Limited scalability on skinny matrices. Mentor utilizes parallelism in the N -dimension to take advantage of column-wise benefits. In real-world scenarios, N may be smaller than #PE of Mentor, especially for skinny matrices. In this situation, additional hardware resources do not significantly improve Mentor's performance. Therefore, we recommend deploying Mentor with several tens of PEs as a lightweight SpMM core. By utilizing multiple on-chip cores, we can support parallel processing on multiple workloads without incurring significant on-chip communication overhead.