2 Background and Motivation 2.1 Sparse-Dense Matrix Multiplication
Given a sparse matrix A of M ×K, a dense matrix B of K × N and a dense matrix C of M × N , SpMM performs the following computation,
C = α ⋅ A ⋅ B + β ⋅ C, (1)
where A ⋅ B is typically the performance bottleneck and thus is the main focus of this work. Since matrix A is often sparse with many zero elements, it is typically stored in a sparse matrix storage format such as the compressed sparse row (CSR) or the compressed sparse column (CSC) formats. In contrast, B is a dense matrix where the non-zeros are stored in row-major or column-major order. As in previous works , we refer to each compressed row/column of the sparse matrix A as a fiber and use row/column exclusively when referring to the dense matrix B.
Inner (M) M-N-K nnz + M ⋅ K ⋅ N nnz+ M ⋅K ⋅N P − < nnz⋅P M ⋅K Moderate Low Inner (N) N-M-K (nnz + K) ⋅ N nnz⋅ N P + K ⋅ N − = 1 1 P + K nnz Moderate Low Outer (M) K-M-N nnz + K ⋅ N n n z+ K ⋅ N ≥ 2 ⋅ nnz ⋅ N < nnz 2⋅nnz+K Good High Outer (N) K-N-M nnz + K ⋅ N n n z+ K ⋅ N ≥ 2 ⋅ nnz ⋅ N < nnz 2⋅nnz+K Good High Row M-K-N nnz + nnz ⋅ N [nnz(1 + N P ), nnz(1 + N )] − [ 1 1 N + 1 P , N 1+N ] Moderate Low Column N-K-M ≤ (nnz + K) ⋅ N nnz ⋅ N P + K ⋅ N − = 1 1 P + K nnz Good Low
2.2 Memory Access Analysis
Our work is motivated by the observation that existing SpMM optimizations built upon the inner, outer, and row-wise products have drawbacks of inefficient off-chip memory accesses. According to Reference , we categorize the dataflows into 6 variants, where (M) and (N ) denotes the stationary dimension in the dataflow. We follow the memory modeling approach for the General Sparse Matrix-Matrix Multiplication (SpGEMM) to analyze the memory access of these implementations. Table quantifies the amount of memory traffic required for moving input data and storing partial results, the data reuse, spatial locality and bandwidth requirements for six computation dataflows with parallelization, in terms of the number of non-zeros (nnz) in A and the matrix sizes (M, K, and N ) of matrices A and B. The number of MAC operations for each SpMM approach is nnz ⋅ N . We also discuss the data locality and bandwidth requirements of the parallelized dataflows in Table .
2.2.1 Inner
Product. This dataflow iterates the fibers (i.e., rows/columns) of sparse matrix A (which is typically stored in the CSR format) and the columns of B to perform vector dot product to obtain a final element:
C[i, j] = K ∑ k=0 A[i, k] ⋅ B[k, j] (2)
As shown in Table , this dataflow has two variants: Inner (M) for M-stationary and Inner (N) for N -stationary. To calculate the entire final matrix C, both require negligible off-chip memory accesses to partial sums produced by dot products, which are often small enough to be stored on-chip. But they require a large amount of off-chip memory traffic to access entire B M times or entire A N times, and thus the memory traffic is M ⋅ K ⋅ N for Inner (M) and nnz ⋅ N for Inner (N). As a result, the data reuse of Inner (M) is nnz⋅N nnz+M ⋅K ⋅N ≈ nnz M ⋅K . This approximation holds because nnz ≪ M ⋅ K ⋅ N . And the data reuse of Inner (N) is nnz⋅N (nnz+K )⋅N = nnz nnz+K .
2.2.2 Outer
Product. This dataflow traverses fibers of A (which is typically stored in the CSC format) and rows of B, performs the outer product (⊗) to produce a partial matrix (multiply stage), SpMM Accelerator Based on Column-Wise Product 79:5 C ′ , which will then be merged to form the output matrix C during a merging phase:
C ′ k = A[∶, k] ⊗ B[k, ∶]. (3)
This dataflow has two variants: Outer(M) and Outer(N). Both dataflow variants traverse A and B once but suffer from large memory traffic requirements for partial matrices and poor output reuse. Thus, the data reuse is less than nnz⋅N nnz+K ⋅N +2⋅nnz⋅N for both Outer(M) and Outer(N). For the typical SpMM computations in deep neural networks (DNNs), matrix B often works as a weight matrix, where N is the hidden layer dimension ranging from 10 to 1000. Under such settings, we can estimate that nnz ≪ nnz ⋅ N , and the data reuse is less than nnz 2⋅nnz+K (Table ).
2.2.3 Row-Wise
Product. This dataflow is a variant of Gustavson's algorithm, which performs intersections, i.e., scalar-vector multiplication between the scalars from fibers of A in CSR and corresponding rows from B determined by the non-zero coordinates in the fibers, and produces a partial row of the final row of C,
C ′ cid [i, ∶] = A[i, cid] ⋅ B[cid, ∶] C[i, ∶] = k ∑ j=0 C ′ j [i, ∶] . ( 4
)
Each intersection takes a scalar and a vector of N elements, i.e., an input traffic of nnz + nnz ⋅ N . Since the partial results sizing N can be held on-chip, the data reuse of SpMM is nnz⋅N nnz+K N ≈ N 1+N ≈ 1.
2.2.4 Column-Wise
Product. This dataflow is also a variant of the Gustavson algorithms, which performs intersections between scalars from B columns and fibers of A in the CSC format, and obtains a partial column of C,
C ′ r id [∶, i] = A[∶, rid] ⋅ B[rid, i] C[∶, i] = k ∑ j=0 C ′ j [∶, i] . ( 5
)
Each intersection consumes a scalar from B and a fiber from A, leading to an input traffic of nnz ⋅ N +K ⋅ N . Note that the non-zeros in B is smaller than N ⋅K. The partial results sizing K can also be held on-chip; thus, the data reuse for column-wise is larger than nnz⋅N (K +nnz)⋅N = nnz nnz+K . Considering that nnz K ranges from 10 to 100 , the data reuse will also approach 1 (Table ).
2.3 Advantages of Column-Wise
Given the dense nature of matrix B in SpMM, we find that the column-wise dataflow can significantly enhance memory efficiency, which is crucial for the memory-bound SpMM. We now consider parallelization to discuss the drawbacks of other methods regarding memory access. We use M or N in Table to indicate that the dataflow is parallelized along the M or N dimension with a parallelism degree P. Figure compares the two variants of the Gustavson algorithm, i.e., the row-wise vs. column-wise approach.
❶ The column-wise approach demonstrates better data reuse. With parallelization, the column-wise approach reduces iterations for accessing A from N to N P and improves data reuse to 1 1 P + K nnz , and Inner(M) can improve reuse to nnz⋅N ⋅P nnz⋅P +M ⋅K ⋅N ≈ nnz M ⋅K ⋅ P. Thus, the reuse ratio of the Inner(M) to column-wise approach is nnz M ⋅K ⋅P ⋅( 1
P + K nnz ) = nnz M ⋅K + 1 M ≪ 1.
In other words, the columnwise approach exhibits better data reuse compared with Inner(M). The memory access of the outer dataflow does not benefit from parallelization, making the column-wise approach more suitable. The parallelized row-wise approach still suffers from irregular data reuse. Figure . Such irregular and challenging-to-capture reuses significantly increase memory traffic. In the worst case, when there are no identical elements in the parallel fibers, the data reuse of this approach is only 1 1+N . Figure (b) shows that the columnwise approach can benefit from a regular memory access pattern, where A[∶, 0] is read once and subsequently broadcasted to multiple B scalars (B[0, 0], B[0, 1], B[0, 2] and B[0, 3]), which maximizes reuse of A, thereby reducing input traffic from (nnz + K) ⋅ N to ( nnz P + K) ⋅ N and increasing data reuse to 1
1 P + K nnz .
❷ The column-wise approach has better spatial locality. The Inner(M) and Inner(N) dataflows can sequentially traverse A and B, but its spatial data locality is limited by the index matching between A fibers and B columns. The outer-product dataflow can avoid this problem and achieve good spatial locality. The row-wise dataflow performs intersections between scalars of A and vectors of B (B[0, ∶] and B[2, ∶] determined by A[0, 0] and A[0, 2] in Figure ), and the irregular distribution of non-zeros leads to poor locality of B. In contrast, the column-wise dataflow guarantees sequential access to the entire A by exploiting the continuity of row-ids of non-zeros in columns of B. The sequential access begins with A[∶, 0], followed by A[∶, 1] and so forth for subsequent columns as shown in Figure ). This access pattern can significantly benefit memory access. Taking the DDR4 on the UltraScale FPGA as an example, the simple address increment pattern can reach an efficiency of up to 94% . These findings highlight the significant performance impact of the column-wise approach with better spatial locality.
❸ The column-wise approach requires low off-chip bandwidth. Although the outer-product dataflow has good spatial locality, it is still limited by the non-parallelizable multiply and merge stages. The merge stage reduces a large number of partial elements and requires high bandwidth SpMM Accelerator Based on Column-Wise Product 79:7  support to minimize overall computational latency. The inner products and Gustavson variants can hold partial results on-chip to reduce the bandwidth requirement.
We also identify that it is easier for the column-wise dataflow to achieve load balancing than the row-wise dataflow. Figure shows that the computation load assigned to each processing engine (PE) depends on the distribution of non-zeros in A for the row-wise dataflow. In contrast, broadcasting ensures even loads among PEs for the column-wise dataflow. This advantage will simplify software or hardware design to balance computational efficiency.
2.4 Prior Accelerators
Table presents a comparison of related accelerators, including specialized works for SpGEMMs (SIGMA, Spaghetti, GAMMA) and GCNs (HyGCN, AWB-GCN, and GCNAX). We consider GCNs because the two stages of a single GCN layer can be organized as two consecutive SpMMs. We discuss the inefficiencies in their dataflows and specific designs.
On the one hand, prior accelerators make critical tradeoffs to overcome the inefficiencies of their dataflows. SIGMA utilizes a bitmap format (using binary to mark non-zero positions) to map sparse matrices to the MAC array and avoid index matching. Still, this format is space-inefficient on highly sparse matrices, leading to decreased input reuse. Spaghetti is the state-of-the-art outer-product-based accelerator, which supports a streaming access pattern to improve output reuse and reduce bandwidth requirement. Unfortunately, it introduces significant overhead of sort-mergers, sacrificing input reuse. To address the issues of irregular reuse and poor data locality, GAMMA employs cache-based on-chip structures to capture irregular reuse of B, reducing memory traffic and enhancing the data locality. However, the performance of the cachebased design heavily depends on the non-zero distribution of the sparse A. Figure shows an emulation of cache behaviors using the same configuration as the FiberCache (3MB, 16-way 79:8 X. Lu et al.
set-associative) on 100 sparse matrices. We see significant variations in cache performance, with a maximum miss rate exceeding 41% and a minimum of 0.08%. Therefore, this approach requires high bandwidth support to mitigate the impact of cache misses. Sextans uses multi-level memory optimizations to buffer partitioned blocks of B on-chip. But it ruins input reuse of A and fails to overlap the memory access by computation. Thus, Sextans suffers from significant memory traffic and high bandwidth requirement. AWB-GCN is designed for consecutive SpMMs, which leverages the column-wise approach and maps the computation of a final column to multiple parallel PEs. This strategy sacrifices the data reuse of matrix A.
On the other hand, the previous non-streaming accelerators suffer from not fully utilizing off-chip bandwidth and PEs. Streaming is a design that can receive, process, and emit data elements at a consistent rate in continuous clock cycles, ensuring efficient data flow . This means that a streaming design has two main features: ❶ A streaming access pattern, which requires continuous transfer of off-chip data with low access latency ); ❷ An optimized data path, which needs to achieve a consistent processing rate between the components in the architecture to ensure high processing element (PE) utilization . As a streaming example, Sextans adopts non-zero scheduling and multi-level memory optimization for efficient memory access. Despite being limited by the inherent disadvantages of the row-wise dataflow, Sextans provides an effective solution by streaming design to accelerate SpMM. However, previous non-streaming designs either do not support streaming access patterns or experience reduced PE utilization due to potential pipeline stalls. Specifically, GAMMA is constrained by the irregular access pattern determined by the distribution of non-zero elements within fibers, and cache misses can cause pipeline stalls in the PEs. HyGCN uses fine-grained parallel execution but does not support a streaming access pattern due to graph partitioning and sparse elimination. Additionally, the tandem engine results in the underutilization of PEs due to workload imbalance between the two execution stages. AWB-GCN maps the partitioned matrix A to multiple parallel PEs, preventing A from being streamed. Furthermore, its auto-tuning unit needs to rebalance the workload after the completion of each column, leading to pipeline stalls and limited parallelism. GCNAX focuses on dataflow optimization about loop order and loop fusion but does not offer a streaming design.
In a nutshell, our Mentor utilizes a column-wise approach without compromising memory accesses. This is achieved by implementing streaming at the software level to enable streaming memory accesses (Section 4) and designing a fully-pipelined hardware architecture at the hardware level (Section 3) to adapt to the created data streams for near-optimal performance.