1 Introduction
Sparse-dense matrix multiplication (SpMM) involves the multiplication of a sparse matrix, A, with a dense matrix, B. This operation is commonly seen in traditional scientific applications and emerging machine learning workloads . There is an extensive body of research for accelerating the SpMM kernel on CPUs , GPUs and purposed-built hardware accelerators . Since the input matrices of real-life SpMM kernels are often too large to be kept in on-chip buffers on hardware accelerators with MBs of static RAM (SRAM), memory optimization is vital for SpMM performance.
Prior work for optimizing SpMM typically follows three types of dataflows for matrix computations: inner, outer, and row-wise products , as depicted in Figure . However, these approaches are inefficient at optimizing off-chip memory access. Specifically, the inner product computes an element of C by performing a dot product, summing a row of A with a column of B. In contrast, the outer product dataflow computes an outer product between a column of A and a row of B to generate a partial matrix, which is then merged to form C. Unfortunately, neither approach effectively exploits the data reuse of both input and output matrices. When processing large matrices, the inner product method results in excessive memory traffic by iterating over the dense matrix B multiple passes. In contrast, the outer product method generates a large number of partial matrices, leading to suboptimal performance.
More recent work exploits the row-wise product for matrix multiplications ]. This approach multiplies each non-zero in a row of A with the corresponding row of B to form a final row of C, where the row in B is determined by the column index of the non-zero in A. While representing a step forward over inner and outer products, row-wise based SpMM accelerators can suffer from poor data locality since the non-zeros used to locate rows in B are irregularly distributed in the sparse matrix A. This often leads to poor memory bandwidth utilization.
We present Mentor, a software-hardware co-design approach to accelerate SpMM, built upon the column-wise dataflow. As shown in Figure , this approach multiplies each element in B with the corresponding column of A to generate a partial column of the output matrix. Our systematic analysis of the memory access pattern of different dataflows for SpMM suggests that the columnwise approach can effectively overcome the drawbacks of inefficient off-chip memory accesses. While some prior works have leveraged this approach for consecutive SpMM operations, they fail to take full advantage of the column-wise approach on individual SpMM computation.
Mentor is designed to take advantage of the column-wise approach by providing a streaming design at the software and the hardware layers. At the software level, we identify optimization opportunities in conventional memory access patterns and propose a pre-processing technique to reconstruct the input matrices. This enables streaming memory accesses for Mentor to improve memory access efficiency when accessing the matrices. Specifically, for dense matrices, this scheme adopts row-major storage rather than column-major used in other column-wise-related works to harvest the data locality and permits the hardware to overlap the computation with memory SpMM Accelerator Based on Column-Wise Product 79:3 accesses by an aggressive optimization. For the sparse matrices, the scheme incorporates a novel compressed format, enabling streaming access to sparse matrices and addressing the read-afterwrite (RAW) conflict by introducing control elements to the conventional compressed format. At the hardware level, Mentor employs a fully pipelined design to process data in a streaming fashion for enhancing throughput and bandwidth utilization. This includes an efficient memory interface with low hardware complexity and a double-buffered on-chip structure to avoid pipeline stall. The hardware design of Mentor is supported by a performance model to derive hardware parameters (e.g., #PEs) for given off-chip memory bandwidths and sparse matrices.
We have implemented a hardware prototype of Mentor on an Xilinx FPGA. We evaluate Mentor across a wide range of input matrices from real-world applications with various sparse patterns and structures. We compare Mentor against four SpMM accelerator baselines based on inner , outer , row-wise , and column-wise dataflows. Compared with the baselines, Mentor achieves a 1.18×-3.98× speedup, with 1.23×-4.93× reduction in the off-chip memory traffic.
This article makes the following contributions:
-It provides a quantitative analysis to demonstrate the advantages of the column-wise approach on SpMM acceleration (Section 2); -It demonstrates how the column-wise dataflow can be employed at the software level (Section 3) and at the hardware level (Section 4.1) to accelerate SpMM; -It presents a performance model to help explore the hardware design space for leveraging the column-wise dataflow design for SpMM (Section 4.2).