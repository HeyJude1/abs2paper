3 Methodology
SpMV is primarily used in iterative algorithms, thus most optimization methods are proposed based on multiple invocations to SpMV during the iterative process. Algorithm 1 presents the outline of the CG algorithm. It can be observed that, in each iteration, the computation result of SpMV (line 4) is used in subsequent dot product (line 5) and scalar-vector multiplication (line 6), generating the multiplication vector p i of SpMV for the next iteration (line 8).
This article focuses on the acceleration and optimization of SpMV on multi-GPU systems while considering its application in iterative scenarios. We consider an iterative framework as shown in Figure . After each GPU completes its SpMV kernel execution, the all-to-all synchronization is performed among multiple GPUs to ensure that each GPU has the complete result vector. Subsequently, multiple GPUs collectively execute other calculations in the iteration. Before executing the next SpMV, each GPU already has the complete multiplication vector, eliminating the need for further data synchronization. Starting from the naive non-zeros-based matrix partitioning, we first propose two-level non-zeros-based matrix partitioning to overlap the kernel computation and data transmission. Considering the irregular distribution in sparse matrices, we then design a longrow-aware matrix partitioning method. Finally, an optimization method that uses a redundant and lightweight kernel execution in exchange for expensive data transmission is proposed.   ;
6 x i = x i−1 + αp i−1 ; r i = r i−1 − αy i ; 7 r New = r T i r i ; β = r N ew rOld ; rOld = r New; 8 p i = r i + βp i−1 ; 9 end 10 x = x i−1 ;
3.1 Non-zeros-based Matrix Partitioning
In SpMV, the calculations for different rows are independent. Therefore, an intuitive partitioning is to evenly divide the sparse matrix by rows into multiple sub-blocks, with each GPU handling one sub-block. However, considering the irregular distribution of non-zeros across different matrix rows, this partitioning method may cause load unbalance among multiple GPUs. One more reasonable partitioning approach is based on the number of non-zeros (NNZ), because it largely determines the workload of SpMV. We refer to this partitioning method as non-zeros-based partitioning in this article. Non-zeros-based partitioning divides the sparse matrix into multiple subblocks, where the number of sub-blocks equals the number of GPUs in the system. Each sub-block consists of consecutive matrix rows and has a similar number of non-zeros to others, approximately equal to nnz/nGPU , where nnz is the total number of non-zeros in a sparse matrix and nGPU is the number of GPU devices in a system. Figure (a) illustrates an example of non-zerosbased partitioning. In the example, the matrix has a size of 7×7 and contains 17 non-zeros. When the number of GPUs is two, the results of non-zeros-based partitioning are A 1 and A 2 , where the first sub-block A 1 consists of the first two rows, containing 8 non-zeros, and the second sub-block A 2 consists of the last five rows, containing 9 non-zeros. In the SpMV algorithm using non-zeros-based partitioning, two threads are allocated to control each GPU device and call the SpMV kernel for the assigned sub-block. Finally, each GPU transmits its calculated results to other GPUs, ensuring that each GPU has the complete result vector. Due to the inherent parallelism in multi-GPU systems, when a GPU completes the kernel execution ahead of others, it can transmit its calculated results to another GPU, and another GPU can proceed with its independent kernel execution while concurrently receiving transmitted data. This enables an initial overlap between kernel execution and data transmission across multiple GPUs. Figure (a) illustrates the computation and transmission process in SpMV using non-zeros-based partitioning. The second GPU takes more time for kernel execution and data transmission, because the subblock it deals with contains more non-zeros and more matrix rows than the first GPU.
One of the advantages of non-zeros-based partitioning is its low partitioning overhead. Taking the most popular sparse compression format CSR as an example, given the number of GPU devices in the system, it only requires traversing the array that stores the offsets of non-zeros for each row. Additionally, compared with rows-based partitioning, the SpMV computation using the non-zerosbased partitioning exhibits better load balance. However, the transmission overhead introduced by multi-GPU computations can significantly reduce the overall performance benefits. In some sparse matrices, the introduced transmission overhead can even exceed the performance gains achieved in the kernel execution. In those cases, the performance of SpMV on multi-GPU systems is inferior to that on a single GPU. Taking the sparse matrix europe_osm as an example, the kernel execution using two GPUs achieves a speedup of 1.48× over one GPU. However, when considering the overhead of data transmission, the overall speedup is less than 1.
3.2 Two-level Non-zeros-based Matrix Partitioning
In non-zeros-based matrix partitioning, each GPU needs to complete the kernel execution for the assigned sub-block before transmitting its calculated results. It means that, on each GPU, the kernel execution and data transmission always run sequentially. Therefore, we propose a two-level nonzeros-based partitioning algorithm to achieve overlap between kernel execution and data transmission on each GPU. Two-level non-zeros-based matrix partitioning divides the sparse matrix into 2 ×nGPU sub-blocks, where each sub-block consists of multiple consecutive matrix rows, and different sub-blocks have a similar number of non-zeros, approximately nnz/(2 × nGPU ).
Figure (b) illustrates an example of two-level non-zeros-based matrix partitioning on two GPUs. First, the non-zeros-based matrix partitioning is used to divide the matrix into two sub-matrix blocks, denoted as A s1 and A s2 . Then, the non-zeros-based partitioning is used again to further divide each sub-block into two sub-blocks, resulting in four sub-blocks. In the example, the subblock A s1 is divided into A s1 1 and A s1 2 , and A s2 is further divided into A s2 1 and A s2 2 . In the computation process, the first GPU is responsible for processing sub-blocks A s1 1 and A s2 1 , while the second GPU handles A s1 2 and A s2 2 . Each GPU launches two CUDA streams to separately run the SpMV kernels for two sub-blocks, enabling the overlap of data transmission for the first sub-block and kernel execution for the second sub-block. As shown in Figure
3.3 Long-row-aware Sparse Matrix Partitioning
Due to the irregular distribution of non-zeros, different sub-blocks often take different data transmission overheads, further affecting the performance benefits of the two-level non-zeros-based partitioning. Observing Figures ) and 4(b) and taking the second GPU as an example, sub-blocks A s1
2 and A s2 2 contain a similar number of non-zeros, thus taking similar kernel execution time.
Optimization of Large-scale Sparse Matrix-vector Multiplication on Multi-GPU Systems 69:9 However, in terms of transmission, given a fixed transmission bandwidth, the time overhead of the transmission process depends on the data transmission amount, i.e., the number of matrix rows contained in each sub-block. Sub-block A s1 2 only contains one matrix row, while sub-block A s2  2 contains three matrix rows. Therefore, in Figure (b), the transmission process D2D{y s1 2 } for the calculated result y s1 2 of sub-block A s1 2 takes nearly two-thirds less time overhead compared with D2D{y s2 2 }. The large transmission overhead of the second sub-block on both GPUs limits the overall performance benefits of the two-level non-zeros-based partitioning. Therefore, we further propose a long-row-aware matrix partitioning algorithm. It first identifies the locations of longrow blocks in the matrix and then uses the non-zeros-based partitioning method to assign the short-row blocks to multiple GPUs, where the short-row blocks are the sub-matrix excluding the long-row blocks. Afterward, the non-zeros-based partitioning method is used again to assign the long-row blocks to multiple GPUs. Now, each GPU is assigned one long-row block and one shortrow block.
Figure (a) illustrates the workflow of the long-row-aware partitioning. Given an m × n sparse matrix A as input, A l and A s represent its long-row block and short-row block, respectively. The objective of the algorithm is to find the positions of A l and A s in A. Let d l represent the ratio of rows in the long-row block to the total matrix rows. The number of matrix rows contained in the long-row block is calculated as m l = d l × m . Letting max denote the maximum NNZ per row in A, we can calculate its corresponding row index rId_max. If rId_max ≤ m l , i.e., the longest row is located within the first m l rows, then the sub-block composed of the first m l rows is considered as the long-row block A l . Otherwise, the sub-block composed of the last m l rows is considered as the long-row block.
For an irregular sparse matrix, when the long-row block and short-row block contain a similar number of non-zeros, the short-row block contains more matrix rows, resulting in higher transmission overhead for the corresponding calculated result. Therefore, the SpMV kernel for the shortrow block should be executed first, followed by the kernel for the long-row block. This allows for the overlap of the result transmission for the short-row block and kernel execution for the longrow block, thereby hiding more of the transmission process. Figure (c) illustrates an example of long-row-aware matrix partitioning. A s1 1 and A s2 1 are the short-row block and long-row block, respectively, assigned to the first GPU, while A s1 2 and A s2 2 are the short-row block and long-row block, respectively, assigned to the second GPU. Figure (c) shows the execution process for this example. Each GPU first executes the SpMV kernel for the short-row block and then for the longrow block. Taking the first GPU as an example, the long-row block A s2  1 and short-row block A s1 1 contain the same NNZ, so we assume that their kernel execution time is largely the same. However, the number of rows in the short-row block is twice that of the long-row block, resulting in double the transmission overhead over the long-row block. In Figure (c), when the kernel execution for the short-row block is completed, the kernel execution for the long-row block y s1 1 = A s1 1 × p and the transmission D2D{y s1
1 } for the short-row block start simultaneously. As D2D{y s1 1 } in Figure ) is longer than D2D{y s1 1 } in Figure (b), the long-row-aware partitioning method can hide more of the transmission process. It can be observed that, compared with the two-level non-zerosbased partitioning, using the long-row-aware matrix partitioning can reduce the time overhead of t 3 − t 2 .
3.4 Redundant-computing Optimization
From Figure (c), it can be observed that the efficiency of kernel execution and data transmission corresponding to long-row blocks affects the overall efficiency of SpMV on multi-GPU systems. The parallel computation on multiple GPUs accelerates the execution of the kernel, but the introduced transmission overhead becomes a critical factor limiting performance improvement. Based on the fact that SpMV on a single GPU does not require data transmission, we further propose redundant-computing optimization. The main idea is to identify the most cost-effective sub-block from the short-row block. Then, every GPU executes the SpMV kernel for this sub-block, without the requirement for transmitting the calculated results to other GPUs. Here, the most cost-effective block refers to the block that contains the maximum number of rows but takes the shortest kernel execution time.
Figure (b) illustrates the long-row-aware partitioning with the redundant-computing optimization. Let A c represent the redundant-computing sub-block. The objective of the algorithm is to find the locations of A l , A s , and A c within the matrix A. Let d l and d c denote the ratios of the matrix rows contained in the long-row block and redundant-computing block to the total number of matrix rows, respectively. We can calculate the matrix rows m l , m s , and m c contained in the long row block, short row block, and redundant-computing block, respectively. Similarly, the row index rId_max of the longest row is calculated, and it determines the location of A l . Assuming rId_max ≤ m l , the first m l rows are composed of the long-row block A l . Conversely, if rId_max ≥ m − m l , then the last m l rows are composed of the long-row block. Otherwise, some m l rows in the middle of A are considered as the long-row block. Taking the first case for example, where the first m l rows are considered as the long row block A l , the algorithm further determines whether the subsequent m c rows or the last m c rows of the matrix have a smaller average NNZ per row. The aveN N Z (m l : m l +m c ) in Figure (b) represents the average NNZ per row in the submatrix block from the (m l + 1)-th row to the (m l + m c )-th row. When the number of matrix rows is the same, the sub-block with a smaller average NNZ per row contains less NNZ, thus taking less kernel execution cost, making it a more suitable candidate for designation as the redundantcomputing block A c . Then, the remaining m s rows are considered as the short row block A s . Therefore, the corresponding partitioning results in the discussed example are ❶ or ❷ presented in Figure (b). The remaining cases follow a similar pattern. Once A l , A s , and A c are located, the non-zeros-based partitioning is used to divide A l and A s into nGPU sub-blocks, respectively. In the example presented in Figure (d), let us take d l = 0.3 and d c = 0.1. This yields m l = 2, m s = 4, and m c = 1. The longest row of the matrix is the first row, satisfying the condition rId_max ≤ m l as depicted in Figure . Therefore, the first two rows of the matrix are considered as the long-row block A l . Next, we compare aveN N Z (3 : 3) = 2 (i.e., the NNZ in the third row of the matrix) with aveN N Z (7 : 7) = 1 (i.e., the NNZ in the last row of the matrix). Consequently, the last row is composed of the redundant-computing block A c , and the middle four rows are considered as the short-row block A s , corresponding to the partitioning result ❷ in Figure .
Building upon the long-row-aware partitioning, the third CUDA stream is launched on each GPU to handle the redundant-computing sub-block. On one hand, the kernel execution for the sub-block can overlap with the transmission process for the calculated result of the long-row blocks on each GPU. On the other hand, its calculated results of SpMV do not require further data transmission.
Figure (d) illustrates the execution process of SpMV using redundant-computing optimization. The short-row block assigned to the second GPU contains fewer non-zeros and matrix rows compared with Figure ). As a result, both the kernel execution time and transmission time are reduced. Each GPU launches the third CUDA stream (Stream 3) to execute the redundant-computing SpMV kernel, allowing it to overlap with the data transmission process of the long-row blocks, D2D{y s2
1 } or D2D{y s2 2 }. Therefore, the redundant-computing optimization enables hiding a portion of the kernel execution while reducing the transmission overhead.