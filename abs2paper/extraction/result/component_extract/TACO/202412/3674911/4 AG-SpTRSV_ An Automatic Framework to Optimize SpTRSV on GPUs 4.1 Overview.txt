4 AG-SpTRSV: An Automatic Framework to Optimize SpTRSV on GPUs 4.1 Overview
To fully consider the aforementioned factors and bridge the performance gaps, we propose AG-SpTRSV, an automatic framework to optimize SpTRSV on GPUs. Compared with existing work , we consider a much larger optimization space, which carefully handles the observed performance issues and consistently achieves good performance on matrices with various sparse patterns. By automatic scheme searching and selecting, AG-SpTRSV is able to provide an efficient target kernel for a specific matrix, with eliminated costs of manual algorithmic design and parameter tuning. AG-SpTRSV consists of four stages. In the Prepare stage, a series of code variants with different parallelism setups and optimization methods are prepared. These code variants are used for different sparsity patterns. In the Transform stage, the sparse matrix is represented as a computation graph. Through graph operations, including node merging and reordering, the original computation graph is transformed into a series of candidate graphs for subsequent optimization and evaluation. In the Schedule stage, the nodes of the candidate graphs are mapped to GPU warps and hardware through multi-hierarchy heuristic scheduling. In the Select stage, AG-SpTRSV exhaustively tests and evaluates the combination of strategies (referred to as schemes). The scheme with the optimal performance is selected for target solving kernel. In this stage, a lightweight learning model is proposed to reduce preprocessing time.
4.2 Code Variant Preparation
As discussed in Section 3, the code implementations in existing work are not consistently optimal in all cases. In the Prepare stage, we abstract the implementation of SpTRSV as Fig. . An example of the fine-grained dynamic parallelism when nд = 4 and nw = 2. Each warp processes 4 independent row groups and each stream multiprocessor is exclusively assigned a GPU block of 2 executing warps.
a unified code template to better represent the solving behavior. Based on the template, we design and implement a series of solution codes with various parallelism and optimization details, referred to as Code Variants, to enhance performance for different sparsity patterns.
4.2.1 Fine-Grained Dynamic Parallelism.
Through the experiments in Section 3.1, we have demonstrated that exclusively utilizing warp-level or thread-level parallelism cannot achieve optimal performance. Although YYSpTRSV proposes to switch between the two algorithms, the parallel granularity still faces a dilemma (discussed in Section 3.1). To fill the gap between the previous two algorithms, we adopt fine-grained dynamic parallelism to better utilize the GPU resources.
As is shown in Figure , a GPU warp is divided into nд smaller units for parallelism, referred to as subwarps. Each subwarp is assigned a row group, which contains several consecutive rows for solution. The nд groups have no inter-group data dependencies so that each subwarp can process an independent task in parallel in a deadlock-free manner. nд is a configurable parameter and remains fixed for each code variant. Generally, 1 ≤ nд ≤ 32, and nд is set as a power of 2 to achieve better performance of reduction.
Within each row group, the subwarps can choose to either collaboratively process each row one by one or individually process separate rows. The choice is based on the average number of nonzeros per row. We use a configurable parameter sw_rnnz as the threshold, which is also fixed for each code variant. When the average non-zero number of the rows in a row group is larger than sw_rnnz, the threads in the subwarp collaboratively process one row to enhance memory access efficiency with more coalesced memory access. Otherwise, the threads in the subwarp individually process separate rows to improve warp utilization and eliminate reduction overheads. We set the value of sw_rnnz from {0, ∞} ∪ {2 i , i ∈ N & 2 i ≤ 32 nд }. The upper bound 32  nд indicates that each thread in the subwarp processes at least one non-zero on average.
It is worth noting that when nд = 1, the method becomes the warp-level and thread-level implementations. When nд > 1 and each row group consists of only one row, the method 70:10 Z. Hu et al. becomes the multiple-row-per-warp implementation mentioned in . Our method follows a more comprehensive design with fine-grained parallelism, which also encompasses existing algorithms.
Section 3.1 also demonstrates that the best performance is not necessarily achieved under the maximum concurrency. Considering that, AG-SpTRSV dynamically adjusts the hardware-level concurrency. Specifically, we set the block size to the maximum possible number (1024 on Ampere). Each thread block contains nw warps that perform the solution, whereas the remaining warps are empty and do nothing. Then, we enforce a thread block to execute exclusively on a stream processor, thus explicitly controlling the number of executing warps.
4.2.2 Adaptive Code Optimization.
When processing a specific row (assumed to be the i-th row), the code implementation can be represented as follows. Firstly, data of the i-th row of the sparse matrix is read in, including the start and end row pointers as well as the column indices of the nonzeros. Next, the warp waits for dependent components to finish processing. After finishing, the values of those components are read in. Then, the computation phase begins. The warp calculates and writes back the value of component x . Lastly, the necessary synchronization information, such as f laд[i] mentioned in Section 2.2, is written back.
We have shown in Section 3.3 that the existing code implementations suffer from non-coalesced memory access, excessive synchronizations, and long-latency division operations. To eliminate these performance bottlenecks, we designed several optimization methods that can be applied under specific circumstances, as is shown in Figure .
Collaborative read. Since SpTRSV is a memory-bound kernel, its memory access efficiency has a predominant impact on its performance. In the existing implementations, each thread within a single warp independently reads the required sparse matrix data on demand, which may lead to inefficient non-coalesced memory access. We use collaborative reading to address the issue. Specifically, each GPU warp allocates a buffer (specifically a piece of shared memory on GPUs) to store data of the sparse matrix. All threads in the warp first cooperatively read data of each row group in sequence. This enables coalesced memory access and improves the utilization of memory bandwidth. When reading finishes, each subwarp begins processing its own row group. The optimization method is applied when the sparse matrix data required by the rows does not exceed the maximum available buffer size, and each row group has sufficient non-zeros to enable coarse-grained coalesced memory access.
Elimination of synchronization. Fine-grained synchronizations for data dependencies (including waiting for dependent rows and writing synchronization information) also incur non-negligible overheads. We introduce the following rules to eliminate synchronization. When all the rows solved by the current kernel are located within the same level, we can eliminate the logic of waiting as all the dependent rows have been processed beforehand. When no rows depend on the current row or the waiting of the rows that depend on the current row has already been eliminated, we can eliminate the logic of writing back synchronization information. This optimization method can be applied when rows of different levels are independently scheduled, which will be further discussed in Section 4.4. AG-SpTRSV adaptively adjusts the synchronization behavior based on the sparse pattern of the matrix, which balances between the fine-grained synchronization methods and the level-set methods .
Early division. When computing component results, the long latency of division operations potentially impacts performance. Therefore, we adopt the optimization to perform divisions earlier. Before waiting for dependent data, the warp/thread calculates the reciprocal of the diagonal nonzero element of the current row. This allows us to use multiplication with the reciprocal instead of division when calculating the final result. This enables warps/threads that need to wait for dependent data (which cannot start their computation immediately) to perform expensive division operations in advance, thereby enhancing computational parallelism. We find that this modification can bring performance improvement of over 20% on certain matrices.
In Table , we compared the code variant of AG-SpTRSV with existing GPU SpTRSV implementations. While all existing implementations focus on some specific sparsity patterns, AG-SpTRSV enables dynamic parallelism and adaptive optimizations to prepare code variants, which can achieve adaptability to matrices with various sparsity patterns.
4.3 Computation Graph Transformation
With fine-grained dynamic parallelism (discussed in Section 4.2.1), a GPU warp is assigned one or multiple row groups to process. How to partition rows of a sparse matrix into row groups crucially impacts performance, as it is not only related to parallelism and task granularity but also affects the data dependencies in the overall solution.
AG-SpTRSV uses computation graphs to represent task partitioning and data dependencies. In a computation graph, a node (noted as N i ) represents a row group, and two types of edges are used to represent the relationships between nodes. A dependency edge connects N i to N j when there is a 70:12 Z. Hu et al.
non-zero element at the position (r 1 , r 2 ) of the matrix, where r 1 ∈ N j and r 2 ∈ N i . The dependency edge indicates that the computation of N i can only begin after the computation of N j finishes. A data edge connects N i to N j when the last row in N i and the first row in N j are contiguous. The data edge indicates that the two row groups are stored continuously in memory and can be further merged into a larger one. The level of node is defined as the topological order of the node in the computation graph. The computation graph also records the level of each node, as we must assign nodes within the same level to one GPU warp to avoid deadlocks.
Directly parallelizing the original computation graph may lead to unsatisfactory performance, as is shown in Section 3 (as well as existing work ). In the Transform stage, AG-SpTRSV performs two types of transformation operations on the original computation graph: node merging and reordering. A set of new graphs with different topological structures are generated, which we refer to as candidate graphs. Those graphs will be further scheduled and measured to achieve better potential performance.
The merging operation traverses the graph along data edges and merges several nodes into a larger one. AG-SpTRSV leverages the following merging strategies for matrices with different sparsity patterns.
-Merge by fixed block size. The merдe_f ixed strategy merges every rb node along the data edge path starting from the first one. After this, each row group contains rb rows. This strategy is suitable for matrices with irregular distribution of non-zeros as each row group contains a similar number of non-zeros and load balancing is achieved. When rb = 1 and 32, the strategy is equivalent to those in and , respectively. -Merge by average non-zeros per row. The merдe_avд strategy calculates the average number of non-zeros of every rb node along the data edge path. If the average number is no more than thresh, the nodes are merged into a larger one. It is based on the intuition that when the numbers of non-zeros in multiple contiguous rows are small, we should group them to avoid idle threads in subwarps. Otherwise, we keep each row as a row group because one single row can be processed by all the threads in a subwarp collaboratively. When rb = 32, this strategy is equivalent to that in . -Merge by the threshold of the number of non-zeros. Sparse matrices usually form irregular distributions of non-zeros, especially for those associated with power-law graphs . The merдe_thresh strategy is designed for rows with a vast number of non-zeros, which have great impact on load balancing. Specifically, the strategy continually merges nodes along the data edge path, as long as the encountered node has no more than thresh non-zeros and the number of merged nodes does not exceed rb. Each long row with more than thresh non-zeros is then kept as a single row group.
After merging operations, levels of nodes should be recalculated as multiple rows from different levels can be merged into the same node.
The reorder operation renumbers the nodes according to the topological order of the graph. After reordering, rows with the close computation order are put together so that parallelism and locality are ensured simultaneously. Node-merging operations are further performed to enhance the performance. Noting that reordering changes the storage data of the original matrix and brings extra costs for preprocessing, we consider it as an optional choice and evaluate its performance gain separately.
Figure shows the resulting candidate graphs with different graph transformation operations. The original computation graph is shown in Figure . After applying the graph operations described above, AG-SpTRSV generates a set of candidate graphs varying in sizes of nodes and connections of dependency and data edges. Each candidate graph (including the original one) represents one specific way of parallelization and has the potential to achieve high performance. AG-SpTRSV then schedules the graphs onto GPU parallelization units to generate schemes (this will be discussed in Section 4.4).
4.4 Multi-hierarchy Heuristic Scheduling
Existing work adopts a simple scheduling strategy, which launches blocks/threads corresponding to the number of computation tasks. In this way, task scheduling is entirely handled by the hardware. However, the hardware scheduler may not always bring the best performance. There are possibilities that blocks with smaller indices but later computation order (i.e., a lower topological order in the computation graph with dependency edges) need to wait for dependency data, whereas blocks with larger indices but earlier computation order (i.e., a higher topological order) are not given enough priority in scheduling. Moreover, the hardware scheduler has no information about data exchange requirements or computation task workloads, thus cannot achieve deep performance optimization.
In the Schedule stage, AG-SpTRSV dives deeper into scheduling from bottom to top, addressing the following issues across three hierarchies: how nodes in the computation graph are grouped, how grouped nodes are scheduled onto warps, and how grouped nodes are partitioned into different kernels. Based on the three hierarchies, we propose a series of heuristic strategies to enhance the performance.
In the first hierarchy, nodes in the computation graph are grouped into node groups. Each node group contains up to nд nodes, where nд is the number of subwarps. A node group is regarded as a computation task of a warp, and each node in the group corresponds to one subwarp. To avoid deadlocks, nodes in one group must be independent of each other. Therefore, AG-SpTRSV group nodes in the same level based on the topological order of the computation graph. We consider two heuristic strategies for grouping:
-The rд_sequential strategy groups nodes sequentially in the order of their starting row indices. Spatially close nodes are more likely to be grouped together, thus resulting in better data locality. -The rд_balance strategy further sorts the nodes within the same level based on the number of non-zeros and assigns them in decreasing order. Nodes with a similar number of non-zeros are more likely to be assigned to the same warp, thus load balancing can be enhanced. In the second hierarchy, node groups are scheduled onto different warps. Scheduling is done in the topological order of the node groups (based on the levels of computation graph). The scheduling decisions have a significant impact on performance, as the distribution of tasks across different stream multiprocessors (SMs) determines load balancing. Moreover, threads on the same SM share the on-chip L1 cache, which is closely associated with data locality. AG-SpTRSV implements scheduling with an explicit approach. Specifically, it launches a fixed number of thread blocks (no more than the capacity of the whole GPU). Each thread block corresponds to one specific SM. Warps in the thread block are statically assigned a set of node groups. In this way, AG-SpTRSV is able to determine rigidly on which thread block and in what order the computation tasks are scheduled and executed. For better load balancing, parallelism, and data locality, we design and adopt the following three heuristic scheduling strategies:
-The ws_rr strategy schedules node groups onto warps in a round-robin way. It is the simplest way to schedule yet sufficient for matrices with a regular distribution of non-zeros. -The ws_balance strategy aims to improve load balancing. For each node group, the strategy chooses the warp currently with minimum workload. The workload of each node group is approximated as its total number of non-zeros. -The ws_locality strategy aims to improve data locality. The strategy schedules the current node group onto the adjacent warp of its parent. An adjacent warp refers to a warp within the same thread block. The intuition of this strategy is that adjacent warps share an L1 cache, which enables cache-level data sharing. When a node group has more than one parent, the warp with the minimum workload is selected for scheduling. In the third hierarchy, node groups of different levels are partitioned and scheduled in one or more kernels. AG-SpTRSV analyzes the number of node groups of each level in the computation graph. For adjacent levels with a small number of node groups, we merge them into one kernel for execution. For levels with a large number of node groups, we execute each of them in a single kernel so that costs of synchronization can be eliminated, as is discussed in Section 4.2.2. We use a threshold thresh_level to determine that thresh_level = α_level × maximum # of warps, where α_level is a variable parameter. The maximum number of warps is a hardware-dependent parameter, calculated by the maximum number of resident threads on the GPU. If the total number of node groups within the current level is more than thresh_level, it is independently processed in a single kernel so that part of the synchronization overhead can be eliminated. When α_level = 0, each level is processed independently, which is equivalent to the level-set methods . When α_level = ∞, all levels are processed by one kernel, which is equivalent to the fine-grained synchronization methods .
Figure shows examples of how candidate graphs are scheduled. With heuristic scheduling strategies, AG-SpTRSV maps computation tasks to the hardware in a multi-hierarchy manner. By now, AG-SpTRSV generates a series of schemes, evaluates the performance, and searches for the best one.
4.5 Automatic Scheme Selection
Compared with the existing work , AG-SpTRSV considers a much larger optimization space. As is shown in Table , the optimization space consists of parallelism setups (including nw, nд, and sw_rnnz), transformation operations with their parameters, and multi-hierarchy scheduling strategies. We refer to the combination of strategies and parameters as a scheme. Within this optimization space, AG-SpTRSV can generate a series of schemes with the potential to achieve
∈ {2 i , 0 ≤ i ≤ 5}), merдe_avд (thresh ∈ {2 i , 0 ≤ i ≤ 5}, rb = 32), merдe_thresh (thresh ∈ {2 i , 0 ≤ i ≤ 5}, rb = 32) } sched
The heuristic scheduling strategy {rд_sequential, rд_balance}× {ws_rr, ws_balance, ws_locality}× α_level ∈ {0, 2, 4, 8, ∞} high performance. In the last Select stage, AG-SpTRSV executes solving kernels and measures the performance of all the generated schemes. The scheme with the best performance is selected and used for the final target kernel. Given the extremely large optimization space, the exhaustive search brings excessive preprocessing overhead. According to Table , there are over 3.2e5 schemes to be evaluated. In practice, we reduce the search space by empirically eliminating schemes that perform poorly on most matrices. The reduced number of schemes to be evaluated is around 1.6e3. However, the number is still too large to provide an efficient end-to-end solution. The introduced overhead may significantly
The average distance between each row and its nearest dependency row.
impact the end-to-end executing time, making AG-SpTRSV impractical for real-world applications. Thus, we further introduce a lightweight performance model based on historical performance results, which enables quick scheme selection.
The performance model takes matrix information as input. We empirically use 10 features to represent the input matrix, as described in Table . These features can effectively represent the matrix size, parallelism, and data dependencies, which can be closely related to the performance of SpTRSV.
m and nnz indicate the size of the matrix. rnnz, rnnz max, and rnnz cov indicate the distribution of non-zeros among rows. lnnz, lnnz max, lnnz cov, and layer num indicate the distribution of nonzeros among levels. dep dist is a customized metric to describe dependency relationships between different rows, calculated as
0≤k <m dep(k) m , dep(k) = 1 k−Col I dx [Row P tr [i+1]−2]] , if RowPtr [i + 1] − RowPtr [i] > 1, 0, otherwise. (1)
When dep dist is large (close to 1), the parallelism among adjacent rows is limited. For such matrices, using larger block sizes (e.g., larger rb for merдe_f ixed strategy) may increase the number of levels in the computation graph, which leads to reduced parallelism. Thus, using smaller block sizes (smaller rb) typically achieves better performance. When dep dist is small (close to 0), sufficient parallelism can be exploited among adjacent rows. For such matrices, using larger block sizes enables coalesced memory access and exploits better parallelism. We use a three-layer multilayer perception (MLP) as the performance model. The dimension of each hidden layer is 32. The input features are first transformed into degree-2 polynomial features and then normalized with standard deviation. The output data is a 0-1 vector with a dimension equal to the total number of schemes (corresponding to the size of the optimization space). If the performance of a scheme achieves 85% of the best, we consider it a "satisfactory" performance and set the value of the corresponding position in the vector to 1. Otherwise, the value is set to 0. The hidden layers use the ReLU activation, while the output layer uses the sigmoid activation. To find out the best scheme for a specific matrix, we input the matrix information and obtain the prediction result. The scheme corresponding to the position with the highest value in the output vector is selected for the target solving kernel. With the proposed performance model, the preprocessing stage only involves extraction of matrix features, performance model prediction, and selected scheme generation. The costs of exhaustive search can be significantly reduced, whereas the selected scheme can still achieve good performance.
AG-SpTRSV: An Automatic Framework to Optimize Sparse Triangular Solve on GPUs 70:17