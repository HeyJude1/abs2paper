5 Experimental Setup
We evaluate AG-SpTRSV on two NVIDIA GPUs: Tesla A100 and RTX 3080Ti. The platform configurations are listed in Table . The nvcc compiler version is 11.4+. The host program and the preprocessing stages are both performed on Intel Core i9-12900KF (Alder Lake architecture, 76.8 GB/s DDR4 memory) with Ubuntu 18.04 and gcc 7.5. The optimization flag is set as -O3.
We compare the performance of AG-SpTRSV with three state-of-the-art implementations: the Sync-free method , YYSpTRSV , and SpTRSV Solve in the NVIDIA cuSPARSE library . To achieve the best performance of YYSpTRSV, we search for its best parameter to switch algorithms from {0, 1, 2, 4, 8, 16, 32} and use the best one for evaluation. We employ cusparseSb-srsv2_solve in cuSPARSE as the baseline routine and utilize two built-in algorithms: using the level information (USE_LEVEL) and not using the level information (NO_LEVEL).
We first evaluate 20 matrices, which are representative of a wide range of scientific fields. Information regarding the matrices is listed in Table , sorted by lnnz (an approximation of the overall parallelism). We divide the matrix into the following three types. TYPE 1 comprises structured matrices with multiple diagonal-like patterns or banded matrices. This type of matrix has a regular distribution of non-zeros but typically exhibits low parallelism. TYPE 2 comprises matrices derived from graph problems, with very high sparsity and high parallelism. TYPE 3 consists of matrices derived from various problems, with uneven and special sparsity patterns, such as locally dense blocks of non-zeros or rows with an excessively high number of non-zeros. We illustrate in Section 5.2 that the performance of AG-SpTRSV is improved with different optimization spaces. We also evaluate the overall performance with 2,219 large matrices (with more than 10 3 non-zeros) in the SuiteSparse Matrix Collection (also known as the University of Florida Sparse Matrix Collection) and show the overall performance comparison in Section 5.3. We also evaluate the ability of the performance model to reduce the search costs while achieving satisfactory performance in Section 5.4.
Without loss of generality, we reserve the lower-triangular-part non-zero elements of all the matrix and add diagonal elements if necessary. All the components in the right-hand-side vector are randomly generated. All experiments are conducted in double precision.
5.2 Performance Gains with Different Optimization Spaces
This subsection analyzes the performance of AG-SpTRSV with the set of matrices listed in Table . All presented times in this subsection are for execution only and do not account for preprocessing time. We compare several versions of AG-SpTRSV with different optimization spaces, constructed by adjusting the searching range of parameters. We evaluate four optimization spaces as follows: MF indicates using only the merдe_f ixed strategy and scheduling by hardware. This is equivalent to the tiling strategy in existing implementations . NM indicates using other node-merging operations of computation graphs in the Transform stage. Scheduling is still handled by hardware. MHS indicates using multi-hierarchy heuristic scheduling strategies in the Schedule stage. RL indicates using the reorder operation to reorder the computation graph by level. We separately evaluate the performance gain of the reorder operation as it introduces 70:18 Z. Hu et al. The performance comparison across the different optimization spaces on NVIDIA Tesla A100 is shown in Figure (a). For TYPE 1 matrices, using only the merдe_f ixed strategy can achieve sufficiently good performance. This is because matrices of this type have a regular distribution of non-zeros, which is suitable for the fixed tiling size. Reordering brings some performance gains for matrices with higher parallelism. For TYPE 2 matrices, reordering can bring significant performance gains. This is due to the great importance of data locality in matrices derived from graphs. For TYPE 3 matrices, using other merge operations and multi-hierarchy heuristic scheduling can bring good performance gains. This indicates that the designed strategies can effectively handle the irregularity of sparse matrices. Moreover, reordering can still yield significant improvements. Compared with the state-of-the-art YYSpTRSV, enabling the optimization spaces of MF, NM, WHS, and RL can achieve speedups of 1.66x, 2.02x, 2.08x, and 2.38x, respectively.
The performance comparison across the different optimization spaces on NVIDIA RTX 3080 Ti is shown in Figure (b). The performance results exhibit similar trends as those observed on A100. Enabling the four optimization spaces achieves speedups of 1.37x, 1.85x, 1.95x and 2.37x over YYSpTRSV respectively. It is worth noting that in many cases, the different optimization spaces may achieve varying levels of performance gains on the two hardware platforms. The best schemes differ across hardware platforms, which indicates the importance of strategy searching and auto-tuning to achieve performance scalability.
Based on experimental data, AG-SpTRSV can achieve promising performance on almost all matrices. However, Figure also shows slight performance degradation on a few matrices compared with YYSpTRSV, such as matrices 12 and 19. Based on our observation, AG-SpTRSV may not perform that well on some matrices with small dep_dist. We speculate that such matrices have simpler data dependencies and unrestricted parallelism. The graph transformation and heuristic scheduling strategies of AG-SpTRSV may introduce unnecessary overhead.
5.3 Overall Performance Comparison
This subsection evaluates the performance of AG-SpTRSV with matrices in the SuiteSparse Matrix Collection. All presented times here are for execution only and do not account for preprocessing time. To avoid interference from extremely small matrices during the evaluation, we filter out matrices with less than 10 3 non-zeros. Except for a small number of matrices that cannot be evaluated due to insufficient GPU memory, we use 2,219 matrices in total and sort them based on their number of non-zeros.
Performance results with SuiteSparse on NVIDIA Tesla A100 are shown in Figure . The geometric average speedups of AG-SpTRSV over Sync-free, cuSPARSE and YYSpTRSV are 3.81x, 3.99x, and 2.14x, respectively. The arithmetic average speedups over the three baselines are 10.56x, 7.35x, and 2.59x. Out of 2,219 test cases, AG-SpTRSV achieves better performance than the three baselines in 96.8% of the cases. The figure also shows that when the number of non-zeros of the matrix increases, the speedups of AG-SpTRSV become more evident and stable, which indicates the advantages of AG-SpTRSV on larger matrices. For large matrices (with more than 10 6 non-zeros), AG-SpTRSV can achieve the geometric speedups of 7.97x over Sync-free, 4.47x over cuSPARSE, and 2.77x over YYSpTRSV.
Performance results with SuiteSparse on NVIDIA RTX 3080 Ti are shown in Figure (b). The geometric average speedups of AG-SpTRSV over Sync-free, cuSPARSE, and YYSpTRSV are 2.98x, 3.57x, and 2.12x, and the arithmetic average speedups are 8.25x, 5.36x, and 2.59x, respectively. that of cuSPARSE and YYSpTRSV (by 2.44x and 43.28x on average), but the execution performance of AG-SpTRSV is higher (with speedups of 10.69x and 2.09x on average). The REC achieves better performance in execution time (1.39x over AG-SpTRSV on average). However, its preprocessing overhead is too high for real-world applications. On average, the preprocessing time of REC is over 10 5 x of the execution time. The number of iterations of solvers in practice rarely achieves this order of magnitude.
Our experimental results also provide suggestions for the selection of SpTRSV implementations: For direct solvers or iterative solvers with few iterations, YYSpTRSV performs better due to its lightweight preprocessing stage. For cases with a significant number of iterations and unchanged distribution of non-zeros, the REC may achieve better execution performance. For typical iterative solvers (with the number of iterations ranging 10 2 ∼ 10 5 ), AG-SpTRSV is a better choice as it offers input-adaptive optimization at a relatively low preprocessing cost.
Furthermore, we find that for TYPE 1 and TYPE 2 matrices, performance models can predict the schemes with performance close to the optimal one (within a margin of error of 5.5%). For TYPE 3 matrices, the margin of error becomes larger (up to 32.8%), as the irregular distribution of non-zeros affects the accuracy of prediction results.
We also evaluate the time consumed by each preprocessing stage of AG-SpTRSV using PMs. The results are shown in Figure . The time consumed by the Select stage remains basically unchanged, as the overhead of the performance model prediction is fixed. On most large matrices (e.g., with more than 10 7 non-zeros), the Transform stage and Schedule stage consume the majority of the time. The time consumption of the above two stages is closely related to the matrix sizes (the number of nodes and the number of non-zeros). The relationship is not strictly linear, as different non-zero distributions and different strategy selections lead to variation in time consumption.