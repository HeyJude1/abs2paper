3 Motivation
To better characterize the performance of existing SpTRSV implementations, we conduct a series of experiments with a set of commonly used sparse matrices with different sparsity patterns on NVIDIA RTX 3080Ti. Detailed matrix characteristics are listed in Table . The interpretation of those characteristics can be found in Table . Based on the experimental results, we can derive several observations that help with performance optimization.
3.1 Dilemma of Parallelism Granularity
Based on the different parallel levels of GPUs, existing work mainly adopts two parallelism granularities, including warp level and thread level . Many employ the fixed parallelism granularity setup, which cannot achieve good performance across various matrices. YYSpTRSV determines the parallelism granularity based on the number of non-zeros per row. However, such approaches can be suboptimal. We evaluate four structured matrices with similar average non-zeros per row as an example. As is shown in Figure (a), atmosmodd and cbi–¥ performs better with the warp-level algorithm, whereas delaunay_n23 and FullChip are better suited for the thread-level algorithm.
We also find that some cases cannot be fully optimized with either warp-level or thread-level parallelism. Taking atmosmodd as an example, though the warp-level algorithm performs better than the thread-level algorithm, the average non-zero per row of the matrix is only 3.97. This results in nearly 75% of threads in the warp remaining idle, leading to very low utilization.  Moreover, the parallelism of the matrix affects the optimal concurrency. Figure shows how the performance varies when different sizes of thread-block are used. atmosmodd has very high parallelism in each level (though limited among consecutive rows); thus, the performance improves as the concurrency increases. By contrast, chipcool0 and cant are band matrices with low parallelism. When the concurrency is high, most threads are idle waiting for data dependencies to be resolved, which leads to unnecessary resource contention and thus brings performance degradation. We argue that a more flexible and comprehensive parallelism setup is required.
3.2 Challenges of Irregularity
Real-world matrices in real-world applications often exhibit irregular distribution of non-zeros. This introduces more challenges to performance optimization due to the mismatch between irregular computation patterns and the GPU hardware design. Our experiments show that the issues of load balancing and data locality brought by irregularity need to be carefully handled.
Figure (a) illustrates the impact of load balancing on performance. wiki -Talk, arabic -2005 and lp1 are matrices with uneven distribution of non-zero elements among the rows, which easily leads to runtime load imbalance. Here, we compare the performance between scheduling with and without the load-balancing strategy (discussed in Section 4.4), which brings speedups of up to 20%.
We also evaluate the impact of data locality by comparing the performance with and without matrix reordering (discussed in Section 4.3). Matrix reordering can efficiently improve data locality 70:8 Z. Hu et al.
by placing tasks with a similar computational order together. Figure (b) shows that reordering can bring significant speedups for matrices with high parallelism.
3.3 Suboptimal Code Implementation
The code implementation of the existing algorithms (shown in Algorithm 2) can be further refined. We observe at least the following three performance bottlenecks through profiling.
-Memory access to sparse matrix and components, especially in the thread-level algorithm, exhibit non-aligned and non-coalesced patterns. This leads to extra memory transactions, which greatly impacts memory access efficiency. -Fine-grained synchronizations, such as reading/writing flags and performing memory fences, introduce non-negligible costs. In fact, some of them are unnecessary. -Division operations are performed to calculate the value of the components. On GPUs, division instruction has high latency, which easily impacts the instruction pipeline.
3.4 Non-orthogonal Optimization Space
In the previous observations, we identify several factors that affect the performance of SpTRSV and summarize their intuitive relationships with the sparsity patterns of matrices. We also find that their effects on performance are not independent. For example, when the granularity of the task changes, the optimal parallelism setup and scheduling strategy may vary. According to our experiments, simply combining the respectively optimal choices of each factor cannot achieve a globally optimal performance and, if anything, falls far short of being satisfactory. All those factors collectively form a vast space, which makes manual optimization extremely tedious and challenging.