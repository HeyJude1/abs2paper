4 Code Generation
This section details the process of code generation from the constraint system solution. We describe how to use this solution to generate concrete index notation , an IR used by the TACO compiler. This IR is then used by TACO to generate the final C code implementation for the contraction tree.
4.1 Concrete Index Notation
As discussed in Section 2 , TACO is a state-of-the-art code generator for sparse tensor computations. While TACO does not address the questions that our work investigates (choice of linear ordering of tensor contractions from a binary contraction tree, selection of fusion structures, and tensor layouts), it does provide code generation functionality for efficient implementations of CSF tensor representations and iteration space traversals. We use concrete index notation , the TACO IR that captures a computation over sparse tensors through a set of computation constructs. The two constructs relevant to our work are forall and where . A forall construct denotes an iteration over some index. A where(C,P) construct denotes a producer-consumer relationship. Here C represents a computation that consumes a tensor being produced by computation P . This construct allows the use of dense workspaces ; as discussed in Section 2 , this is an important optimization in TACO. As an illustration, the concrete index notation we generate from the constraint solution for the running example has the following form:
) = Y(k, i) * D(r, j, k))),
where(forall(q, forall(k, forall(i, Y(k, i) = X(q, i) * C(r, q, k)))), forall(p, forall(q, forall(i, X(q, i) = A(p, q, i) * B(r, j, p))))))))
4.2 Generating Concrete Index Notation
The constraint solver's output can be abstracted as a sequence of pairs A, π , where A is an assignment for a binary contraction and π is a permutation of the indices appearing in the assignment. The permutation is defined by the values of constraint variables lp i,k described earlier and denotes the order of surrounding loops for A. The indices in a reference to a tensor T in A are ordered based on the values of variables dp T , j ; thus, they are consistent with the order of indices in π . The order in the sequence of pairs is defined by the values of variables ap i and represents a topological sort order of the contraction tree. For the example discussed in the previous section, the sequence is
CoNST: Code Generator for Sparse Tensor Networks 82:13
< X[r,j,q,i] = A[p,q,i] * B[r,j,p], [r,j,p,q,i] > < Y[r,j,k,i] = X[r,j,q,i] * C[r,q,k], [r,j,q,k,i] > < R[j,k,i] = Y[r,j,k,i] * D[r,j,k],
[r,j,k,i] > Algorithm 1 describes the creation of the TACO IR from such an input. Function generate is initially invoked with the entire sequence of pairs A, π based on the constraint system's solution. At each level of recursion, the function processes a sequence S of such pairs. There are two stages of processing. In the first stage (lines 3-12), a sequence L of assignments and indices is constructed. One can think of the elements of L as representing eventual assignments and loops that will be introduced in the TACO IR. For example, an index i in L will eventually lead to the creation of forall(i,...) . Similarly, an assignment in L will produce an equivalent assignment in the TACO IR. During this first stage, for each element A, π of S, in order, we need to decide whether the loop structure encoded by π can be fused with the loop structure of the previous element of S, at this level of loop nesting. For example, the sequence shown above contains permutation [r,j,p,q,i] in the first pair of S, followed by [r,j,q,k,i] in the second pair. The processing of the first pair will add index r to L. In the processing of the second pair, the outermost index r matches the current last element of L, and thus r is a common loop for both assignments. The processing of the third pair considers permutation [r,j,k,i] , whose outermost index again matches the last element of L. Thus, at the end of the stage, L contains one element: the index r . In a more general case, a combination of indices and assignments could be added to L. For example, if the input sequence is < A0,[i] > , < A1,[] > , L contains two elements-i followed by A1 -which eventually leads to the creation of where(A1,forall(i,A0)) , as described shortly.
As part of this process, for each index in L the algorithm records the sub-sequence of relevant pairs from S. This information is stored in map M, with keys being the indices that are recorded in L. For the running example, r is mapped in M to the sequence of all three input pairs. This list of pairs is then used in the second stage of processing to generate a construct of the form forall(r,...) .
The second stage (lines 13-18) considers three cases. If L contains a single assignment, this assignment simply becomes the result of IR generation (line 14). If L contains a single index i, this index can be used to create a forall(i,...) construct that surrounds all pairs recorded in M. get (i). This creation is shown at line 16. The pairs in M. get (i) are first processed by a helper function remove and then used to recursively generate the body of the forall . The helper function, which is not shown in the algorithm, plays two roles. Both are illustrated by the modified pairs below, which are obtained by calling remove(r, M. get (r )) :
< X[j,q,i] = A[p,q,i] * B[r,j,p], [j,p,q,i] > < Y[j,k,i] = X[j,q,i] * C[r,q,k], [j,q,k,i] > < R[j,k,i] = Y[j,k,i] * D[r,j,k], [j,k,i] >
First, remove eliminates r from the start of all permutations π . This reflects the fact that a forall(r,...) is created at line 16. Second, the function removes r from all intermediate tensor references for which both the producer and the consumer are in M. get (r ). For example, X[r,j,q,i] appears in the first pair (the producer) and in the second pair (the consumer). Both are surrounded by the common loop r , which means that X can be reduced from order-4 to order-3, and thus the reference is rewritten as
X[j,q,i] . A similar change is applied to Y[r,j,k,i] .
At the next level of recursion, this sequence becomes the input to generate . During that processing, L contains only index j and remove(j, M. get (j )) is called to obtain the modified sequence:
< X[q,i] = A[p,q,i] * B[r,j,p], [p,q,i] > < Y[k,i] = X[q,i] * C[r,q,k], [q,k,i] > < R[j,k,i] = Y[k,i] * D[r,j,k], [k,i] >
Then generate is called on this sequence. At that level of recursion, L contains three indices: p , q , and k . This illustrates the third case in the processing of L. Line 18 shows the creation of a where construct for this case. Since k is the last element of L, the first operand of where is the IR generated for the sub-sequence corresponding to k , which here contains a single pair
< R[j,k,i] = Y[k,i] * D[r,j,k], [k,i] > .
Recall that this first operand of where corresponds to a consumer of a tensor-in this case, tensor Y . The producer of Y appears in the second operand of where , which is generated from the first two pairs from the original sequence:
< X[q,i] = A[p,q,i] * B[r,j,p], [p,q,i] > < Y[k,i] = X[q,i] * C[r,q,k], [q,k,i] >
At line 18, S. truncate denotes an operation to produce this desired prefix of S by excluding the sub-sequence defined by M. get (L. last ()). The IR generated from this prefix itself contains a nested where construct, which captures a producer-consumer computation for X . At the end of processing, the resulting overall structure has the form forall(r, forall(j, where(forall(k, forall(i, A2)), where(forall(q, forall(k, forall(i, A1))), forall(p, forall(q, forall(i, A0))))))) Benchmarks. We evaluate the performance of CoNST-generated code on several sparse tensor networks. Section 5.1 presents a case study of sparse tensor computations arising from recent developments with linear-scaling methods in quantum chemistry . Three tensor networks are used: 3-index integral unrestricted, 3-index integral restricted, and 4-index integral; details on these networks are provided in Section 5.1 . Section 5.2 evaluates performance on the Matricized Tensor Times Khatri-Rao Product (MTTKRP) computation . Section 5.3 presents performance on the Tensor Times Matrix chain (TTMc) expression that is the performance bottleneck for the Tucker decomposition algorithm . Constraint systems. For each of these benchmarks, Table provides details on the Z3 constraint system that was solved to generate the code for our performance evaluations. Column "DimBound" shows the upper bound l on the dimensionality of intermediate tensors; recall from Section 3 that this parameter l is used when generating the constraints. Column "ConsVars" shows the number of constraint variables, while column "Constraints" shows the number of constraints. The last column "SolverTime" shows the execution time of Z3. As can be seen from these measurements, the systems are relatively small and their solutions can be computed very quickly. The follow-up steps of generating the TACO IR and then generating executable code with TACO are also quick, and together take about 0.1 second.
Recall that fusion allows for reduction in the dimensionality (and thus memory usage) of intermediate tensors. As shown in column "DimBound" in Table , the solver can be used to identify fusion structures with low-dimensional intermediates. Comparing with the memory usage in an unfused version (configuration TACO-Unfused, described shortly), we observed that without fusion the memory usage for intermediates in our benchmarks is typically a few megabytes, while the CoNST-generated fusion reduces this memory usage to a few kilobytes.
Performance evaluation. All experiments were conducted on an AMD Ryzen Threadripper 3990X 64-core processor with 128 GB RAM. Optimization flags -O3 -fast-math were used to compile the C code, with the GCC 9.4 compiler. Reported performance results are for single-thread execution. Effective parallelization of the code is a topic for future work. A key challenge is that of achieving effective load balancing across threads because of the significant variance in the work for different iterations of outer-most parallel loops, due to highly variable index-dependent sparsity of inner nested loops. In all experiments and for all evaluated tools, the input tensors are in COO format on disk. The time to read the tensors from disk and to represent them in the CSF formats required by the tools is not included in the measurements.
We compare CoNST against three state-of-the-art sparse tensor compilers and libraries:
82:16 S. Raje et al.
TACO: As discussed in detail earlier, CoNST uses TACO for generation of C code after cooptimization for tensor layout choice, schedule for the contractions, loop fusion, and mode reduction of intermediate tensors. We compare the performance of CoNST-generated code with that achieved by direct use of TACO. This was done in two ways: (1) direct N -ary contraction code was generated by TACO, where a single multi-term tensor product expression was provided as input with the same mode order for tensors produced by CoNST's constraint solver (described in Section 3 ), and (2) TACO was used to generate code for an unfused sequence of binary contractions, in which case results are reported for the best-performing mode order for tensors.
SparseLNR: SparseLNR takes a multi-term tensor product expression and generates fused code for it by transforming it internally to a sequence of binary contractions. We evaluated its performance by providing the same multi-term tensor expression used for comparison with TACO.
Sparta: We used Sparta to compute the sequence of binary tensor contractions produced by CoNST. However, Sparta's kernel implementation internally requires that the contraction index be at the inner-most mode for one input tensor and at the outer-most mode for the other input tensor. If the provided input tensors do not satisfy this condition, explicit tensor transposition is performed by Sparta before performing the sparse tensor contraction. Since the tensor layout generated by CoNST might not conform to Sparta's constraints, we instead performed an exhaustive study that evaluated all combinations of distinct tensor layout orders that would not need additional transpositions for Sparta. We report the lowest execution time among all evaluated configurations.