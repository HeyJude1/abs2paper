2 Background and Overview 2.1 Tensor Networks
We first describe the abstract specification of a tensor network. Such a specification can be lowered to many possible code implementations. Examples of such implementations are also given below. Sparse tensors. A tensor T of order n is defined by a sequence d 0 , . . . , d n−1 of modes . Each mode d k denotes a set of index values:
d k = {x ∈ N : 0 ≤ x < N k },
where N k is the mode extent . Note that the numbering of modes from 0 to n − 1 is purely for notational purposes and does not imply any particular concrete data layout representation; deciding on such a layout is one of the goals of our work, as described later. For a sparse tensor T , its non-zero structure is defined by some subset nz (T ) of the Cartesian product
d 0 × d 2 × • • • × d n−1 .
All and only non-zero elements of T have coordinates that are in nz (T ). Each (x 0 , x 1 , . . . ) ∈ nz (T ) is associated with a non-zero value T (x 0 , x 1 , . . . ) ∈ R .
The tensor expressions described below use tensor references . A reference to an order-n tensor T is defined by a sequence i 0 , . . . , i n−1 of distinct iteration indices ("indices" for short). Such a reference will be denoted by T i 0 i 1 . . . . Each index i k is mapped to the corresponding mode d k of T and denotes the values defined by that mode: i k = {x ∈ N : 0 ≤ x < N k }. The same index may appear in several tensor references, for the same tensor or for different ones. In all such occurrences, the index denotes the same set of index values. For example, an expression discussed shortly contains tensor references X i jqr , A ipq , and B jpr . As an illustration, index j appears in two of these references and is mapped to mode 1 of X and mode 0 of B (and thus both modes have the same extent).
CSF representation. Our work focuses on sparse tensors represented in the widely used CSF format . CSF organizes a sparse tensor as a tree, defined by some permutation of modes d 0 , . . . , d n−1 . This order of modes defines the CSF layout and must be decided when creating a concrete implementation of the computation. The internal nodes of the tree store the indices of non-zero elements in the corresponding mode. The leaves of the tree store the non-zero values. An auxiliary root node connects the entire structure. Using the sparse format abstractions introduced by Chou et al. , the outermost mode is dense (i.e., all index values are represented), while the remaining ones are compressed (i.e., only index values with corresponding non-zero elements are represented). Figure illustrates the CSF representation for an order-4 sparse tensor. When the abstract specification of a tensor expression (or equivalently, of a tensor network) is lowered to a concrete implementation, both tensors and tensor references are instantiated to specific representations. For example, suppose we have a tensor A with modes d 0 , d 1 , and d 2 , and a reference A ipq appears CoNST: Code Generator for Sparse Tensor Networks 82:5  in the tensor network. One (of many) possible implementation is to order the modes as d 1 , d 2 , d 0 in outer-to-inner CSF order. The code references to the tensor would be consistent with this order; i.e., reference A ipq becomes A[p,q,i] in the code implementation.
Tensor contractions and tensor networks. Consider tensors T , S, and R and a binary contraction R i 0 i 1 . . . = T j 0 j 1 . . . × S k 0 k 1 . . . . Let In T , In S , and In R denote the sets of indices appearing in each tensor reference, respectively. Any index i ∈ In R is an external index for this contraction. Any index i ∈ (In T ∪ In S ) \ In R is a contraction index for the contraction.
The non-zero structure of R is defined by the non-zero structure of T and S as follows: (z 0 , z 1 , . . . ) ∈ nz (R) if and only if there exists at least one pair of tuples (x 0 , x 1 , . . . ) ∈ nz (T ) and (y 0 , y 1 , . . . ) ∈ nz (S) such that for each index i ∈ In T ∪ In S ∪ In R , the values corresponding to i in the three tuples (if present) are the same. For any (z 0 , z 1 , . . . ) ∈ nz (R), the associated value R(z 0 , z 1 , . . . ) ∈ R is the sum of T (x 0 , x 1 , . . . ) × S(y 0 , y 1 , . . . ) for all such "matching" pairs of tuples (x 0 , x 1 , . . . ) ∈ nz (T ) and (y 0 , y 1 , . . . ) ∈ nz (S). As a simple example, R i j = T ik × S k j represents a standard matrix multiplication: for any
(a, b) ∈ nz (R) we have R(a, b) = { c: (a , c)∈ nz (T )∧(c, b)∈ nz (S )} T (a, c) × S(
c, b).
A general (non-binary) contraction expression of the form R . . . = T1 . . . × • • • × Tn . . . is defined similarly. Such an expression can be equivalently represented as a tensor network , with one vertex for each tensor reference in the expression, and a hyper-edge for every index. An example of a tensor network representing the tensor expression R i jk = A ipq × B jpr × C kqr × D jkr is shown in Figure (a). Here dashed hyperedges are used to distinguish the contraction indices in the tensor expression (i.e., i, j, and k) from the external indices.
The direct computation of any tensor network (multi-tensor product expression) can be performed via a nested loop, with one loop corresponding to each index, and a single statement that mirrors the tensor expression.  N is the number of operand tensors. Here and for later examples that apply in both the dense and sparse context, we often do not explicitly indicate loop bounds because the form will differ for the dense and sparse case. Note that the figure shows a specific code version with a concrete loop order (e.g., i in the outermost position) and tensor data layouts (e.g., j is the outermost CSF level of D). There are many possible choices for the loop order and the tensor layout. While the loops are straightforward in the dense case, for sparse CSF tensors the code is much more complex, and general techniques for such iteration have been developed .
For the dense case, the complexity of such an implementation is
O(M i M j M k M p M q M r )
, where M x are the corresponding extents. By exploiting associativity and distributivity, the multi-term product can be rewritten as a sequence of binary contractions, with temporary intermediate tensors X and Y as shown in Figure . By using a sequence of binary contractions instead of an N -ary contraction, the complexity is reduced to There exist many different sequences of binary tensor contractions to compute a tensor network, with varying computational complexity. The problem of identifying an operation-optimal sequence of binary contractions for a multi-term product expression is NP-complete , but practically effective solutions have been developed for this problem . We assume that one of these solutions has been applied to produce a binary contraction tree and consider the orthogonal problem of generating efficient code to implement that contraction tree.
O(M i M j M p M q M r + M i M j M k M q M r + M i M j M k M r ).
2.2 Challenges and Overview of Solution
The problem we address in this article is the following: Given a binary tensor contraction tree for a sparse tensor network, generate efficient code for its evaluation. In contrast to the dense case, with sparse tensor contractions a fundamental challenge is that of insertion of each additive contribution from the product of a pair of elements of the input tensors to the appropriate element of a sparse output tensor. The TACO compiler defines a workspaces optimization to address this challenge, where a dense multidimensional temporary array is used to assemble multidimensional slices of the output tensor during the contraction of sparse input tensors. By using a dense "workspace," very efficient O(1 ) cost access to arbitrary elements in the slice is achieved for assembling the irregularly scattered contributions generated during the contraction. A significant consideration with the use of the dense workspaces is the space required: the extents of the workspace array must equal the extents of the corresponding modes of the sparse output tensor and thus can become excessive. By use of loop fusion between producer and consumer contractions to reduce the number of explicitly represented modes in intermediate tensors, we represent all intermediates as dense workspaces and thus make efficient use of TACO's workspaces optimization.
Loop fusion and dimension reduction of intermediates.
In addition to fusion, a critical factor for high performance is the compatibility between loop order and layout order. For sparse CSF tensors, efficient access to the non-zero elements is only feasible if the outer-to-inner order of nested loop indices in the code implementation is consistent with the layout order of tensor modes, in relation to the loop indices that index them. For example, the elements referenced by A[i,p,q] can be accessed efficiently only if i appears earlier than p (which itself appears earlier than q) in the loops surrounding this reference.
To summarize, given a binary contraction tree to implement a general sparse tensor contraction expression, three critical inter-related decisions affect the performance of the generated code:
-Linear execution order of contractions: The fusibility of loops between a producer contraction of an intermediate tensor and a subsequent consumer contraction is affected by the linear execution order of the contractions. -Loop permutation order for each contraction: All surrounding loops of a contraction are fully permutable. The chosen permutation affects both the fusibility of loops across tensor contractions and the efficiency of access of non-zero elements of sparse tensors. -Mode layout order for each tensor: The compatibility of the layout order of each tensor with the loop order of the surrounding loops is essential for efficient access.
These three decisions are inter-dependent. The linear execution order (i.e., the topological sort of the contraction tree) affects which loop fusion structures are possible. The order of loops for each contraction determines what fusion can be achieved, while also imposing constraints on the data layouts of tensors that appear in the contraction tree. In this article, we propose a novel integrated solution that considers these three decisions in a single formulation. Our approach creates a constraint system that encodes the space of possible decisions and their interdependence. This system is then solved using the Z3 SMT solver . The solution is used to create a legal fused loop structure that reduces the size of intermediate tensors while ensuring the compatibility constraints described above.
✓ ✗ ✓ Data layout selection ✗ ✗ ✗ ✓ Schedule for contraction trees ✗ ✗ ✗ ✓
To the best of our knowledge, this is the first work that takes such an integrated view and provides a general approach for code generation for arbitrary tensor contraction trees. Table contrasts our work with the three most closely related state-of-the-art systems for sparse tensor computations described below. Our experimental evaluation presents comparisons with all three existing systems. Section 6 provides further details on these and other related efforts.
The CoNST system leverages, as its last stage, the code generator for sparse tensor computations in TACO . The main focus of TACO is the generation of efficient code for N -ary contractions with arbitrarily complex tensor expressions. While TACO can be used to generate code for a sequence of binary sparse tensor contractions, it does not address optimizations like loop fusion across tensor contractions, tensor mode layout choice, or the choice of sequence of tensor contractions for a given contraction tree. In our experimental evaluation (Section 5 ), we show that code generated by CoNST achieves significant speedup over code directly generated by TACO.
SparseLNR builds on TACO to implement loop fusion optimization. It takes a multi-term tensor product expression as input and generates fused loop code for a sequence of binary tensor contractions corresponding to the input tensor product expression. In our experimental evaluation, we compare the performance of code generated by SparseLNR with code generated by CoNST and demonstrate significant speedups.
Sparta implements a library for efficient tensor contraction of arbitrary pairs of sparse tensors. Since a library is being created, this work does not address any optimizations like loop fusion across contractions, data layout choice for tensors, or the schedule of contractions for a contraction tree. We performed extensive experimentation to compare the performance of code generated by CoNST with the best performance among all valid tensor layout permutations for unfused sequences of contractions executed using Sparta. These experiments demonstrate very significant performance gains for CoNST.