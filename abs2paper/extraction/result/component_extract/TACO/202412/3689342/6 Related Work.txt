6 Related Work
A comparison between CoNST and the three most related prior efforts was presented in Section 2 and summarized in Table . As shown in the previous section, significant performance improvements can be achieved by the code generated through CoNST's integrated treatment of contraction/loop/mode order for fused execution of general contraction trees, compared to (1) code directly generated by TACO , (2) fused loop code generated by SparseLNR , and (3) calls to the Sparta library for sparse tensor contractions. A variety of sparse formats are possible in TACO through format abstractions ; our work focuses on the popular CSF representation. We note that very recent developments have advanced the TACO compiler to allow some forms of sparse workspaces for intermediate tensors; our work was completed before this feature was available and only uses dense workspaces previously supported by TACO.
A number of efforts have addressed loop-level optimization of dense tensor contractions for CP Us and GP Us . However, none of them can be directly adapted for optimizing sparse tensor contractions because of the need to use a compact data representation for sparse tensors. Cociorva et al. addressed loop fusion in the context of optimizing dense tensor networks. Similar to CoNST, the reduction of the order of intermediate tensors is addressed. However, the main focus of that work was to identify more aggressively fused configurations that further reduced the space for intermediate tensors at the price of redundant computation. CoNST does not introduce redundant operations in its optimization and does not incorporate any such space-time tradeoff considerations.
Sparse fusion is an inspector-executor strategy for iteration composition for fused execution of two sparse kernels. This approach optimizes kernels with loop-carried dependencies using runtime techniques. Tensor mode layout and its interactions with iteration order and mode reduction of intermediate sparse tensors are not considered by this existing work. In contrast, our work considers these factors in compile-time code generation for a general contraction tree.
The sparse polyhedral framework defines inspector-executor techniques for optimization of sparse computations, e.g., through runtime iteration/data reordering. It has been applied to individual tensor contractions where iteration code is derived using polyhedral scanning. This approach does not consider fusion or reordering of loops/modes. In contrast to inspector-executor approaches, which often involve non-trivial "inspector" overhead in analyzing the sparsity pattern for each execution instance, CoNST uses a purely static compile-time approach. It thus avoids such overhead and the generated code can be used for different input tensors without any instancespecific runtime analysis.
SparseTIR is an approach to represent sparse tensors in composable formats and to enable program transformations in a composable manner. The sparse compilation support in the MLIR infrastructure enables integration of sparse tensors and computations with other elements of MLIR, as well as TACO-like code generation. SpTTN-Cyclops is an extension of the Cyclops Tensor Framework (CTF) to optimize a sub-class of sparse tensor networks. In contrast to CoNST, which can handle arbitrary sparse tensor networks, SpTTN-Cyclops only targets a product of a single sparse tensor with a network of several dense tensors. Indexed Streams develops a formal operational model and intermediate representation for fused execution of tensor contractions, using both sparse tensor algebra and relational algebra, along with a compiler to generate code. Tian et al. introduce a DSL to support dense and sparse tensor algebra algorithms and sparse tensor storage formats in the COMET compiler , which generates code for a given tensor expression. Zhou et al. propose a set of techniques to optimize tensor networks including use of loop fusion. Using manual implementation of the proposed techniques, performance improvement is demonstrated over code generated by TACO on a set of benchmarks. Finch is a recently developed framework that supports efficient code generation for computations on sparse matrices/tensors that exhibit structured sparsity. None of these efforts address the coupled automated optimization of tensor layout, contraction schedule, and mode reduction for intermediates in fused code being performed by CoNST.