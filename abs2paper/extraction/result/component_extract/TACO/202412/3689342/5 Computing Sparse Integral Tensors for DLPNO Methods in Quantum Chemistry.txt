5 Computing Sparse Integral Tensors for DLPNO Methods in Quantum Chemistry
Recent developments in predictive-quality quantum chemistry have sought to reduce their computational complexity from a high-order polynomial in the number of electrons N (e.g., O(N 7 ) and higher for predictive-quality methods like coupled-cluster ) to linear in N , by exploiting various types of sparsity of electronic wave functions and the relevant quantum mechanical operators .
The few efficient practical realizations of Domain-based Local Pair Natural Orbital (DLPNO) and other similar methods, e.g., the Orca package , have developed custom implementations of sparse tensor algebra, without any utilization of generic infrastructure for sparse tensor computations. Below we present a case study that demonstrates the potential for using CoNST to automatically generate code that can address the kinds of sparsity constraints that arise in the implementation of DLPNO and similar sparse formulations in quantum chemistry.
A key step in the DLPNO methods is the evaluation of matrix elements (integrals) of the electron repulsion operator that was first formulated in a linear-scaling fashion by Pinski et al. . The first stage of the DLPNO integral evaluation involves a multi-term tensor product of three sparse tensors; Figure (b) shows a sparse tensor network corresponding to the expression:
E Ki ˜ μ = I K μν × C μi × ˜ P ν ˜ μ .
The indices of the tensors correspond to four pertinent spaces, ordered from least to most numerous: (1) localized molecular orbitals (indexed in the code by i), (2) atomic orbitals (indexed by μ and ν ), (3) projected atomic orbitals (indexed by ˜ μ), and (4) density fitting atomic orbitals (indexed by K).
The sparse structure of tensors as well as the ranges of loops in the code are governed by various sparsity relationships or sparse maps between pairs of index spaces, as illustrated in Figure (a) (reproduced from Pinski et al. ). This enables a reduction of the number of executed operations, and only a subset of all elements of this tensor network are evaluated. Figure (c) shows a four-term  sparse tensor network where an additional 0/1 sparse matrix L Ki has been added to the base tensor network in Figure (b), corresponding to the known sparse map L(K → i). This can equivalently be expressed as a multi-term tensor product expression:
E Ki ˜ μ = I K μν × C μi × ˜ P ν ˜ μ × L Ki .
The inclusion of such sparse maps as additional nodes in the base tensor network has the same beneficial effect of reducing computations as the manually implemented restriction in Orca . In our experimental evaluation, we evaluate both forms of the sparse tensor networks in Figure , representing the unrestricted form (Figure ) and the restricted form (Figure ).
We computed the DLPNO integrals for 2-D solid helium lattices with the geometry described in . The "small" input used a 5 × 5 lattice (25 atoms) and "medium"/"large" inputs used a 10 × 10 lattice (100 atoms). The following orbital and density-fitting basis sets were used: 6-311G and the spherical subset of def2-QZVPPD-RIFIT , respectively, for the "small" and "medium" inputs, and cc-pVDZ and cc-pVDZ-RIFIT for the "large" input. All quantum chemistry data was prepared using the Massively Parallel Quantum Chemistry package .
Figure (a) presents measurements for the transformed 3-index integral E Ki ˜ μ in unrestricted form (Figure (b)). CoNST-generated code is about two orders of magnitude faster than the N -ary code generated by TACO as well as SparseLNR (for this case SparseLNR was unable to perform loop fusion and simply lowered the input to TACO). TACO-Unfused is much faster than N -ary but is still about five to six times slower than the code generated by CoNST. The best of the comprehensively evaluated versions for Sparta is about an order of magnitude slower than CoNST's code. We note that a direct comparison with the domain-specific implementation in Orca is very challenging because its implementation of DLPNO-CC fuses tensor contraction with other computation. For example, the 3-index MO integral evaluation fuses contraction with the evaluation of AO integrals, and the 4-index integral evaluation in ORCA uses pre-computed 3-index integrals stored on disk.
The performance data for evaluation of E Ki ˜ μ using the restricted form (Figure (c)) is presented in Figure (b). Significant speedups can be seen between the execution times in Figure (a) and 7 (b) (the Y-axis scales are different) by use of the additional tensor L Ki for CoNST, SparseLNR, and TACO N -ary, with the speedup with use of CoNST being roughly the same. However, TACO-Unfused does not improve as much, causing its slowdown with respect to CoNST to get worse. No data for Sparta is presented in Figure (b) because of a constraint of Sparta that a tensor product must have a contraction index, which is not the case for the tensor product with L Ki . A subsequent step after formation of the 3-centered integrals is to use them to construct 4-index integrals (Equation ( ) in Reference 33 ):
V i j ˜ μ ˜ ν = E Ki ˜ μ × E K j ˜
ν , using the 3-index input tensor E obtained via the unrestricted form.
Performance results are reported in Figure . CoNST again achieves significant speedup over the alternatives. For this experiment, we could not use the large dataset because of insufficient physical memory on our platform.
5.2 Sparse Tensor Network for CP Decomposition
Canonical Polyadic (CP) decomposition factorizes a sparse tensor T with n modes into a product of n 2-D matrices. For example, a 3-D tensor T i jk is decomposed into three dense rank-r matrices A ir , B jr , and C kr . The CP decomposition of a sparse tensor is generally performed using an iterative algorithm that requires n Matricized Tensor Times Khatri-Rao Product (MTTKRP) operations . For a 3-D tensor, the three MTTKRP operations are as follows:
A ir = T i jk × B jr × C kr B jr = T i jk × A ir × C kr C kr = T i jk × A ir × B jr .
Figure shows the performance for MTTKRP operations for each of the three modes for sparse tensors from the FROSTT benchmark suite . We used the same four sparse tensors (Flickr3d, Nell1, Nell2, and Vast3d) used in the experimental evaluation of SparseLNR . The rank of factor matrices was set to 50. The time to perform the MTTKRP operation for the three modes varies quite significantly; this is in part due to the the highly non-uniform extents of the three modes for the tensors (as seen in Table ). For the MTTKRP expression, SparseLNR was not able to perform its loopFusionOverFission transformation, so the code and performance are essentially identical to TACO N -ary. Considering CoNST, unlike with the DLPNO benchmark (Section 5.1 ), CoNSTgenerated code is not always fastest. For the first two operations, CoNST achieves a speedup between 2.0 × and 4.8 × over other schemes, but relative performance is lower for the third one, ranging between 0.9 × and 1.0 × over the best alternative. For this case, the size of the intermediate tensor is small and the binary tensor contractions are efficient without fusion, whereas the mode  order that enables fusion results in code with slightly lower performance than the unfused code. However, when considering the total time for all three operations, as needed in each iteration in CP decomposition, CoNST achieves a minimum speedup of 2 × over the alternatives, across the four benchmarks. Sparta times are not reported because it could not be used: it does not handle contractions with "batch" indices that occur in both input tensors and output tensors, as occurs with the second tensor contraction in the binarized sequence for each MTTKRP.
5.3 Sparse Tensor Network for Tucker Decomposition
Tucker decomposition factorizes a sparse tensor T with n modes into a product of n 2-D matrices and a dense core n-mode tensor. For example, a 3-D tensor T i jk is decomposed into three rank-r matrices A ix , B jy , C kz , and core tensor G xyz . The computation is generally performed using the High Order Orthogonal Iteration (HOOI) iterative algorithm that requires n Tensor Times Matrix chain (TTMc) operations . For a 3-D tensor, the TTMc operations are as follows:
A iyz = T i jk × B jy × C kz B jxz = T i jk × A ix × C kz C kxy = T i jk × A ir xr × B jryr .
Figure presents execution times for the alternative schemes on the four FROSTT tensors. The mode-2 contraction for Flickr3d and mode-3 contraction for Nell-1 tensor ran out of memory for all methods on 128GB RAM. TACO-Unfused and Sparta ran out of memory for a larger set of runs because they form high-dimensional sparse intermediates in memory. The rank of decomposition was 16 for Nell-1 and Flickr-3d tensors, and 50 for Vast-3d and Nell-2 tensors. For the TTMc operation, SparseLNR is not able to perform its loopFusionOverFission transformation, so that performance is identical to TACO N -ary. Sparta runs a flattened matrix-times-matrix operation for a general tensor contraction and uses a hashmap to accumulate rows of the result. Since the matrix being multiplied is dense, the hashmap simply adds an overhead. Overall, CoNST generates code that achieves significant speedups over the compared alternatives.