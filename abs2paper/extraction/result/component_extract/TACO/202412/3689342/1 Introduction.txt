1 Introduction
This article describes CoNST, a Co de generator for N etworks of S parse T ensors. A tensor network expresses a collection of tensor contractions over a set of tensors. Tensor contractions 82:2 S. Raje et al.
are higher-order analogs of matrix-matrix multiplication. For example, the binary contraction Y i jlm = U i jk × W klm represents the computation ∀ i, j, l, m : Y i jlm = k U i jk × W klm . Multi-tensor product expressions, e.g., Z im = U i jk × V jl × W klm , arise commonly in many domains of scientific computing and data science (e.g., high-order models in quantum chemistry , tensor decomposition schemes ). They involve multiple tensors and multiple summation indices, e.g., ∀ i, m : Z im = j, k, l U i jk × V jl × W klm .
These multi-tensor products are also referred to as tensor networks , represented with a node for every tensor instance and edges representing variables that index the various tensors. The figure on the right illustrates this representation. As explained later in Section 2 , such a network is typically computed efficiently using a tree of binary contractions.
Considerable prior research has been directed at the optimization of dense tensor contractions and the optimization of tensor networks where the component tensors are dense . A few efforts have also addressed the optimization of tensor networks with sparse tensors under some restrictions . However, optimization and effective code generation for arbitrary sparse tensor networks remain an unsolved challenge.
A fundamental difficulty in developing efficient sparse versions of tensor computations in comparison to the corresponding dense versions is the fact that some compact representation such as Compressed Sparse Fiber (CSF , , detailed in Section 2 ) must be used to represent the non-zero elements, with the implication that arbitrary slices of a multi-dimensional tensor cannot be efficiently extracted. In contrast, with dense tensors, arbitrary elements or contiguous slices along any combination of tensor modes can be easily and efficiently accessed. Therefore, while code generation for the application of an arbitrary combination of loop transformations like tiling, permutation, and fusion is quite straightforward for the dense case, the same is not true for optimization and code generation for a collection of sparse tensor contractions.
Loop fusion transforms a sequence of perfectly nested loops into an imperfectly nested loop, where a set of one or more outermost loops with exactly matching loop bounds from each of the loop nests is pulled out and made common surrounding loops for a sequence of lower-dimensional loop nests containing the non-common loops. Consider the simplest case of loop fusion across a sequence of two perfectly nested loops, where the first loop nest produces an array that is consumed by the second loop nest. The cache reuse distance (defined as the number of distinct data elements accessed between two successive accesses to a given data element) for the fused version of the code can be significantly lower than the unfused version. This is because lower-dimensional slices of the produced/consumed array (corresponding to fixed values of the fused loop iterators) are produced/consumed in temporal proximity with the fused version, whereas all accesses to the entire array happen for the first loop-nest before any accesses from the second loop-nest for the unfused code, resulting in much larger reuse distances.
The above benefit of improved data locality and reuse in cache for fused producer/consumer loops applies to both dense and sparse tensor contractions. However, for the sparse context, an additional benefit accrues from loop fusion. When a set of common surrounding loops between a producer loop-nest and a consumer loop-nest is fused, it is not necessary to allocate the full space for the temporary array that is produced/consumed, but only as much as needed for lowerdimensional slices corresponding to fixed values for the fused loop iterators. This is because space used for a previous slice (corresponding to some fixed values of the fused loop iterators) can be CoNST: Code Generator for Sparse Tensor Networks 82:3 reused for the next slice. Further, if a sufficient number of loops are fused and the size of the full product data space for the lower-dimensional slice is small, a dense representation can be used instead of an explicit sparse representation for the slices of the intermediate temporary tensor between the producer and consumer statement, thereby lowering data access overheads .
Although a few efforts have been directed toward compiler optimization of sparse matrix and tensor computations , the current state of the art does not adequately address a number of critical inter-dependent aspects in the generation of efficient fused code for a given tree of sparse binary contractions.
Sparse tensor layout mode order. We focus on the widely used CSF format, which is commonly used for efficient sparse tensor computations. Since CSF uses a nested representation with n levels for a tensor of order n, efficient access is only feasible for some groupings of non-zero elements by traversing the hierarchical nesting structure. Selecting the nesting of the n modes of a tensor is a key factor for achieving high performance. Prior efforts in compiler optimization and code generation for sparse computations have not explored the impact of the choice of CSF nested layout mode order on the performance of contraction tree evaluation.
Loop fusion to reduce intermediate tensors. The temporary intermediate tensors that correspond to inner nodes of the contraction tree could be much larger than the input and output tensors of the network. By fusing common loops of the nested loops that produce and consume an intermediate tensor, the size of that tensor can be reduced significantly (as illustrated by an example in Section 2 ). A reduction of the size of an intermediate tensor can enable significant reduction in the number of cache misses if the reduced intermediate can fit in cache but the intermediate without loop fusion does not. Further, a dense representation of the intermediate becomes feasible, which further improves performance due to the reduced cost of tensor element accesses .
Inter-dependence between loop order, mode order, and contraction order. In addition to selecting the layout mode order for each tensor in the contraction tree, code generation needs to select a legal loop fusion structure to implement the contractions from the tree. Such a fused structure depends on the order of surrounding loops for each contraction, on the order in which the contractions are executed, and on the choice of layout mode order. No existing work considers the space of these inter-related choices in a systematic and general manner.
Our solution. We propose CoNST, a novel approach that considers the above factors in an integrated manner using a single formulation. This formulation encodes several inter-related goals. First, for each contraction, we ensure that the order of loops that surround it (in the fused loop nest) is compatible with the layout mode order of all tensors that participate in the contraction. This allows for efficient traversal of non-zero elements of these tensors. Second, we produce a valid topological sort of the contraction tree (i.e., each producer contraction appears before the corresponding consumer contraction). Third, the surrounding loops for each producer-consumer pair allow for valid fusion-and not only for this pair, but also for all other contractions that appear between the pair in the topological sort. Finally, the resulting fusion allows for significant reduction of intermediates: specifically, all intermediate tensors are guaranteed to be of order at most l, where l is a small constant limit (e.g., l = 1 ) given as a parameter to our tool.
To find a solution that satisfies these goals, we formulate a constraint system in which constraint variables are used to encode all relevant choices: order of surrounding loops, order of tensor layout modes, and topological order of contractions. The system is then solved by the Z3 SMT solver and the result is used to create the desired fused loop structure and tensor mode layouts for the entire contraction tree. This structure is lowered to the IR of the Tensor Algebra Compiler (TACO) , which is then used to generate the final executable code. The main contributions of CoNST are:
-We design a novel constraint-based approach for encoding the space of possible fused loop structures and tensor CSF layouts, with the goal of reducing the order of intermediate tensors. This is the first work that proposes such a general integrated view of code generation for sparse tensor contraction trees. -We develop an approach to translate the constraint solution to the concrete index notation IR of the TACO compiler. -We perform extensive experimental comparison with the three most closely related systems: TACO , SparseLNR , and Sparta . Using a variety of benchmarks from quantum chemistry and tensor decomposition, we demonstrate significant performance improvements over this state of the art.