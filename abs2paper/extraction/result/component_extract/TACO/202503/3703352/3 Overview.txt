3 Overview
As depicted in Figure , the flow of ApSpGEMM includes two stages. The first stage involves preprocessing the sparse matrices, consisting of the Matrix Pre-analysis and Matrix Splitting steps. ApSpGEMM 20:5 Fig. . The workflow of ApSpGEMM. Matrix pre-analysis to obtain matrix features, matrix splitting to generate panels, in-core computation to accomplish specific panels multiplication, and heterogeneous collaboration implements parallel computing between CPU and GPU.
The second stage is out-of-core computing, which comprises the In-core and Heterogeneous Collaborative computing steps.
Matrix Pre-analysis. Matrix Pre-analysis is the foundational stage, providing crucial insights into the distribution of NZs within the matrix. This information guides subsequent steps such as Matrix Splitting and In-core GPU computation. However, it is noteworthy that the analytical cost of this stage should be balanced against potential performance gains. For example, in the case of nsparse , allocating approximately 30% of the execution time for pre-analysis to analyze the features of NZs distribution underscores the need for a fast, efficient, and lightweight analyzer capable of extracting essential feature information swiftly.
Matrix Splitting. As illustrated in Figure , these matrices exhibit various distributional features for NZs, such as band distribution, centralized distribution, and diagonal distribution. These features significantly impact computational efficiency in the core. For example, in the "cite-Patents" matrix of Figure (a), loading various rows into thread blocks directly can lead to some blocks being underutilized, while others become densely occupied. Hence, it's crucial to devise a strategic algorithm based on the distinct distributional features of NZs. This algorithm optimally splits the matrix to make the most of the computational and storage resources on the target core.
In-core GPU. This step mainly involves multiplying panels from matrix A with panels from matrix B on GPU. Several crucial considerations apply here. Firstly, the unique distribution features of the panels require implementing different multiplication operations tailored to panels with different features. Secondly, determining optimal panel sizes for thread blocks is a challenging task. Lastly, carefully allocating thread blocks and threads is crucial for memory access and load balance.  Heterogeneous Collaboration. When dealing with large-scale matrices that exceed the available GPU graphics memory capacity, the concurrent use of multiple heterogeneous cores proves effective. However, this approach introduces challenges like transfer latency, differing computational capabilities, and the complex coordination of scheduling. Therefore, strategies are needed to allocate panels of varying sizes and features between the GPU and CPU. It's also crucial to minimize communication and computation latency in this heterogeneous computing paradigm.
Considering the general evaluation metrics for matrix computations, the objective of Ap-SpGEMM centers on addressing large-scale SpGEMM computations across two stages while minimizing the total computational time, denoted as T . Mathematically, this optimization problem can be formally expressed as Equation (1):
Arдmin(T ) =t 1 (pre-analysis) + t 2 (splittinд) + t 3 (In-core computation) + t 4 (heteroдeneous collaboration), (1)
where each component t i signifies the time associated with its respective step.
It should be noted that the panels generated during the matrix splitting stage will impact the computational performance in the second stage. While we analyze the distribution features of NZs in Figure , we do not adopt band or localized centralized splitting methods because these approaches cannot be extended to general matrices. Instead, we opt for the versatile approach of ApSpGEMM 20:7 sparsity splitting. Its inherent adaptability allows us to efficiently handle matrices with different features, playing a crucial role in achieving the seamless integration of in-core computation and heterogeneous collaboration steps.