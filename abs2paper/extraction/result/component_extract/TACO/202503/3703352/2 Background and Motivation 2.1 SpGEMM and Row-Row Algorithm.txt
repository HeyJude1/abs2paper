2 Background and Motivation 2.1 SpGEMM and Row-Row Algorithm
This study concentrates on the compressed sparse rows (CSR) format, which is the most commonly employed data structure for representing sparse matrices . As shown in Figure , the CSR format comprises two position index lists: "rowptr" and "colptr", along with a list of data. Specifically, the "colptr" contains the column indices of the NZs, the "data" stores all of the NZs values, and the "rowptr" indicates the starting position of each row's NZs in the "data". In SpGEMM, the computation of the sparse matrices A and B, and the resultant matrix C is represented by
C i j = k A ik • B k j .
Currently, a widely adopted algorithm for sparse matrix multiplication is Gustavson's row-row formulation , as shown in Algorithm 1.
2.2 Motivation of This Work
In the field of scientific computing, there are large-scale sparse matrix computations. For example, "nlpkt200", "uk-2002", and "stokes", originating from deep learning, graph processing, and semiconductor technology, respectively, have NNZs reaching billions. The NNZs in the resulting matrix C = A × A is 5 to 10 times greater than that of matrix A itself. Although many efforts have been devoted to optimizing SpGEMM computation , these methods assume sufficient computation core memory . A natural approach is to evenly split large matrices into tiles for iterative computation, but this wastes CPU computing resources and leads to load balancing and memory access issues on GPUs, resulting in low computational performance.  for all B k j in row B do 7: This work attempts to address the above issues through CPU and GPU collaboration. However, heterogeneous collaborative SpGEMM faces the following challenges: (1) imbalance of sparse matrix loads and irregular memory access on the GPU due to the distribution features of NZs, (2) different matrix computation latency on the CPU and GPU due to variations in sparsity, and (3) additional latency overhead from temporary data transfers between heterogeneous cores.
value = A ik × B k j 8: if C i j C i * then 9: insert(C i j , C i * )
We have designed a four-step approach ApSpGEMM to address the aforementioned challenges. Based on the features of NZs distribution, ApSpGEMM split the matrix into sub-matrix named panels, where the size of the panels depends on the splitting algorithm and the matrix itself. These panels serve as the fundamental units for computation and transmission on both GPU and CPU.