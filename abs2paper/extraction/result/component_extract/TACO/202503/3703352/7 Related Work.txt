7 Related Work
SpGEMM in parallel is more complex than other sparse kernels like sparse matrix-vector multiplication (SpMV) and sparse-dense matrix multiplication (SpMM) due to the impact of the NZs distribution features on data loading and memory access. Since the pursuit of optimizing SpGEMM has garnered significant scholarly attention in contemporary discourse. Over the years, SpGEMM has garnered significant attention on various modern parallel platforms , such as GPUs , FPGAs , specific devices , and distributed clusters . This section commences by scrutinizing scholarly investigations that have concentrated on the GPU acceleration of SpGEMM. Subsequently, we delve into an exposition of optimization endeavors pertaining to matrix multiplication within heterogeneous computational environments.
A substantial body of research has been dedicated to the optimization of SpGEMM on GPUs. Notably, Bellet et al. introduced the Expansion, Sorting, and Compression (ESC) approach, which dissects the computation process into three principal stages: ESC. In the initial phase, this  method generates intermediate products by expanding sparse matrices (Expand). Subsequently, these intermediate results are subjected to sorting based on their respective row and column indices (Sort). Finally, the approach amalgamates values with colliding indices to derive the ultimate result (Compress). Rivera et al. have directed their attention toward optimizing GPU resource utilization, particularly when the input matrix exhibits specific structural features. To this end, they have proposed two distinct algorithms, denoted as TSM2R and TSM2L, tailored for computing two categories of "tall-and-skinny" matrix-matrix multiplications on GPU architectures. Furthermore, various studies have explored techniques aimed at refining load balancing in the context of SpGEMM optimization. For example, Nagasaka et al. have employed a two-step binning process, one for symbolic and another for numeric stages. Lee et al. have devised a block reorganizer facilitating parallel splitting and gathering of computations for individual blocks. Merging strategies play a pivotal role in SpGEMM optimization and involve the utilization of sorted lists of intermediate results, typically through merge-sort-like algorithms . RMerge , for instance, decomposes input matrices into sub-matrices, which are efficiently merged using specialized algorithms. Similarly, bhSPARSE dynamically selects among different merging solutions and optionally integrates elements of the ESC approach.
Effective matrix splitting assumes paramount importance within the domain of SpGEMM computations. This significance arises not solely from its role in enhancing load distribution but also from its substantive contribution to the optimization of data locality . Illustratively, the work has split rows into 38 bins based on computational workload and has assigned distinct optimization methods to each bin. Akbudak et al. introduce a meticulously crafted hypergraph partitioning (HP) model, strategically designed to mitigate communication costs while simultaneously achieving a well-balanced workload distribution. In a complementary vein, Ballard et al. have introduced a comprehensive framework that delineates the minimal communication prerequisites for both parallel and sequential SpGEMM computations. Furthermore, they have formulated a methodology for the identification of communication-optimal algorithms tailored to the specific features of input matrices, employing the hypergraph partitioning paradigm as the foundational approach. Building upon these endeavors, Selvitopi et al. have extended the applicability of the HP model to encompass diverse parallel SpGEMM algorithms, including variants such as outer-product, inner-product, and row-by-row-product computations. Collectively, this concerted research effort underscores the paramount importance of proficient matrix partitioning in the continual advancement of SpGEMM optimization. Efforts to address the computational challenges posed by large-scale matrix multiplication have led to investigations into parallelization strategies that harness both CPUs and GPUs. Benatia et al. introduced a methodology that involves the horizontal splitting of the input matrix into multiple block rows, coupled with the application of a machine-learning-based performance model for the predictive determination of optimal sparse formats. Subsequently, a mapping algorithm is employed to judiciously allocate these block rows among the available CPUs and GPUs within the computational system. In a related vein, Xia et al. proposed a splitting approach that distinguishes between out-of-core and in-core computations, with the aim of achieving the synchronization of computation and communication between CPUs and GPUs. This strategy is thoughtfully designed to minimize reliance on dynamic memory allocation, thus enhancing efficiency.