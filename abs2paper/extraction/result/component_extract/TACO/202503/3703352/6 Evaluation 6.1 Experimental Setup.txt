6 Evaluation 6.1 Experimental Setup
We conducted experiments using the NVIDIA GeForce RTX 3080 and the Intel(R) Xeon(R) Gold 5117 platforms. Table provides detailed specifications for both setups. The host operating system was Ubuntu Linux 18.04. We utilized driver version 470.57.02 and the CUDA 11.6 toolkit for GPU implementations. Compilation was done with NVCC compiler version 11.6.124. In our comparative analysis, we benchmarked ApSpGEMM against cuSPARSE, AC-SpGEMM , spECK , and TileSpGEMM .  Row_Pair[ ]=Sparsity_Mean_Sort(row A i , row B j ) 7: end for 8: for G = 0 to R × length(Row_Pair)-1 do In-core Computation 12: end for 13: for C = R × length(Row_Pair) to length(Row_pair) -1 do 14:
Assign Row_Pair[C] to CPU 15: end for 16: Combine final result matrix C
6.2 Matrices Datasets
Our computational efforts focused on the SpGEMM operation denoted as C = AA T , aligning with established conventions in prior SpGEMM research . We selected experimental matrices from the SuiteSparse Matrix collection and the Network Repository. Specifically, we selected matrices with NNZs ranging from 3 million to 150 million for matrix C. The matrices designated for the execution of heterogeneous cores were excluded. Approximately 400 matrices met our criteria in both repositories. However, many were duplicates, resulting in 273 distinct matrices.
For precision, we used double data type in our experiments. We meticulously chose nineteen matrices for detailed analysis. The first nine underwent SpGEMM exclusively on the GPU, while the remaining nine, due to their size, underwent SpGEMM on both the CPU and GPU. Table   For each matrix, the values of "n", "NNZs(A)", "flops(AA T )", and "NNZs(AA T )" are all expressed in millions (M).
provides a comprehensive overview of these matrices. "Abbr" stands for matrix name abbreviation. "n" represents the number of rows (and columns) in matrix A, while "NNZs(A)" indicates the total non-zero elements. "flops(AA T )" measures the floating-point operations needed for AA T multiplication, considering the multiply-add operation as two flops. "NNZs(AA T )" is the total NZs in the resulting matrix C. Finally, the "Compression Ratio" parameter shows the ratio of half the flops to the NNZs(AA T ) ratio, indicating the average number of floating-point operations required to generate a NZ in the resulting matrix.
6.3 In-Core GPU Performance
Performance Comparison. Figure provides a performance comparison between our proposed method and four prominent SpGEMM techniques using 273 matrices where C is computed as C = AA T on RTX 3080. Notably, cuSPARSE, a vendor-provided library, shows limitations in handling a subset of matrices in the dataset. In contrast, AC-SpGEMM, spECK, TileSpGEMM, and our method demonstrate a broader capability, successfully performing computations on nearly all matrices.
Analyzing the data, we observe distinct trends in the performance curves. cuSPARSE starts with the lowest performance point and exhibits a gradual increase. AC-SpGEMM slightly surpasses cuS-PARSE initially, while spECK consistently achieves over 40 GFlops for most matrices. TileSpGEMM and our method both approach the 40 GFlops threshold initially. Upon closer examination, our method consistently outperforms others, especially for matrices with higher compression ratios. We identify the peak performance point (dark blue point) furthest from the fitted line. The peak GFlops for cuSPARSE, AC-SpGEMM, spECK, TileSpGEMM, and our method are 65. .54 GFlops, respectively. Compared with cuSPARSE, AC-SpGEMM, spECK, and Tile-SpGEMM, our method achieves 3.03x, 2.38x, 1.84x, and 1.20x higher peak GFlops, respectively. Assessing average GFlops across the five methods, we find values of .55, and 58.62 GFlops, respectively. Our method outperforms its counterparts with average GFlops that are  2.31x, 1.77x, 1.28x, and 1.12x greater, respectively. In summary, our method consistently delivers superior computational performance for SpGEMM compared with the other four state-of-the-art methods.
In addition, Table reports the comparison of SpGEMM's wall clock time cost against dense GEMM on various tasks. In these comparisons, SpGEMM uses ApSpGEMM and DGEMM uses CUBLAS. Due to GPU memory size limitations, the size of DGEMM is restricted to below 50000×50000. As shown in the results, the time cost of ApSpGEMM remains at the millisecond level, whereas GEMM is generally at the second level. The computation time of SpGEMM is positively correlated with the compression ratio, while GEMM's time is related to the number of rows and is independent of NNZs.
Single-step Time. Table and Figure (a) offer detailed insights into the performance of the 10 small matrices at in-core GPU. A thorough analysis of Figure (a) highlights our method's outstanding performance, surpassing others in 8 of the 10 matrices assessed. While TileSpGEMM excels in "dela" and "af" matrices, it's worth noting our approach remains competitive even in these cases. Our method's performance boost is most noticeable in matrices with higher  compression ratios. In contrast, aside from cuSPARSE, the four other methods show relatively similar performance in matrices with lower compression ratios.
Significantly, our method notably outperforms in four matrices: "mem", "msd", "human", and "nd3k". This superiority stems from two main factors. First, the remarkably high compression ratios in "human" and "nd3k" matrices provide a broader scope for our method to excel in computational efficiency. Second, the distribution of non-zeros in the "mem" and "msd" matrices naturally aligns with the reordering rule based on sparsity. This means that matrix reordering and load assignment take up a smaller portion of the overall computation time.
We conducted a series of precise experiments to comprehensively analyze the time distribution of the different steps. This study involved the execution of 10 carefully selected matrices, with a detailed recording of the time taken by each step. Figure ) visually represents the percentage of time attributed to each step in relation to the overall computation for each matrix. It's important to note that Global load, Local load, and Compute are integral components of the in-core GPU computation phase. Figure (b) clearly shows that matrices "mem" and "msd" allocate the least time to the reordering step, while dedicating a significant portion to the computation step. On average, across all thirteen matrices, the time distribution among the steps is approximately as follows: Analysis accounts for about 5%, Reorder constitutes roughly 18%, Load Balance occupies around 16%, and Computation predominates with an approximate share of 61%.
6.4 Heterogeneous Collaboration Performance
Allocation Ratio. To verify the best CPU/GPU panels scheduling ratio R, we conducted a series of performance evaluations with nine large-scale matrices on two types of heterogeneous cores. The outcomes, shown in Figure , demonstrate progressive performance improvement with increasing R, from 55% to 65%. The peak performance is notably observed between 60% and 65%. However, once R exceeds this range, performance starts to decline, and there's a significant drop after surpassing the 70% threshold. It's worth noting that the optimal allocation ratio (R) varies for two heterogeneous platforms. For instance, in "lj", "soc-lj" and "com-lj", the maximum GFlops are achieved at R values of 55% and 60%, respectively.
Performance Comparison. We systematically performed a comprehensive performance analysis on three matrices, each with distinct configurations: single-CPU, single-GPU, and heterogeneous collaboration (Hete) setups. For single-CPU SpGEMM, we utilized Gustavson's row-row algorithm. In single-GPU processing, we employed a straightforward chunking approach for matrix loading and subsequent SpGEMM computations. In the heterogeneous setup, we followed the approach outlined in Section 5.4 to coordinate the execution of SpGEMM operations. The findings presented in Figure strongly emphasize the superior performance effectiveness inherent in the heterogeneous collaboration, surpassing both GPU and CPU settings. Heterogeneous collaboration improves GFlops by an average of 7.21 times and 2.25 times compared with single CPU and single GPU, respectively. It's noteworthy that the performance enhancement is more pronounced for "stokes", "j", and "uk", attributed to their relatively higher compression ratios.