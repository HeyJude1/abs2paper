1 Introduction
SpGEMM is widely applied in various fields, including AMG , graphic processing , deep learning and geometric transformations in CAD/CAE. SpGEMM operates two sparse matrices A and B to compute a result C = A Ã— B. To enhance SpGEMM performance on GPUs, the widely used method is Gustavson's row-row algorithm , which computes the result matrix C in parallel by aligning the non-zero elements (NZs) of A in the corresponding rows of B. Due to the sparse structural features of the input and output matrices, accelerating SpGEMM on GPUs has long been a noteworthy area of focus. As a result, a number of recent efforts have been made to develop GPU implementations for SpGEMM .
Existing approaches assume that the entire SpGEMM computation can be completed within the available memory of a single GPU. However, the sizes of sparse matrices can be quite large in real world. Koichi Shirahata et al. found that graphs with over 100 million vertices and over 1 billion edges (requiring over a terabyte of uncompressed storage) are very common in social networks and scientific computing. Due to memory constraints , even the most advanced and memory-efficient SpGEMM implementations are unable to handle these large-scale matrices on GPUs. Recently, the development of unified memory has provided a new approach to address memory limitations. Therefore, the collaborative computation of large-scale SpGEMM by heterogeneous CPUs and GPUs is worth attention.
Previous work HPMaX has attempted to address dense matrix multiplication (DGEMM) through collaborative CPU and GPU. However, the random distribution of NZs makes computing SpGEMM on heterogeneous cores more challenging compared with DGEMM. First, in the kernel, the regular distribution of NZs in DGEMM allows for predictable memory access patterns. This means that threads can read and write multiple consecutive NZs while ensuring load balancing. However, in SpGEMM, the distribution of NZs is random and irregular. The number of NZs (NNZs) each thread is responsible for is unpredictable, and reading NZs one by one incurs significant memory access overhead. Secondly, due to differences in computational resources and the irregular distribution of NZs, the CPU and GPU incur different latencies when computing SpGEMM. This implies that it is hard to guarantee computational parallelism if we allocate different parts of the matrix to the CPU and GPU separately without constraints. Finally, regardless of whether it's computing input matrices or storing output matrices, the temporary data transfer between CPU and GPU are inevitable. However, on PCIe-based heterogeneous devices, these temporary data transfers significantly increase the computation's waiting latency.
To address the aforementioned challenges, we propose ApSpGEMM, an adaptive-panel-based heterogeneous collaborative approach for large-scale SpGEMM. We first designed a lightweight analyzer to extract the distribution features of NZs. Next, we define an efficient sparsity ordering rule to quickly reorder and split the matrix based on the NZs' features. To achieve load balancing ApSpGEMM 20:3 and reduce memory access overhead, ApSpGEMM uses an adaptive thread allocation algorithm for the three types of panels from the matrix splitting. Considering the computational differences between the CPU and GPU, we introduce the concept of core affinity to adaptively allocate panels across heterogeneous cores. Lastly, we proposed an asynchronous multiplication approach and carefully designed scheduling strategies to overlap the multiplication and transmission of panels across different cores.
We evaluated the computational performance of matrix multiplication for a set of sparse matrices selected from the matrix collection. Our experimental results demonstrate that, in the majority of matrices, ApSpGEMM improves performance by 1.12 to 2.31 times compared with the state-ofthe-art in-core GPU method. Through the implementation of asynchronous overlap scheduling and adaptive panel allocation, heterogeneous collaboration further enhances the multiplication of large-scale matrices by 2.25 to 7.21 times.
Contribution In summary, we make the following contributions.
-We introduce ApSpGEMM, an adaptive-panel-based heterogeneous collaborative approach, aimed at addressing the challenges posed by the irregular distribution of NZs and memory limitations encountered in large-scale SpGEMM computations. -We present a comprehensive four-step design, including Matrix Pre-analysis, Matrix Splitting, In-core Computation, and Heterogeneous Collaboration to optimize the SpGEMM. -We introduce the novel concept of core affinity analysis, which carefully considers the impact of sparsity on panel multiplication performance across different cores, thereby enabling the development of an adaptive allocation scheme for panels. -We propose an asynchronous multiplication approach and scheduling strategies to effectively overlap the multiplication and transmission of panels across different cores, mitigating the impact of temporary data transfer latencies. -Through extensive experiments, we demonstrate that our method achieves significantly higher GFlops compared with existing methods for the C = AA T operation, and large-scale matrices perform better on heterogeneous cores than on single devices.