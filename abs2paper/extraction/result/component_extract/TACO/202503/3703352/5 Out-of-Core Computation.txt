5 Out-of-Core Computation
5.1 In-Core Computation
In Section 2.1, to parallelize Gustavson's algorithm on the GPU, the critical task is parallelizing the two for-loops at lines 2 and 3 within Algorithm 1. This means that in the GPU kernel, each thread block is responsible for computing panel multiplication involving distinct rows. Each thread within a block computes the corresponding NZs within these rows. Efficient parallelization depends on meticulous consideration of two key aspects for blocks and threads: load balancing and memory access optimization.
(a) Effective management of rows and NZs is central to load balancing. Allocating an optimal workload to each thread block is fundamental, as excessive workloads may lead to frequent global memory accesses to transfer rows to shared memory, causing a performance bottleneck. Conversely, an overly conservative allocation may result in inefficient use of shared memory resources. Within a single block, maintaining a balanced distribution of workload among threads is also vital. Imbalances can lead to some threads bearing excessive computational loads while others are underutilized. Thus, achieving global and local load balance is paramount in GPU parallelization.
(b) Memory access involves threads retrieving rows from panel B p (as referenced by the NZs of panel A p ) from global memory and loading these rows into the shared memory. This means that each NZ in the A p requires one global memory access. This can result in a significant overhead in memory access operations, especially when compared with the computation itself. To address load balancing and memory access challenges and lay the groundwork for Section 5.2, we propose the following solutions for SpGEMM, SpMM, and DGEMM:
SpGEMM. We use a systematic approach to achieve global load balance. This involves categorizing contiguous rows from A p into discrete bins. We leverage the knowledge from the matrix pre-processing step, generating multiple bins, which are subsequently allocated to distinct thread blocks. Each thread block is responsible for executing the multiplication operation for its designated bin.
Previous methods used atomic operations to allocate a fixed number of adjacent rows into the corresponding bins . Although this method is simple, it is inefficient. Firstly, it performs unnecessary binning operations for entirely empty rows. Secondly, it focuses on the number of rows rather than the memory size occupied by the rows, which leads to wasted shared memory space. We attempt to address the aforementioned issues through a dynamic binning approach, where each bin is allocated a different number of rows based on the size of the shared memory. However, dynamic binning requires sufficient prior knowledge to find suitable rows. Fortunately, the matrix pre-analysis and reordering steps solve this problem. Given a panel, not only are all rows reordered according to a descending order rule, but the NNZs of each row are also known. Within the limits of shared memory size, deciding the number of rows for each bin is straightforward.
The goal of local load balancing is to allocate threads within a given block so that each thread is responsible for processing certain NZs in a row of A p . This means that the threads need to call CUDA cores to access global memory to retrieve the relevant NZs of B p and perform elementwise multiplication on the NZs they are responsible for. Therefore, allocating the optimal number of threads for a row is crucial to ensuring efficient memory access and fast element-wise multiplication operations.
On the one hand, allocating an insufficient number of threads for one row in A p would require multiple iterations per thread, leading to suboptimal memory access and frequent CUDA core launches. Conversely, allocating an excessive number of threads could result in a notable surplus of idle threads. On the other hand, although matrix reordering and dynamic binning reduce the risk of thread load imbalance to some extent, the differences in NNZs between rows still exist. Therefore, if a fixed thread allocation scheme is used for each row, the long and short rows within the panel will still affect the balance of thread workloads. However, using a dynamic thread allocation method, such as adaptively assigning the number of threads for each row, would introduce significant decision-making overhead.
After weighing the pros and cons of the two thread allocation strategies, our goal is to ensure that each thread's iteration count closely matches the number of rows that the respective thread group must handle. In a thread block with T threads, we divide it into G groups, each containing M threads, where M = T /G. As shown in Figure , the local load balancing strategy encompasses various thread group configurations. For example, when M = 8, it requires 4 iterations; when M = 4, it requires 3 iterations, and when M = 2, it involves 4 iterations to complete the processing tasks, as depicted.
Using insights from the pre-analysis of the matrix, we start by initializing the value of M based on the average row length across all rows in A p . We then proceed to systematically assign these thread groups to A p . However, given the potential existence of long rows, it's crucial to note that the maximum iteration count a thread in such a row would need to perform is defined as iter thread = N NZ max /M. To ensure that this maximum iteration counts closely aligns with the number of rows, denoted as n rows , that each group needs to process, we make an adjustment to the value of M using a heuristic approach, as shown in Algorithm 2.
SpMM. The operation involves multiplying a sparse panel A p by a dense panel B p , resulting the output panel denoted as Y , as depicted in Figure
if G > N NZ A then 11: G ← N NZ A 12:
end if 13: end while are contiguous. This means that in Algorithm 1, the middle loop (line 3) reads a small amount of NZs from A p , while the inner loop (line 4) reads a large amount of NZs from B p . As a result, the NZs from A p occupy only a small amount of space in shared memory, while most of the space is occupied by the NZs of B p . Therefore, the main memory access overhead comes from the frequent exchange of B p 's NZs between global memory and shared memory.
To address this issue, our approach focuses on improving the reutilization of NZs within A p . We observed that NZs sharing the same column in A p can simultaneously access elements of the same row in B p from global memory. Consequently, the number of times NZs in the same column of A p access global memory can be reduced to once. Additionally, the more NZs there are in the same column of A p , the greater the benefit derived from this process. Based on this observation, we define a dense column as one with more than one NZ, and otherwise, it is considered a sparse column. The upper limit of NZs in dense columns depends on the thread group size and the size of shared memory. We assume the shared memory size is S, the number of threads in the group is M, the number of columns in B p is K, and the average NZs in dense columns is Q, with each NZ represented as 8 bytes of storage. Therefore, the NZs read by the threads must satisfy 4(MK + MQ) < S. Thus, the size of a dense column is 2 < Q < S 4M − K. In dense columns, threads access global memory to load the corresponding rows from B p into shared memory, and NZs in these dense columns share the data from the referenced rows. As exemplified in Figure (a), A p contains 13 NZs, requiring a total of 13 global memory accesses to load the corresponding rows in B p . In contrast, in Figure (b), dense columns (columns 1, 2, and 4) only require three threads to access global memory, totaling three times, to load the vectors x 1 , x 2 , and x 4 , respectively. For sparse columns, it takes four global memory accesses to load the vectors x 0 , x 3 , and x 5 . The number of global memory accesses is reduced from 13 to 7.
DGEMM. Dense panels multiplication efficiency is significantly higher than that of SpGEMM and SpMM in the kernel. Dense panels inherently possess a high degree of data reuse and follow a sequential element access pattern. As a result, in the kernel, there is no need for specific discussions regarding memory access and thread allocation to multiply dense panels with every row/column. Currently, there is ongoing work aimed at reducing the computational complexity of DGEMM by decreasing the number of multiplication operations, such as AlphaZero and Strassen. ApSpGEMM focuses on kernel-level optimization, while AlphaZero and Strassen focus on operator-level optimization, and the two are not in conflict. Therefore, while we do not specify which DGEMM algorithm is optimal, these works are compatible with ApSpGEMM. Incorporating Strassen into the kernel configuration of the ApSpGEMM to accelerate dense panels multiplication is a natural choice.  Configuration. ApSpGEMM uses the maximum available shared memory (128 KB on RTX 3080) and the largest kernel size (1024 threads) to guarantee full hardware utilization. It is important to note that the shared memory size rated for the RTX 3080 is 64 KB, and the L1 cache size is also 64 KB. ApSpGEMM combines shared memory and L1 cache, which are shared among 1024 threads. During In-core Computation, some data is read-only, such as the NNZs information corresponding to rows or columns, sparsity list S, and the rowind indices. These data are characterized by being frequently accessed and read-only during kernel execution. To maximize the storage of NNZs data in shared memory, ApSpGEMM defines the above read-only data in "__constant__" and stores them in the constant memory (64 KB).
ApSpGEMM loads the target bin into shared memory, while adjacent bins are preloaded into the L2 cache, thereby reducing global memory access overhead. Additionally, a group may consist of one or more warps. On the RTX 3080, each warp is fixed at 32 threads by default. Therefore, in ApSpGEMM, the number of threads in a group is usually set as a multiple of 32.
5.2 Heterogeneous Collaboration
As described in Section 2.2, there are two key challenges in the heterogeneous collaboration step:
(1) The computational latency differs between CPU and GPU for panels with different distribution features. Synchronizing the computation time between the CPU and GPU to ensure they complete almost simultaneously maximizes parallel processing performance. Therefore, it requires establishing a discerning criterion for adaptively allocating panels to the appropriate cores.
(2) Empirical investigations using matrices from the SuiteSparse Matrix Collection, executed on an NVIDIA GeForce RTX 3080 GPU and an Intel(R) Xeon(R) Gold 5117 CPU, have revealed a significant presence of data transfer overhead between these heterogeneous cores, as illustrated in Figure . In light of this observation, optimizing the data transfer process becomes a pivotal consideration. Panels with varying sparsity levels from Section 4.2 are allocated for computation on either the CPU or GPU. To empirically evaluate the performance implications of sparsity levels on computation, we conducted a series of tests measuring computation times for panels with varying sparsity levels, utilizing both the GPU NVIDIA GeForce RTX 3080 and the CPU Intel(R) Xeon(R) Gold 5117, as elucidated in Figure . The outcomes of these experiments reveal a discernible trend, wherein panels featuring higher sparsity levels demonstrate superior suitability for GPU-based computations, whereas those with lower sparsity levels exhibit enhanced compatibility with CPU-based ApSpGEMM computations. We refer to this trend as panels affinity, where Af f inity = Spar sity(A p )+Spar sity(B p ) 2
. Figure (b) demonstrates the impact of affinity on SpGEMM in heterogeneous processors in realworld applications. We illustrate this process by multiplying the matrix "gupta3" from an optimization problem domain by its transpose. The "gupta3" matrix has 16,783 rows and columns with 9,323,427 NZs. The table in Figure (b) shows that the computation time for the multiplication of row A 5839 is smaller on the CPU, whereas for row A 15785 , the computation time is smaller on the GPU.
To synchronize the computation times on heterogeneous cores, determining an allocating ratio for panels becomes crucial. This ratio defines how panels are allocated for processing on the CPU versus the GPU, aiming to minimize the discrepancy in their computation times. We define computational coefficient as K = CPU exe /GPU exe , where CPU exe and GPU exe represent the respective computation times for the CPU and GPU to perform a single multiplication operation. Consequently, we derive the GPU-to-CPU allocation ratio as R = K/(K + 1). Based on our experimental observations, we have determined that setting R within the range of 60% to 65% yields optimal performance for the input matrix on our specific experimental platform. It is important to note, however, that the optimal value of R may be influenced by changes in the configuration of the CPU or GPU. Therefore, there may be a need to adjust R to match different hardware setups. Nonetheless, the fundamental principle of selecting an appropriate R to distribute computational tasks between the CPU and GPU remains an important consideration.
As shown in Figure , it is clear that the time needed for data transfer between the CPU and GPU exceeds the time allocated for computational tasks. This disparity can be attributed to the inherent limitations of the PCI-e architecture, which serves as the interconnect between these heterogeneous cores. In this architecture, each data transfer direction is handled by a single engine, restricting the CPU and GPU to perform data transfers in only one direction at a time. This inherent constraint introduces the potential for data transfer bottlenecks during concurrent operations.
To improve efficiency and mitigate this issue, we leverage the concept of computation and transfer overlap. Each computation on the heterogeneous cores involves two separate data exchanges: first, the transfer of CPU panels to GPU memory, and then the transmission of computed results from the GPU back to the CPU memory. By carefully coordinating these operations, we establish a synergistic relationship between computation and data transfers. This helps minimize waiting overhead and maximizes the potential for asynchronous concurrency.  As depicted in Figure , our approach to achieving overlap involves meticulous synchronization of the i-th computation with both the (i-1)th GPU-to-CPU transfer and the (i+1)th CPU-to-GPU transfer. This synchronized orchestration ensures that while the GPU is actively engaged in the computation of the current iteration (i-th), the CPU concurrently executes two essential tasks: receiving the results of the subsequent iteration (i-1)th and preparing to transfer data from the preceding iteration (i+1)th. This simultaneous handling of data transfers and computational tasks optimizes the temporal efficiency of the entire process, as exemplified in the figure Furthermore, incremental matrix splitting offers unique advantages over the notion of splitting the entire matrix in a single step followed by panel transfer. This step-by-step approach maintains the effectiveness of the splitting process while strategically allowing overlap between the splitting and data transfer operations. This concurrency facilitates the simultaneous execution of panel splitting and data transfer, leading to improved overall efficiency in the matrix multiplication process.
Algorithm 3 describes the panels' allocation process. Line 6 represents the result of averaging the sparsity of one row from matrix A with one row from matrix B, followed by sorting. This means that each pair {row A i , row B j } in Row_Pair[ ] is ordered by their affinity for the GPU, from high to low. Lines 8-14 represent the allocation of panels for GPU and CPU based on affinity and the allocation ratio. The index corresponding to R serves as the dividing line. The row pairs before this index are allocated as panels for the GPU to perform in-core computation, while the row pairs after this index are calculated by the CPU. Notably, the method of overlapping computation and transfer is applied in lines 10 and 11.