5 Evaluation
We split the results between CIFAR-10 (Section 5.1) and ImageNet (Section 5.2). We first analyze the accuracy impact of the optimization techniques and choose maximally compressed models for each technique that maintains accuracy. Then, we analyze the inference performance of these models using the algorithms and compilation configurations on the CPUs and GPUs. Section 6 gives a high-level discussion of the results.
5.1 CIFAR-10
5.1.1 Accuracy. The models' accuracy with varying levels of compression can be seen in the first row of Figure . For the four models, Table shows the baseline (dense) top-1 accuracy on CIFAR-10. We observe that, for weight pruning (Figure ), the accuracy is maintained for all models until 95% pruning, at which point all models see a drop in accuracy at 99%. However, the drop in accuracy for MobileNetV1 and V2 is higher, likely because they have fewer parameters.
We also observe this trend in channel pruning (Figure (b)), where VGG-16 and ResNet18 maintain their accuracy for longer compared to the MobileNets. However, for all models, the drop in accuracy is earlier compared to weight pruning, with the elbow appearing at 50% pruning for the MobileNet models and 80% for VGG-16 and ResNet18. We also observe that after the elbow the drop is higher, to around 10% accuracy or equivalent to random guessing.
For data-type quantization in Figure (c), we observe almost no change in accuracy for float16 across the four models. The output is not bit-wise identical, however, at most, this represents a 0.03% difference in top-1 accuracy. For uncalibrated int8 quantization, all models see a drop in accuracy, with MobileNets V1 and V2 seeing the highest drops, to 10.0% and 16.4%, respectively. However, with calibration, all models recover significantly, with MobileNetV1 losing the most accuracy at 1.7%. Table shows the elbow points of accuracy we take for the inference experiments.
5.1.2 Inference -CPU (Untuned).
The first two rows of Figure show the untuned performance of the CIFAR-10 models when running on the CPUs of the HiKey (4(a)-4(d)) and i7 (4(e)-4(h)) platforms, with varying compression strategies and convolutional primitives. The overall trends, including the fastest combination of parameters under different settings, are shown in Table .  For the untuned baseline, we observe that the i7 and HiKey differ in fastest algorithm, with direct and GEMM for all models, respectively. However, in most cases across devices, GEMM algorithms are the fastest, with one exception for VGG-16 on the HiKey. In terms of compression techniques, int8 has the fastest overall time in three out of four cases on the i7, whereas weight pruning is the fastest on the Hikey in three out of four cases.
If we take the best baseline time for each model, then we can compute an expected speedup given the compression ratio of each model optimization technique. For example, on the HiKey for MobileNetV1, with a pruning rate of 95%, we could expect an ideal speedup of 20×. However, in this case, we only achieve a speedup of 2.6× for the best weight pruning algorithm (GEMM), i.e., 13.0% of the expected speedup. On average, for weight pruning, we achieve 11.5% and 21.8% of the expected speedup on the HiKey and i7, respectively.
For channel pruning, the elbow points for the models (see Table ) show that channel pruned models are less compressed than the weight pruning models, means that we would [expect] the latter to always be slower than the former. However, we observe several cases where a channel pruning model is faster, namely, for all VGG-16 variants, except for GEMM on the HiKey, which is slightly slower. On average, for channel pruning, we achieve 77.9% and 83.9% of the expected speedup on the HiKey and i7, respectively.
For float16, we observe a slowdown when compared to the baseline (float32) in every case. For int8, we generally see a speedup relative to the baseline, with some notable exceptions using spatial pack. In some cases, int8 gives the best time overall, as seen in the last column of Table . On average, we achieve 33.6% and 157.2% of the expected speedup on the HiKey and i7, respectively.
5.1.3 Inference -CPU (Tuned).
The last two rows of Figure show the tuned performance of the CIFAR-10 models when running on the HiKey (4(i)-4(l)) and i7 (4(m)-4(p)) CPUs, with overall trends shown in Table . Comparing to the untuned results in Section 5.1.2, we observe that tuning has created some significant differences in the relative performance of the experiments, beyond reducing the inference time significantly. For example, spatial pack is now predominantly the best algorithm for dense models on both CPUs. For weight pruning, direct is the best algorithm, and this combination is predominantly the fastest inference time across models and CPUs. On average, we achieve 9.5% and 6.9% of the expected speedup on the HiKey and i7, respectively, less than untuned but faster in absolute terms. For channel pruning, sparse direct is also the best algorithm, and we still observe that channel pruning is faster than weight pruning in most cases of VGG-16. On average, we achieve 39.8% and 26.6% of the expected speedup on the HiKey and i7, respectively. Also, float16 still gives a consistent slowdown, and these differences are exacerbated when compared to the untuned results. The relative speedups of int8 on the i7 have disappeared with tuning, with an average slowdown of 2.0×.
5.1.4 Inference -GPU (Untuned).
The first two rows of Figure show the untuned performance of the CIFAR-10 models when running on the GPUs of the HiKey (5(a)-5(d)) and Xavier (5(e)-5(h)) devices, with overall trends shown in Table . We note that the HiKey's inference time on the GPU is much higher than on the CPU, e.g., the dense direct MobileNetV1 is almost 7× slower on the GPU. For the pruned experiments, we see speedups most consistently using spatial pack, which is different from the CPU where we almost always saw a slowdown. In terms of expected speedup, for the HiKey and Xavier, respectively, for weight pruning, we achieve 7.4% and 2.2%, and for channel pruning, we achieve 41.1% and 26.0%. For float16, unlike the CPU, we observe speedups in several cases, however, this behavior is not consistent across models, algorithms, or devices. For example, on the HiKey using GEMM with MobileNetV1 (Figure (a)), float16 provides a slowdown, whereas, for other models on the HiKey, we observe a speedup (similar to the Xavier). On the Xavier, float16 provides a speedup in all cases except for direct convolution where we observe small slowdowns. On average, float16 achieves 51.9% and 49.4% of its potential speedup on the HiKey and Xavier, respectively.
5.1.5 Inference -GPU (Tuned).
The last two rows of Figure show the tuned performance of the CIFAR-10 models on GPUs, with overall trends shown in Table . As noted in Section 4.4, we cannot provide tuned results for sparse models on the GPU. The HiKey GPU is still slower than the HiKey CPU (tuned), with the best dense result being 3.1× slower on average. The Xavier does not get any improvement when tuning; we discuss this issue in Section 6.5. For quantization, on the HiKey, taking the best result for each model, we achieve 58.9% and 44.4% of the expected speedup on average for float16 and int8, respectively; the Xavier achieves 49.0% and 28.4% of its expected speedups.
5.2 ImageNet
5.2.1 Accuracy.
For the four models, the baseline (dense) top-1 accuracy on ImageNet is shown in Table . EfficientNetB0 has the highest accuracy, which may be surprising, given it has fewer parameters. However, EfficientNetB0 is more recent and thus exploits a number of newer machine learning techniques to improve its parameter and training efficiency.
The accuracy on ImageNet with varying levels of compression can be seen in the second row of Figure . We observe a similar trend to the CIFAR-10 models, namely, the smaller models (Efficient-NetB0 and MobileNetV2) lose their accuracy more quickly than the larger ones (DenseNet161 and ResNet50). We also observe that all models lose more accuracy earlier when compared to CIFAR-10 pruning. This suggests that the CIFAR-10 models are more overparameterized.
MobileNetV2 Direct − − Direct Direct i8 i8 i8 i8+Direct ResNet18 Direct − − Direct Direct i8 i8 i8 i8+Direct VGG-16 Direct − − Direct Direct i8 i8 i8 i8+Direct MobileNetV1 Direct − − Direct Direct i8 i8 i8 i8+Direct
The best inference times are shown in bold. Shortened names are used for brevity: WP (weight pruning), CP (channel pruning), i8 (int8), f16 (float16).  Fig. . Experiments comparing the compressed ImageNet models chosen from obvious elbows of accuracy, with varying algorithmic primitives, benchmarked on the i7 and HiKey CPU platforms, with and without auto-scheduling.
For data-type quantization, we observe a similar trend as CIFAR-10, namely, a negligible difference in accuracy for float16. For ResNet50, we see a large drop in accuracy for uncalibrated int8 quantization, recovering to around a 3.0% accuracy reduction. For MobileNetV2, we observe a huge drop in accuracy for the uncalibrated model, down to around 0.09%, recovering to around a 6.6% accuracy reduction. This drop was much higher than we expected, so we also tried importing the Keras definition of MobileNetV2 and observed the same behavior.
For EfficientNetB0, we also observe a huge drop in accuracy to 0.08%, however, the recovery is much smaller than MobileNetV2's, reaching only 0.43% accuracy. This is due to architecture features of the model that make it less suitable for quantization, which we discuss in Section 6.2. For DenseNet161, we cannot run the int8 model in TVM due to an unsupported quantized operation. This excludes it from collection of uncalibrated accuracy and inference time results. However, when calibrated in ONNXRuntime, we reduce accuracy by 1.9%.
5.2.2 Inference -CPU (Untuned).
The first two rows of Figure show the untuned performance of the ImageNet models when running on the i7 and HiKey CPUs, with overall trends shown in Table . For dense models, we observe that in all cases on the HiKey GEMM gives the best performance, which matches its behavior as seen for CIFAR-10. For the i7, GEMM is fastest for the large
models (ResNet50 and DenseNet161), however, direct is fastest for the small models (MobileNetV2 and EfficientNetB0); on CIFAR-10, direct was consistently the fastest on this CPU.
For weight pruning, we find that by taking the best-performing variants as before, we achieve 30.3% and 41.8% of the potential speedup for the HiKey and i7, respectively; significantly higher than CIFAR-10. For channel pruning, this is 60.2% and 84.2%, respectively, which is 16.7% less than CIFAR-10 for the HiKey and 0.3% more for the i7. For quantization, we see similar trends to CIFAR-10, namely, a slowdown using float16 and a speedup using int8. For int8, we achieve 25.0% and 73.0% of the expected speedup on the i7 and HiKey, respectively, lower than CIFAR-10.
5.2.3 Inference -CPU (Tuned).
The last row of Figure shows the tuned performance of the ImageNet models when running on the i7 CPU. We note that tuning on the HiKey CPU (and GPU) was not practical, since the two variants we attempted took over 140 hours each, so we do not include any of the 57 variants required for each device. For the dense case on the i7, we see that spatial pack is consistently the best, matching the observed trends on tuned CIFAR-10. For the pruned models, we do not see any cases where pruned models are faster than a dense float32 implementation. This is contrasted with CIFAR-10, where we observe this in every case.
5.2.4 Inference -GPU (Untuned).
On the Xavier in the dense case, spatial pack is the best algorithm for the larger models (ResNet50 and DenseNet161), and direct is the best for the smaller models (MobileNetV2 and EfficientNetB0). On the Hikey, for the smaller models, GEMM was the best, and for the larger models, direct was the best. However, on the HiKey, dense ResNet50 and DenseNet161 experiments using spatial pack crashed with the error CL_INVALID_WORK_GROUP_SIZE. This means that TVM is exceeding the number of supported work items (see the OpenCL specification for more details ). If we run auto-tuning, then TVM can configure the work group size, which could avoid this issue.
For the sparse experiments, spatial pack using weight pruning was consistently the best across both GPUs, however, only outperformed the baseline in one case, ResNet50 on the Xavier. On the Xavier, we found that sparse direct experiments did not halt, even allowing hours for a single run. GPU memory utilization was at its maximum, suggesting that some inefficiency in this algorithm/hardware combination. Again, auto-tuning may make this variant viable, however, as highlighted in Section 4.4, we cannot tune sparse models on GPUs.
For ImageNet, quantization was not consistently the best compression technique for the GPUs, unlike CIFAR-10. For several cases, weight pruning performed best. On the HiKey, using int8 for EfficientNet was the fastest approach, however, it should be noted that in this case the accuracy drop was prohibitively large, as discussed in Section 6.2.
5.2.5 Inference -GPU (Tuned).
As discussed in Section 5.2.3, collecting tuned results for the HiKey GPU was not practical. In addition, we again observed no speedup on the Xavier when tuning, hence, we do not include the graphs.