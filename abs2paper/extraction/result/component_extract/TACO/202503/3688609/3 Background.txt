3 Background
In this section, we discuss the necessary background of the techniques in DLAS explored in our experiments. Note that providing a complete background of every technique in DLAS is beyond the scope of this article, as deep learning innovations are rapid and continuous, making any attempt at comprehensiveness quickly outdated.
3.1 Datasets & Problem Spaces
We focus on two common image classification datasets, CIFAR-10 and ImageNet . CIFAR-10 images are 32×32 pixels across 10 classes, while ImageNet images are 224×224 pixels across 1,000 classes.
3.2 Models & Neural Architectures
There is a wide range of DNN architectures available, with a variety of popular models for each. Convolutional neural networks (CNNs) are commonly leveraged for image classification tasks and are characterized by their use of convolutional layers. Other layers may include batch normalization and pooling layers, fully connected layers, and activation functions such as ReLU. The topology of these networks are generally deterministic, directed acyclic graphs. Some neural architectures may include skip-connections , where activations from previous layers are reused in later layers, or depthwise separable convolutions , which can reduce memory and computational requirements.
When training DNNs, we repeatedly present the full dataset (each round called an epoch) and assess ultimate performance against a separate test set. Algorithms like stochastic gradient descent (SGD) iteratively update the DNN weights to enhance accuracy between epochs. Hyperparameters like the learning rate (LR) influence weight adjustments each epoch, usually reducing as the network learns.
3.3 Model Optimizations
It has been observed that DNN models are overparameterized, and similar accuracies can be achieved with smaller models . As a result, a wide range of model optimization and compression techniques has been proposed. Figure shows some common compression techniques, with an uncompressed (or "dense") convolution shown in Figure .
Pruning is a family of techniques that sets weights/parameters in a DNN to zero. This can reduce the computational or memory overheads of a given DNN model, with potential accuracy loss. There are two general types of parameter pruning: unstructured and structured, each of which can be used in either global or layer-wise configurations . Unstructured pruning removes individual parameters, as seen in Figure (b), whereas structured pruning removes whole groups of parameters, such as blocks, or channels, as seen in Figure . With global pruning, given some pruning target (e.g., 50% of parameters should be pruned), the pruning algorithm will find the best parameters to prune across the whole DNN model, meaning that some layers will be more or less pruned than others. For layer-wise pruning, we prune by a pre-defined amount per layer, e.g., 50% pruning. All types of pruning apply some scoring function to the weights to determine which are the least important and therefore are likely to have the lowest impact on accuracy if removed. A popular approach is "L1-pruning, " where we prune parameters that have the lowest absolute value, i.e., closest to zero. Other ranking approaches include gradient-based methods and Taylor series expansion.
Another compression technique is data-type quantization, which reduces the number of bits used to represent data, visualized in Figure , where a continuous function is approximated with nine distinct values. Typically, DNNs are trained using float32, however, when they are deployed, we can reduce the precision. Common quantized data-types include float16 and int8, as well as emerging machine learning specific types such as bfloat16. For some types of data-type quantization, it may be necessary to add additional operations to the DNN, e.g., in many int8 DNNs, we need to store the partial-sums of MACs (multiply-accumulate operations) using up to 16 bits, and then quantize back into int8. Other layers, such as layer norm and softmax, may also require higher precision.
Note that for many model optimization techniques, including pruning and data-type quantization, it may be necessary to use some form of post-training fine-tuning or calibration to try and recover some of the lost accuracy. This can involve retraining the non-pruned parameters of the model in the case of pruning, or adjusting constants used in the rescaling operations in the case of data-type quantization, or some combination of the two.
From a systems perspective, many model optimization techniques do not necessarily provide speedups unless lower levels of the stack adequately support it; for example, hardware that can compute using data-types with fewer bits or algorithms that exploit the pruning to skip operations.
3.4 Algorithms & Data Formats
The layers of a given DNN model can be implemented in a variety of ways, as long as they still provide the same output. However, the behavior of a given implementation is influenced by the size and shape of the data, the properties of the hardware we are running on, as well as the choices around how we exploit model optimization techniques.
For the CNN models we evaluate in our work, the most important component to optimize is the convolutional layers, since they are generally the most compute-and memory-intensive. Algorithms used in this work implementing convolutional layers include direct, GEMM, and spatial pack convolution. Direct convolution applies the convolution in a manner similar to the textbook definition "sliding-window" and does not reshape input data and weights. GEMM convolution reshapes input data into a 2D array, potentially replicating elements using a reshaping algorithm known as "im2col. " This means that the convolution can be computed as a matrix multiplication. Spatial pack convolution reshapes both in input data and weights, although the weights can be reshaped offline, with data packed into tiles that are ideally loaded once. Unlike GEMM convolution, the size of the reshaped input data is the same size, and tiling on inputs and weights is intended to exploit data reuse and SIMD vectorization. Simplified implementations of these three algorithms are given in the Appendix using the TVM tensor expression language.
Another important component of how we format data is the layout, which is how we order the data in memory, since memory may have a different structure to a given tensor (e.g., 1D memory but 4D tensors). For 2D arrays, common formats include "row-major" and "column-major" order, with the former meaning that data in the same row is contiguous in memory, and the latter meaning data in the same column is contiguous in memory. Similarly, for 4D data, which is more relevant to our image classification CNNs, two common formats are NCHW and NHWC, with N representing the batch size and C, H, and W representing the number of input channels, the input height, and input width, respectively. The algorithms in listings 1-3 are in the NCHW format, however, spatial pack (Listing 2) temporarily reshapes data to NCHWc, where c is an inner tile dimension to exploit vectorization.
To achieve savings from pruning, the Algorithms & Data Formats layer is critical, since the chosen algorithm must support "sparsity, " i.e., exploiting the zeros generated by pruning to skip computation or reduce memory usage. The computational savings are enabled by the fact that, regardless of the value of an input, a multiplication by zero will have no impact on the final output; and the memory savings come from representing sequences of zeros in a more compressed format. In this work, we use the popular CSR (compressed sparse row) format, which represents 2D data using three arrays: (1) The non-zero elements of the parameters (data); (2) the original column index of the corresponding parameters (indices); and (3) the first non-zero elements in each row, as well as the final non-zero element (indptr). Other formats include BSR (block sparse row) and COO (coordinate list).
3.5 Systems Software
The systems software most commonly exposed to machine learning practitioners is the DNN framework. Common frameworks include PyTorch , TensorFlow , JAX , and MXNet , all of which are focused on DNN training. Other frameworks may focus exclusively on deployment, such as TensorFlow Lite and TensorRT . These frameworks include utilities for defining, saving, and loading models; passing, processing, and inspecting data; and invoking and profiling training and inference. Underlying these DNN frameworks are the kernel libraries that execute the critical computations of the DNNs (e.g., the convolutional layers). For example, for Nvidia GPUs, many frameworks leverage the cuDNN library , which is a collection of optimized CUDA kernels to run common DNN operations.
An alternative to using optimized vendor libraries is using a tensor compiler such as TVM or IREE . They generate code for a specific DNN and hardware backend, and when leveraged correctly can outperform vendor libraries, especially for operations that may be less popular or optimized. TVM uses a "compute schedule" programming paradigm, similar to Halide , where a high-level description of the computation is complemented by "schedules. " Schedules are platform-specific transformations that are applied to the code for each algorithm to enable better performance. Examples of schedule language primitives include parallelism, loop unrolling/splitting/reordering, and vectorization.
Tensor compilers can be taken even further by tuning the code (i.e., schedule) for each layer, using tools such as AutoTVM and Ansor . Specifically, Ansor is an auto-scheduling system built on top of TVM that automatically searches for optimized schedules for a given DNN model on a given hardware platform. It leverages a genetic algorithm and learned cost model to iteratively explore schedule transformations. A tuned schedule alters a given operation to exploit how the data sizes and access patterns interact with the target hardware, e.g., changing the schedule of a DNN layer of a given size to better exploit the cache memory. This is contrasted to an untuned schedule, which is designed to be more generic, and does not exploit knowledge of how the hardware and a particular DNN layer interact.
Below the level of tensor compilers are programming paradigms and general-purpose compilers. LLVM is a cross-platform compiler infrastructure that higher-level compilers such as TVM can lower their optimized code to, which is then converted to a binary. For hardware accelerators, systems such as CUDA and OpenCL provide a programming interface for GPUs, and more specialized accelerators may define their own libraries. Generally, when using accelerators, we still require CPU-side host code to manage accelerator calls and data transfers.
3.6 Hardware
Hardware devices that DNN models commonly execute on include CPUs and GPUs, as well as more specialized accelerators. CPUs are generally complex independent processing cores, typically with one to several dozen cores on a single chip. GPUs are generally simpler interdependent processing cores, typically with dozens to several thousand cores on a single chip. Note that this comparison is a simplification, since these cores are not equivalent, and they may vary in speed, programmability, and other features.
Vector or SIMD instructions allow multiple data to be loaded or computed upon using a single instruction. For example, the Intel instruction MOVAPS loads four float32 values in a single instruction, which in theory represents a 4× speedup compared to loading them one-by-one. Practically, micro-architectural considerations means that this speedup may vary. Different architectures may support varying maximum SIMD length, e.g., Intel's AVX instructions support up to 256 bits, whereas Arm Neon supports up to 128 bits. Different hardware may also have varying support and levels of optimization for different data-types, e.g., a CPU may support float16 data-types but actually execute them as float32 instructions; whereas a GPU may have explicit float16 instructions that can be exploited.
Another important aspect of hardware is the memory system, with fast, small caches being close to computation and larger, slower memories higher up the hierarchy. As well as memory management within processor hardware, data transfers between CPU and an accelerator can also be a critical bottleneck to efficient processing.