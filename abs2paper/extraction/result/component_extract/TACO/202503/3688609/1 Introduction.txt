1 Introduction
Recent years have yielded rapid advances in deep learning, largely due to the unparalleled effectiveness of Deep Neural Networks (DNNs), such as Convolutional Neural Networks (CNNs) and Transformer architectures , on a variety of difficult problems . Despite increases in algorithmic efficiency , the trend with DNN architectures is increased size and deployment costs as demands for more powerful and general solutions grows . As such, creative approaches are required to deploy DNNs on hardware with limited resources to enable a variety of emerging applications (e.g., autonomous driving , collision avoidance for quadcopters ). However, often these optimization approaches come with limited benchmarks and few comparisons, and there may be a disconnect between machine learning-based and systems-based optimizations due to disparate communities with varying core competencies.
Even in a simplified view of the relevant components of deep learning deployment (e.g., machine learning, software, and hardware), it is evident that choices in one area can have consequences for the choices in others . For example, constrained CPU hardware will require appropriately resource-conservative software and DNN models that fit limited memory and latency budgets. Alternatively, a new DNN architecture with a novel operation will need an optimized software kernel to execute it on hardware that can provide the required inference time. Without broad awareness of these interactions from practitioners, potential performance may be lost, since novel machine learning techniques may not be fully exploited by systems techniques , or machine learning practitioners may not be aware of under-utilized resources available on their hardware platforms.
In this work, we outline a high-level "stack-based" conceptual model of the relevant techniques pertaining to DNN acceleration and demonstrate how they are linked with a multi-level experimental analysis. Our goal is to enable a more comprehensive understanding of the performance available under different constraints of inference accuracy, execution time, memory space, and energy consumption. We introduce the Deep Learning Acceleration Stack (DLAS), which provides a model for both machine learning and systems researchers to reason about the impact of their performance optimizations. We propose DLAS as six layers, 1 as shown in Figure , covering parameters more relevant to machine learning (Datasets & Problem Spaces, Models & Neural Architectures, and Model Optimizations) and parameters more relevant to systems (Algorithms & Data Formats, Systems Software, and Hardware). Each of the layers can be further decomposed into sub-layers, however, the intent of DLAS is to provide a starting point for reasoning about across-stack optimization and encourage co-design of accelerated DNN deployments. The core contributions of this work include: -We introduce the Deep Learning Acceleration Stack to reflect the different layers of optimization, from both machine learning and systems, that can be applied to run a DNN model more efficiently on a given target device. -We select parameters to vary at each layer of DLAS (two datasets, four models, three compression techniques, three algorithms, two compilation techniques, four hardware devices), which gives us a vertical slice of DLAS to explore, presenting results on the inference time and accuracy impacts of our variations. -We develop an experimental framework based on Apache TVM to evaluate our parameters in a consistent environment, extending TVM where required and possible. -We explore across-stack interactions and make 13 key observations from our results, discussing their consequences and highlighting potential improvements that could be made from other works. The rest of the article is organized as follows: In Section 2, we motivate and describe DLAS. Section 3 gives the necessary background to understand the optimization techniques we explore across DLAS. For our evaluation, we describe the experimental setup in Section 4, and the results are presented in Section 5. In Section 6, we discuss the results of our evaluation as well as related work that could be exploited in further exploration.