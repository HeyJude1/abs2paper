4 Experimental Setup
Our experiments represent a vertical slice of DLAS, which demonstrates the design choices available and interactions that occur. Therefore, we do not optimize for every technique that may influence a given result. Additional techniques from the literature that could be used to further optimize performance, as well as discussion of the trends in our results, are given in Section 6. We develop our case study using Apache TVM (extending it where necessary), a state-of-the-art tensor compiler that is competitive with other optimized runtimes and has the advantage of being able to generate code for multiple targets (e.g., GPUs and CPUs of various architectures).
4.1 Models & Neural Architectures
As highlighted in Section 3.1, we investigate two image classification datasets: CIFAR-10 and Ima-geNet. For CIFAR-10, we use model definitions from a PyTorch-based library , which we train from scratch. We consider four architectures: ResNet18 , MobileNet V1 and V2 , and VGG-16 . ResNet18 and VGG-16 are larger models, and MobileNets V1 and V2 are designed to be more resource-efficient. To train the models, we used SGD to minimize the cross-entropy loss (averaged across all data items), which penalizes the network for making incorrect classifications. We used a 1cycle LR scheduler with momentum 0.9, weight decay 5 × 10 −4 , and an initial LR of 5 × 10 − , trained for 200 epochs.
For our ImageNet models, we use pre-trained models from the TorchVision repository . We consider four architectures: DenseNet161 , EfficientNetB0 , ResNet50 , and Mo-bileNetV2 . ResNet50 and DenseNet161 are larger models, and MobileNetV2 and EfficientNetB0 are designed to be more resource-efficient. These models are pre-trained with the training configurations described in the TorchVision documentation.
4.2 Model Optimizations
We explore three approaches to compression: (1) global L1 unstructured pruning, which we call "weight pruning" (Figure (2) global L1 structured pruning over convolutional channels, which we call "channel pruning" (Figure )); and (3) data-type quantization, exploring float16 and int8 quantization (Figure ). Our pruning techniques explore the impact of increasingly higher compression ratios, 2 thus for the evaluation of inference time and for each model and pruning technique, we select the pruning level that has the highest compression ratio before a significant accuracy drop (elbow point) occurs. To implement our pruning, we leverage PyTorch Lightning , a wrapper of PyTorch that simplifies pruning. We apply pruning iteratively, starting with the pre-trained unpruned dense models. To reduce accuracy loss, at each pruning step, we apply fine-tuning. For weight pruning, we start by pruning at 50%, then increase in step sizes of 10%, additionally pruning at 95% and 99%. For channel pruning, we start by pruning at 5%, then increase in step sizes of 5%, additionally pruning at 99%. In total, each pruning technique gets the same number of fine-tuning epochs shared evenly in each pruning step. However, we perform channel pruning in a more fine-grained way to compensate for its more coarse-grained approach to removing weights. For our CIFAR-10 models, we use: 210 epochs of fine-tuning; an initial LR of 5 × 10 −2 ; and SGD with momentum 0.9, a weight decay 5 × 10 −4 , and the 1cycle LR scheduler . For our ImageNet models, we use: 140 epochs of fine-tuning; an initial LR of 1 × 10 −3 ; and SGD with momentum 0.9, weight decay 5 × 10 −4 , and the cosine annealing LR scheduler .
For data-type quantization, we use TVM's native conversion tool. To recover the accuracy from quantizing from float32 to int8, we use ONNXRuntime's post-training quantization tool (with the default static quantization configuration) and the relevant validation dataset. For float16, as we show later in our results, there is no accuracy loss, thus, we do not perform any additional steps to recover lost accuracy.
4.3 Algorithms & Data Formats
We evaluate three algorithmic primitives for the convolutional layers: (1) direct, (2) GEMM, and (3) spatial pack convolution. We use both dense and sparse versions of these algorithms, which we implement or extend within TVM v0.8.0. The same high-level algorithm implementation is used for both CPU and GPU. All of our algorithms use the NCHW data layout, and for both weight and channel pruning, we use the CSR sparse data format.
4.4 Systems Software
For each convolution algorithm, we implement a minimal TVM schedule that uses thread parallelism. However, since TVM's performance comes from optimized schedules, unoptimized algorithms may give an unrealistic indication of the best algorithm. Thus, rather than hand-optimize the schedules and risk introducing bias from inconsistent levels of optimization, we leverage the Ansor auto-scheduler to generate optimized schedules for each DNN layer and algorithm. For CPU code, we use TVM's LLVM backend with AVX and Neon extensions for the Intel and Arm CPUs, respectively. For GPU code, we generate OpenCL and CUDA kernels for the Arm and Nvidia GPUs, respectively.
For our auto-scheduling (or "tuned") experiments, we allow Ansor to explore up-to 20,000 program variants, with early stopping permitted if no speedups have been observed after 1,000 variants. Auto-scheduling sparse computations is not fully supported by TVM. Thus, we employ an approach in TVM called "sparse sketch rules, " where we describe a starting point for the autoscheduler to begin schedule generation. This works for the CPU, however, TVM is unable to support auto-scheduled sparse computations on GPUs in the versions of TVM we have evaluated. This is because the auto-scheduler has two conflicting requirements: (1) cross-thread reduction, requiring partial sums that must be computed across GPU threads simultaneously; and (2) loops parallelized over threads must request a static number of threads. Both of these conditions cannot be satisfied, since the size of our reduction loop for our algorithms varies, depending on how many non-sparse elements there are in a given portion of the computation. Thus, we cannot tune pruned models on the GPU in our evaluation, which highlights a type of barrier faced by across-stack acceleration researchers, i.e., limited software support for a given combination of DLAS parameters.
4.5 Hardware
Table shows the hardware platforms used in our experiments. For CPU experiments, we use an Intel i7 workstation machine and the HiKey 970 development board. The i7 has 6 cores, but due to hyper-threading, 12 threads are exposed. By default, TVM uses 1 thread per core, a default we follow in our experiments. The HiKey board has an Arm big.LITTLE architecture, meaning that it has 4 more powerful cores (A73@ 2.4 GHz) and 4 less powerful cores (A53@1.8 GHz). For our experiments, we use only the A73 (big) cores, which is the default for TVM. In principle, with 1:11 For the HiKey 970, we only use the A73 CPU.
appropriately configured load balancing between cores, using all cores could bring a performance improvement. However, this is outside the scope of this work, and as we will discuss in Section 6.6, exposes further across-stack considerations. For our GPU experiments, we leverage the GPU cores of the HiKey 970 and an Nvidia AGX Xavier devices.
4.6 Evaluation Methodology
On both CPU and GPU experiments, we ensure that devices are single-tenant and repeat inference experiments 150 times. We use TVM's time_evaluator function with a single input image (i.e., batch size 1). For auto-scheduling, we run our search across 20,000 program variants once per experiment and evaluate the optimized binary 150 times. We report the median inference time, disregarding an initial warm-up run.