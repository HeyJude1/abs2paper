6 Discussion and Related Work
Our experiments show a large number of results, even from varying a small number of parameters across DLAS, which highlights a critical challenge: the huge design space for across-stack DNN acceleration. Other studies try to make more generalized claims of the impact of a specific parameter by exploring a significantly reduced design space, limiting the number of parameters explored at some layers of DLAS or keeping them fixed. For example, Hadidi et al. do not explore the impact of compression techniques or algorithms. Similarly, the stack presented by VTA focuses on a given compiler framework and accelerator, which is less general than DLAS.
Our results show that the interaction between different layers of DLAS can cause significant performance variations. This is our core observation: The number of design choices available for DNN acceleration continues to grow, and including additional parameters in a study can significantly change the performance optimization landscape. Some examples of this include:
-For MobileNetV2 on CIFAR-10, the fastest overall combination of algorithm and compression technique is weight pruning and GEMM (see Table ). This is true in the untuned cases of both the HiKey and i7 CPUs. However, adding in just one additional DLAS parameter (tuning) changes this, such that for the HiKey, weight pruning and direct convolution is the fastest, whereas for the i7, dense and spatial pack convolution is the fastest. This example shows that introducing just one new DLAS parameter can expose differences between other parameters (the hardware devices) not previously evident.
-Similarly, for ImageNet in the untuned case on the CPU, weight pruning is consistently the fastest compression technique on both devices. However, on the i7, for MobileNetV2 only, int8 quantization is the fastest option. -Finally, for CIFAR-10 on the GPUs, direct convolution is consistently the fastest algorithm in the untuned case, however, on the HiKey for VGG-16, spatial pack convolution is the fastest.
Overall, these examples show how multiple across-stack interactions can both unlock higher performance, but also present the challenge of how to efficiently explore the design space, i.e., choosing DLAS parameters that are likely to give high-performance improvements without significantly increasing the number of experiments required.
Identifying the across-stack problems relevant to DNN acceleration has been explored in other works. For example, Sze et al. give a comprehensive overview of relevant DNN acceleration parameters, but do not simplify this overview into a concise structure, which is critical for efficient discussion and analysis. Accelerating DNNs is the combination of several NP-hard optimization problems, so, at best, researchers can create principled heuristics that try to give the highest speedups, while keeping the evaluation costs low. Our experiments used a modified tensor compiler (TVM) , which helped reduce the cost of DSE by automatically generating code for different combinations of parameters. Techniques and software like this will be increasingly relevant as the number of design choices continues to grow .
In Section 5, we observed many variations across our experiments with a number of non-trivial dynamics emerging. Our first observation is (I) MACs, accuracy, and inference time are not strongly correlated, along with 12 additional observations, (II)-(XIII). These observations are not intended to be definitive, and large changes in the results could be expected by bringing in new techniques from DLAS. However, the key point of this work is that across-stack interactions of machine learning and systems optimizations can be non-trivial, and bringing in additional features may significantly accelerate or impede a given technique. We refer the reader to other characterization works and surveys that highlight cutting-edge techniques from across DLAS .
6.1 Datasets & Problem Spaces
Between datasets, models showed similar trends in terms of accuracy, with the accuracy losses due to compression being higher for ImageNet models. The computational requirements of models for each dataset varied, i.e., CIFAR-10 models have fewer MACs and parameters than ImageNet models (Tables ), which we would expect to have effects on the inference behavior. For instance, (II) tuned sparse CIFAR-10 models were faster than the dense baseline, but this was not the case for ImageNet models. Possible explanations include overheads in the sparse algorithm and data format choices, which could be exacerbated by the larger ImageNet models; and for tuning, both sets of models were given the same number of trials despite ImageNet models being larger.
6.2 Models & Neural Architectures
As a related observation to (I), we note that (III) model size is not strongly correlated with accuracy, however, smaller models are more vulnerable to compression. For example, EfficientNetB0 has a higher baseline accuracy than both DenseNet161 and ResNet50, despite having at most 21% of the parameters. This can be understood as EfficientNetB0 being a more recent model that exploits novel architectural and training techniques to achieve better parameter efficiency. However, other works have shown that if we keep the model architecture the same, then more parameters generally means higher accuracy . For CIFAR-10, we observed that ResNet18 and MobileNetV2 had higher baseline accuracies than VGG-16, despite both having fewer parameters. A similar explanation of using techniques such as residual blocks can explain this behavior. However, as (III) notes, models with fewer parameters were more vulnerable to compression.
Another related observation is that EfficientNetB0 had the highest accuracy drops for int8 quantization out of any model. We understand this to be due to EfficientNetB0's architecture not being amenable to post-training quantization. These issues were highlighted and corrected by EfficientNet-Lite , which removes the squeeze-and-excitation networks and replaces the swish activation functions with ReLU6 activations .
6.3 Model Optimizations
Overall, the best model optimization technique varied. (IV) Weight pruning tended to give better compression ratios and speedups than channel pruning, however, achieved less of its expected speedups. We also observed that (V) quantization's speedup varied by hardware platform and data-type.
6.3.1 Pruning.
As observation (IV) notes, weight pruning was generally faster than channel pruning, however, the former achieved a lower proportion of its expected speedups. There were several cases where a less compressed channel pruning model was faster than a more compressed weight pruning model. However, this appears to come [from] algorithmic interactions, since taking the best variant across algorithms it was rare for channel pruning to outperform weight pruning. Overall, the relative performance of pruned models on the GPUs was worse than on the CPUs. This is because sparse computations are irregular and thus cannot easily take full advantage of the greater number of cores available in a GPU that dense computations can. However, some across-stack techniques can improve sparse performance on GPUs . Since we could not tune sparse computations on the GPUs, we cannot comment how well they would perform if the code further optimized. Other pruning techniques that we did not explore include layer-wise pruning and gradient-based methods, tradeoffs of which are discussed in Blalock et al. . We only evaluated pruning techniques in terms of accuracy and compression, however, as other work has noted, pruning also comes with non-negligible costs due to retraining, which may be a bottleneck to DSE .
6.3.2 Data-type
Quantization. (VI) float16 had a negligible impact on accuracy, when calibrated int8 lost 1.8% and 22.2% for CIFAR-10 and ImageNet models on average, respectively. Excluding Ef-ficientNetB0, ImageNet models lost 3.8% of accuracy on average. With regards to inference time, as observation (V) notes for float16 on the CPUs, we observed a consistent slowdown when compared to float32. This is because the hardware runs float16 computation using software emulation. Although there are savings in memory footprint, the overhead involved in the emulation clearly outweighs these savings, with the differences exacerbated when tuning. For int8 on the CPUs, we generally observed a speedup, since the CPU ISAs can use SIMD instructions to compute int8 computations relatively efficiently.
In the untuned int8 case on the CPU, we observe some cases where we get higher than expected speedups; for instance, many of the GEMM and direct cases on the i7 in Figure . The highest of this is for VGG-16 with CIFAR-10 using GEMM, where we observe a speedup of over 9.2×. We hypothesize that using smaller data sizes is reducing cache usage, meaning that the algorithm has to fallback to slower caches less in the int8 case. However, this advantage is significantly reduced when we tune. We believe that Ansor is not fully exploiting the performance potential and search space of int8, since it was (1) initially designed with float32 computation in mind, and (2) int8 can require a more complex sequence of instructions to be generated to run efficiently, which Ansor does not appear to factor in.
For float16 on the GPUs, contrasting to the CPUs, we observed in general a reduction in inference time on both GPUs, as they have direct hardware support for float16 instructions. Like the 1:21 CPU, we also saw speedups with int8 models, in general, marginally higher than with float16. However, if we take the best times across all algorithms, we did not see any cases where float16 or int8 achieved close to an ideal speedup of 2× and 4× relative to float32.
6.4 Algorithms & Data Formats
We made several observations for algorithms and how they interacted with other layers of DLAS. In the dense case, when tuned, (VII) spatial-pack convolution is generally the best algorithm on the CPU (when tuned), and (VIII) direct convolution is generally the best algorithm on the GPU (when tuned). Observation (VIII) goes against conventional wisdom, where we would normally expect GEMM to be faster on the GPU. However, we should note that we are using a custom implementation of GEMM within TVM, rather than an optimized BLAS library. When the algorithm was not tuned, the best algorithm varied more, especially on the GPUs. For example, with ImageNet on the HiKey, GEMM was faster for smaller models, and direct was faster for larger models. In the sparse case, (IX) GEMM is generally the best algorithm on CPUs, and direct is generally the best on GPU. For quantized models, the best algorithm varied on the CPUs and HiKey GPU, whereas on the Xavier, direct was generally the fastest.
In the evaluation, we used the same convolution algorithm for all layers of a given model, but we could vary the algorithm used per-layer, which could bring significant performance speedups. However, as other work has noted, data format transformation overheads between layers need to be considered . We also did not explore all possible algorithms available, such as Winograd convolution . However, we chose three common algorithms to keep the across-stack evaluation tractable. Other works have explored algorithmic tradeoffs in more detail .
As discussed in Section 6.3.1, (X) for the pruning techniques, we rarely observed the expected performance improvements when compared to the dense implementations. Partly this can be attributed to the inherent overheads of sparse data formats-CSR must store up to three values for every nonzero element. This, coupled with irregular data access patterns means that the sparse algorithms do not realize their full potential. CSR is not the only way to represent sparsity, and alternative data formats (and their complementary algorithms) may provide different tradeoffs . For example, for channel pruning, we could store a list of the indices of pruned channels and store the non-pruned channel data in a dense format. This could allow us to leverage a more "dense-like" algorithm with an overhead for looking up pruned indices. Alternatively, we could use a format called block-sparse row (BSR), which is similar to CSR but represents blocks of sparse parameters, rather than individual weights. This could allow us to reduce the overheads of the sparse storage format, with greater savings with larger block sizes at the risk [of] higher accuracy loss. However, to fully exploit this, we would need to change the pruning method to prune blocks.
6.5 Systems Software (XI) auto-tuned code dramatically accelerates inference time and can change the best algorithm or compression technique.
However, we observed no speedup tuning the Xavier. When testing with server-class Nvidia GPU platforms, we observed speedups using auto-scheduling and the same evaluation code. Our conclusion is that some aspect of the AGX Xavier's software stack was incompatible with Ansor, but we could not detect it. It is well known that auto-scheduling can provide significant speedups , but the search time required is non-negligible. We could not collect auto-scheduled results on the HiKey for the ImageNet models, since they took too long to tune (over 140 hours each). This highlights a key issue with auto-tuning, namely, the cost of search, especially on constrained devices. Approaches that significantly prune the search space and techniques such as transfer-tuning can reduce the search time.
As highlighted in Section 6.3.2, we observed that (XII) Pruned models saw a lower relative speedup when tuned. A more specialized sparse compiler, such as the emerging SparseTIR system , could reduce the impact of these overheads. This was also observed for the quantized models, which may require similar optimization support.
We initially experienced an issue compiling int8 EfficientNet, as TVM assumed that in a multiplication operation only the left-hand operand would be pre-quantized. However, the structure of EfficientNet violated this assumption, which necessitated a bug-fix that we pushed upstream. This highlights that assumptions that systems software make about the properties of workloads may not always hold, especially when novel DNN architectures emerge.
All of the models were defined in PyTorch and evaluated in TVM, however, there are other DNN frameworks available, and the relative performance of different DNN frameworks has been well studied . Other systems-software dimensions to consider are hand-tuned kernel libraries such as oneDNN , cuDNN , or other deep learning compilers such as TensorRT or IREE . Collage can explore varying the backend for different subgraphs of the same DNN, which can bring significant performance improvements.
6.6 Hardware
As expected, (XIII) the i7 CPU was generally faster than the HiKey CPU, and the Xavier GPU was generally faster than the HiKey GPU. Unfortunately, we did not see improvements when tuning on the Xavier GPU. Other aspects of the hardware that could be better utilized include the big.LITTLE architecture of the HiKey CPU (we only leveraged the big cores), hyper-threading on Intel CPU (we ran one thread per core), or leveraging both the CPU and GPU in parallel for the Xavier and HiKey. However, across-stack optimizations would be required to exploit these features properly . We also did not leverage the Xavier GPU's 64 tensor cores in addition to its 512 general purpose CUDA cores. TVM supports tensor cores but requires manual schedule re-design for each algorithm. MetaSchedule can expand auto-scheduler search spaces to include hardware features such as tensor cores, which could ease [investigation of] this dimension.
6.7 Evaluation Methodology
For the experiments, we kept the batch size as 1, took the median of 150 runs, disregarding the first warm-up run. Although this is a common deployment and evaluation scenario, it is important to be aware that this is not the only one, and experimental design should reflect which deployment case is being considered when evaluating models . For instance, for edge deployment, we may expect the batch size to be small, whereas on the cloud it may be large. Increased batch sizes mean increased memory requirements and inference latency, but also potentially higher throughput. For the use of 150 runs, disregarding the first run, there could be deployment scenarios where we are more interested in the performance of these initial warm-up runs, before the cache behavior becomes more regular.