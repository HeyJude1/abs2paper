4 MobiRL: System Design 4.1 The Overview of Our Design
In this work, we propose MobiRL, an intelligent RL-based CPU/GPU frequency scheduling mechanism for mobile systems. MobiRL leverages an RL model to learn how to optimize power and UI smoothness simultaneously by adjusting CPU/GPU frequency. Several challenges need to be addressed for MobiRL to achieve its goal: (1) The state space and action space are complicated, making MobiRL challenging to converge. (2) MobiRL has two optimization goals, i.e., UI smoothness and power consumption, which are difficult to achieve at the same time on mobile systems. Optimizing UI smoothness (needs more power) may negatively affect another goal, and vice versa.
(3) Since the battery power of mobile devices is limited, the ML models used in this work must be low overhead. To address the above challenges, we design a new ML-based approach, a DDPG model customized for frequency scheduling on mobile systems. Our design has a reduced action space and a reward function that optimizes power consumption while avoiding frame drops. Our design can work on mobile systems with a low overhead.
Figure shows MobiRL in a nutshell. MobiRL aims to schedule CPU/GPU frequency to optimize power and UI smoothness simultaneously. A typical RL framework employs an agent to learn how to conduct an action based on the state acquired from the environment and subsequently improves the agent's decision-making by learning from the reward. MobiRL's RL model takes CPU/GPU frequency scheduling actions according to the mobile system's current status. After the 12:8 X. Dou et al. frequency scheduling action, the reward evaluates the mobile system's power consumption and UI smoothness. MobiRL works as a component in the OS.
4.2 Design Details of MobiRL
4.2.1 Environment.
The RL environment, in this context, refers to the mobile system. In our design, it includes a system monitor and frequency controller. The system monitor monitors and collects the mobile system's status. It can collect essential information from the OS during the last frame rendering each time some frames are rendered. The information includes (1) RL model input features (refer to Section 4.2.3); (2) features used for the MobiRL control flow, e.g., CPU/GPU frequency limits; and (3) features used for performance evaluation, e.g., frame rendering time, allowed frame rendering time, device temperature, voltage, and current. In MobiRL, the default setting is to collect this information each time 10 frames are rendered to reduce the feature collection overhead.
The frequency controller conducts and validates CPU/GPU frequency scaling actions. It provides interfaces that allow the scheduler to perform frequency scheduling actions. It manages frequency limits by updating editable configuration files in the CPUFreq and devfreq subsystems. MobiRL controls parameters, including CPU cluster 0/1/2 and the GPU's upper frequency and lower frequency limits, as listed in Table . The frequency limits can be one of the predefined values in Table . During online training, MobiRL may have frequency limit configurations that lead to excessive power consumption or continuous frame drops when it explores in the scheduling space. To ensure reliability, MobiRL identifies these configurations through offline sampling and prunes them (i.e., pruned values in Table ), preventing scheduling actions that may cause serious performance issues. Additionally, MobiRL can be easily extended to manage other resource dimensions, including the number of cores enabled in each CPU cluster, DDR frequency, and so forth.
4.2.2 Agent.
The agent is the DDPG model customized for mobile systems. DDPG is a reinforcement learning model that uses an Actor-Critic framework. We employ DDPG for two reasons:
(1) DDPG is suitable for continuous state space on mobile systems, and (2) DDPG has an Actor-Critic architecture. The Critic provides a stable baseline for the Actor, leading to stable and efficient learning. MobiRL's DDPG model learns to conduct CPU/GPU frequency scheduling actions that can maximize long-term rewards. DDPG has four deep neural networks, i.e., Actor, Critic, Actor target, and Critic target. The Actor network is used to infer a frequency scheduling action given a state S t . The Critic network infers the Q-value given a state S t and an action A t . The Q-value is the expected cumulative reward of conducting the action A t given the state S t . The Actor and Critic target networks have identical structures as the Actor and Critic networks, respectively. Compared to Actor and Critic networks, Actor target and Critic target networks have lower learning rates, which can prevent drastic changes in the action selections during training, enhancing the stability of the model training process. In DDPG, each network (structures for Actor/Critic network) has an input layer, output layer, and two hidden layers. Each hidden layer has 40 neurons. We use this setup because adding more layers does not further improve the performance but incurs higher training overhead. We customize the DDPG model for deployment on a mobile system. In the Actor network and Actor target network, we used softmax as the activation function in the output layer, thereby transforming the frequency scheduling into a classification problem. Doing so simplifies the action space, speeding up the ML model convergence. All other layers use ReLU as the activation function. Actor and Critic networks use Adam optimizer for gradient updates.
As shown in Figure , the control flow of MobiRL's DDPG model has two phases, i.e., learning phase and prediction phase. During the learning phase, we build a DDPG model using TensorFlow on a Linux server. By interacting with the mobile system through adb, we obtain experiences including runtime traces collected by the system monitor, frequency scheduling actions predicted by the model, and corresponding reward values. The neural network parameters are updated based on these experiences. During the prediction phase, we deploy a simplified version of MobiRL on the mobile system for low-overhead frequency scheduling. Experiences obtained during the learning phase can be stored and exported for further model updates. To train MobiRL's DDPG model, the following steps are required:
(1) Interact with the environment. When DDPG performs scheduling, the state S t is fed into the Actor network. The Actor network outputs π (S t ) = A t , where π represents the policy function of the Actor network. Gaussian noise is added to A t based on a mean μ and a standard deviation σ to encourage the agent to explore the environment more effectively.
As the Actor network uses softmax as the activation function in the output layer, A t represents the probability distribution of available actions. Algorithm 1 is invoked to obtain the actual frequency scheduling action. In Algorithm 1, the action with the maximum probability in the softmax output is selected. After the action is conducted, the reward R t is calculated based on the UI smoothness and the power consumption according to the reward function (Equation ( )). The state transitions to S t +1 after the action is conducted. The tuple <S t , A t , R t , S t+1 > is stored in the experience pool for experience replay. (2) Sample from the experience pool. Randomly select a batch of tuples from the experience pool for model learning and gradient updates. The default batch_size is 64, as shown in Table .
12:10 X. Dou et al.
Obtain the actual frequency-adjusting action using the Actor network's output Input: The Actor network's output A 1 max_value ← max (A); 2 max_index ← A.indexOf (max_value); 3 Get the target parameter and scaling direction of the action corresponding to max_index from the action space; 4 if the action is an idle action that does not adjust any configuration parameter then (3) Update the Critic network. The Mean Squared Error (MSE) between the predicted Q-value and the target Q-value is used as the loss function for the Critic network. The gradient of this loss function with respect to the parameters of the Critic network (i.e., the set of weights and biases) is computed and used to update the network's parameters. Specifically, the predicted Q-value is calculated as Q(S t , A t ), where Q represents the action-value function of the Critic network. The target Q-value is calculated as
y t = R t + γQ (S t+1 , π (S t+1 )). Here, γ (γ ∈ [0, 1]
) is a discount factor that defines the weight of immediate rewards and future rewards. A higher value of γ gives more weight to future rewards, and the agent tends to prioritize long-term benefits, while a lower value of γ places more emphasis on immediate rewards. Q represents the action-value function of the Critic target network. π represents the policy function of the Actor target network. (4) Update the Actor network. The goal of the Actor network is to maximize Q(S t , π (S t )), i.e., to maximize the expected cumulative reward of the state-action pair (S t , π (S t )). Therefore, the negative Q-value is used as the loss function for the Actor network. The gradient of this loss function concerning the parameters of the Actor network is computed and used to update the network's parameters. (5) Update the target networks using soft updating. The soft updating blends the parameters of the Actor and Critic networks into the Actor target and Critic target networks, achieving smooth updates for the target networks and avoiding fluctuations in target values. The update processes are as follows:
θ Q = τθ Q + (1 − τ )θ Q θ π = τθ π + (1 − τ )θ π . (1)
MobiRL 12:11 Here, θ represents the parameters in the neural network. θ Q , θ Q , θ π , and θ π correspond to the parameters in the Critic, Critic target, Actor, and Actor target networks, respectively. The hyperparameter τ (τ ∈ [0, 1]) controls the update rate of the target networks.
4.2.3 State.
The state represents the input features of the MobiRL ML model. As shown in Figure , MobiRL captures the system states of the mobile system, including system load, device temperature, cache miss rate, and so forth. The states are inputs of the ML model. They accurately reflect the current status of mobile systems. The Actor network predicts a frequency scheduling action based on the state. In MobiRL, the state includes 20 operating system-related features and 4 TOP-APP-related features, as shown in Table . Operating system-related features include (1) OS performance counters (i.e., CPU load, IPC, and cache miss rate), (2) CPU/GPU/DDR frequency and frequency limits, and (3) device statuses (i.e., device temperature, whether the screen is being touched or not). TOP-APP-related features include the cache miss rate of the process, the CPU load generated by the process, and the ID of the CPU cluster where the main thread and the render thread are running. The features are normalized into [0,1] according to Equation (2):
Feature normalized = Max − Feature Max − Min , ( 2
)
where Feature is the raw value, and Max and Min are predefined maximum and minimum values of each feature, respectively.
4.2.4 Action.
The action represents the frequency scheduling decisions made by the DDPG model in response to the mobile system status. The Actor network outputs a k-dimensional vector A = {a 1 , a 2 , . . . , a k }, k i=1 a i = 1. As the Actor network uses softmax as the activation function, A represents the probability distribution of available actions over the action space. For the eight configuration parameters in Table , we design an action space that contains 17 frequency scheduling actions. For each configuration parameter, there are two actions: one for scaling up and another for scaling down the parameter. Additionally, there is an idle action that does not adjust any configuration parameter. Each time the Actor network outputs A, the action with the highest probability is selected as the chosen action.
A frequency configuration parameter can take a number of predefined discrete values. For example, the upper/lower frequency limits of CPU cluster 0 can take 16 predefined discrete levels ranging from 300 MHz to 1,804.8 MHz. We set the maximum step size to be three levels for a specific frequency scheduling action. Additionally, there is a constraint that the upper frequency limit must be greater than or equal to the lower frequency limit. Actions that violate the constraint are invalid. To avoid invalid actions, we compute the upper and lower bounds for scaling up or down each resource configuration parameter based on the current frequency limit settings and the parameter's counterpart. The counterpart represents another frequency limit of the same CPU cluster or GPU. For example, the counterpart of CPU cluster 0's upper frequency limit is CPU cluster 0's lower frequency limit. Algorithm 1 shows more details.
As illustrated in Figure , MobiRL's ML model can intelligently output frequency scheduling actions that optimize the UI smoothness and power consumption. The actions can be the most appropriate, as the customized DDPG in MobiRL can learn and get the optimal policy. 4.2.5 Reward. The reward function defines the objective of MobiRL. It is designed to minimize power consumption while ensuring UI smoothness. MobiRL schedules each time it receives the system status information the system monitor collects. The reward function is invoked before the scheduling to calculate the reward for the last state-action pair. The reward includes both the current reward and the historical reward. The current and historical rewards are calculated using the same reward function (Equation ( )), but they differ in the data range used for computation. The current reward is calculated using all data from the conduction of the last action up to the present. On the other hand, the historical reward is calculated using all data from the past 3 seconds. The final reward value is the sum of the current and historical rewards, each multiplied by a weight of 0.5. MobiRL has two optimization goals, i.e., UI smoothness and power consumption. To optimize them simultaneously, the reward function is designed as a piecewise function. When there are frame drops, i.e., the frame rendering time t exceeds the allowed frame rendering time t allowed , MobiRL only optimizes the UI smoothness, i.e., r = r UI . MobiRL learns to optimize the power consumption when there are no frame drops, i.e., r = r power . The reward function is shown as follows:
r = r UI if t ≥ t allowed r power if t < t allowed r UI = −α log(t/t allowed ) r power = β(1 − p/TDP), ( 3
)
where t denotes the maximum frame rendering time of all data used to calculate the reward. t allowed is the allowed frame rendering time, which is typically set as 1 s/FPS −1 ms in the industrial track. This is because frame drops may occur when the frame rendering time is close to 1 s/FPS. α and β are the weight parameters of r UI and r power , respectively.
The UI smoothness reward r UI penalizes frame drops. When the frame rendering time exceeds the allowed frame rendering time, i.e., t > t allowed , the ratio of t and t allowed is greater than 1, and r UI yields a negative reward value. The power reward r power encourages lower power consumption. It is designed as 1 minus the ratio of measured power consumption and the thermal design power (TDP) of the processor. TDP refers to the power consumption of a processor under the maximum theoretical load. The TDP of Snapdragon 888 is 5 W. The lower the measured power consumption, the higher the r power value. As in Figure , the rewards are the feedback after actions are conducted. MobiRL can have ideal solutions as the reward function penalizes frame drops and encourages lower power consumption. Moreover, MobiRL can meet user demands with minimum power consumption by using the reward function, avoiding overheating and voltage spikes. Therefore, MobiRL can reduce device heat and increase the processor core's lifetime.
4.3 The Central Control Logic
The central control logic manages the data and control flow of MobiRL. It has a learning phase and a prediction phase.
4.3.1 Learning
Phase. Each time a configurable number (default as 10) of frames is rendered, MobiRL obtains the data collected during the last frame (Section 4.2.1). The collected data is added to the list of runtime traces. Upon receiving the data, MobiRL conducts scheduling and extracts the state S t from the collected data and feeds it into the Actor network to obtain the frequency scheduling action A t . After conducting the action, MobiRL obtains S t+1 . The reward R t is calculated before the next scheduling process. The tuple <S t , A t , R t , S t+1 > is stored in the experience pool for experience replay. After a round of scheduling, MobiRL extracts a batch of tuples and updates the networks (Section 4.2.2).
4.3.2 Prediction Phase.
During the prediction phase, we deploy a simplified version of MobiRL on mobile systems for low-overhead frequency scheduling. We deploy only the Actor network on the mobile system leveraging TensorFlow Lite. When 10 frames are rendered, MobiRL collects the mobile system state using the system monitor. MobiRL feeds the state into the Actor network to predict a frequency scheduling action and conducts it. Therefore, the sampling rate and the frequency of MobiRL's decision-making are six times per second (6/second). In practice, MobiRL's action is prompt enough to handle transient load and user demands. It can quickly schedule CPU/GPU frequency limits to achieve the frequency limit configuration that can maximize the long-term rewards within seconds.
4.4 Implementation
We implement the MobiRL training framework and train the DDPG model on a server running Linux 5.15.0. We also implement MobiRL on the latest real smartphone (Table ) by integrating it into Android's system_server process so that MobiRL has permission to collect system status and conduct frequency scheduling actions. The well-trained model is deployed on the mobile system using TensorFlow Lite v2.5.0. TensorFlow Lite also supports on-device training . But we decided to train the model on the server to avoid the high overhead during the model training. Both CPU and GPU can be used for on-device inference of the model. Moreover, we use the default schedutil and msm-adreno-tz governors for CPUFreq and devfreq subsystems, respectively. The model training hyperparameters are determined using Hyperopt , which can infer the parameter configuration that yields the highest performance by sampling in the exploration space. We use the average reward value after convergence as the objective to minimize and then define a value range for each parameter to be tuned, for example, Actor learning rate ∈ [0.001,0.0001], Critic learning rate ∈ [0.01,0.001]. Then we employ the Random Search algorithm in Hyperopt to sample several configurations in the exploration space. The parameter configuration that yields the highest reward after convergence is used. The training parameters are in Table .