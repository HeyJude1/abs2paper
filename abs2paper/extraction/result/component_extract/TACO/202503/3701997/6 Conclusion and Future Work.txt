6 Conclusion and Future Work
We propose MemoriaNova, a framework that includes two innovative algorithms, BTSearch and GenEFlow, for optimizing memory and inference latency in distributed deep learning on edge devices. The BTSearch method optimizes the cumulative memory overhead of models structured as DAGs. Through meticulous exploration of the operator execution order, BTSearch effectively minimizes memory usage during model inference. This application significantly enhances memory efficiency and enlarges the latency optimization search space. Our experimental results demonstrate that BTSearch achieves up to a remarkable 12% reduction in memory overhead. GenEFlow targets the optimization of communication latency in distributed inference tasks from a holistic model perspective. It strategically configures operator placements by leveraging GAs to minimize communication delays across distributed edge devices and offering a comprehensive search space for model partitioning. Our empirical evaluations indicate that GenEFlow achieves impressive results, with a 33.9% reduction in inference latency. With the popularity of large language models, our future work will consider how to deploy large language models with higher memory requirements in memory-constrained edge devices and optimize their inference performance.