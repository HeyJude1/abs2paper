1 Introduction
The artificial intelligence paradigm has experienced significant advancement and widespread applications across various domains. Deep learning (DL) methods have achieved stateof-the-art results in many machine learning applications , such as object detection, image classification, and face recognition . Traditionally, the inference task of DL models occurs on high-performance cloud servers, necessitating large data transfers and incurring substantial time overhead. To address this challenge, deploying models on edge devices near data sources becomes common . Consequently, researchers explore distributed inference mechanisms that distribute inference workloads across multiple edge devices to mitigate latency . Beyond reducing network transmission load, deploying DL models at the edge confers additional benefits . These include reduced latency, enhanced privacy and security, improved reliability, and offline capability. These advantages make edge deployment an attractive option for various applications requiring real-time or near-real-time processing and decision-making capabilities.
However, inference tasks are often computationally intensive, and the limited resources of edge devices can exacerbate overall latency. For example, in a smart home, the camera processes real-time video data and recognizes visitors. Subsequently, the camera sends the visitor information to the smart speaker, which provides voice announcements based on the recognition results and performs corresponding actions as instructed by the homeowner, such as opening the door or sending an alert. Meanwhile, environmental sensors continuously monitor indoor air quality, temperature, and humidity, adjusting the operation of air conditioners or humidifiers based on the analysis results to ensure a comfortable and healthy home environment. These devices require complex deep learning models for inference, which exceeds the capabilities of a single device.
Although distributed inference has attracted much attention, several challenges remain to be solved. The first challenge is addressing the memory constraint of edge devices during model distribution. Edge devices such as intelligent surveillance cameras , intelligent door locks , smart TVs , and smart speakers typically have limited memory. In contrast, several sources of memory overhead exist when conducting distributed inference. A DL model can be abstracted as a directed acyclic graph (DAG), which means there may be more than one reasonable operator execution order of the model. According to Reference , operator execution order influences the lifetime of intermediate tensors of the model, leading to variable memory overhead. Besides, partitioning a model involves operator partition while an operator's type and partition number cause additional memory overhead. Existing methods like in References and only consider latency optimization, not memory constraints. HMCOS reduces the memory footprint of inference tasks by adjusting operator execution order but only on a single GPU. Moreover, traversing the topological sorting of directed acyclic graphs is a P-Complete (PC) problem mathematically . Efficiently conducting this search remains a challenging problem.
The second challenge lies in determining a suitable model partition configuration to minimize inference latency. Common distributed strategies for model partitioning encompass horizontal, vertical, and hybrid partitioning. We delve into addressing model partitioning issues under the hybrid partitioning strategy, which considers both horizontal and vertical partitioning, along with the interdependence among operators. This process involves considerations of dimension, partition number, and proportions. The partitioning of operators impacts both computing and communication time, thereby influencing overall inference latency. Moreover, the decision on operator partitioning affects the following adjacent operators. Thus, partitioning a model for reduced inference latency presents a complex optimization problem. Unfortunately, existing solutions often provide coarse-grained approximations. For instance, References and address the operator partition problem individually, which may not guarantee optimal results. These methods typically focus on a single operator partition dimension. Additionally, Reference employs an approximation method to transform the optimization problem into a linear program, which introduces errors and diminishes effectiveness.
To address the challenges mentioned above during the optimization of inference latency of DL models on memory-constrained distributed edge devices, we conduct a memory-time cost analysis of operator partitioning in model parallelism and propose two optimization methods, namely BTSearch and GenEFlow. BTSearch graph state backtracking algorithm traverses all topological sorting in a DAG structure model. It guarantees to find the optimal operator execution order of a DL model. The result execution order has minimal overall memory overhead without considering operator partition, which enlarges the search space for operator partition optimization. We apply an efficient pruning strategy on BTSearch. The strategy prunes the branches with no potential for better results according to the state of the computation graph. GenEFlow is a GA-based method aiming to optimize the inference latency while satisfying the memory constraints of the edge devices. We model the partition decision of the whole model as a chromosome and consider different operator partition dimensions, thus constructing a more comprehensive search space. GenEFlow can search for the optimal solution from a global perspective through these designs. Moreover, we use constraint violation parameters to guarantee memory constraints.
Our main contributions are as follows: We analyze the memory-time cost of operator partitioning and operator execution order in model parallelism. Specifically, we examine the available partitioning methods for each operator and their memory overhead, calculating the memory consumption for different partitioning methods to determine the optimal partitioning method for each operator. Additionally, we analyze the impact of operator execution order on memory, finding that adjusting the execution order under memory constraints reduces the maximum memory overhead and increases the available memory space per device. We propose the BTSearch, which employs efficient pruning strategies to optimize the execution order of operators in DL models with DAG structures. BTSearch reduces the overall memory overhead and provides a more extensive search space for optimizing inference latency. (3) We introduce the GenEFlow method, which optimizes inference latency for distributed edge devices without altering the model computation results. GenE-Flow models the model partition decision as a chromosome and employs GAs for optimization. GenEFlow considers two dimensions of operator partitioning and covers a more extensive search space, offering a more comprehensive search space and robust solution than traditional methods. (4) We merge BTSearch and GenEFlow into MemoriaNova and validate it on 11 deep-learning models. Our results demonstrate significant improvements in memory optimization and inference latency reduction. Specifically, BTSearch achieves up to 12% overall memory optimization, while GenEFlow reduces model inference latency by 33.9% in our distributed edge device system.