3 Design 3.1 Overview
This section introduces MemoriaNova, a comprehensive approach designed to optimize DL models for edge devices. Within MemoriaNova, we present two core algorithms: BTSearch and GenEFlow. BTSearch focuses on exploring the computational graph of the target DL model to identify the optimal operator execution order, thereby expanding the search space for GenEFlow. Subsequently, based on hardware specifications and the determined execution order, GenEFlow utilizes the information acquired from BTSearch to optimize the model's parallel configuration. This process aims to minimize inference latency while adhering to the memory constraints of each device. Figure provides an overview of our methodology, illustrating the seamless integration of BTSearch and GenEFlow to achieve enhanced DL performance.
3.2 BTSearch: A Backtracking Algorithm for Optimizing Model Operator Topological
Sorting To reduce the memory overhead from the sequence of operator executions, we bring up BTSearch. BTSearch is a graph state backtracking algorithm that aims to find an operator execution order that minimizes memory overhead and widens optimization opportunities for operator slicing efficiency gains.
The computational graph of a DL model can be represented as G = {V, E}, where V is vertices and E is edges. An edge e i j ∈ E signifies a connection, implying op i precedes op j during inference. Sorting all operators ensures no path from op j to op i , termed topological sort. Computing sorts for a graph is a PC problem, typically requiring exponential time. In the worst-case scenario, it requires exponential time to traverse all topological sorts of a directed acyclic graph. Fortunately, multi-branch DL models typically exhibit a concatenated parallel structure, where the topological structure comprises several small-scale parallel structures. The fact results in a relatively smaller number of possible topological sorts. For ease of algorithm description, the following definitions are provided.
Definition 3.1 (Operator State).
In the process of an inference task for a DL model, the state of an operator is defined as a Boolean variable, indicating whether the operator has completed its computation. For example, b i denotes the state of the operator op i .
Definition 3.2 (Computational Graph State).
In the process of an inference task for a DL model, the states of all operators in the computational graph constitute the current state of the graph, denoted as State G = b 1 , b 2 , . . . ,b N (where N is the number of operators in the computational graph).
We aim to optimize operator execution to maximize available memory during model inference, expanding efficiency optimization opportunities. The evaluation metric Metric(Order i ) sums the memory overhead of each operator in a topological order: The pseudocode for BTSearch is shown in Algorithm 1. BTSearch's input is the computation graph of a DL model, and its output is the optimal topological order under a certain metric condition. BTSearch perform a backtracking iteration on the graph that has not yet started computing. Based on the current state of the graph, all legal next states are derived and recursively processed in sequence. As the main steps, BTSearch initializes the graph state and the current local order first. And then, it calls the backtracking algorithm to obtain the optimal order. BTSearch's backtracking recursive function first determines the recursion exit. If all operators have been executed, i.e., State G = true, true, . . . , true, then it is necessary to check whether the metric value of the currently found topological order is better. If so, then update the current best result. Then, the function returns. Parse the current graph state. Based on the current graph state, the status of each operator, the list of currently executable operators, and the tensor information stored in memory can be parsed. Loop through the current list of executable operators. For each operator in the list, assume the operator is chosen as the next to be executed, add it to the current local order list, and update the graph state. Recursively call the backtracking function with the parameters updated in the previous step. Finally, the regional order list and graph state were restored to the state before the last operator was chosen.
Metric(Order i ) = N j=1 (Mem f ull − Mem op j e ).
The graph state parsing function calculates the list of currently executable operators and the memory overhead based on the current graph state for all operators in the graph that still need to be executed loop through. Identify all directed edges that have the operator as the endpoint; for each such edge, increment the in-degree of that operator by one. Finally, Check the in-degree of all operators, adding operators with an in-degree of zero to the list of executable operators.
Additionally, the input tensors of all operators with an in-degree of zero are set as intermediate result tensors, and the memory overhead of all intermediate result tensors is calculated based on the current graph state.
3.2.1 Pruning Optimization Based on State
Marking. During backtracking, repeated state transitions may lead to the same graph state. As the backtracking is depth-first, if a certain state recurs, then all subsequent iterations from that point have been processed, indicating subsequent local optimal solutions. The graph state updates with each recursive call, enabling the following optimization: Maintain a state marking dictionary outside the function to record encountered graph states and their local metric values. Before the loop, check if the state is recorded in the statemarking dictionary. If the state is recorded, then prune if the current metric exceeds the recorded value; otherwise, continue execution as usual. Finally, update the dictionary after the loop.
3.2.2 Hash Optimization for the Parsing Function.
Even with previous optimization, redundant computations may occur during backtracking. Hash optimization eliminates redundant computations by recording graph states and parsing results in a dictionary. Check if parsing results exist in the dictionary; if found, return them; otherwise, compute and register the results.
3.2.3 Time Complexity Analysis. The algorithm has an exponential time complexity of O(2 n )
for general directed acyclic graphs. In practice, most deep learning models exhibit a topology characterized by a series-parallel graph. In such a graph, it is assumed that the structure consists of N parallel graphs concatenated, with each parallel graph containing M branches and each branch comprising K nodes. After pruning, each serial subgraph is processed only once. Best-case time complexity per subgraph is O(M * K), while the worst-case is O(K M ), and overall complexity is
O(N * M * K) ∼ O(N * K M ).
In practice, with limited branches and operators in serial subgraphs, the algorithm's execution time is acceptable.
3.3 GenEFlow: GA-based Model Parallel Scheduling Optimization Method
To decrease the inference latency of the target model by optimizing the model parallel schedule, we devise GenEFlow, a GA-based method, to optimize model parallel schedules to reduce inference latency. GenEFlow operates in a router-edge devices setup, considering broadcast and point-to-point communication. It abstracts model operator partition optimization as a chromosome configuration. Furthermore, GenEFlow constructs a search space, defines an objective function, and iteratively refines configurations using GAs. It ensures legal configurations and employs a GA Solution to minimize model inference latency in distributed systems.
3.3.1 Search Space Construction.
We optimize the model's slicing configuration using a GA instead of optimizing operators individually. Operators execute synchronously, involving data transfer and computation stages. An operator's execution time is linearly related to its scale. By uniformly partitioning operators, parallel execution time decreases. Memory constraints guide the maximum splits per operator. Memory calculations inform optimal partitioning methods, detailed in Table . The computation of convolutions, typically large, benefits from efficient partitioning, reducing memory overhead.
Chromosome Encoding. Based on the current graph state, k 1 and k 2 are calculated from relevant parameters. If k 1 < k 2 , then partitioning occurs along output channels (cout). Otherwise, it is along feature map height (fmh). Chromosome encoding for all operators' partitioning configurations is necessary to invoke GAs. For a single operator op i , its partitioning encoding vector is defined as follows: The encoding vector needs to satisfy the following constraints:
ì x i = [x 0 , x 1 , . . . , x n ]. (6)
x i ∈ N, i ∈ [0, . . . , n], ( 7
)
x 0 = 0, (8)
x n = lenдth, (9)
x 0 ≤ x 1 ≤ . . . x n , ( 10
)
where lenдth is the size of the operator along the partitioning dimension, and n is the number of edge devices in the system. The partitioning encoding vector assigns tasks to devices based on output tensor indices, which are crucial for constraint calculations. Note that when x i−1 = x i , it signifies that device d i will not be assigned the computation task for the current operator. This characteristic is used in the subsequent calculation of constraint violation parameters.
From the partitioning vector of a single operator, where each partitioning operator corresponds to a single gene in the GA's chromosome representation, the chromosome encoding for the entire model's partitioning configuration can be obtained as
ì X = [ ì x 1 , ì x 2 , . . . , ì x N ]. (11)
Through chromosome encoding analysis, we form a comprehensive search space. It fulfills distributed system memory needs and encompasses varied operator partitioning configurations. This space is denoted as a set as follows:
{ ì X } s.t. ( ),( ),( ), . In Section 3.2, considering the example model, Figure illustrates the relationship between chromosome encoding and model partitioning configuration. With three devices, operators execute in Order1: Conv1, Conv2, Conv3, Sum. In the figure, x i, j denotes the partitioning vector elements for the ith operator. The range [x i, j−1 , x i, j ) assigns computation to the jth device. If x i, j−1 = x i, j , then it implies device j is not involved in operator i computation.
3.3.2 Objective Function.
The GA adopted in GenEFlow is a single-objective optimization GA, and the optimization target is the single inference latency. Therefore, for any given valid chromosome encoding, it must be mapped to inference latency. This mapping is the optimization objective function.
Device Modeling Optimization and Communication. In our distributed edge device system, each of the n edge devices is linked via a router. Two communication methods are employed: point-topoint and broadcast. Point-to-point involves direct communication between two devices through the router. Broadcast sends data from one device, transmitting it to multiple devices through the router. This modeling mirrors real-world scenarios like interconnected smart home devices.
Chromosome Encoding to Inference Time Mapping. The algorithm calculates inference latency by processing operators sequentially in a deep-learning model. Its execution phase is divided into data synchronization and computation phases. The predecessor operators of the operator op i are defined as pred(op i ). For any op j ∈ pred(op i ), there exists an edge e ji in the computation graph G. Similarly, the successor operators of the operator op i are defined as succ(op i ). For any op j ∈ succ(op i ), there exists an edge e i j in the computation graph G. During data synchronization, the algorithm determines the distribution of output tensors from predecessor operators to calculate data transfer amounts. The operator type and its partitioning method have an impact on communication mode (broadcast or point-to-point). This information is encapsulated in the chromosome ì X for inference latency calculation. (i) If op i adopts cout partitioning, then data from op j is synchronized to all devices. The total transferred parameters amount to M out (op j ), incrementing all Comm[j] elements. (ii) If op i uses fmh and op j cout partitioning, then devices need partial feature map data. Data transfer is computed based on feature map indices, excluding portions saved on d k . Transferred data amount: M out (op j ) × C comm /C j f ull × (x j e − x j s ). (iii) If both op i and op j use fmh partitioning, then the data required by op i on d k as input and currently not held by the current device d k needs to be transferred from other devices to d k . As for the transferred data, it is calculated as M out (op j ) × H comm /H j f ull , where H comm = max(x j e − x j s , max(0, x j e − x j k ) + max(0, x j k−1 − x j s )). The communication volume for convolutional operators is depicted in Figure . Cases (a) and (b) represent Case (i), while (c) and (d) correspond to Cases (ii) and (iii). In (c) and (d), the characteristic of convolution determines that there may be duplicated data in Input i , marked by shaded areas. For other scenarios, M need in op j 's output and M hold on d k are computed. Data to be transferred are M need − M hold . Broadcast communication is used if data are required by multiple devices, considering a single transmission's data volume.
Calculation of Data Transfer
Communication Time and Computation Time. According Algorithm 2, the total data communication volume for op i transmitted in device k is CommNum+ = Comm[j] , where op j is the predecessor operator of op i , and Comm[j] indicates the data amount transferred from the ith predecessor operator to device d j . Then, the total communication time of operator op i is calculated as CommTime←CommNum / D.Bandwidth.
We assume that for a specific operator and device, the execution time is linearly related to the size of the input or output feature map. Therefore, a linear function Y i can be used to calculate the computation time of operator op i on device d k as CompTime = Y i (x i,k − x i,k−1 , op i ). Here, x i,k − x i,k−1 represents the part of the operator split on device d i corresponding to the partitioned dimension.
Therefore, the time expense for this operator op i on device k is DeviceTime = CommTime + CompTime, and the longest time spent on each device for operator op i is the time cost TmpTime i of this operator. Summing up, all operators' time yields the total inference latency FinishTime. When calculating the time cost of each operator op i , the communication time CommOpTime i generated by this segment is the communication time of that operator. Summing up the communication times of all operators gives GenEFlow the total communication time.
3.3.3 Constraint Violation Parameters.
We utilize the high-performance GA library Geatpy to implement the optimization iteration process. In the iteration process of Geatpy, The constraint conditions considered are Legitimacy of chromosome parameters; (ii) Legitimacy of the total memory in the distributed system; and (iii) Legitimacy of memory on each device in the distributed system.
Only the chromosomes (model partitioning configurations) that pass all three legitimacy checks are considered legal. Constraint violation parameters define the degree of violation for a specific constraint in the optimization problem. For example, assuming a constraint in the optimization problem is a ≤ b, the constraint violation parameter corresponding to this constraint is a − b. The larger this value, the higher the degree of constraint violation.
Legitimacy of Chromosome Parameters. For the chromosome ì
X = [ ì x 1 , ì x 2 , . . . , ì x N ],
where ì x i = [x i,0 , x i,1 , . . . , x i,n ], it corresponds to the partitioning configuration of the ith operator in the execution sequence. The parameters in it need to satisfy the constraint conditions given by ( ), ( ), , and . The constraint condition is ensured by specifying that the parameters within the chromosome are integers when defining the optimization problem, and there is no need to add it to the constraint violation parameters. Constraint condition (8) corresponds to two constraint violation parameters,
cv 1,i = x i,0 , cv 2,i = −x i,0 . (13)
Similarly, constraint condition (8) corresponds to two constraint violation parameters,
cv 3,i = x i,n − lenдth, cv 4,i = lenдth − x i,n , (14)
where lenдth is the size of op i in the corresponding partitioning dimension. Constraint condition corresponds to n constraint violation parameters,
cv 5,i j = x i, j−1 − x i, j , j ∈ [1, 2, . . . , n]. ( 15
)
Legitimacy of the Total Memory in the Distributed System. In Section 3.3.1, we discussed the impact of the total available memory in the distributed system on the upper limit of the number of partitions in the model partitioning configuration. Assuming the upper limit of the number of partitions for op i is k max , then op i corresponds to a constraint violation parameter,
cv 6,i = n − n j=1 I j − min(n, k max ), i ∈ [1, 2, . . . , N ], (16)
where
I j = 0 x i, j−1 = x i, j−1 1 x i, j−1 x i, j−1 . ( 17
)
Legitimacy of Memory on Each Device in the Distributed System. During inference, each operator's execution on devices must adhere to device memory limits. Given the fixed execution order and result tensor storage on devices, memory consumption per operator on each device is derived from model partitioning configuration ì X . Assume device memory limits as M limits = [M l 1 , M l 2 , . . . , M ln ], where M li denotes the ith device's available memory. For the operator op i , it has a total of n constraint violation parameters on various devices,
cv 7,i j = M e,i j + M o,i j − M l j , j ∈ [1, 2, . . . n]. ( 18
)
The memory consumption during operator execution on device d j is denoted as M e,i j , and M o,i j represents the memory consumed by other tensors on d j during op i execution. Based on the analysis in Section 3.2, it can be inferred that the memory overhead of operators during execution is always greater than or equal to that of the intermediate stages. Therefore, it is only necessary to ensure that the execution phase complies with the memory constraints.
The vector representing the constraint violation parameters for a single operator is given by
ì cv i = [cv 1,i , cv 2,i , cv 3,i , cv 4,i , cv 5,i1 , . . . , cv 5,in , cv 6,i , cv 7,i1 , . . . , cv 7,in ]. ( 19
)
The vector representing the constraint violation parameters for the entire model is as follows:
ì CV = [cv 1 , cv 2 , . . . , cv N ]. ( 20
)
3.3.4 GA Solving.
The model parallel scheduling problem seeks to minimize inference latency by optimizing partition vectors for each operator in the distributed system. GAs are well suited for this nonlinear optimization task. However, conventional crossover operations may disrupt superior chromosomes, affecting overall performance. Therefore, we adopt a single-objective GA with an elite preservation strategy. This approach initializes a large population and computes fitness based on latency. A new population is generated through crossover and mutation operators, preserving privileged individuals. The process continues until convergence or a specified generation limit is reached, resulting in optimized model parallel scheduling.