5 Related Work
Optimizing DL models for deployment on edge devices. Deploying DL models on edge devices poses challenges due to limited resources. Lightweight models like MobileNets , Single Shot Detector , YOLO , and SqueezeNet are designed for edge deployment, utilizing techniques such as filter decomposition and specialized convolution filters to reduce computations while maintaining accuracy. Model compression methods, including parameter quantization, pruning, and knowledge distillation, aim to minimize accuracy loss in existing models. DeepIoT offer pruning methods for IoT devices, enabling immediate deployment on edge devices. Knowledge distillation trains smaller models to mimic larger ones, while Fast Exiting provides approximate classification results by utilizing initial layer computations. Techniques like AdaDeep , and DeepMon combine compression methods to meet the accuracy and resource constraints. These methods aim to reduce model complexity for efficient inference on distributed edge devices while preserving computational integrity.
Distributed DL inference optimization. Deploying DL inference tasks across distributed systems involves optimizing efficiency through various strategies . In the simplest approach, individual model operators are distributed across devices for sequential execution , enhancing throughput via pipeline formation. Guo et al. adopt hierarchical optimization, employing a GA to vertically partition models and reduce pipeline latency.
In scenarios where devices execute tasks serially, pipeline design aims to enhance computational throughput . Alternatively, parallel execution partitions models into sub-models deployed across devices, leveraging internal parallelism for improved resource utilization and reduced latency.
DeepThings , proposed by Zhao et al., adopt a classic model horizontal partitioning approach, dividing the model into independent sub-models to fully utilize each device's computational resources without inter-device data transmission overhead. , which partitions model operators without overlap to minimize computational overhead. CoEdge employs inter-device communication for overlapping input data, dividing each operator execution phase into data transfer and execution phases. However, while efficient, CoEdge's optimization process may lead to suboptimal solutions due to its greedy approach. EdgeFlow extends the theoretical analysis for heterogeneous distributed edge devices by considering data transfer and computation phases during operator partitioning. It converts partitioning problems into linear programming and adopts a greedy approach to achieve local optimality. However, EdgeFlow may need to pay more attention to the impact of operator execution orders on performance, potentially leading to suboptimal solutions.
Optimization utilizing a model's directed acyclic graph structure. The optimization method utilizing DAG structures organizes tasks or dependencies into directed acyclic graphs to streamline computational processes efficiently. IOS allowed parallel execution of operators within stages, employing a dynamic programming algorithm to find optimal execution schedules. However, its coarse optimization granularity limits scalability to distributed devices. HMCOS optimized memory usage by simplifying DAG structures through a hierarchical perspective, reducing memory overhead during inference. AGO partitioned computation graphs into subgraphs, optimizing operator execution efficiency for specific convolution operators. While effective, it is limited to certain convolution types and complex DAG handling. PEFT scheduled DAG tasks onto heterogeneous devices, considering earliest start times and device completion times. However, it must address task division possibilities and may not fully optimize memory usage. Applying PEFT directly to DL models' DAG structure may underutilize device resources and provide suboptimal memory optimization results.