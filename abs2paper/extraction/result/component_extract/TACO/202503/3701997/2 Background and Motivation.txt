2 Background and Motivation
2.1 Operator Partition Methods and Memory Overhead Analysis
2.1.1 Operator Partition Optimization. Operator partition optimization is vital for efficient model inference on edge devices. It breaks down complex tasks into smaller distributable operators across multiple devices, reducing inference latency and maximizing resource utilization. Determining the correspondence among the input data, operator parameters, and output data becomes necessary to accomplish this objective. The convolution operator's partitioning along the feature map's high dimension is illustrated in Figure (a), with no processing done on the channel dimension, remaining consistent with the original operator.
After partitioning, the output tensors are executed on different devices, each storing a copy of the operator parameters (Kernel). The input tensor is partitioned according to the convolution computation rules, resulting in a small amount of duplicate data, as shown in the gray area in Figure (a). Following this partitioning process, the subsequent equation provides the calculation formula for the input data range when partitioning the feature map's high dimension for the convolution operator. If the output tensor's high dimension range is [x s , x e ), then the corresponding input tensor range is given by the following:
[x s × S − P, (x e − 1) × S + K h − P], (1)
where S represents the Stride, P represents the Padding, and K h represents the height of the convolution kernel.
2.1.2 Analysis of Memory Overhead in Operator
Partitioning. Partitioning operators impact computation time and memory. Concurrently, parallel execution reduces computation time but may raise memory overhead. Additionally, partitioning strategy and device setup determine the balance between time and memory. While parallel execution reduces the computation time by distributing the workload, it may introduce additional memory overhead due to data duplication and synchronization requirements across devices. The choice of partitioning strategy and device configuration plays a crucial role in determining the tradeoff between computation time and memory overhead. Figure shows convolutional output channel partitioning, where kernels partition without redundant data. Each device retains a copy of the input tensor. Various partitioning methods result in different memory overheads due to input tensor and kernel memory footprints.
To determine the optimal partitioning method, we perform memory calculations for the obtained operator execution order. We consider different partitioning optimization methods from a memory perspective. The partitioning optimization methods for various types of operators and the resulting memory overhead are shown in Table . In the table, "cout" denotes "Channel out, " and "fmh" denotes "Feature map height. " "len" represents the length of the vector. The operators listed in indicates that partitioning the operator will not incur additional memory overhead. The memory analysis and the handling of the partitioning overhead for convolutional operators are particularly beneficial, given that convolutional operators typically have a large parameter size in DL models.
Taking the convolution operator as an example, we introduce the method for determining its partitioning. Assuming there are n devices in the distributed system, each device has an available memory limit:
M = [m 1 , m 2 , . . . ,m n ]. (2)
The total available memory limit for each device is as follows:
M f ull = n i=1 m i . (3)
For the current convolution operation Conv, memory allocation includes M in for input, M out for output, M ker nel for parameters, and M other s for intermediate tensors. The operator is partitioned into k 1 partitions along the output channel dimension, respecting device memory limits. We have the following:
M other s + k 1 * M in + M ker nel + M out ≤ M f ull . (4)
The current convolution operator is partitioned along the height dimension of the output tensor feature map, with the number of partitions being k 2 . Similarly,
M other s + M in + k 2 * M ker nel + M out ≤ M f ull . (5)
Based on the current operator parameters and the current state of the computation graph, we can calculate the values of k 1 and k 2 and then round them down to yield the final results. When k 1 < k 2 , we adopt output channel (cout) partitioning for the current convolution. Otherwise, we assume feature map height (fmh) partitioning.
2.2 Analysis of Operator Execution Order on Memory Overhead
In DL model inference, the operator arrangement in computational graphs impacts memory usage. Sequential execution causes fluctuating memory footprints, especially in models with multi-branch structures. Variability arises from memory allocation for tensors, parameters, and results. Memory remains constant for simpler models with one input/output tensor. However, complex models with multi-branch structures introduce memory management challenges. Different operator execution orders impact memory overhead, emphasizing the need for efficient topology sorting. Optimization can reduce memory overhead, leading to smoother inference processes. The following example illustrates this process.   The model in Figure For a float32 data type, the memory space for tensor T 0 is calculated as follows:
Mem(T 0 ) = 1 × 3 × 224 × 224 × 4/1, 024 = 588KB.
Similarly, the memory space occupied by tensors T 1 to T 4 is 12,544, 6,272, 6,272, and 6,272 KB, respectively. Based on the earlier analysis of memory overhead during the operator execution process, we divide the entire inference process into several execution stages and interval stages. The execution stage represents the process where an operator is actively performing computations. In contrast, the interval stage corresponds to the period when one operator has completed execution, and the execution of the next operator has not yet commenced. Memory overhead during execution and interval stages is denoted as M e and M i , respectively. The memory overhead analysis for all valid operator execution orders of the example model in Figure is provided in Table . The values in the table round to the nearest whole integer. Taking Order1 as an example, the inference process proceeds as follows: (1) Before the execution of the first operator, only the input tensor T 0 is present in memory, with a memory overhead of M i1 = M(T 0 ) = 588 KB; (2) during Conv1's execution, memory usage is M e1 = M(T 0 ) + M(T 1 ) + M(Conv1 ker nel ) = 13, 139 KB; (3) before the execution of the second operator MemoriaNova: Optimizing Memory-Aware Model Inference for Edge Computing 3:7
Conv2, the intermediate result tensors to be stored in memory are T 0 and T 1 , with a memory overhead of M i2 = T 0 + T 1 = 588 + 12, 544 = 13, 132 KB; (4) during the execution of the second operator Conv2, in addition to the memory space required for Conv2 computation, tensor T 0 needs to be additionally saved. The memory overhead is calculated as ) during the execution of the fourth operator Sum, assuming an in-place addition method where the input and output tensors share the same memory space, the memory overhead is calculated as M e4 = T 2 + T 3 = 6, 272 + 6, 272 = 12, 544 KB; and (9) after the completion of all operators' computations, the output tensor T 4 needs to be stored in memory, with a memory overhead of
M e2 = M(T 1 ) + M(T 2 ) + M(Conv2 ker nel ) + M(T 0 ) = 12,
M i5 = T 4 = 6, 272 KB.
In Order1, the maximum memory overhead is 19,692 KB. Order2 and Order3 are similar to Or-der1, and their maximum memory overhead is 25,376 KB. Hence, optimizing memory usage by adjusting the order of operator execution is crucial in limited memory scenarios. This minimizes overhead, increases memory space, and reduces computation time, especially for intensive tasks like partitioning operators. Efficient topology sorting becomes pivotal in managing memory overhead and enhancing model performance in constrained environments. Therefore, adjusting execution order impacts memory overhead, highlighting the importance of efficient topology sorting for improved performance. Even with similar maximum overhead for Order2 and Order3, differences in local memory overhead exist. Computation time improvement in inference tasks can involve sacrificing memory space via operator slicing. Additionally, the number, method, and ratio of sliced sub-operators cause computation time and additional memory overhead. Adjusting operator execution order under limited memory can reduce maximum memory overhead, increase available memory space, and reduce computation time through operator slicing.