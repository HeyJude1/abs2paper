3 Related Work
Blocksome et al. developed a single-sided communication interface for the IBM Blue Gene/L supercomputer, leading to a threefold improvement in maximum bandwidth . Faraj et al. proposed various enhancements by thoroughly utilizing the interconnection architecture and hardware capabilities of IBM Blue Gene/P, enabling near-peak efficiency in message passing interface (MPI) collective communication on the Blue Gene/P system . IBM developed the LAPI (Low-level Applications Programming Interface), which is a low-level, high-performance communication interface available on the IBM RS/6000 SP system . It provides an active message-like interface along with remote memory copy and synchronization functionality. However, the limited set from the LAPI does not compromise on functionality expected on a communication API. Even worse, the topology mapping library and LAPI are designed for IBM supercomputers, resulting in difficulties in adaptation to applications running on general supercomputers, especially for Graph500 testing on other supercomputers.
To improve task mapping on the Blue Gene/L supercomputer, a topology mapping library was incorporated into the BG/L MPI library, enhancing both communication efficiency and application scalability . This library could also offer scalable support for the MPI virtual topology interface. To tackle the significant challenges posed by Blue Gene/Q's extreme parallelism and scale, Kumar et al. developed the Parallel Active Message Interface (PAMI), a communication library designed to maximize the performance of large-scale supercomputing applications . IBM created the LAPI as a high-performance, low-level communication tool for the IBM RS/6000 SP system . This interface provides features akin to active messages, along with capabilities for remote memory copying and synchronization. While LAPI's limited functionality set might appear constrained, it still meets the essential requirements for a communication API. However, the design of both the topology mapping library and the LAPI is specific to IBM supercomputers, which presents challenges when adapting these tools for applications on other general-purpose supercomputers. This issue is particularly pronounced during Graph500 testing on non-IBM systems.
Unlike the communication enhancements tailored for IBM supercomputers, Shida et al. created a specialized MPI library and low-level communication infrastructure for the K supercomputer, leveraging Open MPI and addressing the tofu topology . This approach, while similar in intent to IBM's solution, is specifically designed to optimize performance for the K supercomputer rather than for IBM's systems. To explore how MPI communication affects Graph500 performance, Li et al. performed a comprehensive examination of MPI Send/Recv and MPI-2 RMA, uncovering several performance bottlenecks. They then introduced an advanced, scalable design for Graph500 utilizing MPI-3 RMA, which enhanced GTEPS and achieved a twofold speed increase on the TACC Stampede Cluster . Despite these advancements, translating these insights and optimizations into a universal communication library applicable to the Graph500 benchmark remains a challenge.