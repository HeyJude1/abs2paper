1 Introduction
C is a widely used programming language, one that is heavily relied upon in many codebases. In 2022, on GitHub, C and C++ accounted together for 4.6M repositories , this being a 23.5% growth compared to the previous year . According to the TIOBE index , C is the second most popular programming language.
4:2 A. Lepori et al. The C language was developed in the 1970s at about the same time as the first microprocessor . Parallel computing only became widespread decades later, driven by the end of Dennard scaling . Hardware efforts to continue growth following Moore's Law past the power wall lead to a renewed focus on parallel computing as a way to continue improving performance . C was not designed with parallel programming in mind, as its inception was much earlier than the explosion of parallel computing. Libraries provide support for parallel paradigmssuch as OpenMP or MPI -but add a difficult-to-manage layer of complexity. The flexibility and low-level nature of C code make debugging challenging, even more so when the code is executed in parallel . This makes parallelizing code a long and complex process.
Frameworks such as Polly and Pluto promise automatic parallelism-these are, however, limited to static control parts (SCoPs). The Intel compiler offers the icc parallel mode that can identify and auto-parallelize several predefined programming patterns. Data-centric methods try to push the boundary even further by leveraging complex dataflow analysis techniques to better understand data movement and dependencies. On modern systems, data movement is the most expensive operation in most programs, concerning both time and energy consumption , and tends to be the biggest bottleneck in computations . Many frameworks and compilers such as HPVM , Halide , Jax , and DaCe leverage dataflow analysis extensively in their pipeline.
The major obstacle in the static analysis of data accesses is, unfortunately, the use of pointers. Though they are an integral part of the C language, they often become a barrier on the road to performance. The Intel C/C++ programming guidelines [2] also suggest to use array notation rather than pointers. This issue can be seen in the two examples in Figure . On the right side of the figure, we can see that the inner loop only has independent data accesses, and modern compilers are able to detect that. But no compiler we tested was able to detect that the outer loop has no data dependency. Every access done with p[k] is different because of the pointer movement in the outer loop. Current analysis methods do not consider this type of access, as it would require complex and expensive pointer tracking, and even then it would require a correlation between the pointer movement and loop iterations.
Our solution aims to address this challenge by tracking data containers (as opposed to pointers) across the entire program. We then split pointers into the data container and an integer index to it. This effectively makes data accesses explicit while preserving execution semantics, enabling better analyzability of the code. A state-of-the-art data-centric compiler can then attempt to parallelize and improve the performance of the code. The implementation of our methods is explained in Section 2. Afterwards, in Section 3, we discuss the limitations and motivation behind them. In Section 4, we will discuss possible solutions to mitigate the limitations, showing some specific transformations handling special cases.
Iterating Pointers: Enabling Static Analysis for Loop-based Pointers 4:3 Fig. . Representation of data access patterns with pointer movement (left) and static accesses with adjunct (right). Annotated with the accessed memory locations and pointer values. In the case of pointer movements, the pointer is both the container and the offset, while with the adjunct the pointer is only the container.
-We introduce a novel method to statically split pointers into a data container and an index to it, creating explicit data accesses to aid further analysis. -We generate a parallel version of the Mantevo HPCCG benchmark, automatically finding all parallelization opportunities the benchmark developers envisioned manually. Our automatic workflow outperforms the manually tuned version by up to 6%. -We automatically parallelize the previously serial PBKDF2 algorithm implemented in OpenSSL and obtain up to an 11Ã— speedup.