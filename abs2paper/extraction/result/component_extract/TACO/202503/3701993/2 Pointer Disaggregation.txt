2 Pointer Disaggregation
We propose a static method to improve the analyzability of pointers that are used as iterators over data containers. We first provide a high-level overview explaining our method. We follow with some notes toward a proof and the description of the technical implementation. Then, we compare our method with previous work. We also provide some examples and results to show the value of our transformation.
2.1 High-level Overview
Pointers are used both as handles for data containers and to iterate over them. By splitting these two semantically different use cases, we intend to improve element-sensitive analysis of pointer accesses. For each pointer, we create an adjunct int variable that represents the offset from the start of the data container. Note that the type of the adjunct should be big enough to address every element plus one in the connected data-container. The goal of the adjunct is to allow iteration over the data container without modifying the handle used to access said data container. We need the adjunct type to be plus one larger than the size of the data container, as with loops containing *p++, we could have the need to get the address after the last element.
Figure shows a high-level example of how the adjunct is used. We can see from the figure that the value of x after both executions is the same. For general code, it is important to note that p[i] is semantically equivalent to *(p + i), as per C99 standard . In Figure , we present a minimal Fig. . Difference in the assembly representation (compiled with gcc 12.2 and -O2) between a code using pointer movement and one with static access after the adjunct is introduced.
example to show the value of introducing the adjunct variable. We provide the resulting assembly to show that when using the adjunct, the compiler is able to correctly identify that p ad j is only used as an iterator similar to i and merges the two, resulting in more compact, efficient code.
The adjunct transformation enhances code analyzability and exposes additional parallelization opportunities. Such transformed pointers are equivalent to arrays, in that the symbol represents a data container that is statically known. The adjunct variable dictates how the pointer is accessed. Because it is an int variable, there are more compiler analysis methods available to track the value. Inside loops a compiler could apply Loop Strength-Reduction (LSR) to simplify addressing computations. Note that this can only be done after the adjunct transformation is applied, as LSR only acts on integer values and not addresses (pointers). If instead the adjunct variable is never used-if a pointer is never moved, for example-then the compiler will be able to optimize away the adjunct with constant propagation, resulting in no performance losses. A summary of the transformation is described in Figure . To note is that the expression p = q x is a combination of p = q; q = q x. Because of this fact, we do not detail this case in the following sections, as it follows from the other statements.
The adjunct type is deliberately a signed integer to replicate the behavior of actual pointer movement. This is because of the non-wrapping behavior when the adjunct becomes negative. It is worth noting that accessing a pointer with a negative offset might be considered valid behavior under certain circumstances. For instance, if a programmer is confident that the memory is initialized within the same program, such as when an opaque library provides a pointer within an array, and the programmer is aware of the underlying structure.
Function arguments and calls undergo minimal modifications. When a function is called, it is treated as a dereference, and we pass the pointer with the adjunct offset applied. The function argument declarations remain unchanged, meaning that the adjunct information is lost after the function call. This is generally not an issue, as subroutines often do not require the additional information. Moreover, this problem is mitigated by the compiler's automatic inlining or by manually annotating functions to force inlining, thereby eliminating the function call.
2.2 Notes toward a Proof
We provide a proof sketch that the transformation showed in Figure does not modify the accesses done with pointers. First, for initialization of a pointer, let p be a pointer to some data. Then, let p ad j Iterating Pointers: Enabling Static Analysis for Loop-based Pointers 4:5 Fig. . Transformation summary to improve pointer analyzability. Both p and q are int pointers (int* p, q), x is an int expression, and is any integer binary operator. Note that the data-type int is only for explanation purposes and can be substituted with any other type. The adjunct-type int can be interchanged with any integer type that fits the size of the data-container.
be a new unique integer variable not already present in the program and initialized to zero when the memory of the pointer is first initialized, i.e., we have p = malloc(...); p ad j = 0, where p ad j was not previously present in the variable definitions. This is a new variable definition without any name conflict, hence it will not influence other parts of the program.
Let S be an arbitrary n length sequence of statements s.t.
:= p + x i |x i ∈ Z, 0 ≤ i < n) .
Then, we define the syntax S; p by using the sequential operator ; with a variable name at the end as the value of p after execution of S. By our definition, we have that:
(S; p) = p + n i=0 x i .
Let S be an n length sequence of statements (note that x i are the same values as in S and in the same order).
S = p ad j := p ad j + x i |x i ∈ Z, 0 ≤ i < n
As before, by definition, we have that
(S ; p ad j ) = p ad j + n i=0 x i .
We can see that
(S; p) = p + n i=0 x i = p + p ad j + n i=0
x i − p ad j = p + (S ; p ad j ) − p ad j .
By using the sequential operator ; we define a program sequence that defines the pointer p, integer p ad j , and executes all statements in S. This represents an arbitrary program from the definition to the access of a pointer p with arbitrary pointer movements in between. We note that (S ; p) = p and (S; p ad j ) = p ad j , as the statements do not affect the evaluated variable. Then, we have that if we substitute any execution p = p + x i with p ad j = p ad j + x i i.e., executing S instead of S, then This means that by substituting S with S and accessing every pointer using p + p ad j as their address, we access the same memory location. Hence, the program will be semantically equivalent. S and S only contain statements referring to p or p ad j for simplicity of the proof. Any other statement could be interleaved including control flow statements. By getting all possible execution paths and removing all statements that do not act directly on p, then we have the same case as S that can be transformed in S .
The proof sketch for the correctness of the pointer assignment transformation can be found in Appendix A.
2.3 Technical Implementation
We will now discuss the algorithm that governs our approach. The algorithms are applied to the AST (abstract syntax tree) to produce the modified code including the adjunct transformation. It is important to note that the whole transformation is source-to-source. The implementation leverages libclang to parse the source code. We keep an adjunctMappinд map that maps pointers to its adjunct. As previously mentioned, the type of the adjunct variable should be an integer type big enough to address each element of the data-container. We mostly use int as it is big enough for most practical purposes, but it could be easily interchanged with bigger or smaller integer types.
For every variable declaration, we check if it is a pointer type. In that case, we append an additional variable declaration for the adjunct variable with type int and initialize it to 0. We keep a mapping between the pointer variables and their adjunct. We analyze every array subscript expression (a[i], where a is the array expression and i is the index expression) and verify if the array expression is in the mapping. In case we have a match, we modify the index expression to sum the additional adjunct variable. Note that we can handle unary expressions of the type *p easily by transforming them to p[0] with a simple transformation (which does not change the semantics as per C standard E1[E2] is equivalent to *(E1 + E2)).
Iterating Pointers: Enabling Static Analysis for Loop-based Pointers 4:7 for all array subscript expression of the form 𝑎𝑟𝑟𝑎𝑦
[𝑖𝑛𝑑𝑒𝑥] do if 𝑎𝑟𝑟𝑎𝑦 ∈ 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔 then 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡 ← 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔[𝑎𝑟𝑟𝑎𝑦] 𝑖𝑛𝑑𝑒𝑥 = 𝑖𝑛𝑑𝑒𝑥 + 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡
Finally, we examine every assignment. Here, we have two cases: p = p x or p = q. For both, the left expression must be in the mapping. In the first case (p = p x) the right side must be a binary operation for which the first operator is the same as the left side of the assignment (p = q
x, p and q must be the same pointer). We then substitute the pointer p with its adjunct (p_adj = p_adj x). In the second case (p = q), we check that the right side is also a pointer in the mapping. Then, we append an additional statement that overwrites the adjunct of the left side with the adjunct of the right (p_adj = q_adj).
for all assignment of the form 𝑙ℎ𝑠
= 𝑟ℎ𝑠 do if 𝑙ℎ𝑠 ∈ 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔 then if 𝑟ℎ𝑠 is a pointer ∧ 𝑟ℎ𝑠 ∈ 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔 then 𝑙ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 ← 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔[𝑙ℎ𝑠] 𝑟ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 ← 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔[𝑟ℎ𝑠] 𝑙ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 = 𝑟ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 else if 𝑟ℎ𝑠 == 𝑝𝑡𝑟 𝑒𝑥𝑝𝑟 then if 𝑙ℎ𝑠 == 𝑝𝑡𝑟 then 𝑙ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 ← 𝑎𝑑 𝑗𝑢𝑛𝑐𝑡𝑀𝑎𝑝𝑝𝑖𝑛𝑔[𝑙ℎ𝑠] 𝑙ℎ𝑠 = 𝑙ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 𝑟ℎ𝑠 = 𝑙ℎ𝑠𝐴𝑑 𝑗𝑢𝑛𝑐𝑡 𝑒𝑥𝑝𝑟
2.4 Previous Approaches
A similar approach was already done specifically for digital signal processing (DSP) applications . Their approach could also be extended to general programs and modifies the source code to remove pointer movement. Our biggest innovation is the use of the adjunct that enables runtime-determined pointer movements. Previous methods only handled static or compile-timedecidable pointer increments. For example, the expression:
if (exp) { x = ptr++; } else { y = ptr + 2; } cannot be handled by previous approaches, as it cannot decide at compile time if the pointer must be incremented by 1 or 2. With our adjunct approach, it can be handled as the same branch just by modifying the adjunct instead of the pointer.
While points-to analysis represents a cutting-edge method widely integrated into modern compilers, its effectiveness in element-sensitive analysis, particularly concerning pointer movements, remains limited. This constraint poses challenges for loop optimization strategies such as vectorization or parallelization, as points-to analysis struggles to grasp element-sensitive aliasing patterns. Left side: example code using pointer movements where the points-to analysis inside GCC is unable to ensure non-aliasing accesses. Right side: code after adjunct transformation, where GCC is able to statically decide non-aliasing using points-to analysis and integer analysis. Note the difference in the CFG of the resulting assembly code.
As illustrated in Figure , conventional points-to analysis, as implemented in GCC 13, fails to statically discern element-sensitive aliasing in code involving pointer movements. However, following our transformation, GCC successfully identifies the absence of access collisions. It is worth noting that even after the adjunct transformation, GCC continues to leverage points-to analysis to recognize the independence between accessed elements. The shown example in Figure includes only a 5-iteration loop to more obviously present the missing vectorization check branch.
Figure illustrates that the transformation is also beneficial for dynamic allocations. In such cases, the base pointer must be stored somewhere to ensure proper memory deallocation. However, even with the base pointer known, the compiler often struggles to fully resolve all aliasing information.
Sui et al. proposed additional analysis techniques for pointer loops, enhancing fieldsensitive and element-sensitive methods. However, their work does not address pointer movements, which remain a challenge for compilers, even advanced ones. Our transformation effectively eliminates pointer movements, allowing existing points-to analysis techniques, including advanced element-sensitive approaches, to be applied with greater efficacy. By incorporating pointsto analysis with our approach, we enhance the depth of insights and enable advanced code optimization. Our methodology involves modifying the source code, rendering it compatible with various analyzers and optimizers. While our primary focus lies in data-centric frameworks, which often struggle with handling pointers, it is essential to note that our approach is not limited to this domain. The modified source code generated through our method seamlessly integrates with any existing or future frameworks, offering versatility and long-term applicability.
Scalar Evolution (SCEV) and other loop analysis methods aim to scrutinize loops for potential optimizations and performance enhancements, including vectorization. However, as demonstrated in numerous examples, current implementations often struggle to identify such opportunities, particularly concerning pointers. Although there have been proposals for improving analysis techniques , finding techniques that are both safe and general enough remains challenging. Our transformation addresses this challenge by providing additional information to the compiler, enabling it to leverage existing SCEV techniques for performance improvements. With Iterating Pointers: Enabling Static Analysis for Loop-based Pointers 4:9 better analysis techniques, more patterns can be matched. Importantly, our transformation does not hinder the adoption of new techniques, as the array access pattern is well-established and commonly utilized, making it compatible with existing analyzers. While advanced analyzers may eventually incorporate pointer movements into their analyses, currently, this remains a significant challenge, and most analyzers do not account for it. Hence, we contend that pointer movement continues to pose a problem, and our transformation provides a valuable solution in the current landscape.
2.5 Practical Results
PBKDF2. To showcase the transformation in practice, we test it on the code snippet from PBKDF2 first presented in Section 1. We expect the inner loop to be vectorized by every compiler. What we want to test is whether compilers are able to discover the independence between the iterations of the outer loop and the effects that the adjunct transformation has on it. The code was run both with normal pointer movements ("no adjunct") and with the transformation ("with adjunct" variables). It was compiled with different compilers: GCC 12.2.2, Clang 15.0.6, and Polly (same Clang version). We also used the DaCe 0.14.1 data-centric framework to analyze and optimize the code with the resulting file being compiled with GCC. The code was instrumented with PAPI to measure the execution time and the number of instructions. For each test, 10 runs were executed and the median is reported, with the 95% confidence interval being reported. For the number of instructions, the error range is negligible (±1, 000 instructions) and not reported. Runs were done with n = 100 and cplen = 100 • 10 6 . As DaCe and Polly produce OpenMP code, we report different values for different thread counts. We compare results with Polly, as it is another compiler that offers automatic parallelization. Note that after the adjunct transformation the code only has array accesses, which is a pattern supported by polyhedral compilers.
Figure shows that the adjunct transformation gets a minimal improvement in single-threaded performance.While with multiple threads the data-centric compiler DaCe was able to identify the parallelization opportunity on loop . Polly, however, is not able to identify the parallel loop either with or without the adjunct transformation. This is the case, because the array-style access of p is non-affine, as it uses the p_adj variable, which is non-affine w.r.t. loop iterators, which is a requirement for the loop to be a SCoP. Polly only acts on SCoPs of programs. Note that as DaCe uses a data-centric IR that utilizes static and scope-defined data containers (for arrays), pointer movements are not supported directly. The adjunct transformation is a way to support pointer movements in the data-centric IR without the need to change its inner workings.
The number of instructions (Table ) neither increase nor decrease with the adjunct. As Polly is a collection of transformations on the LLVM-IR that is compiled using Clang, the number of instructions is identical (Polly was not able to find any SCoPs, hence no transformations were  applied). Surprisingly, DaCe has the same number of instructions as GCC. Even if it uses GCC to compile to machine instructions, this is unexpected, as DaCe applies multiple transformations to the code. This can be explained, as even after the transformations, the computation is mostly the same only with additional annotations for parallel execution. This proves the value of the transformation, but also that the transformation benefits compilers with data-centric IR the most: They can leverage the improved analyzability of pointer movements. Data centric-paradigms otherwise represent pointers as indirection that is challenging-or even impossible-to handle. All compilers see a minimal improvement, but they are less able to capitalize on the improved analyzability.
Another instance where pointer movements within loops are prevalent is in compression algorithms. As an illustration, we applied our transformation to the compression function of the Lempel-Ziv-Oberhumer (LZO) algorithm . This algorithm is commonly utilized within the Linux Kernel for file-systems and memory, supporting live compression/decompression operations. Notably, it finds applications in BTRFS , SquashFS , initramfs , zram [8], and zswap .
Following a similar approach to previous benchmark studies of the LZO algorithm , we executed multiple iterations of the algorithm, ensuring each run involved significant computation without increasing memory usage. The compression was performed on a 1 MB file with 2,000 iterations and a block size of 256 KB. As before, we report the median with a 95% confidence interval using the same compiler versions. We compared the original implementation compiled with GCC against DaCe with the adjunct transformation. It is noteworthy that the DaCe code was executed with a single thread, as no significant parallelization opportunities were identified.
Our results (Figure ) indicate a slight improvement, thanks to the additional vectorization opportunities provided by the transformation. However, achieving more substantial improvements, along with potential parallelization opportunities, proves challenging due to loop-carried dependencies, which would require an algorithm redesign. Nevertheless, in compression algorithms, the adjunct transformation can facilitate modest yet discernible enhancements in runtime performance.