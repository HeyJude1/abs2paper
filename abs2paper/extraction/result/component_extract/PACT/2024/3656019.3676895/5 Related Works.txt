5 Related Works
This paper proposes a new pre-trained multi-modal code representation technique for LLVM IRs. For most source code based optimization tasks, analyzing code can provide pointers to the pertinent optimizations. In fact, most compiler optimizations are code dependent. Therefore, a suitable code representation technique is also essential for using deep learning (DL) to make optimization decisions in HPC. To this end, several code representations have been proposed , which have been used to good effect for optimization tasks such as CPU/GPU device mapping, thread coarsening factor, loop vectorization, etc. to name a few. Preliminary works in this field such as , focused more on lexical tokens which often fails to capture code semantics. The next generation of representational learning works leverage LLVM IRs to make semantic features available to DL models. However, the embeddings generated by these often require advanced modeling techniques such as GNNs for each individual task. In contrast, for downstream tasks, our approach can leverage transfer learning to generate learned embeddings that can be easily modeled with simple MLP layers to get better results than these.
An alternative to DL-based auto-tuning is to use non-neural network based machine learning approaches. Several works have used ML for a variety of tasks. propose machine learning based approaches for auto-tuning OpenMP applications. Artemis is another work that performs automatic parameter tuning using machine learning. ytopt , BLISS are examples of learningbased tuners that employ Bayesian optimization for online tuning tasks. These approaches are often domain or application specific. Although often faster than search-based alternatives, these do need multiple code executions to identify good performing parameters.
Studies highlighted so far in this section were all proposed as means to improve upon traditional search based auto-tuning. Works such as ActiveHarmony , OpenTuner have leveraged several search space optimization techniques to reduce the auto-tuning overhead compared to brute-force tuning. These optimization techniques include Hillclimbers, random search, Nelder-Mead, and many more. However, due to their sampling overhead, works such as ytopt and BLISS were proposed to reduce tuning overhead.
DL-based approaches, including ours, further help alleviate such overhead by making predictions without having to execute applications. This helps with configuring commonly used parameters across applications, without having to devote significant resources to the tuning process.