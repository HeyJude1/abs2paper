3 MIREncoder
Most source-code based performance optimization tasks in HPC usually involve compilable languages such as C, C++, CUDA, and so on. A large number of these languages can be compiled and optimized using the LLVM infrastructure. LLVM IRs are a portable, high-level assembly language that can be optimized with a variety of transformations over multiple passes. It is fairly simple to extract IRs from source code such as C, C++. IRs generated from source code are usually devoid of most stylistic choices and redundant code. This is why we choose to work with IRs for performance optimizations. Figure shows a high-level overview of our approach.
For the first modality, we first tokenize the input IRs into meaningful "tokens" before they are mapped to an embedded dimension. Our approach then learns the embedding of the IR instructions after splitting them into sub-words. For the second modality, the IRs are first converted to dependence graphs that include in them data flow, control flow, and call flow information that represents the semantic information in the source code. These two modalities are then passed into the modeling pipeline either for pre-training or inference. The following paragraphs outline our pipeline.
3.1 Tokenization
Simply put, tokenization is the process of breaking down a piece of text into smaller units called tokens, and assigning a numerical value to each token. A deep learning (DL) model does not understand text or images in its raw form. It needs to be represented as numbers for the model to make sense from it. This is why tokenization is extremely important for such works. In this paper, our tokenization process follows the same approach taken while designing and training the BERT model. However, the pre-trained BERT tokenizer readily available online is trained on natural language (NL). However, source code (IRs in our paper) is more structured than NL, and quite possibly has fewer "words". Thus, we had to train our tokenizer from scratch. We initially collect a large set of IRs by compiling programs in existing datasets into their LLVM IRs. For training the tokenizer, we have used C, C++, and CUDA code from CodeNet , HPCorpus , and LS-CAT . We first define a set of special tokens to handle unknown inputs, and a token that will be used during Masked Language Modeling. 10, 000 unique programs are randomly selected and compiled into LLVM IRs. These are then passed through a WordPiece tokenizer, as done in BERT, and trained to generate a learned vocabulary. BERT uses a sequence length of 512. However, for the sake of simplicity and faster training, we limit the sequence length for each encoded IR statement to 64. Increasing the sequence length might improve results, but the aim of our work is to extract features from IRs, rather than have code generation capabilities. Thus, such an approach might be sufficient for performance optimization tasks, as we will show later. [SEP]'] In Figure , we show an example of the tokenization process with our trained tokenizer. For this example, we select an IR statement from a file that was not used to train the tokenizer. We feed the statement to the tokenizer, which outputs a sequence of numbers (input ids). This is what a DL model will work with. To show that the encoding is correct, we decode the tokenized input ids to show that it is exactly the same as the given IR input, with a few minor but important differences. As shown in Figure , the outputs are in array format, as the tokenizer decodes each input id individually. The array includes a '[CLS]' and a '[SEP]' token at each end. The '[CLS]' token is used to denote the class of the input, if applicable, and the '[SEP]' token is used to separate two statements in the same input. The upper case alphabets in the inputs have also been converted to lower case to make the sequences case-insensitive. If we remove the first and last tokens in the array, and join the elements, we end up with the same output as the input, which shows the success of our tokenizer training process.
3.2 Graph Generation and Pre-Processing
Several works ( ) have outlined that simply looking at source code as a stream of lexical tokens is not sufficient to represent code. Modeling IRs only as stream of tokens does not provide enough details about the structural properties of the program. Code structure can highlight dependencies in source code. It can show the flow of execution in source code, or can also show dependencies between variables. Given that such dependencies are sparse in nature, a graph seems to be an appropriate data structure to represent such structure and dependencies. The dependencies also highlight the meaning of a source code. The sequence of execution or the control flow, how the variables are dependent on each other or the data flow and the function call stack in a program are indicators of the underlying semantics of source code. Prior literature has used such structural and semantic information to good effect. We build on these ideas and work with graphs generated from IRs as the second modality. These graphs are generated with a tool called PROGRAML . The generated multigraphs contain data-flow, control-flow, and call-flow dependencies in them. During pre-training, these graphs allow our model to extract semantic and structural features from source code (IRs). This is necessary as code structure and semantics should dictate the performance of an application/kernel. An example of such a graph is shown in Figure .
The nodes in our generated graphs (example shown in Figure ) contain IR statements. These form the node features in our graphs. Node features are used by Graph Neural Networks (GNNs) in forward and backward propagation during training. However, DL models cannot use such statements directly. Therefore, we use the trained tokenizer described in Section 3.1 to convert the IR statements into sequence of numbers. These become the node features and are used in the pre-training process by the GNN layers.
3.3 Pre-Training MIREncoder
In this section we outline the pre-training process of MIREncoder. The quality of a pre-trained model usually depends on the pretraining tasks considered. For this work, we have used three pretraining tasks; one task that targets each modality, and another one that is used to explicitly link together the two modalities. Namely, the pre-training tasks are Masked Language Modeling, Graph Auto-Encoding, and IR Code-Graph Matching.
3.3.1 Masked Language Modeling. Masked Language Modeling
(MLM) is a widely used pre-training task in natural language based pre-trained models. It is also commonly used as a pre-training task in studies working with programming languages such as CodeBERT .
MLM for this paper can be defined as follows. Given an IR statement ùëê as input, we select a random set of positions ùëö ùëê that will be masked out; i.e. replaced with the '[MASK'] token. Following ideas presented in , we mask out 15% of the input. The task in this pre-training step is for the model to successfully predict the masked out words from the adjoining context. This is a self-supervised approach as the model is expected to produce the correct outputs without any explicit labels. Throughout the training process, the model is updated based on the difference between the predicted words and the actual words in the statements.
However, it is worth noting that the '[MASK]' token does not appear during the downstream tasks. To overcome this, as done in , we perform the following steps:
‚Ä¢ Select 15% of the token positions at random.
‚Ä¢ Randomly replace 80% of the selected positions with the '[MASK]' token. ‚Ä¢ Replace 10% of the selected positions with a random token.
‚Ä¢ Keep the token unchanged for the remaining cases.
These steps help the model learn the meaning of a word in the context of a statement, and not assign a single meaning to a word. Also, not including the '[MASK]' token in each statement during pre-training ensures that the model does not always expect that token. For this pre-training task, we use transformer layers with attention mechanism for improved training.
3.3.2 Graph
Auto-Encoding. Graph Auto-Encoders (GAEs) like traditional auto-encoders also aim to reconstruct the given inputs. The aim of this pre-training task is for the model to produce a learned low-dimensional embedded representation from the IR graphs during the downstream tasks. During pre-training, our model setup follows the widely used encode-code-decode setup. An input graph is first fed through GNN layers (Graph Convolution Layers or GCN ) to produce node embeddings in a two-dimensional latent space. This forms the encoder part of the network. In the decoder part of the network, the aim is to reconstruct the graph from the low-dimensional encoded form. The aim is not to reconstruct the original nodes, but to reconstruct the adjacency matrix identical to the input graph through an inner product between latent variables in order to understand the structure of the graphs. Now the multi-graphs used in this study have three sub-graphs in them denoting control-flow graphs, data-flow graphs, and callflow graphs. However, it is quite difficult to auto-encode graphs with multiple edge types. Therefore, we tweak the training process slightly by extracting each sub-graph from the IR multi-graph, and train the auto-encoder for each of the three sub-graphs. But, we do not train the model thrice. The modeling and the loss calculation phases are updated to work with the node features and adjacency matrices of each sub-graph. The loss is back-propagated as an aggregation of the difference in graph reconstruction of each subgraph. There are two main benefits to this: i) calculating the loss and back-propagating over the whole graph instead of each sub-graph allows the model to improve its learning over the whole graph and enables it to implicitly learn the relations between the three types of semantics in the graphs (control-flow, data-flow, call-flow), ii) it improves overall training time when compared to training three separate GAEs, one for each sub-graph.
3.3.3 IR-Graph Matching.
Here, we propose a novel pre-training task IR-Graph Matching to link the two modalities together. The modalities considered in this paper have different data structures, one being a sequence of tokens, the other being a graph. Intuitively, it might be difficult for the model to understand how these two modalities are linked together, and by extension, difficult to link the syntax and structure.
Therefore, we propose this pre-training task where the aim is for the model to correctly identify if the code sequence and the code graphs are from the same IR source. We setup this as a binary classification task, where the inputs are the code sequences (ùëÜ) and the code graphs (ùê∫). Positive and negative samples are automatically generated as data pairs to train the model. Positive samples are those where ùëÜ and ùê∫ are from the same IR, while the negative samples are those where the graphs and sequences are from different IRs. Negative samples are selected in 50% cases by randomly selecting a different IR from the dataset. The code graph of the negative sample is paired with the code sequence to create the negative data pair.
As outlined in Section 3.3.1, the Masked Language Modeling task is performed on IR statements. However, in this task, we need to work with whole files to match text in IR files to the corresponding graphs. Although embedding an IR statement/instruction to a sequence of length 64 might work, embedding a complete file with a large number of statements to a sequence of length 64 will not provide enough information to the model. Therefore, we embed each statement in the file, and then aggregate all the vectors. The aggregated input and the generated code graph with the embedded node features (Section 3.2) are then trained together as a binary classification problem. The transformer layers used in Section 3.3.1 and the GCN layers used in Section 3.3.2 are reused to model the code sequences and the code graphs. Their outputs are concatenated and passed through linear layers with binary cross-entropy used for the loss calculations.