1 Introduction
The complexity, scale, and heterogeneity of HPC hardware has increased significantly over the past several years improving performance over traditional multi-core systems. However, this has also opened up new opportunities of performance optimizations. Performance engineers and application developers devote considerable time in trying to tune and optimize hardware and software knobs. However, it is extremely difficult to adapt to a constantly changing landscape. Automated techniques are thus necessary to help optimize performance of HPC applications. Prior Works. A large chunk of performance gains for parallel applications come from compiler optimizations, such as those seen in LLVM and GCC. Although such optimizations are painstakingly designed, it might not work in all cases due to the variety of applications seen in HPC. In addition to compiler-driven optimizations, runtime performance tuning by online auto-tuners also help identify configurations/parameters that might often be non-intuitive. Although this improves performance, it comes with significant tuning overhead.
Machine learning (ML) based techniques have also been widely used for such performance optimizations. Several works have used ML to model handcrafted features for specific tasks . These handcrafted features are not universal and might not be suitable for other optimization tasks. To overcome these shortcomings, studies based on code representational learning were proposed. Most of these works proposed a means of representing source code in a way understandable by machine learning models. Various works designed representations on top of source code for tasks such as variable misuse and method name prediction. However, such representations put a lot of emphasis on stylistic choices in source code, are language dependent, thus are not ideal candidates for performance optimization tasks of compilable source code. Our proposed approach can, on the other hand, work with multiple languages as shown later in Section 4.
These aforementioned representations are also not adept at capturing program dependencies. Thus LLVM IR based approaches have been proposed. Several works have outlined IR-based code representations for downstream optimizations. However, these are dependent on manual design choices and heuristics. Additionally, these representations usually need complex, resource intensive modeling techniques for each downstream task and might increase the barrier to entry for new researchers. Working with self-supervised pre-trained models and using transfer learning for downstream tasks might help alleviate such shortcomings. This is our aim in this work.
To better represent source code/IRs, we believe modeling both syntax and semantics are equally important. And modeling each as separate modalities seems logical. However, representing source code as each such modality, and re-training from scratch for each target task adds complexity and increases resource requirements. Therefore, we propose an IR-based pre-trained encoder for performance optimizations. This allows us to remove dependency on individual programming languages and target optimizations on both CPUs and GPUs with the same pre-trained encoder. Our Contributions. In this paper, we have proposed an IR-based self-supervised multi-modal pre-training approach (MIREncoder) with the aim of generating encodings/features for downstream tasks. Unlike prior code representations, our pipeline is completely self-supervised and only needs an LLVM IR as input for both pretraining and target optimization tasks. The IR statements in the input files are modeled to extract syntactic features during the pre-training process. This represents the first modality in our pretraining pipeline. The input IRs are also converted to multi-graphs that includes data-flow, control-flow, and call-flow information. This forms the second modality of our approach.
MIREncoder employs three pre-training tasks. The first modality, or IR statements are pre-trained on the task of Masked Language Modeling (MLM) with a Transformer based model. MLM is widely used in pre-training deep learning approaches with code or text generation capabilities. The second modality, or code graphs, are pre-trained with an auto-encoding task (Graph Auto-Encoder), where the aim is for a Graph Neural Network (GNN) based model to reconstruct the input graph. To the best of our knowledge, this study is the first to pre-train a multi-modal encoder using Transformers and GNNs to model individual modalities for parallel code. We also propose a new pre-training task to link the two modalities. We design a pre-training task to match the code graphs to the tokenized IRs (IR-Graph Matching). This allows our pre-trained model to better understand how the IR text translates to its corresponding graph, thus implicitly allowing the model to understand and link the syntactic, semantic, and structural aspects of the input IR.
We will show in later sections that the features/embeddings generated by our pre-trained model helps us match or outperform the state-of-the-art task specific approaches. Our MIREncoder-based embeddings lead to accuracy of upto ≈ 94% for CPU/GPU device mapping, speedups of upto 1.3×, 1.32×, ≈ 3× on thread coarsening, loop vectorization, and OpenMP paramter tuning tasks. Our predictions also reduce error rates by upto ≈ 40% and ≈ 70% over the state of the art for NUMA/Prefetcher optimizations, and tuning thread block sizes for CUDA code respectively.
To summarize, the contributions of this works are as follows:
• A multi-modal IR-based pre-training approach for source code representation. • A novel pipeline that aims to i) model IRs as streams of lexical tokens with transformers, and ii) as multi-graphs with GNNs, to extract and understand syntactic, semantic, and structural features.
• A novel pre-training task, IR-Graph Matching, to link the two modalities and help the model relate syntactic, semantic, and structural features. • Extensive experimental evaluations on six downstream tasks, including CPU/GPU device mapping, thread coarsening, loop vectorization, OpenMP parameter tuning, NUMA/ Prefetcher optimization, and tuning CUDA code with thread blocks, with superior results over state of the art. • Analysis of the importance of each modality and the overheads of our pipeline.