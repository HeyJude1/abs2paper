4 Experiments
In this section, we outline the experiments undertaken to show the strength of our approach. We test our pre-trained model on six downstream tasks different from each other. For each task, we work with metrics used in prior works for evaluation. Experimental setup and evaluation metrics are outlined in more detail in the corresponding sections.
A few things, however, are common for all experiments. For each downstream task, the pre-trained model is not fine-tuned. The pretrained model is set to inference mode to generate embeddings for input IRs. A few trainable linear/MLP layers are added to the pretrained model to perform task specific training. For downstream tasks, only these final layers are trained which substantially reduces the optimization/tuning overhead. In the following sections, we outline each downstream optimization task performed in this paper, and compare and contrast our results with the state of the art.
4.1 Heterogeneous Device Mapping
Grewe et al. proposed the device mapping task to map OpenCL kernels to the CPU or GPU. This task has been widely used to evaluate the performance of code representations. We also use this task to evaluate the effectiveness of our approach and compare against the state-of-the-art results.
Dataset. We use the dataset published by Ben-Nun et al. for this experiment. It has 256 unique OpenCL kernels from seven benchmark suites comprising of AMD SDK[6], NPB , NVIDIA SDK[17], Parboil , Polybench , Rodinia and SHOC . The data size and workgroup size were varied for each kernel to obtain a labeled dataset with 670 CPU or GPU-labeled data points for each of the two devices, AMD Tahiti 7970 and NVIDIA 970.
Baseline. We have compared the results of our approach with prior works with the same dataset. Prior evaluations were presented in terms of accuracy and performance improvements (speedups). We have also adhered to these metrics. For analysing speedups, we use the same static mapping baseline proposed in .
Results. We use our pre-trained model to encode the available IRs, and perform the classification experiments. The MIREncoder pipeline is first used to embed each IR statement in a file. The generated embeddings are then aggregated to encode the first modality. For the second modality, the IRs are first converted to graphs as outlined in Section 3.2, and are fed through the Graph Auto-Encoder (GAE) layers to encode the graphs. These two sets of embeddings (sequences of vectors) are passed through three linear/MLP layers to train and validate the model. As done in prior works, we also add transfer and workgroup sizes from the dataset to the feature set before passing the feature set onto the linear layers. Following techniques used before , we have used ten-fold stratified cross-validation to evaluate our results.
Our experimental setup leads to state-of-the-art results in identifying the correct device. We achieve accuracy of 93.7% and F1-score of 0.94 in identifying the best device on the NVIDIA GPU. On the AMD GPU, we achieve accuracy and F1-score of 93.6% and 0.92. We see that our approach is better or equivalent in all cases compared to prior works in literature. The accuracies are shown in Table , and the numbers in parenthesis shows the improvement in accuracy by using MIREncoder over prior works.
The model predictions also lead to significant performance improvement over static mappings. On the NVIDIA 970 system, our approach leads to speedups of 1.28√ó compared to oracle speedups  of 1.34√ó. The oracle speedups are calculated by analyzing the execution time on the best device and comparing it to the static mapping baseline. On the AMD Tahiti system, our predictions lead to speedups of 2.24√ó versus oracle speedups of 2.39√ó.
4.2 Thread Coarsening
Thread coarsening is used to increase the work done by a single thread by fusing two or more concurrent threads. Thread coarsening factor (TCF) corresponds to the number of threads that can be fused together. Selection of an optimal TCF can lead to substantial improvement in performance on GPU devices and a naive coarsening could lead to slowdown. Due to differences in architectural characteristics across devices, a TCF that gives the best speedup on one GPU might show degraded performance on another GPU . As an example, nbody kernel has a higher degree of Instruction Level Parallelism and can be better exploited by VLIW-based AMD Radeon than SIMD-based AMD Tahiti .
Dataset. In this experiment, we follow the experimental setup proposed in and reused in to predict the best thread coarsening factor from {1, 2, 4, 8, 16, 32}. We use the dataset provided in , which consists of 68 data points from 17 OpenCL kernels on 4 different GPUs, namely AMD Radeon 5900, AMD Tahiti 7970, NVIDIA GTX 480, and NVIDIA Tesla K20c. These include kernels from AMD SDK[6], NVIDIA SDK[17] and Parboil benchmarks.
Baseline. As done in prior works , we evaluate the predictions from our model in terms of performance improvement/speedups over default coarsening behavior. The results from our approach has been compared against prior works on this dataset.
Results. For this experiment, we pass the input IRs through our pre-trained encoder as before to generate the embeddings. The embeddings are then passed through linear layers to train and validate the best thread coarsening factors. Similar to prior works on this task, we also perform leave-one-out cross validation and report the geometric mean speedups across all folds in Table . We observe that our approach performs better in all cases. In Table , speedups are presented for each device included in the dataset.
4.3 Loop Vectorization
Modern compilers can automatically detect when loops should be vectorized so that multiple iterations of the loop can be performed together. When compilers vectorize loops, it must determine the number of instructions to pack together and the interleaving level (stride). This task was proposed in as a potential candidate for DL-based optimization. Modern compilers allow users to select and define the vectorization factor (VF) and interleave factor (IF) to control the loop vectorization process. However, manually evaluating and testing all possible combinations might not be feasible, especially for a large number of applications. To this end, we propose a MIREncoder-based static loop vectorizer.
Dataset. We define a search space based on ideas in by considering pairs of VF and IF and execute them to create a dataset. Their definitions are given below in Equation ,
ùëâ ùêπ ‚àà [2 0 , 2 1 , ... ùëÄùê¥ùëã _ùëâ ùêπ ], ùêº ùêπ ‚àà [2 0 , 2 1 , ... ùëÄùê¥ùëã _ùêº ùêπ ], (1)
where we set MAX_VF and MAX_IF to 64 and 16 for the architecture under test (Intel Skylake). We reuse the set of kernels collected in and execute them with each (VF, IF) pair to create a dataset of kernels, (VF, IF) pairs, and their runtimes. The training and testing set were defined separately in and we follow the same setup here as well. Overall, we collect more than 273ùêæ samples for training. We label the kernels with the best (VF, IR) pair by selecting the vectorization/interleave factor with the fastest runtime. For the test set, we perform the same steps to create the test set.
Baseline. For this experiment, we select the default LLVM Loop Vectorizer as a baseline and evaluate the predicted performance with respect to this. We compare our work with Neurovectorizer . This paper first proposed this task as suitable for DL-based tuning. They used inst2vec embeddings with reinforcement learning for their experiments.
Results. We follow the same steps as before in this experiment as well. We pass the IRs through the pre-trained model to generate the embeddings. The embeddings are then passed through the trainable MLP layers to train and test the model. Both vectorization factor and interleave factor can be varied during compilation. Therefore, we depend only on the compiled IR for training and testing. We train our model on the training data collected on our Intel Skylake server, and test it on the collected test set. From our experiments we see that MIREncoder-based vectorization leads to mean speedups of ‚âà 1.32√ó over LLVM vectorization heuristics. We repeat the same experiments as done in on the same Skylake server and observe ).
4.4 Tuning OpenMP Runtime Configurations
OpenMP is one of the most widely used shared memory programming models. It is mostly used to parallelize sequential code by inserting pragmas. Dutta et al. in , proposed a GNN-based tuner (PnP Tuner) for identifying the best OpenMP parameters for improving performance. They also used power limits to increase the size of the search space and evaluate the impact that limiting power has on OpenMP applications. We build on ideas presented in that paper to set up our own tuner based on MIREncoder. Dataset. As in , we work with 25 applications from the Polybench benchmark suite. We define the search space as done in (Table ) with 504 configurations. We also modify the data sizes used to run these applications by changing compile time options provided by the benchmark suite. We use two input sizes for the purposes of evaluation. For each application, input size, and parameter set (power limit, # of threads, schedule, chunk) we compile and execute the application to collect the runtimes and generate a dataset of 25200 samples. We also collect the runtimes for each of these applications when run with default OpenMP settings (all threads, static scheduling, compiler defined chunk sizes) at Thermal Design Power (TDP). We collect this data on a 64-core Intel Skylake system with a TDP of 150ùëä .
Baseline. The metric of choice for evaluating the performance of our MIREncoder-based tuner is speedups as done in . We calculate the speedups with the default OpenMP configurations at TDP as the baseline and compare the performance of our predicted configurations with those predicted by the PnP tuner, the current state-of-the-art for this experiment. In fact, PnP tuner also works with graphs generated from IRs. Their approach, like ours, also aims to model control and data flow in a program with the help of GNNs. PnP tuner uses RGCN (Relational Graph Convolutional Networks) as the GNNs of choice.
Results. Following the approach in , for this set of experiments we consider application speedups instead of performance improvement of individual OpenMP loops. Also, the OpenMP parameters were modified at runtime. Thereby, all OpenMP loops in an application were run with the same set of parameters. The modeling process used in this experiment is also similar to the ones used in previous sections. For validation, we perform leave-one-out validation as done in . Each application is assigned to the test set, while all other applications are assigned to the training set. We repeat this for all applications in the dataset. Based on our experiments, we find that the tuner designed with MIREncoder embeddings has better or equivalent performance to PnP tuner. It is able to identify configurations that lead to faster code execution in most cases,
4.5 Optimizing NUMA/Prefetcher Parameters
TehraniJamsaz et al. in proposed a novel GNN-based LLVM IR modeling technique for optimizing NUMA (Non-Uniform Memory Access) and Prefetcher configurations. In particular, this work built on top of prior works to explore the impact of various cache prefetching options along with NUMA-related hardware parameters such as number of threads, degree of NUMA node, thread mapping and page mapping. The authors used graph embeddings generated from LLVM IRs to statically map each kernel to the best NUMA/prefetcher configuration. Dataset. In , the authors used data from 57 parallel kernels from Rodinia , NAS Parallel Benchmarks , CLOMP , and LULESH . The data was collected on Intel SandyBridge and Intel Skylake processors on a search space with 288 and 320 configurations respectively. This dataset was pared down to 13 configurations as the authors found that 99% of the performance gains were obtained using these 13 configurations. To increase the quality of their code modeling and improve results, TehraniJamsaz et al. augmented the dataset by re-compiling the kernels in the dataset with 1000 different compiler sequences. We use this collected dataset to further test the strength of our approach.
Baseline. In this study, we are using a pre-trained model to generate the embeddings for each IR. In addition to testing the quality of optimizations made by our MIREncoder embeddings, we also use this experiment to highlight reduced data requirements when a pre-trained model is used to generate features. Transfer  learning allows us to achieve this as MIREncoder generates learned embeddings, thus implicitly transferring its knowledge to the tuner. Therefore, during training we only use ‚âà 5% of the complete dataset for training. During validation and testing, the authors in used 10-fold validation. We also use the same folds for our tests and compare the results from MIREncoder with the state of the art in this dataset. As with prior works on this task, we also use error rate (relative difference between best and predicted performance) as the evaluation metric.
Results. To perform 10-fold validation, we first separate the validation set from the training set by assigning the kernels specified in each fold to the validation set. During training, we only select ‚âà 5% of the IRs in the dataset at random for the kernels in the training set. However, we validate on all IRs corresponding to the kernels in the validation set. Training with such reduced data also produces good results as we can leverage transfer learning from our pre-trained model to generate embeddings for the IRs in the training set. For both SandyBridge and Skylake, we outperform in 8 out of 10 folds (Figure ). The modeling for this experiment only uses simple MLP layers in contrast to , which trains resource intensive GNNs for each experiment. Overall, across 10 folds, MIREncoder embeddings help reduce performance error rates by ‚âà 15% (Sandy-Bridge) and ‚âà 29% (Skylake) over . MIREncoder embeddings outperform Perfograph in 8 out of 10 folds for SandyBridge improving error rates by ‚âà 14%. It outperforms Perfograph in all cases for Skylake, improving error rates by ‚âà 40%.
4.6 Tuning Thread Blocks for CUDA Programs
So far the auto-tuning experiments have targeted programs written in C and C++. However, state-of-the-art GPUs have contributed immensely to performance improvement of HPC workloads and CUDA is often the language of choice for programming such GPUs, specifically NVIDIA GPUs. With this in mind, we have tried to optimize the performance of CUDA kernels in this section. As in the prior sections, we work with a previously published dataset and use a MIREncoder based tuner to identify the best parameters to run CUDA kernels.
Dataset. To address the lack of large scale datasets suitable for machine learning based optimizations of CUDA kernels, Bjertnes et al. published the LS-CAT dataset with 19, 683 CUDA kernels. They also open source scripts to modify the input matrix sizes, and , , , , , , , , , , , (
thread blocks used to execute these kernels. We compile and run these CUDA kernels with the matrix sizes and thread blocks shown in Table to collect a dataset with more than 2.7 million samples on an NVIDIA A100 GPU. From the collected dataset, we identify the minimum runtime of each kernel and input matrix. The block size corresponding to the fastest runtime is then selected as the best configuration. This processed and labelled data is then used to train a simple MLP model on the MIREncoder embeddings to predict the best configuration for a CUDA kernel and input matrix unknown to the model. Baseline. To the best of our knowledge, this study is one of the first works to perform optimizations using this dataset for CUDA code. To evaluate our MIREncoder representation, we use the embeddings from three prior works (IR2Vec , PROGRAML , Perfograph ), and adapt the modeling techniques specified in those papers to the best of our ability for this task. We follow the same strategy used in and Section 4.5 and use error rates (relative difference between best and predicted performance) as a metric to present and compare the results for this section.
Results. The LS-CAT dataset does not have a designated test set. Therefore, we perform 10-fold validation as done in prior works
4.7 Observation and Analysis
In this section, we outline and analyse the merits of our approach. We primarily hope to show the importance of each modality to our pre-training pipeline. We will also show how using our approach reduces the overheads associated with deep learning based performance optimization. Ablation Studies. Ablation studies are commonly used in deep learning to highlight the importance of individual components of the modeling process. Here, we hope to highlight the impact of each modality on the modeling process. We first remove the modules associated with each modality from the pipeline and pre-train the uni-modal models from scratch. However, for each uni-modal model, we only train it on one pre-training task as each pre-training task was designed with a modality in mind. For example, a Masked Language Modeling pre-training task would not be appropriate for the code graph modality. And the IR-Graph Matching task is dependent on both modalities being a part of the pre-training process. With this setup, we pre-train our uni-modal models and follow the same experimental setups as before. The uni-modal pre-trained models are tested on three tasks from the previous sections, namely heterogeneous device mapping (Section 4.1), thread coarsening (Section 4.2), and loop vectorization (Section 4.3). Case Study 1 (CS1): Each experiment shows that our pre-training is highly dependent on each modality. For device mapping, the quality of the predictions fall significantly when modality 2 (code graphs) is not included. When modality 1 (IR text) is removed, the performance drops, but less drastically. When we only pre-train with modality 1, performance drops by ‚âà 37% and ‚âà 14% for the NVIDIA and the AMD GPUs, whereas performance drops by ‚âà 8% and ‚âà 4% when only code graphs are used for pre-training (Table ). The higher dependence on the code graphs is expected as code semantics dictate which device is chosen as the best one for some of the kernels in this dataset. For example, the makea kernel from the CG application in NPB , has a faster runtime on the GPU with a smaller input size, whereas it is mapped to the CPU when run with larger inputs. This behavior can be due to the presence of a number of function calls inside the parallel kernel. Such semantic details might be difficult for an NLP-style model to understand. However, a graph that embeds such dependencies as edges between nodes can help highlight such semantic information to the model.
Case Study 2 (CS2): When predicting the thread coarsening factors, we see that not including the code graphs has a smaller impact on thread coarsening factors than device mapping. Moreover, using only the code graphs leads to a bigger drop in application performance than when using both modalities. We see that performance drops by 5% when the code graphs are not used, whereas performance drops by 11.7% when only code graphs are used (Table ).
Case Study 3 (CS3): We also test how unimodality impacts the performance of loop vectorization. Loop vectorization is an important compiler optimization for modern processors. For this set of experiments as well, we see that removing a modality impacts the performance of the predictions (Table ). Using IRs only in textual format the performance drops by ‚âà 30%, and when we use only the code graphs as a modality the performance of our vectorizer drops by ‚âà 12%. Analyzing Overheads. Most advanced DL-based works usually have significant training and inference overheads. We use the experiment in Section 4.4 as a template to evaluate the overhead of our approach. We first train and test the MIREncoder-based tuner and PnP tuner from Section 4.4 and capture the wall times. The PnP tuner is a GNN based code modeling approach that first proposed this downstream task. Across all experiments, to reduce overhead, we simply generate embeddings from the pre-trained model instead of fine-tuning it. PnP on the other hand needs to train GNNs for each experiment. Compared to our MIREncoder-based tuner, which only trains a few MLP layers, training and tesing a GNN based model is much more expensive as shown in Table .
Most studies working with pre-trained models usually suggest fine-tuning the pre-trained models for downstream tasks. However, in this work, we do not fine-tune our pre-trained model for downstream tasks. The embeddings generated by MIREncoder are good enough to be used with simple shallow networks. To show this, we perform a set of tests with two setups: i) we use our regular set up where we do set the pre-training model to inference mode and use only the final MLP layers for training and testing, and ii) we do not set the pre-training model to inference mode, and use the complete network to fine-tune for downstream tasks. We observe that for the experiment in Section 4.4, performance (speedups) improves < 5% when we fine-tune. However, the training time balloons by 238√ó. This is a significant increase in overhead for fairly marginal gains. We avoid this overhead by not fine-tuning, but simply generating the embeddings to achieve good results as shown in Section 4. Such overheads are seen even for our relatively small model with 22 million parameters. Recently, large language models, with billions of parameters, have been proposed for addressing compiler optimizations. This would increase the training time exponentially, especially when computational resources are limited. Thus, new innovative techniques, like the one proposed in this paper, is necessary to reduce overheads and large-scale resource dependence. Multi-modality allows us to work with a small model and helps offset the loss in learning when a small uni-modal model is used.