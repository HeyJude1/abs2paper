6 Discussion
In this work, we have proposed a pre-trained multi-modal encoder for IRs with source code based performance optimizations in mind. Such an approach enables a model to understand syntactic, semantic and structural characteristics of source code. Prior works in this domain often depend only on NLP-style stylistic choices or compiler based code semantics and might require advanced modeling techniques with significant overheads.
Not only do our embeddings help reduce overhead on downstream tasks (Section 4.7), our pre-trained model is itself much smaller in scale than the latest pre-trained models in literature. Very large models, such as LLMs, often have billions/trillions of parameters. This makes training and fine-tuning them quite expensive, often requiring multiple state-of-the-art GPUs. Our pretrained multi-modal model on the other hand, only consists of 22 million parameters, and can be easily trained using a single GPU. However, most very large models have text or image generation capabilities; our model does not. This is by design as the aim of this work is to simplify and speed up the process of deep learning based performance optimization in HPC.
Moreover, for downstream tasks, we do not need to fine-tune the pre-trained model as is often necessary for larger models. We simply put the pre-trained model in inference mode, and output the embeddings of an input LLVM IR. Transfer learning allows us to do this and still achieve good results across multiple languages (C, C++, CUDA) and programming models (OpenCL, OpenMP). Because the pre-trained model has already been trained to understand and relate code syntax, semantics and structure, during downstream optimization tasks, the pre-trained model can leverage its prior knowledge to generate good quality embeddings. This also allows us to reduce data requirement while training DL models, as shown in Section 4.5, where we train our model with only 5% of the data that the state of the art had been trained on.
Additionally, our pipeline is modular by design. This can inspire future research on how each modality can be represented. For example, we could replace the graphs used in this work by other graphical representations such as ASTs, Perfograph, Graph2Par and evaluate their impact. We hope to do this in future.