7 Conclusion and Future Works
This paper proposes MIREncoder, a multi-modal pre-training approach to encode/embed LLVM IRs for easy use by deep learning based models targeting performance optimizations in HPC. Our pretrained encoder will allow researchers to focus more on adapting deep learning for HPC optimization problems instead of focusing on how it can be done. Moreover, as seen widely in literature, it is often possible to re-train existing pre-trained models for multiple domains. With this in mind, our model has been designed to be smaller in scale compared to existing pre-trained models. This would allow further research on such topics, and would not make researchers completely dependent on high-end and large-scale resources as is the case with very large models. Our aim with this paper was to propose a pre-training pipeline for HPC that would be small-scale. We helped alleviate the loss in learning from using a smaller model by introducing multi-modality to help our model better understand code "meaning". Our experimental results and further analysis support our claims of better performance with reduced overheads. Furthermore, our pre-trained model could easily be used in conjunction with online auto-tuners to help aid the search process. We hope to investigate this in future.