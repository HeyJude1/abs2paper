IV. EVALUATIONS
In Section IV-A, we evaluate the performance of g-SpMM and g-SDDMM. In Section IV-B, the ablation study shows the importance of our optimizations. In Section IV-C, the productivity of our DSL compiler is compared to the human expert. In Section IV-D, we evaluate the autotuning with the cost model. Finally, in Section IV-E we evaluate the GNN model end-to-end training and inference performance.
Evaluation Environment: For all experiments, we maintain a consistent setup. We utilize an NVIDIA V100 GPU with 16GB of device memory and employ CUDA 11.1. Sparse tensors feature 32-bit indices, while dense tensors use 32-bit floating-point values in row-major format.
Benchmarks: All g-SpMM kernels and g-SDDMM kernels have been tested using the REDDIT dataset from Hamilton et al. , as well as the OGBN-PROTEINS and OGBN-PRODUCTS datasets from the Open Graph Benchmarks . In the REDDIT dataset, nodes represent forum posts, while edges indicate that users have commented on both connected posts. The OGBN-PROTEINS dataset features nodes as proteins and edges as biologically meaningful associations between them. In the OGBN-PRODUCTS dataset, nodes symbolize products purchased on Amazon, and edges signify that the connected products were bought together. These three datasets are widely used as benchmarks for evaluating GNN models. Basic statistics for each dataset are provided in Table . Evaluation Methodology: We evaluate G-Sparse by comparing it with state-of-the-art GPU implementations, including DGL v0.9.1 , FeatGraph , Sputnik , and cuSPARSE , to demonstrate its performance advantages in kernel performance benchmarks, end-to-end model training, and inference benchmarks. We use DGL to conduct performance evaluations and integrate G-Sparse kernel implementations into DGL for comparison. In end-to-end model benchmarks, DGL employs the cuSPARSE vendor library whenever possible.
To ensure fairness, all kernel execution measurements are initiated from DGL's Python interface, as performance overhead exists when calling native functions from Python.
We investigate the kernel performance of g-SpMM and g-SDDMM under various scenarios by comparing them across different feature lengths ranging from 1 to 1024, with each length being a multiple of 2, for each dataset.
a) G-SpMM Performance: For simplicity, we focus on the g-SpMM with the copy_lhs binary operation and the sum reduction type, which can be supported by cuSPARSE.
As illustrated in Figures 10, our performance surpasses cuSPARSE, DGL, FeatGraph, and Sputnik for most tested datasets and feature lengths.
Compared to the DGL with cuSPARSE implementation, our kernels achieve an average speedup of 3.2×, 2.6×, and 1.5× on REDDIT, OGBN-PROTEINS, and OGBN-PRODUCTS, respectively. We observe a greater acceleration on the REDDIT dataset, likely because it is more unbalanced than OGBN-PROTEINS and has more non-zeros per row on average than OGBN-PRODUCTS, as Table shows. Consequently, our row balance and data reuse optimizations have a more significant impact on its performance.
Compared to FeatGraph, we achieve up to a 3.4× speedup and an average of 1.9× speedup. FeatGraph's performance is comparable to cuSPARSE but falls short when the feature length is smaller than 32. Although both FeatGraph and the latest version of DGL apply the same optimization techniques, FeatGraph can adjust and tune the number of CUDA blocks for different datasets and feature lengths, resulting in higher performance than DGL.
Compared to Sputnik, we obtain up to a 2.1× speedup and an average of 1.4× speedup. While Sputnik employs hierarchical 1-D dimensional tiling, we utilize 2-D shared memory optimization, which offers better data reuse efficiency. Furthermore, Sputnik does not apply the 1-D stride register tiling as we do.
b) G-SDDMM Performance: G-Sparse consistently outperforms or matches DGL and FeatGraph for g-SDDMM kernels in all tested cases. Figure illustrates the performance Fig. : G-SpMM and g-SDDMM benchmarks for REDDIT, OGBN-PROTEINS and OGBN-PRODUCTS datasets. When the feature length is 512 (g-SpMM) and 1024 (g-SpMM and g-SDDMM), we encountered out-of-memory on both of the baselines and G-Sparse on OGBN-PRODUCTS. comparison for g-SDDMM kernels. G-Sparse achieves approximately 1.02× to 3.37× speedup and an average of 1.46× speedup over DGL.
Although both DGL and FeatGraph employ warp shuffle optimizations, we achieve higher performance for feature lengths smaller than 32. This is because DGL and FeatGraph only apply warp shuffle optimizations for feature lengths equal to or greater than 32. Since the lane width of warp shuffles can be an integer multiple of 2, we can perform warp shuffles for feature lengths ranging from 2 to 32.
We conduct an ablation test for g-SpMM and g-SDDMM kernels with different feature lengths on the REDDIT dataset.
As shown in Table , 2-D shared memory tiling and row balancing greatly impact performance and do not exhibit significant performance changes with variations in feature   lengths. We observe that warp shuffles provide considerable benefits for g-SDDMM kernels but are not robust to the size of the feature lengths.
Conversely, 1-D stride register tiling influences approximately 10% of the performance and shows no benefit when the feature length is 8. We find that this is likely because when the feature length is smaller than the warp size of 32, the GPU warp is not fully utilized, leaving no room for register tiling.
Our G-Sparse implementations are not only more efficient but also require fewer lines of code compared to DGL. G-Sparse codes are 3.6× and 4.4× shorter than DGL without counting DGL's utility code lines while achieving 2.5× and 1.45× average speedup over DGL for g-SpMM and g-SDDMM computations, respectively.
In terms of details, DGL requires 313 core code lines along with an additional 176 code lines for g-SpMM computations, while our G-Sparse implementation only needs 86 core code lines. For g-SDDMM computations, DGL requires 319 core code lines with the same extra utility code lines, while our G-Sparse implementation only needs 72 core code lines.
We apply autotuning across all experiments to obtain the best-performing kernel. Our autotuning system combines a cost model with a genetic search algorithm, taking approximately 8 to 20 seconds for each experiment. Table VI shows the autotuning time of g-SpMM for the REDDIT dataset. Autotuning results can be saved and reused, allowing us to save time when autotuning the same dataset and the operations with the same shapes.
We compare autotuning results with those obtained by domain experts who manually tuned the scheduling over several days. As seen in Figure , kernels with autotuning outperform those without autotuning in all cases, achieving a maximum speedup of 3.7×. This can be attributed to the fact that autotuning combines domain expertise with comprehensive search spaces that are too large for experts to consider every aspect thoroughly.
We find that autotuning is more adept at identifying the optimal solution for allocating GPU resources, such as the number of threads per block and number of registers, which are often mutually constrained, compared to domain experts. As demonstrated in Table , the optimal number of threads for warp shuffle in g-SDDMM does not always equal the feature length or 32 (the warp size). Because more warp shuffle instructions require more threads and registers, and the total number of threads and registers is limited, autotuning is employed to discover the best configuration. Moreover, the performance is close between G-Sparse and G-Sparse without autotuning on Figures ) and 10(f). We observe that, in these cases, the schedules of autotuning are highly consistent with those written by our experts. Overall, autotuning leads to higher or the same performance as the experiments without autotuning in all cases. We train the cost model using three different kernels on the REDDIT dataset. Each kernel has approximately 20 to 1500 performance samples, depending on its search space size. For training results, we predict the performance of training samples and then measure their correlation with real performance measurements. For testing results, correlation coefficients are calculated using different kernels that were not part of the training process on all three datasets. This process is repeated for g-SpMM and g-SDDMM, respectively. As observed, even though the cost model is trained on the REDDIT dataset, the training correlation reaches 0.88 and 0.99, while the inference correlation achieves 0.74 and 0.89 for g-SpMM and g-SDDMM, respectively. This indicates the relatively good generalization of our cost model. Figure demonstrates that the cost model is more efficient than random sampling. With the same number of trials (three), the best performance of the kernel from the cost model achieves a 1.0× to 2.9× speedup on REDDIT compared to the random sampling method. When the feature length is less than 8, there are only tens or hundreds of candidates to try, so the cost model and random sampling exhibit little difference.
Herein, we compare the end-to-end performance of GCN , GAT , and GraphSage models with different kernels on the REDDIT dataset. The G-Sparse implementations are integrated into DGL for performance testing, and DGL utilizes the cuSPARSE library if cuSPARSE supports the computation. In all three models, the number of layers is two, and the hidden size is 256 for each layer. GCN and GraphSage use g-SpMM with the aggregation type of sum, while can also use other types such as mean and max. GAT employs both g-SpMM and g-SDDMM with more aggregation types and incorporates both vertex and edge feature embeddings.
The full-batched training is used for all models. The reasons come from two aspects. On one hand, our core interest lies in the performance improvement brought by the accelerated sparse computations and the full-batched training has a stable speed per epoch. On the other hand, the current implementation of G-Sparse still has one limitation that uses the number of non-zeros in the sparse matrix as the input feature to our DNNbased autotuner (refer to Algorithm 4). We plan to overcome this limitation in our future work to integrate DSL compilation with autotuning techniques to effectively accelerate samplingbased GNN learning. Nonetheless, the mini-batched training is supposed to demonstrate consistent acceleration results with the full-batched, since the overhead of graph sampling in mini-batched training can be removed or hidden in practice. For instance, two popular approaches can be used. One is to generate sub-graph examples beforehand and store them on disk for future use in training/inference - . The other is to exploit parallel data prefetching in data loaders , , . no no no middle Sputnik no no no high DGL yes no no middle FeatGraph yes yes limited middle G-Sparse yes yes yes high We have verified that the training accuracy and the test accuracy obtained by using our kernels in DGL, within the same number of epochs, match that of DGL for each pair of the model and dataset. We trained the three models with 200 epochs to check for accuracy. The test accuracy of G-Sparse matches that of the original DGL implementations -94.0% for GCN, 93.7% for GraphSage (full batch), and 93.1% for GAT.