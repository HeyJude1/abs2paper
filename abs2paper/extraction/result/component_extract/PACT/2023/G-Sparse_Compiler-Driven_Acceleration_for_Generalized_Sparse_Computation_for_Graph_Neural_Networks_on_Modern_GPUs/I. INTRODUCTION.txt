I. INTRODUCTION
Graph Neural Network (GNN) is an emerging field in deep learning and have been applied to a wide range of real-world applications, including but not limited to bioinformatics , social science , recommendation systems , and quantum chemistry . Graph-structured representation learning using † Corresponding authors; ‡ The authors contributed to this work when they worked in Ant Group.
GNN models is crucial for many real-world graph-related applications, such as node classification - , graph isomorphism , and diffusion patterns . In contrast to conventional deep learning models like DNNs, which rely heavily on dense matrix operations, GNNs involve message-aware propagation operations that recursively update each vertex's features based on its neighbors. When working with real-world graph datasets, their sparsity and irregularity pose performance and design challenges for recent state-of-the-art frameworks, such as TensorFlow , PyTorch , and MXNet .
Generally, the computational patterns of GNNs can be described using message-passing primitives. For instance, DGL introduces generalized sparse computations, specifically generalized Sampled Dense-Dense Matrix Multiplication (g-SDDMM) and generalized Sparse Matrix-Matrix Multiplication (g-SpMM), to express these message-passing primitives. As illustrated in Figure as an example, the aggregation of features for target vertices from neighboring vertices can be represented as an SpMM operation when performing an aggregating sum operation. Similarly, the aggregation of features for target edges can be represented as an SDDMM operation between the source vertex features and the target vertex features. These operations are considered generalized because the aggregation type in GNNs can include sum, max, and so on. However, these operations lead to a large overhead of GNN training. The SpMM-and SDDMM-like operations account for more than 60% of the total execution time , , and become the bottleneck of improving GNN learning speed, motivating us to accelerate them.
Challenges: There exist three challenges to optimizing the SpMM-and SDDMM-like operations, including the limitation of current sparse matrix computation optimizations, computational libraries, and Domain-Specific Languages.
Firstly, compared with conventional dense matrix calculations that employ easy-to-use tensor-driven parallelism, sparse matrix operations are more challenging due to their irregular workload, sparse memory access, and limited data reuse. As n1 1 x x n2 2 x x n3 3 x x n4 6 x x 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 n4 0 1 0 0 0 * 1 x x n1 2 x x n2 3 x x n3 4 x x 5 x x = x x x x x x x x x 6 x x n4 x x x Adjacent Matrix Feature Matrix Feature Matirx Fig. : A g-SpMM operation in GNNs with an aggregate sum.
On the left, we aggregate feature vectors of vertex n 1 , n 2 and n 3 to the vertex n 4 . On the right, we represent the graph as an adjacent matrix so that the features of vertices are computed as an SpMM.
a result, accelerating sparse matrix computation on modern accelerators is essential and challenging for efficient GNN training and inference. To improve the performance of sparse computations in GNN, DGL and FeatGraph attempt to leverage node-centric and edge-centric parallelism. These works bridge GPU-accelerated graph processing and GNN learning by transforming graph data into specific structures and performing scattering and gathering operations along edges and vertices. However, these efforts largely ignore the systematic optimization opportunities presented by the architectural features of the underlying hardware, such as load balancing, shared memory tiling, and register tiling, which have been proven significant for GNN sparse matrix performance. Moreover, their performance is difficult to tune without providing in-depth systematic task scheduling approaches and programming interfaces.
Secondly, although cuSPARSE and Sputnik have considered load balancing and other sparse-specific optimizations, they are mainly targeted for scientific computations and conventional Deep Neural Networks (DNNs) such as Transformers and ResNet-50 , and provide very limited support for the operators in GNNs. For instance, in g-SpMM, they only support the case of aggregated summation for source node, and cannot support the case of the edge with more than one feature as input or max as an aggregation function. In particular, even though this operation in GNNs does not require the value array in the CSR sparse matrix, filling the CSR value array with the full assignment value of 1.0 in these existing computational libraries is still compulsory. These redundant computations and memory-accessing operations can bring significant performance degradation to the GNN training procedure.
Finally, existing Domain-Specific Languages (DSLs) such as TVM , Halide , and Ansor separate algorithms from the schedule and utilize autotuning strategies for efficient automatic code generation, but are not efficient for GNNs. These DSLs mainly target the image processing and natural language processing domain, which commonly utilize interval analysis and provide support for processing regular dense matrices. However, the optimization for sparse computations in the GNN DSL domain is still very limited. One of the reasons is that the DSLs cannot deal with the non-rectangular buffer boundary inference problem in sparse computations. Moreover, they lack sophisticated system optimizations and corresponding autotuning strategies for GNNs and their sparse computations, for example, not supporting sparse-specific optimizations such as row load balancing, adaptive warp shuffle, and hierarchical memory tiling optimizations.
To address these technical challenges, we propose G-Sparse, a compiler-driven approach for enabling highly effective acceleration of generalized sparse computations in GNNs running on modern GPUs. G-Sparse is a new DSL that extends Halide, and it is capable of efficiently and automatically generating highly optimized code for general sparse kernels. The performance of kernels generated within seconds can surpass the code manually optimized by experts over several weeks. In summary, we make the following contributions:
• We present 2-D shared memory tiling, 1-D register tiling, and row load balancing optimizations for g-SpMM operations in GNNs.