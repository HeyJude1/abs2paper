III. SYSTEM DESIGN
A. Overview
Figure illustrates the workflow pipeline of the G-Sparse DSL compiler. In contrast to conventional GNN system optimization, we leverage compiler techniques to enhance the performance of g-SpMM and g-SDMM operations and enable autotuning. This includes optimizing GPU memory access, workload balancing, and SIMD-aware execution. In the following sections, we discuss g-SpMM and g-SDDMM optimizations and their corresponding schedules using our extended G-Sparse DSL.
For g-SpMM, we introduce the buffer-bound inference technique to enable 2-D shared memory optimization, the buffer binding index expression to facilitate row load balancing optimization, and the 1-D stride register tiling to optimize data reuse at the register level. As demonstrated in Table optimizations, which have not been employed by previous DSLs, are crucial for enhancing the performance of sparse computations. We adopt DGL's g-SpMM interface, which employs the CSR format for the adjacent sparse matrix in g-SpMM.
For g-SDDMM, we apply the warp shuffle optimization and our autotuning strategy with a more extensive search space to achieve better performance. Warp shuffle functions utilize registers, rather than shared or global memory, for thread communication within a warp and have been used in sparse linear algebra acceleration , . We adopt DGL's g-SDDMM interface, which employs the COO format for the sparse matrix in g-SDDMM.
Lastly, we introduce our autotuning approach that incorporates the genetic search algorithm with an extensive search space. This enables the automatic search for optimal results without human intervention. Table presents the notations used in our system implementations.
B. G-SpMM Optimizations
We implement g-SpMM algorithms in Halide DSL and introduce 2-D shared memory tiling, row load balancing, and 1-D register tiling optimizations. As an illustration, Figure shows the SpMM DSL and Figure shows the CUDA code generated by the DSL. a) 2-D Shared Memory Tiling with Buffer Bound Inference: The G-Sparse compiler aims to optimize memory hierarchy tiling for g-SpMM computations. As shown in Figure , we reuse the column index array and values array of the sparse matrix in the GPU shared memory to prevent accessing global memory every time when computing the same row of the output matrix. Additionally, preloading the sparse matrix with the GPU coalesced threads into the shared memory allows us to avoid wasting global memory transactions if we Fig. : The SpMM DSL description.
were to serially read the sparse matrix values in one row, as mentioned in Section II-B0b. This improves performance.
The G-Sparse compiler implements this optimization by extending the Halide DSL. Halide employs interval analysis to infer the loop extent and allocation size, recursively inferring from the output loop extent and allocation size back to each function. However, Halide can only analyze the bounds of a variable and a parameter, not buffer access. When Halide encounters a situation it cannot infer, given the row index is i, it pessimistically assumes that the bound is infinite, which is
bounds(A.rowP tr(i)) = [−inf, +inf ]
. As a result, the bound analysis of the non-rectangular buffer A.colIdx cannot proceed, and in the end, Halide can only perform the most pessimistic analysis and then load the entire A.colIdx before computing the kNnzTile. Although this approach can make the global memory load of A.colIdx efficient (Section II-B0b), it will cause repeated calculations and loading, and the entire nnz elements are often too large for the GPU's shared memory. For instance, REDDIT has approximately 114 million non-zero elements, while the shared memory size of the NVIDIA GPU V100 is only 96K.
To address this issue, we introduce non-rectangular buffer bound inference to determine the bound and allocation size Binding mi and ri to GPU threads while loading A.colIdx 8:
for mi ← 0 to mextent in parallel do for ri ← 0 to kextent in parallel do for mi ← 0 to mextent in parallel do for ri ← 0 to kextent do O(i,j) ← sum(U(i,colIdxTile(start+reduceDomain)))
Recall that A.rowPtr(i) is a scalar integer value and has a single point interval for each row. Therefore, the bounds of A.colIdx are calculated as follows:
[A.rowP tr(i) + min(reduceDomain),
A.rowP tr(i) + max(reduceDomain)]
Line 11 in Algorithm 1 shows that we preload A.colIdx for kMTile rows and with kNnzTile indices of non-zero elements for each row, the inner reduction dimension iterating from 0 to kNnzTile -1. For each row, the preload size is only related to the size of reduceDomain domain. We get the loop extent and allocation size is [min(ri), max(ri)]. Therefore, the loop extent for A.colIdx each row is equal to ri's loop extent: min(nnzP erRow −ro×kN nzT ile, kN nzT ile), where nnzPerRow means the number of non-zero elements per row and ro means the outer reduction dimension split from nnzPerRow with a factor kNnzTile.
We find that this loop extent has a constant upper bound kNnzTile. Therefore, we can optimistically allocate kNnzTile shared memory size for A.colIdx each row in Algorithm 1 (line 5). Instead of pessimistically putting all of A.colIdx into shared memory as Halide will do, we only put chunks of kNnzTile size into shared memory while still maintaining the check of the dynamic memory bounds in the computation.
b) Row Load Balancing with Buffer Binding Index: Sputnik observes that the NVIDIA Volta thread block scheduler employs a round-robin strategy. Therefore, Sputnik uses row swizzle load balancing for sparse matrix multiplication to balance the workload. It first sorts the row indices based on the number of non-zero items in each row and stores the sorted indices in an array called balancedIdx. As a result, the first GPU thread block will compute on balancedIdx[0], which points to the longest row, and the second GPU block will compute the second-longest row.
To incorporate this row load balancing strategy into our DSL, we introduce the bind buffer (line 5 in Figure ) schedule primitive and a corresponding lowering pass to support indexing with a buffer. We use the BindBuffer function (lines 1-3 in Algorithm 2) to bind each row index access expression exprM Algorithm 2 Lowering pass for buffer binding index. return halideExprs with the balancedIdx buffer, which stores the row indices sorted by nnzPerRow. Lines 4-11 in Algorithm 2 shows the lowering pass. Internally, the computation iterates through the expressions to find every expression associated with a bound buffer, and replaces it with the row indices expression, and then invokes the loading expression of balancedIdx buffer with indices. As a result in the CUDA code shown in Figure (lines , when the rowPtr is requested with an indexed GPU thread block vm, its index vm needs to be replaced with balancedIdx .
This way, when we assign GPU blocks to each row of the output matrix, the ith ∈ {0, m} GPU block is responsible for computing the balancedIdx[i]th row of the sparse matrix, instead of the ith row. As Figure illustrates, the A.rowPtr is indirectly accessed from the balancedIdx buffer.
c) 1-D Stride Register Tiling: Besides using shared memory preload, the other way is to reuse the value of the sparse matrix in the register when computing the result, i.e., register tiling, which is 1-dimension here.
As shown in Figure , we compute the output in registers using 1-D tiling. During the register tiling computations, we load the A.colIdx into the register from the shared memory. Since the row dimension remains constant, the register value of A.colIdx is reused in the tiling computations for different values of the input matrix U, thereby reducing the memory access of A.colIdx.
Additionally, rather than performing tiling in the continuous memory of the output matrix, we do tiling across a specific length of elements. This approach is necessary to ensure that the global memory access of the dense output matrix is contiguous and coalescing when accessed using GPU threads. If we were to tile adjacent elements, the global memory access with threads would be in stride but not coalescing, resulting in wasted global memory access transactions.
C. G-SDDMM Optimizations a) Adaptive Warp Shuffles:
A g-SDDMM operation is depicted in Figure . G-Sparse's g-SDDMM kernels follow the edge parallel paradigm and tune the size of edge parallelism and the lane width inside the warp shuffle. We assign a specific number of edges to a block, use adaptive lanes for computing features to intermediate results, and apply warp shuffle instructions to reduce intermediate results in registers. Algorithm 3 provides simplified pseudo kernel code for comput-0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0
• ( t 1 t 2 t 1 t 2 * t 1 t 2 t 1 t 2 ) t 1 t 2 t 1 t 2 t 1 t 2 r Adjacent Matrix A Feature Matrix U Feature Matrix V stage1 stage2
Fig. : A g-SDDMM operation. In stage 1 , threads t 1 and t 2 separately compute the partial dot product result of matrix U and V . Stage 2 uses warp shuffle to sum up the partial results in t 1 and t 2 to get the final result r.
Algorithm 3 Adaptive warp shuffle. for all ro ← 0 to extent do 5:
regInThread ← sum(load(U ) * load(V )) Use warp shuffle to sum up the regInThread in each thread.
6:
for all ri ← 0 to adaptiveLaneW idth do 7:
regResult ← sum(warp shuffle(regInThread, ri))  ing g-SDDMM within a warp. Within adaptiveLaneW idth number of lanes in a warp, line 5 computes a dot product between vertex feature matrix U and another vertex feature matrix V , storing the result into the register regInT hread. Lines 6 -7 accumulate the results in regInT hread within the lanes to yield the final result regResult. The g-SDDMM is implemented in our DSL, as demonstrated in Figure . Figure also presents the specific CUDA code generated by our G-Sparse compiler.
Although DGL and FeatGraph utilize techniques like warp shuffles, we find that DGL does not apply autotuning, and FeatGraph applies tuning only with the size of GPU blocks. Moreover, they do not apply warp shuffles for feature lengths less than the warp size of 32, thus not achieving optimal performance in some cases. Additionally, the lane width of warp shuffles for different feature lengths is fixed at 32 for both DGL and FeatGraph. However, as Table IV in Section IV-D demonstrates, 32 is not always the optimal lane width. We use an adaptive shuffling lane width for warp shuffle, ranging from 2 to 32 (powers of two), to achieve the best performance.
D. Autotuning with Cost Model a) Cost Model:
To make performance tuning easy in the search space, task schedules over the GPU threads can be abstracted using the DSL compiler. The following hyperparameters constitute the search space:
• kMTile and kNTile represent the thread block tiling sizes for row and column dimensions of the output matrix, respectively. • kNnzTile represents the shared memory tiling size in each nnzPerRow. • kNRegTile denotes the size of 1-D stride register tiling.
• kLaneWidth represents the lane width of warp shuffles.
• Whether to use row load balancing, 2-D shared-memory optimizations for A.colIdx or A.values, and warp shuffles. Nevertheless, the search space is so large that recent GNN models are hard to tune. Taking executing g-SpMM operations over REDDIT dataset as an example. When the length of features equals 256, the size of the search space could be estimated as 1.5 billion sizes. Conducting a brute-force search will take weeks or even months with every candidate's real run time on specific architectures.
To address this challenge, we propose an autotuning method based on a cost model-driven technique. Inspired by , , , our cost model improves their performance on sparse matrix operations with a neural network structure and training target. Most of the previous works are designed for dense matrix computations. In contrast, our cost model return costMetrics is designed specifically for sparse matrix computations. We ultilize metrics calculated in Algorithm 4 for g-Spmm and g-SDDMM respectively. Additionally, we also add the shape information of the input and output matrix to the metrics for training. While previous works aim to predict the schedules directly, we use the cost model to predict the cost of each schedule in the search space and then fine-tune it with genetic search or random sampling. Finally, we choose the best configuration from the fine-tuned options.
Our cost model employs a simple and trainable DNN. As illustrated in Figure , the first layer of our model structure is a log function, followed by a second layer consisting of batch normalization and three dense layers. The final layer is the normalization layer. The loss is calculated using mean squared error, and we normalize the real data to better reflect the true trend: loss = MSE(predict y , normalize(y)). P redict y represents the predicted cost, while normalize(y) refers to the normalization of the actual cost, specifically representing the run time observed on the hardware. Ultimately, we utilize the Pearson correlation coefficient to measure the accuracy of our training and prediction. b) Genetic Search and Random Sampling: After the cost model predicts the cost of the schedules in the search space, we choose 5 to 20 schedules that exhibit the lowest cost values as predicted by the cost model to fine-tune the performance with genetic search and random sampling algorithms.
We customize a genetic search algorithm to search for the optimal schedule, similar to the approach in . The population size is set at 16, the elite size is 6, and the search concludes after three generations of breeding. We employ a roulette wheel algorithm for candidate selection during the crossover. The mutation probability is set at 20% to escape local optima.
Furthermore, crossover and mutation procedures can generate invalid configurations (e.g., the number of threads exceeding 1024). In such cases, we choose the most similar yet valid candidate from the remaining candidates, where similarity is measured by Euclidean distance. A candidate is returned once it has only one gene differing from the invalid candidate. Consequently, the time complexity of the similarity calculation is linear to the generation size, and the best time complexity is constant. Lastly, a multi-threaded compilation of the schedules is used to accelerate the process.
Fitness: The fitness is designed as the reciprocal of the execution time on the GPU hardware:
fitness = 1/execution time(candidate) ( 3
)
The candidate is executed 10 times, and we take the average time results of these runs. Before that, we execute once to warm up. Random sampling: We employ random sampling to find the optimal performance when requiring a fast search time. After trimming the search space, we randomly sample the remaining candidates. Three candidates are selected and executed. Ultimately, the best-performing candidate is chosen as the final schedule.