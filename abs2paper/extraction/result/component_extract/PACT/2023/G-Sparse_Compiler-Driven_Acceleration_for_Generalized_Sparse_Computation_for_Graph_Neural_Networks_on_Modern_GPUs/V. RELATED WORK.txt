V. RELATED WORK
Sparse computations in GNNs dominate the overall runtime in training and inference. Recently, a few implementations have been proposed to speed up the sparse computations in GNNs. DGL and FeatGraph utilize node and edge parallelism and perform thread-level parallelism for feature dimension . DGL employs the cuSPARSE library when the latter supports the required computation. Table compares the features of G-Sparse with both DGL and FeatGraph.
Sputnik primarily targets DNNs and introduces shared memory optimization, vector instructions, GPU row swizzle load balancing, and other optimization techniques. Table compares the features of G-Sparse with Sputnik. spECK optimizes the load balancing problem for sparse computation. Ge-SpMM optimizes the performance of g-SpMM for GNNs, supporting generic SpMM sparse computation and using the coalesced row caching method to access global memory efficiently. ES-SpMM co-designs edge sampling and SpMM kernel to fit the graph into shared memory, reusing data to accelerate sparse computations. FusedMM mainly supports generic GNN sparse computation on CPUs, optimizing load balancing and efficient use of memory bandwidth.
Halide , TVM , Cortex , AKG , Triton , Ansor , and Tiramisu are all DSL compilers targeting image processing and DNNs, primarily consisting of dense matrix computations. TACO is a tensor algebra compiler capable of expressing sparse computations. Scheduled TACO supports split, collapse, and reorder schedules. However, scheduled TACO does not support non-rectangular buffer bound inference and bound bind index expression introduced by us for sparse computations.
We implement G-Sparse as a DSL by extending Halide to describe the computations in GNNs. Additionally, we integrate more optimizations into the schedule strategy, including data reuse and row balancing, and introduce autotuning to obtain optimal results by exploring more schedule space.