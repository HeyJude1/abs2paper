IV. DATA STRUCTURES FOR TREE TRAVERSAL
This section will introduce the optimized data structures that form the basic components of the presented inference function. These structures will be illustrated using an exemplary binary decision tree as shown in Fig. , which is comprised of seven split nodes (S 0 to S 6 ) represented by circles and eight leaf nodes (L 0 to L 7 ) represented by squares, having a maximum depth equal to three (the root node S 0 is considered to be at depth 0, nodes S 1 and S 2 at depth 1, and so on).
The processing of a decision tree for a given input sample, also denoted as tree traversal, starts at the root node and ends when a leaf node is reached. Each split node that is visited during a tree traversal, selects a feature of the input sample and compares it against a threshold value. Based on the outcome of the comparison, processing continues with either the left or the right child node. In this example, the left child node is selected if the feature value is less than or equal to the threshold, and the right child node is selected otherwise, as indicated by the symbols along the edges.
Each leaf node contains a label of a type that is determined by the model type. For example, for random forests the label can be an integer or floating-point value for regression, or a class identifier or set of class probabilities for hard or soft voting classification, respectively. The prediction result for a given input sample is derived from the labels in the leaf nodes that have been reached by traversing all trees in the ensemble.
A commonly used approach is to map the decision-tree nodes according to a breadth-first order within an array in the memory, starting with the root node being mapped at an offset 1. This allows the node interconnection structure to be encoded through the node offsets, without requiring explicit fields to define the child nodes of each split node (e.g., using pointers or offsets): the left and right child nodes of a given split node that is mapped at offset k, are mapped at offsets 2 × k and 2 × k + 1. This approach, however, requires the decision tree to be a perfect tree (as described in Section III) such as the tree shown in Fig. . In this figure, the node offsets according to a breadth-first order are shown next to each node in red.   The main advantages of this data structure are its compactness and the simple calculation of the offset of the child nodes from the parent node offset enabling efficient SIMD vectorization. An important disadvantage is the requirement of a perfect tree, which limits its application to relatively shallow trees. Another disadvantage is that the nodes on traversal paths between the root node and leaf nodes are typically not mapped near to each other, except in small trees, but are scattered throughout the memory array, resulting in minimal spatial locality and therefore low caching efficiency.
To overcome these disadvantages, an optimized breadth-first tree structure is proposed, henceforth referred to as OBF. The OBF scheme divides trees into multiple partitions, which are expanded to perfect subtrees as shown in the example in Fig. . In this example, the colored nodes comprise the original tree structure, and the other nodes were added to expand the shown partitions to perfect subtrees. The nodes at the lowest level in a partition are denoted as result nodes. A result node can either correspond to a leaf node in the original tree structure (and will then be represented by a square), or it can comprise a reference (e.g., pointer or offset) to another partition containing the next nodes from the original tree structure (in which case it will be represented by a circle). A tree traversal is completed if a result node of the former type is accessed and the label can be retrieved from that result node. If a result node of the latter type is accessed, then processing continues with the partition that it refers to.
As can be understood from the example in Fig. , this approach allows non-perfect trees to be handled more efficiently by only 'covering' the actual existing subtrees using partitions instead of expanding the entire tree to become a perfect tree for the maximum tree depth. As a result, however, tree traversals from the root to a leaf node can be of different lengths, resulting in certain tree traversals reaching leaf nodes earlier than other tree traversals, which makes SIMD vectorization more difficult. The OBF scheme handles this by iterating the processing of the last partition for the tree traversals that finished earlier at leaf nodes that are not at the maximum tree depth until all tree traversals are completed. These iterations are represented by the looping edges attached to result nodes corresponding to leaf nodes in Fig. .
The partition size is selected such that a partition fits in one or two cache lines to optimize spatial locality. In this paper, a fixed partition depth of 4 levels is used, corresponding to a perfect subtree with 15 split nodes and 16 result nodes. Fig. illustrates the corresponding data structure for each partition, which consists of three arrays, with the first two arrays storing the feature selectors and threshold values (32 bit single-precision floats) of the 15 split nodes in the partition, and the third array comprising the 16 result nodes (32-bit values) as mentioned above. A vector containing 16 singlebit flags indicates for each result node if it corresponds to a leaf node and contains a label, or if it contains a reference to another partition. Using a separate array for the feature selectors makes it possible to reduce the memory footprint by adapting the data type to the actual number of features used in the given model, e.g., by using bytes for supporting models with up to 256 features, 16-bit shorts to support up to 64K features, and 32-bit words otherwise. The entries in each array are sorted according to the breadth-first order of the nodes and indexed using the offset calculation described above.
Another common approach is to apply a depth-first ordering of the nodes within a memory array, starting with the root node being mapped at offset 0, which is illustrated in Fig. . This node interconnection structure is also encoded through the node offsets: the left child node of a split node mapped at offset k is mapped at offset k + 1 and the right child node at offset k + m d with the 'offset increment' m d being dependent on the depth d at which the parent node is located within the tree. For the example in Fig. , m 0 = 8, m 1 = 4, and m 2 = 2, with m d+1 = md 2 . Similar to the OBF structure, this structure also requires a perfect tree to work correctly.
An optimized depth-first tree structure is proposed, which will hereby be denoted by ODF, which modifies the basic depth-first tree structure to exploit node-level access proba-  bilities. In order to illustrate this, the nodes are colored to show a heatmap which reflects the node access probability as indicated by the legend shown in Fig. . This information can, for example, be derived from leaf cardinality information generated during training, which reflects the number of training samples that are 'mapped' on each path between the root node and any leaf node. Alternatively, node access probabilities can also be generated for the internal decision-tree ensemble representation to which an imported model is converted as was described in Section II, using training or similar data.
As can be seen in Fig. , paths that consist of sequences of 'left-child-nodes' (e.g., S 0 , S 1 , S 3 , and L 0 ) are mapped on consecutive offsets which results in higher cache locality when traversing those paths. As also can be seen, these paths do not necessarily align with the most probable (frequently) travelled paths (e.g., S 0 , S 2 , S 5 , and L 5 ) as indicated by the heatmap color. In order to 'correct' this alignment, the ODF scheme applies a per-node selection of the comparison operator (either less-than-or-equal-to or greater-than) such that the child node that is most likely to be accessed next corresponds to a positive comparison result and 'becomes' the left child node. The resulting structure for the example tree is shown in Fig. . In this example, the compare operators of nodes S 0 , S 1 , and S 5 have been changed as reflected by the symbols at the edges. Because of the above adaptation of the comparison operator in each node, now large parts of the most likely traversed paths are mapped on consecutive offsets, thus improving cache locality. To realize this, the applied comparison, ≤ or >, has to be encoded in the data structure. ODF does this by using the most significant bit of the feature selector field.
To support deeper trees, ODF removes the need for perfect trees by supporting an 'optional' selection of the offset increment for the right child node for each individual split node by storing it explicitly in the node structure in an additional field, while the offset of the left child node remains equal to the offset of the parent node incremented by one. This is illustrated by the example of a non-perfect tree in Fig. in which the offset-increment selection is applied to all split nodes as can be seen from the differences between the offsets of the right child nodes and the parent nodes that can be selected independently.
This flexible offset-increment selection can be applied to all split nodes in the entire tree as shown in Fig. ; however, it is more efficient to use the above discussed structure shown in Fig. for traversing nodes near the root node and using the structure shown in Fig. for nodes at the greater tree depths. The depth at which processing switches between the two structures constitutes an additional implementation parameter that is selected by the data-structure selector component in Fig. when a model is imported. The first structure will be denoted as ODF1 and the second as ODF2.
With the requirement for perfect trees removed, the traversal of different paths can now take a variable number of steps. In order to enable efficient SIMD vectorization, the leaf nodes are implemented and processed as a special kind of split nodes for which the comparison always produces a negative result. By using a zero offset increment when processing these nodes, the tree processing function will simply start looping over the same leaf node upon arrival. This continues until all parallel tree traversals have arrived at a leaf node. These loops are illustrated in Fig. . This concept is similar to the partition-level iterations applied by the OBF scheme discussed in Section IV-A, but applied at the node-level.
The OBF scheme maps the leaf nodes on the last consecutive offsets in each partition as can be seen in Fig. . The depth-first ordering applied by the ODF scheme, however, interleaves the offsets upon which the split nodes and leaf nodes are mapped as Figures show. Because of this, an array-based data structure as shown in Fig. for the OBF scheme does not make sense for the ODF structures discussed here. Instead, each node will be mapped on a singlenode structure combining multiple fields storing the node's feature, node-level comparison type flag, threshold, and offset increment (for the ODF2 structure), as is shown in Figures . For efficient processing, all fields are 32 bits wide.