VII. EXPERIMENTS
A large number of experiments were performed using the publicly available classification and regression datasets listed in Table that are frequently used in publications for benchmarking inference algorithms. Multiple gradient boosting and random forest models with various tree count and depth combinations were trained for each dataset using XGBoost 1.7.4, LightGBM 3.3.5, and Scikit-Learn 1.2.2. The trained models were both serialized using pickle and exported to ONNX format. The resulting files were then loaded for performing inference experiments with the native runtime (i.e., the scheme that was used to train the model), with ONNX Runtime 1.14.1 and with our inference function. The LightGBM models were also scored using lleaves 1.0.0.
The experiments were carried out on a single node of a shared-memory NUMA system, having 128GB of main memory and two Intel ® Xeon ® Gold 6230 CPUs running at 2.10GHz. Each CPU had 20 cores, dedicated 32KB L1 and 1MB L2 caches and a shared 27.5MB L3 cache, and supported AVX2 and AVX-512 SIMD instructions. The system was running Ubuntu 20.04.6 LTS. As discussed in Section II, the inference function was implemented as a C++ extension module that is accessed through a Python script. The C++ code was compiled using gcc 9.4.0 with the -O3 flag. All timing measurements were performed as part of the Python script. For each model many prediction requests were executed such that the test data available for the corresponding data sets as listed in Table was covered at least once by all batches and such that there were at least 64 prediction requests for each batch size. The batches were selected in various ways from the test data, but the exact order and alignment did not appear to have much impact on the measured performance. Finally, the arithmetic mean was determined over all measured predict times for each given batch size.
Inference is typically executed as part of an application framework, implying that in addition to the predict function other tasks are executed on the CPU, which also consume computer resources and therefore could degrade predict performance. It is assumed that there are mechanisms in place (e.g., by setting processor affinity attributes) that dedicate one or multiple cores to the inference task and that the execution of the application framework will not significantly impact the predict operation nor affect decision-tree data stored in the caches of those cores. This seems to be in line with the way in which experimental results are obtained and presented by most of the published related work.
Fig. shows the average predict time (latency) per input sample that was measured on the test system for four singlethreaded predict functions implemented as part of the ODF scheme that involve various ways of parallelization using AVX2 SIMD instructions as described in Section VI, for a range of batch sizes. Each function is identified using the representation of the inner/outer loop, SIMD vectorization and multithreading defined in that same section. The model was trained using XGBoost for the epsilon dataset with the number of trees, T , tree depth, d, and number of CPU threads, thr, listed in Fig. . Fig. shows a similar graph for 12 multithreaded predict functions (also for ODF and implemented using AVX2 instructions) for a model trained for the HIGGS dataset. Both figures illustrate that there can be considerable differences in the performance of the predict functions for the same batch sizes.
Intuitively it can be expected that for short batch sizes, which provide less options to parallelize over the input samples, predict functions that parallelize traversals over multiple trees using either SIMD vectorization and/or multithreading will perform better. This can be seen clearly in Fig. for batch sizes below 8 samples, for which predict functions ti and it, which use SIMD vectorization to process multiple trees in parallel, both perform well. It is interesting to see in Fig. , that when in addition also multithreading is enabled over the tree loop (functions Ti and iT), the performance becomes worse in comparison to ti and it. The latter can be explained by realizing that the multithreading over the tree loop will introduce additional tree structures into the cache, which can result in cache conflicts ('thrashing') between the parallel tree traversals which consequently degrades performance. The extent to which these cache conflicts occur between parallel tree traversals depends on several factors, including obviously the number of parallel traversals, the size of each tree and the number of active paths (e.g., the heatmap in Fig. ). In a similar way, when tree traversals are parallelized to process multiple input samples, then the size of these samples (i.e., the number of features) and how these samples are accessed can also result in cache conflicts and affect performance. The selection of the best predict function taking into account these effects, is automatically done based on benchmarking data as was described in Section II.
In this section the performance of the inference function that is proposed in this paper and comprises the various components discussed in the previous sections is compared to state-of-the-art algorithms. Figures show the measured performance expressed as average predict latency per input sample for three models trained and scored using XGBoost, LightGBM and Scikit-Learn (random forest) as described in Section VII-A. The graphs also show the inference performance for the ONNX Runtime library and for the dynamically selected OBF and ODF predict functions implemented using AVX2 and AVX-512 SIMD instructions.
The OBF and ODF schemes outperform XGBoost, Light-GBM, Scikit-Learn, and ONNX Runtime throughout the entire range of batch sizes, with the largest performance gain occurring for short batch sizes. The latter was to be expected, given that most decision-tree algorithms are optimized for processing  larger batch sizes. The fact that OBF and ODF also achieve the best performance for longer batch sizes, is clearly due to the dynamic predict function selection, which also appears to be very effective in this case.
Table shows performance results, in terms of latency per sample, for a large variety of model parameters, batch sizes and number of threads. This table consists of five horizontal sections, one for each dataset listed in Table , and three vertical sections corresponding to XGBoost, LightGBM and Scikit-Learn, respectively. Each intersection of a horizontal and vertical table section corresponds to a model that is trained using one of the three schemes for one of the five datasets.   which show the measured average prediction times per sample for the native runtime, for the ONNX Runtime library, for lleaves (LightGBM models only), and for OBF and ODF, respectively, for four different batch sizes (the maximum batch size for scoring models trained using the Covertype dataset was reduced because of the limited number of test samples -see Table ). The last row shows the model sizes in bytes, which equal the serialized (pickled) file sizes, except for the ONNX Runtime library which does not support serialization and for which the ONNX file size is used instead, and except for lleaves for which the size of the exported binary ELF file is used. The best (i.e., smallest) latency and model size values are highlighted using a bold font.
Because random forests can involve deeper trees than gradient boosting models , the latter models (XGBoost, LightGBM) were trained for a larger number of shallower trees and the former (Scikit-Learn) for fewer but deeper trees. It also appeared that for several experiments involving multithreading, Scikit-Learn and lleaves performed better using single-threaded operation. Although these two schemes do not support an automatic reduction of the number of actually used threads to improve performance, we decided to list the better single-threaded latency values (marked with an '*') for both schemes in Table for having a fairer comparison. For the same reason, we also limited the number of threads used for scoring the Scikit-Learn models.
As can be seen from Table , the dynamically selected OBF and ODF predict functions outperform all other schemes across all models. Performance gains of one to two orders of magnitude are achieved for single-sample prediction compared to XGBoost, LightGBM and Scikit-Learn, and a factor two to ten for longer batches for most models. The AVX2 implementations of OBF and ODF predict functions outperform ONNX Runtime on average by more than a factor four and lleaves by about a factor three for the models in Table . The AVX-512 implementations of the OBF and ODF predict functions outperform the AVX2 implementations on average by about a factor 1.5. Table II also shows that for all models, OBF or ODF achieve the smallest (serialized) model sizes.
Detailed analysis and further experiments have revealed that compared to a non-parallel implementation of the OBF and ODF predict functions, the performance was improved by approximately a factor 2 and 3 by only using AVX2 or AVX-512 SIMD vectorization, respectively, by about a factor 6 when only using multithreading, and by a factor of 11.4 and 14.2 when using both multithreading and AVX2 or AVX-512 SIMD vectorization, respectively. Note that these are average values for all experiments listed in Table . These numbers can change substantially for individual models, batch sizes and number of CPU threads.
For determining the average latency numbers listed in Table II from the measured prediction latencies, the following coefficients of variation (standard deviation divided by mean) were observed for the respective batch sizes 1, 128, 1024 and 8192 (2048 for the Covertype models): 1.2, 0.20, 0.071, and 0.027. Because delays can be amortized more easily over longer batches, obviously the shortest batch sizes show the most variability. This is also reflected in the observed 95th and 99th percentiles which were on average 28%, 20%, 9.4% and 4.5%, respectively 67%, 59%, 13%, and 4.8% greater than the mean value.