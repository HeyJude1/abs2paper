I. INTRODUCTION
Decision trees in machine learning have a long history. Automatic Interaction Detection (AID) and THeta Automatic Interaction Detection (THAID) are often considered to be the first published decision tree algorithms for regression and classification, respectively, which were combined and extended into the Classification And Regression Trees (CART) algorithm . Later, ensemble learning methods such as bagging , gradient boosting - and random forests , were developed, which combine multiple decision trees to substantially improve prediction accuracy over individual decision trees. Despite this long history, decision trees and ensemble methods such as random forests and gradient boosting are still among the most frequently used machine learning algorithms today, as shown by a recent Kaggle survey . This is because of the inherent advantages coming from the relatively simple concept, support for numerical and categorical features and interpretability of predictions.
Although basic research on decision trees has existed for decades, the widespread deployment of machine learning in recent years has boosted research into innovative approaches to further increase performance. This research was initially centered around training, but inference has gradually become more important as well, as can be seen from the increase in related publications in this field - . Originally, these efforts were focused on the inference of larger input batches, for which the prediction latency of an individual input sample was less important and processing delays could be amortized over the entire batch. This has fundamentally changed with the emergence of new applications that rely on real-time online predictions involving only one or a few input samples. Such applications are typically encountered in the financial services sector and include the scoring of credit card transactions for fraud detection as well as real-time anti-money laundering operations, for example. Low latency operation is critical because the inference operation is part of a real-time processing pipeline, in which individual transactions or small batches of transactions are processed on-the-fly. In addition, small memory footprints are important in shared, multi-user applications, e.g., in cloud serving, where multiple models may simultaneously be held in memory to support concurrent model inference requests.
This paper addresses these challenges in multiple ways. A first contribution consists of optimized versions of conventional breadth-first and depth-first decision tree traversal algorithms that enable efficient use of SIMD vectorization and the exploitation of node-level access probabilities to speedup the processing of both shallow and deep tree structures. This is in contrast to state-of-the-art schemes, where the use of SIMD vectorization is typically limited to shallow trees and is not combined with optimizations based on node-level access characteristics. A second contribution enables efficient inference for individual samples and small to large input batches through a novel concept in which a collection of predict functions is designed, with each function implementing a different combination of parallelization using SIMD vectorization and multithreading. The most suitable and performant predict function is then selected dynamically during inference based on model, request and platform parameters. To the best of our knowledge, this is the first time that such a concept has been proposed for decision tree inference. The remainder of the paper is organized as follows.
Section II describes the design of the proposed inference function. Section III discusses related work. Optimized data structures for breadth-first and depth-first tree structures are presented in Section IV. Sections V and VI describe how the tree traversal algorithms for those data structures can be accelerated using SIMD vectorization and multithreading, respectively. Section VII presents experimental results and a performance comparison with other state-of-the-art schemes and Section VIII concludes the paper. II. DESIGN OVERVIEW Fig. shows a block diagram of the CPU-based decisiontree inference function that is presented in this paper. It supports importing random forest and gradient boosting models trained in Scikit-Learn , XGBoost and LightGBM and exported to PMML [24], ONNX or selected proprietary formats. Imported models are first converted into an internal decision-tree ensemble representation that is used as input to a data structure selector, which selects a data structure type out of two candidates (the OBF and ODF structures that will be introduced in Section IV) based on model and platform parameters. The selected data structure is then generated and optimized for the given platform, taking into account cache sizes, the availability of SIMD instructions, and other platform characteristics. For each data structure type, multiple predict functions have been implemented, involving different combinations of SIMD vectorization and multithreading, as will be discussed in Section V. At inference time, the predict function that is expected to perform best is selected based on prediction request parameters, in particular the batch size and the number of CPU threads available for the given prediction request. None of the steps shown in Fig. changes the basic functionality of the imported model, and, consequently, the inference function will generate exactly the same prediction results as if the original model was scored in its native runtime, i.e., Scikit-Learn, XGBoost or LightGBM.
The selection of the best data structure and predict function involves complex dependencies on platform parameters (e.g., cache sizes, SIMD instructions), model parameters (e.g., tree counts and depths), and prediction request parameters which renders the creation of general selection rules for the above data structure selector and dynamic predict function selector impractical. Therefore, instead a benchmarking function is applied, as shown in Fig. , which, using training data or other data serving this purpose, evaluates the performance of the various predict functions for both data structure types and for a range of prediction request parameters. This information is then used by the data structure selector and the dynamic predict function selector. If no data is available for the benchmarking operation, then the selection will be based on previously collected benchmarking data for general models. Ongoing research investigates the possibility of training a machine learning model for data structure and prefix function selection, however, this is outside the scope of this paper.
The inference function is implemented in C++ code as part of an extension module that can be accessed in Python using a Scikit-Learn-like interface.