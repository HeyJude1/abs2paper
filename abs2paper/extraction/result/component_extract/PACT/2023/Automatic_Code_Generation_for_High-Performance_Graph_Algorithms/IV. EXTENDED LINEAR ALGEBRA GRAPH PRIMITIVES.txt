IV. EXTENDED LINEAR ALGEBRA GRAPH PRIMITIVES
As described in Section II, traditional linear algebra primitives are not sufficient to implement efficient graph algorithms. For this reason, the GraphBLAS standard introduces extended linear algebra operators that leverage the nature of graph data structure to reduce computation and increase performance. This section describes the design and implementation of such extended primitives in our work.
A. Semiring
Semiring is an algebraic structure similar to a ring, but without the requirement that each element must have an additive inverse operator. Semiring operators combine a pair of linear algebra operations into a singular one. For example, the DSL expression
C[i, k] = A[i, j] @(+, * ) B[j, k] (1)
where A and B are the input matrices and C is the output matrix, contains the semiring operator @(•, •). The semiring operator takes as input the pair of operations that needs to be combined. In this example, @(+, * ) combines addition and multiplication, thus it is equivalent to the standard matrix multiplication operation. Listing 3 demonstrates the use of the any-pair semiring between a dense vector and a sparse matrix (Line 22), where the first operator returns true if both the elements in the two input vectors are non-zero (paired); the second operator returns true if any pair was found in the output of the first operator. Our compiler also supports plus-times, min-first, any-pair, and plus-pair.
The compiler lowers the semiring operator @(•, •) down to the tensor algebra dialect (Line 7 in Listing 1). Since semiring may contain different operations, the specific mapping of semiring to TA operators depends on the particular semiring used. For example, @(+, * ) maps to ta.mul, while @( , * ) maps to ta.elews_mul. Additionally, our compiler extends the semantics of the tensor algebra dialect operators to include the semiring type and the pair of operations required. Listing 1 shows how the ta.mul operator in the tensor algebra dialect is extended to represent semiring type plus_times (note the semiring attribute). In the next step, the compiler lowers the tensor algebra IR to index tree IR, where we perform most code optimizations and transformations for sparse computations. Listing 2 shows the result of this lowering step for the plus_times semiring described in Listing 1. Finally, the compiler lowers the index tree IR to the SCF IR and then to LLVM IR and machine code for execution.
B. Masking
In the GraphBLAS standard, masking is a technique to selectively apply operations to certain elements of input matrices or tensors. In our DSL, masking is represented by the operator • , which takes as input a matrix that represents the mask and has identical dimensions of the matrix to which masking is applied (hence the dimensions of the mask matrix are omitted). For example:
C[i, k] M = A[i, j] * B[j, k] (2)
performs a matrix-matrix multiplication between the inputs A and B. In this expression, the masking operator applied to C, C[i, k] M , limits the scope of scalar operations to be performed and which elements of C need to be updated.
The mask matrix M is a binary matrix in which m ij = 1 iff element c ij needs to be updated with the result of the computation. Figure (a) and 4(b) illustrate how masking works when applied to a sparse matrix and a sparse vector, respectively. In the Figures, A and x are the original structures, while B and y are the masks. Our code generation algorithm for masking operation can be classified into two classes, depending on the sparsity structure of the masking matrix: push-based masking and pullbased masking . As an example, let us consider the case C<M > = A * B (spGEMM). The push-based masking algorithm is driven by the input matrices. The algorithm traverses matrix A by rows and "pushes" the non-zero elements into the corresponding rows of matrix B. First, by analyzing the nonzero elements in a row of A, the algorithm identifies the specific entries in B that contribute to the output C. Next, the compiler performs a linear combination between those elements from the row of A and corresponding rows of B. Finally, the row of A selects the row of M with the same row index, reducing the number of output elements to be computed. In contrast, the pull-based masking algorithm is driven by the matrix mask. In this case, the algorithm first traverses every non-zero element of the mask M and "pulls" the corresponding row from matrix A and the column from matrix B. First, by analyzing a non-zero element in M , the algorithm determines which row of A and which column of B should be pulled out. Second, the algorithm does a sparse dot product between these selected vectors.
While the goal of both push-based masking and pull-based masking algorithms is to eliminate unnecessary computations, the pull-based masking approach is preferred when the matrix mask M has a very low density, as it only examines the elements in the input matrices determined by the mask. However, in this case, the input B needs to be stored in columnmajor storage such as CSC when performing a sparse matrixsparse matrix operation. In contrast, the push-based masking approach is more general and allows B to be stored in a rowmajor format, such as CSR. Thus, this approach is preferred when the matrix mask M has a relatively high density. In general, the best approach depends on the sparsity level of the mask M : the compiler is designed to favor the approach driven by the most sparse matrix to eliminate as much unnecessary computation as possible.
Note that, in both approaches, there is an additional cost of accessing the mask matrix M and determining which elements pertain to the computation. However, the savings introduced by eliminating unnecessary computations greatly outweigh this additional cost, as shown in the next Section V.