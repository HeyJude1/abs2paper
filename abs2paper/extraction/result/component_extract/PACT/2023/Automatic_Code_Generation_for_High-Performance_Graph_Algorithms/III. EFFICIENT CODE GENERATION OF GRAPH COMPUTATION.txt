III. EFFICIENT CODE GENERATION OF GRAPH COMPUTATION
The proposed compiler is based on the MLIR framework, which provides a multi-level IR and an infrastructure to perform progress lowering. The key insight in MLIR, hence in this work, is that different optimizations can be performed at each level of the IR stack (or dialects), from high-level, domain-specific optimizations at higher levels to architecturespecific optimizations at low levels and that optimizations and dialects can be composed to efficiently generate executable code for various target architectures.
A. Index Tree Dialect
This work introduces a new MLIR dialect, the Index Tree dialect, between the COMET's existing tensor algebra dialect and the MLIR Structured Control Flow (SCF) dialect with the specific objective of performing efficient code transformation and optimizations for sparse computation. The index tree dialect is a representation of an index tree notation, which is widely adopted by various code generation models of tensor computations , , . While the COMET tensor algebra dialect is designed for traditional tensor algebra operators, the index tree dialect provides a generic and efficient representation of computing expressions (operators and their relations) based on two types of nodes, indexation, and computation nodes. The specific instantiations of the nodes are determined by the computation expressed in the tensor algebra dialect during lowering. However, the index tree nodes are not limited to the traditional tensor algebra operators and can be used to implement extended operators, such as semiring and masking, as described in Section IV. As discussed later, these extended operators are fundamental to achieving high performance with graph algorithms.
Listing 1 and Listing 2 show examples of code (matrix multiplication) represented in the tensor algebra dialect (ta.mul) 1 %C = "ta.mul"(%A, %B) { 2 formats = ["CSR", "CSR", "CSR"], Listing 1: The sparse matrix-matrix multiplication operation represented in the tensor algebra dialect. Multi-dimensional memory references are indexed with affine maps.
3 indexing_maps = [ 4 affine_map<(d0, d1, d2) -> (d0, d1)>, //C[i,k] 5 affine_map<(d0, d1, d2) -> (d1, d2)>, //A[i,j] 6 affine_map<(d0, d1, d2) -> (d0, d2)>], //B[j,k]
%1 = "it.computeRHS"(%a, %b) ...{} -> tensor< * xf64> %2 = "it.computeLHS"(%c) ...{} -> tensor< * xf64> %3 = "it.compute"(%1, %2) {semiring = "plus_times"} %4 = "it.indices"(%3) ...{} %5 = "it.indices"(%4) ...{} %6 = "it.indices"(%5) ...{} %7 = "it.tree"(%6) ...{}
Listing 2: The sparse matrix-matrix multiplication operation represented in the index tree dialect highlighting the tree structure with index and compute nodes.
and lowered to index tree dialect, respectively. The code in Listing 2 can be read from bottom to top, i.e., it.tree operation represents the root of the index tree. The it.indices operations represent the various indices used in the tensor multiplication operation, which would be i, j, k. These nodes of the tree will directly map to three nested loops needed for this operation. The next three operations in the index tree dialect represent the body of the loop. In this case, compute operation forms a segue between index operations and the compute operations. The compute operation maintains the compute expression that is formed via the computeLHS, computeRHS operations. Once the lowering to SCF dialect is complete, the code is ready to be consumed within the downstream MLIR infrastructure to generate machine code.
B. Code Optimizations and Transformations
To provide efficient code generation for sparse kernels, this work addresses three major challenges during the code generation: 1) high insertion cost into sparse output tensors 2) the unknown size and distribution of nonzero elements in sparse output tensors, and 3) the difficulty of parallelization.
1) Workspace Transformation: In general, inserting a new element into the sparse data structure of the output tensor has a high time complexity. To eliminate this complexity, most compiler frameworks and libraries store the output of a sparse computation in a dense format. Although this approach greatly reduces the cost of computation, it may lead to "densification" of the data structures and increased memory footprints for the output results, which may result in unnecessary wasted space and an inability to execute.
To address these challenges, we developed a novel approach called workspace transformation to store sparse output directly in sparse formats such as CSR. The workspace transformation introduces temporary intermediate dense data structures
i,j,k i,k j j k,j i j C ij +=A ik * B kj W j +=A ik * B kj W j =0 C ij =W j W j +=A ik * B kj W j =0 C ij =W j (a) (b) (c) Index node Compute node
Fig. : The index trees (a) before and (b,c) after applying the workspace transformation on C in index j, given a matrix multiplication operation. (called the workspace) to avoid irregular access to the original sparse data structures. This not only simplifies the code generation algorithm, but also improves the data locality of the generated code by narrowing some irregular access to the dense data structure. Unlike previous work that requires users to determine the index to apply workspace transformation, in this work, the compiler automatically identifies the index involved based on the storage format.
i,j,k i j C ij =A ij .* B ij W j =B ij W j =0 C ij =A ij .* W j (a) (b) Index node Compute node
The workspace transformation can be applied to two types of indices in a sparse-sparse expression: input indices associated with sparse dimensions in both input tensors and output indices associated with sparse dimensions in the output tensor. The workspace transformation performed depends on the indices to which it is applied.
Consider two sparse matrices A ik and B kj stored in the CSR format. The expression C ij = A ik * B kj produces a sparse output C ij , where the index j is associated with a sparse dimension. In this case, the compiler applies the workspace transformation to the index j as output index. The index trees before and after workspace transformation are shown in Figure : Figure shows the original index tree for a pure sparse matrix multiplication operation; Figure shows the index tree after applying the workspace to C on index j. The data in dimension j of C is represented with a temporary dense vector W ; Figure shows the index tree after enabling loop-invariant optimizations, in which unused indices of the leaf nodes are removed. In particular, the temporary dense vector W is independent of the index k, so k can be safely removed. Before workspace transformation, the time complexity of random access to the sparse output index j is O(log n) from searching (assuming the indices are sorted), and the insertion complexity is O(n) from data movement. After workspace transformation substitutes the sparse index j with the dense vector W , the complexity of both random access and insertion is reduced to O(1). Therefore, the workspace transformation can provide asymptotic performance improvement for the sparse outer index. Now consider the element-wise expression
C ij = A ij . * B ij
, where index j is associated with a sparse dimension in both the input matrices A and B. In this case, we apply the workspace transformation on the input matrix B. The index trees before and after workspace transformation are shown in Figure , where the data in dimension j of B are temporarily preserved by a dense vector W . Before workspace transformation, iterating the sparse input index j on both the sparse matrices A and B simultaneously requires a merge while loop with compound conditionals. After workspace transformation, the linear traverse of index j in matrix B can be replaced by random access to the dense vector W . This can not only improve the performance asymptotically, but also substitute the merge while loop with a simple for loop, which enables additional optimization potentially customized for for loops.
The workspace transformation is applied on the Index Tree dialect. Compared to the kernel fusion optimization in COMET , which removes redundant computation and performs memory optimization for intermediate tensors created by the fusion of multiple operations, the workspace transformation increases the performance of a single operation. The workspace transformation proposed in this work and the kernel fusion can potentially be composed together to result in overall higher performance. We leave this evaluation as future work.
2) Two-Phase Computation: One of the challenges of sparse computation comes from the unknown size and distribution of the output tensor. First, the number of nonzero elements in the output tensor is unknown before computation, which makes memory management very difficult. A general method is to allocate a very large chunk of memory to avoid the case where the output size exceeds the allocation, which results in redundant memory usage. Second, it is hard to know beforehand how nonzero elements are distributed among different rows (in case of row-major storage) in the output tensor. To update the output tensor in parallel, it is common to use a lock on the critical data structure, which results in high synchronization overhead.
To determine the needed size and real distribution of the output tensor, this work generates the code with two phases for sparse computation. The first phase is called the symbolic phase. It follows the same procedure of the given sparse computation (e.g., SpGEMM) in a "symbolic" way that it does not execute the computation, but only records the nonzero distribution of the output. After that, the symbolic phase can also determine the true number of nonzero elements in the output tensor and then allocate the sparse data structure with only the needed memory size. The second phase is called the numeric phase. It performs the real "numeric" computation with the prior knowledge from the symbolic phase. For some components of the sparse data structure (e.g., the index array in CSR), the output can be placed directly at the correct location Fig. : An example of parallel numeric phase. After the symbolic phase, thread t knows where to insert the result into the data array using a private workspace.
that is provided by the symbolic phase. Therefore, the twophase computation can minimize memory usage effectively.
3) Automatic Parallelization: Another challenge of sparse computation is the inefficient parallel execution due to the unknown distribution of the output nonzero elements. Unlike the dense matrix computation that can be parallelized by simply dividing the regular output, the sparse computation generates irregular sparse output. A na√Øve parallelization method would require locks on the critical sparse data structure, which may result in a high synchronization overhead.
The use of two-phase computation enables efficient parallelization without locks. First, the symbolic phase can be naturally parallelized among one dimension of the tensor. It does not have update conflicts because the symbolic phase does not perform numeric computations but only records the output distribution. Second, the numeric phase can also be parallelized according to the locations provided by the symbolic phase as shown in Figure . Different parts of nonzero elements can be updated simultaneously without conflicts.
Moreover, the codegen can generate a private workspace for each worker during parallel execution. In this way, multiple workers will not have writing conflicts with each other, and a worker can reuse their private workspace without unnecessary reallocation.