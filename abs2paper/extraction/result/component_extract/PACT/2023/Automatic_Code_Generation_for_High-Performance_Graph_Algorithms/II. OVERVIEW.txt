II. OVERVIEW
This work proposes a domain-specific compiler framework to develop efficient graph algorithms represented by linear algebra operations. Our work adheres to the GraphBLAS standard, which provides a comprehensive set of graph primitives for sparse matrices and vectors of various types and extends the traditional linear algebra operators with semirings and masking to achieve higher performance. The GraphBLASbased approach provides a consistent way for graph algorithm implementation through common graph primitives that can be optimized using well-studied techniques, and it avoids the complexity of writing different ad-hoc implementations common with traditional vertex-or edge-centric approaches - . In most cases, the algorithmic complexity of the graph algorithms implemented using linear algebra is close to the complexity of the implementation based on vertex-or edgetransverses . Given a graph G(V, E), where V is the set of N vertices (or nodes) in the graph and E is the set of M edges that connect two vertexes, such that e ij ∈ E iff there are two vertexes v i , v j ∈ V and there exists an edge between the two. While graphs can be represented in various forms, such as a list of edges and the vertices they connect, a graph G(V, E) in algebraic implementations of graph algorithms is represented as an adjacency matrix A of size NxN, where the elements a ij = 1 iff there exists an edge e ij ∈ E. Once a graph is represented as an adjacency matrix, the implementation of graph algorithms can leverage well-defined linear algebra operations. For example, visiting all neighbors from a source vertex v i only requires multiplying the adjacency matrix A by a vector x that has all zero elements except x i . Similarly, multiplying A by itself k times yields relations between vertices at distance k. For example, A 2 x and A k x return the neighbor lists that are two and k hops away, respectively. Given the nature of graphs, generally N 2 M , hence A is very sparse. Storing A in a dense format unnecessarily increases the memory and computing requirements and results in inefficient execution. This framework employs sparse linear algebra operators to reduce both the memory and computing requirements, as only the non-zero elements of A need to be stored in memory and considered during the computation.
The proposed DSL is based on index notation (or Einstein notation), which is a concise, expressive, and widely used way to express dense and sparse tensor computations. For example, the multiplication of two matrices A and B can be expressed as C ij = A ik * B kj , where i and j are the free indices that appear in the output, whereas, the remaining indices are the summation indices, k in this case. This concisely represents the following operation for each entry of the output matrix C: N k=1 a ik b kj , assuming that all matrices are of size NxN. The Einstein notation is adopted and supported in many common programming models to express tensor operations, such as the numpy.einsum API in NumPy , PyTorch (torch.einsum) and TensorFlow (tf.einsum). It is also the input language in deep learning frameworks such as Tensor Comprehension , and sparse tensor compilers such as TACO and COMET , . All these libraries and frameworks implement some variant of the original Einstein notation to expand the expressiveness. In this work, we adopt a consistent Einstein notation semantic as used by numpy.einsum and state-of-the-art compilers , . We refer the reader to the numpy.einsum API page for a more comprehensive description of the notation. Note that a summation or contraction index is implied if an index variable appears on the right-hand-side tensors but not on the left-handside tensor. In addition, a custom function can be specified to express other types of reduction other than summation, such as
A i = max(B ij ).
It is also possible to express operations such as MTTKRP (Matricized Tensor Times Khatri-Rao Product) as A ir = B ijk * D jr * C kr , where the index variable j and k are summed. The sparse formats of each tensor are specified as type annotations, and a compiler will automatically generate code from the expression in the back end. Also, note that for some operation sequences, the order of evaluation can result in different asymptotic time complexities, such as a chain of matrix multiplications . In this work, we assume such order is already determined and do not attempt to reorder matrix multiplications.
From an implementation point of view, the proposed compiler is based on the Multi-Level Intermediate Representation (MLIR) framework and built on top of COMET , . COMET is a dense and sparse tensor algebra compiler that targets multiple architectures. It has been extensively used to optimize dense tensor contractions within the NWChem quantum chemistry framework , . The work introduces specific code optimizations and transformations for sparse linear algebra operators, and DSL support to implement algebraic formulations of graph algorithms. MLIR, which is part of the LLVM ecosystem, provides a solid foundation to build new compiler frameworks and a set of common optimizations and code transformation passes, such as loop unrolling, tiling, and vectorization. New optimizations and architectures added to the MLIR framework will be readily available to the proposed compiler and its users.