6 Discussion
We now discuss three salient features of our proposal.
â€¢ Translating input UWOmp ğ‘ğ‘Ÿğ‘œ programs to mUWOmp ğ‘ğ‘Ÿğ‘œ code. We use the following simplification steps (similar to that of Aloor and Nandivada ), to convert any general UWOmp ğ‘ğ‘Ÿğ‘œ code to mUWOmp ğ‘ğ‘Ÿğ‘œ , before we invoke our CPS translator. We apply these steps until there is no further change.
Step 1. A sequence of statements as the body a parallel-for-loop. The full body is moved to a separate function and a call to that function is replaced with the body of the parallel-for-loop.
Step 2. One or more serial-loops inside the code invoked from a parallel-for-loop. We transform each such serial-loop to a recursive function and replace the loop with a call to that function.
Step 3.Set of Statements inside the parallel-region and not a parallelfor-loop or barrier statement. Similar to Step 1, we first move the set of statements to a separate function (say foo). The statements include the set of sequential statements until we hit a barrier statement or parallel-for-loop. Then, since the code has to be executed by all the workers, we replace the sequence of statements with the following code:
#ompfor for (int i=0;i<T;++i) {foo(â€¢ â€¢ â€¢ );} Note: The arguments to foo are the list of free variables and T denotes the number of workers executing this code.
â€¢ Optimization for Static Scheduling. We optimize the postbox for all-to-all based synchronization kernels and static scheduling policies by using a worklist implemented as a single array of closures with two indices (left and right) per thread. When a thread hits a barrier, it resumes executing the continuation only after (i) the thread has finished executing all the other closures in its worklist (between left and right), and (ii) the remaining activities have also reached the barrier. This approach simplifies maintenance and reduces memory overhead.
â€¢ Maintaining the thread-id. The UW model gives a guarantee that each iteration is executed by a unique worker. Thus, querying the thread-id at any point in the iteration should return the same value (consistent thread-id requirement). However, in our proposed solution, an iteration is divided into one or more closures executed by different threads. To satisfy the consistent thread-id requirement, we store the expected thread-id of each iteration in the closure, and modify the omp_get_thread_num function to access this closure.
â€¢ Compiling UWOmp ğ‘ğ‘Ÿğ‘œ code with OpenMP disabled. Unlike regular OpenMP codes, as is usual with codes using point-to-point synchronization, the semantics of UWOmp ğ‘ğ‘Ÿğ‘œ may not match with their serial counter-parts (obtained by compiling the code by disabling OpenMP). Successive-Over Relax 128K 12 Seidel2D 2 dim Gauss Seidel 128K 13 IA 1 dim Iterated Avg. 4K 14 HP 4 dim Heated Plate 4K â€¢ Signal/wait outside parallel-for-loops. UWOmp ğ‘ğ‘Ÿğ‘œ assumes that during execution, signal/wait functions are never invoked from outside parallel-for-loops. To handle invocation of signal/wait outside parallel-for-loops, we ensure that signal/wait (and their CPS counterparts) will abort if not invoked inside a parallel-for-loop.
â€¢ Possibility of deadlocks. Similar to clocks (X10), and phasers (HJ), programs written in UWOmp ğ‘ğ‘Ÿğ‘œ can also deadlock. For example, iterations may wait for each other, without sending signals.
But if a UWOmp ğ‘ğ‘Ÿğ‘œ program has no such dependencies (using signal/wait commands) causing circular-wait, then the translated code will not lead to circular-waits (and hence no deadlocks).
â€¢ Multi-file compilation. For ease of presentation, the paper discussed the concepts assuming that there is a single file. To support multi-file compilation, we require that all files are compiled with a suitable option (e.g., -uwpro) or none are compiled with the option.