4 UWOmp 𝑝𝑟𝑜 to Efficient OM-OpenMP
We now present the translation rules used to convert input UWOmp 𝑝𝑟𝑜 code to efficient OM-OpenMP code. The main idea behind our translation is that in the generated OM-OpenMP code, the activities of the parallel-for-loop are stored as closure (in one or more work-queues) to be executed by different workers. When an activity encounters a wait operation, it enqueues the continuation to the work-queue of the parent activity and continues executing other activities in the work-queue. Figure shows the block diagram of our translation. The input UWOmp 𝑝𝑟𝑜 code is fed to the Simplifier module which converts the given input code to a representative subset of UWOmp 𝑝𝑟𝑜 code called mUWOmp 𝑝𝑟𝑜 . The mUWOmp 𝑝𝑟𝑜 code is input to the 'CPS Translator' module which converts the code to CPS form called UWOmpCPS 𝑝𝑟𝑜 . The UWOmpCPS 𝑝𝑟𝑜 code is input to the 'OM-OpenMP Translator' module which translates the code to conforming OpenMP code such that it does not invoke barriers inside work-sharing constructs (parallel-for-loops). Finally, a post-pass step introduces type-specific reduction operations. We now describe these important modules of our translation scheme.
4.1 Simplifier
For the ease of explaining the translation scheme, like Aloor and Nandivada , we use a representative subset of the input UWOmp 𝑝𝑟𝑜 language called miniUWOmp 𝑝𝑟𝑜 (mUWOmp 𝑝𝑟𝑜 ); Section 6 discusses how any general UWOmp 𝑝𝑟𝑜 program can be translated to mUWOmp 𝑝𝑟𝑜 code. Figure shows the grammar of mUWOmp 𝑝𝑟𝑜 . A mUWOmp 𝑝𝑟𝑜 program consists of a sequence of function declarations (FuncDecl) followed by the MainFunc. FuncDecl can have an assignment statement, a function call, return statement, barrier statement or a statement generated by Seq(X): the program formed from X closed under sequential constructs. MainFunc consists of a parallel region which in turn consists of a sequence of parallel-for-loops or barrier statements. Each parallelfor-loop is a normalized loop whose body is a function call.
4.2 CPS Translator
Our translation scheme is inspired by that of Aloor and Nandivada , who translate an input program to an IR (called UWOmpCPS), before lowering it to OM-OpenMP. UWOmpCPS is an extension to CPS (Continuation Passing Style ); its choice was inspired by the fact that CPS naturally provides support for operations like wait and continue. A UWOmpCPS program is similar to a program in CPS form, except that the former may include parallel-for-loops and barriers. One of the sources of overheads of the scheme of Aloor and Nandivada was that all the methods were converted to CPS form. We observe that since only the activities of parallel-for-loops can synchronize with each other (point-to-point or all-to-all), we need to CPS transform only those functions that may be invoked by the iterations of the parallel-for-loop. Further, in the input UWOmp 𝑝𝑟𝑜 program, thread-level barriers (invoked via #pragma omp barrier), may appear outside the work-sharing constructs.
Based on these points, we first present a modified UWOmpCPS grammar and then discuss the modified translation rules. 4.2.1 UWOmpCPS 𝑝𝑟𝑜 : Modified CPS IR.The grammar for the modified IR (called UWOmpCPS 𝑝𝑟𝑜 ) is shown in Figure . Some of the main differences between UWOmpCPS and UWOmpCPS 𝑝𝑟𝑜 are as follows: (i) A program may consist of both CPS (CPSFuncDecl) and non-CPS (FuncDecl) functions. (ii) A CPSParRegion may contain a set of parallel-for-loops in CPS form (CPSParLoop) or barriers (BarrierStmt). (iii) A CPSParLoop can specify a schedule and related options (represented as schedOpt). Note that Stmt denotes any sequential statement, FuncDecl is any regular C function declaration, FunCall is any regular non-CPS function call statement. As is standard in CPS translation, the continuation object is passed as an additional argument to each CPS function call (CPSFunCall). 4.2.2 Generation of code in CPS form.Figure shows the translation rules. Here, a rule of the form 𝑋 ⇒ 𝑌 is used to denote that input code 𝑋 is transformed to the output code 𝑌 in UWOmpCPS 𝑝𝑟𝑜 . The RHS (𝑌 ) may contain further terms with indicating that those terms need to be further transformed. Here, we use #ompparallel as a shortcut for #pragma omp parallel, and #ompfor for #pragma omp for nowait schedOpt. Our CPS transformation starts by transforming the parallel-region (Rule 9). Rules 1-8 are the standard CPS translation rules used to convert a given input program to CPS Form. Note: We only handle code that is reachable from a parallel-for-loop and if any function is called from outside the parallel-region then it is left as it is. Rule 10 has two substeps: (i) the function (fun) called in the body of the parallel-for-loop is translated to CPS form (using the standard CPS transformation rules, [Rules 1-8, Figure ], by passing the identity function 𝑖𝑑 as the continuation. Here, 𝑚𝑘𝐶𝑙𝑠𝑟 is a macro that creates a closure by taking three arguments: a function pointer, the list of arguments required for the function (obtained by invoking a compiler-internal routine bEnv), and a continuation to be executed after executing the function. (ii) The call to fun is replaced by its CPS counterpart by passing the continuation as an additional argument. If a barrier is encountered, it is left as it is (Rule 11).
4.3 OM-OpenMP Translator
We now discuss how we translate code in UWOmpCPS 𝑝𝑟𝑜 format to OM-OpenMP code. We emit code such that each iteration of the parallel-for-loop creates a closure object and enqueues to a work-queue. The details of the work-queue depend on the scheduling policy of the parallel-for-loop. For static scheduling policy, the activities to be executed by each thread is fixed a priori and thus we maintain a local worklist for each thread. For guided or dynamic scheduling, all closures are pushed to a global 'work queue'. Each thread dequeues closures from the queue and executes the same. Figure shows the rule to translate the parallel-for-loop. Line 4 in Figure calls the function getScheduler that takes the scheduling policy string 𝑠𝑐ℎ𝑒𝑑 and threadID 𝑡𝑖𝑑 as parameter. This string is obtained using a call to the function getSchedule using the 𝑠𝑐ℎ𝑒𝑑𝑂𝑝𝑡 string as parameter. Figure shows the pseudocode of the getScheduler function that returns a pointer to the corresponding scheduler function and assigns the worklist to be used by each thread (WL[𝑡𝑖𝑑]). The function assigns the 𝑠𝑐ℎ𝑒𝑑𝑃𝑡𝑟 to the corresponding scheduler function, depending on the value of 𝑠𝑐ℎ𝑒𝑑. We emit a parallel-for-loop that pushes the closure for each activity to WL[𝑡𝑖𝑑] (Lines 5-9 in Figure ). Finally, we invoke the appropriate scheduler (Line 10 in Figure ). K 𝑇 fun ( args ) { S } // fun is called from within parallel -for -loop
⇒ 𝑣𝑜𝑖𝑑 funCPS ( Clsr K , args ) { K S }
2. K fun (a1 ,... , a𝑛 ) ⇒ funCPS (K ,a1 ,... ,𝑎𝑛 )
K S1; S2 // S1 has no call , return // or pragmas inside .   If schedOpt is omitted, then getSchedule sets the schedule to static. Similarly, if schedOpt=runtime, then getSchedule obtains the schedule from the language-specified environment variable.
⇒ S1 ; K S2 4. K { S } ⇒ { K S } 5. K S /
⇒ if (e) { K S1 } else { K S2 } K Y 9. K #ompparallel { S } ⇒ #ompparallel { K S }
4.4 Post-Pass: Type Specific Reduction Ops
The final step in our translation process introduces type specific reduction operations for the operations specified in the OpenMP specification . As mentioned in Section 3, the reduction-related  wait commands (waitRed and waitAllRed, Figure ) in the input UWOmp 𝑝𝑟𝑜 code take a reduction operation and a reduction variable rVar (which stores the reduced result), as additional arguments to the wait command. The compiler uses the declared type of rVar (say, int) to replace the user specified reduction operation (say, ADD representing the '+') with the actual reduction function (say, ADDint) in the wait commands. For each of the primitive types 𝑇 , our runtime provides functions for performing the reduction (for example, ADDint, ADDdouble, and so on).
In addition to introducing the type-specific reduction operation, the reduction procedure needs a method to copy values from one variable to the other (for example, to copy the final computed value to the reduction variable). Similar to the type-specific reduction operations, for each primitive type 𝑇 , our runtime provides functions for performing the copy operation (e.g., 'COPYint(int *from, int *to)'), which is passed as an additional argument to the wait method calls. For example, the command waitAllRedCPS (𝐾, 𝑖==1, ADD, 𝑥), where 𝑥 is the reduction variable of type int, gets replaced by waitAllRedCPS (𝐾, 𝑖==1, ADDint, 𝑥, COPYint).
4.5 Example translation
For a better understanding of our translation scheme, Figure describes the steps to transform a sample UWOmp 𝑝𝑟𝑜 code to OM-OpenMP code. Figure shows the input UWOmp 𝑝𝑟𝑜 code and Figure shows the CPS transformed version. The standard set of CPS transformation rules is applied to the function f to convert it to fCPS, and generate other CPS functions (pCPS1 and pCPS2). We avoid showing the second argument to mkClsr as it depends on the actual statements following the call (for example, S2 and S3). The parallel-for-loop body creates an identity closure K to denote the continuation after executing the parallel-for-loop. It calls fCPS with closure K as an argument.  Figure shows the OM-OpenMP translated code of the UWOmpCPS 𝑝𝑟𝑜 code. This step emits code to identify the appropriate scheduler (Lines 2-7 from Figure ), and wraps the call to function fCPS inside the closure C before enqueuing the closure in the appropriate worklist WL . Finally, Figure shows the OpenMP translated code with the postpass translation rules applied on the waitRedCPS method.