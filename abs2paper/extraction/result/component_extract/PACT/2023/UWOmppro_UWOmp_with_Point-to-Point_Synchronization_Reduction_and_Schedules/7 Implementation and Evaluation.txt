7 Implementation and Evaluation
We implemented our proposed language extension, translation and the runtime support for UWOmp ğ‘ğ‘Ÿğ‘œ in two parts: (i) the translator has been written in Java in the IMOP Compiler Framework -approximately 8000 lines of code (ii) the runtime libraries are implemented in C -approximately 2000 lines of code. IMOP is a source-to-source compiler framework for analyzing and compiling OpenMP programs. To compile the generated OpenMP codes, we used GCC with -O3 switch (includes tail-call optimization). We evaluate our proposed translation scheme and the runtime using 14 benchmark kernels from various sources (details in Figure ). These include all the kernels used by Aloor and Nandivada (except FDTD-2D, which we could not compile/run using the baseline compiler of Aloor and Nandivada) and a few additional kernels: WF, Jacobi1D, Stencil4D, and HP. For each kernel, we indicate the type of synchronization needed and if it uses reduction operations. Note that point-to-point kernels, can also be written using all-to-all synchronization.
To demonstrate the versatility of our proposed techniques, we performed our evaluation on two systems: (i) Dell Precision 7920 server, a 2.3 GHz Intel system with 64 hardware threads, and 64 GB memory, referred to as HW64. (ii) HPE Apollo XL170rGen10 Server, a 2.5 GHz Intel 40-core system, and 192GB memory, referred to as HW40. All numbers reported in this section are obtained by taking a geometric mean over 10 runs. For each benchmark kernel we chose the largest input such that the 10 runs of the UWOmp ğ‘ğ‘Ÿğ‘œ kernel would complete within one hour on HW64. In this section, for a language ğ¿ ğ‘¥ , we use the phrase "performance of an ğ¿ ğ‘¥ program" to mean the performance of the code generated by the compiler for ğ¿ ğ‘¥ , for the program written in ğ¿ ğ‘¥ .
We show our comparative evaluation across four dimensions: (i) UWOmp ğ‘ğ‘Ÿğ‘œ kernels that perform all-to-all synchronization with no reduction operations (kernels 1-7); we compare the performance of these UWOmp ğ‘ğ‘Ÿğ‘œ codes against their UWOmp++ counterparts. (ii) UWOmp ğ‘ğ‘Ÿğ‘œ kernels that perform only point-to-point synchronization, with no reduction (kernels 8-12); we compared their performance with that of their all-to-all versions written in UWOmp ğ‘ğ‘Ÿğ‘œ and standard OpenMP. Note: we could not successfully run the code generated by the UWOmp++ compiler for the all-to-all UWOmp++ versions of these codes and hence we do not show a comparison against these codes. (iii) UWOmp ğ‘ğ‘Ÿğ‘œ kernels performing reduction operations (kernels 13-14); we compare the performance of these kernels with their OpenMP original benchmarks. We first rewrote these kernels to use our reduction algorithm and compare them with their standard OpenMP benchmarks. (iv) Impact of the scheduling policy; we present a comparative behavior of all the kernels by varying the scheduling policy. We also found that the UWOmp ğ‘ğ‘Ÿğ‘œ codes scale well with the increasing number of threads. to lack of space, the details are made available in the extended report . Evaluation of all-to-all synchronization. For the benchmark kernels 1-7, Figure shows the percentage improvement of UWOmp ğ‘ğ‘Ÿğ‘œ codes over their UWOmp++ counterparts, for varying number of threads.
Our evaluation shows that except for KPDP in one particular configuration (64 cores on HW64 and 40 cores on HW40), the UWOmp ğ‘ğ‘Ÿğ‘œ codes perform better than their UWOmp++ counterparts. Even for that particular configuration the performance degradation is minimal (<7%). One common pattern we find is that if a kernel has a lot of computation (for example, LELCR, LCS and WF) UWOmp ğ‘ğ‘Ÿğ‘œ outperforms UWOmp++ significantly, in contrast to kernels with very low computation (for example, KPDP and MCM) where our comparative gains are less. Overall, we find that the percentage improvements varied between âˆ’4.0% to +98.1% on the HW64 system and between âˆ’6.6% to +89.5% on the HW40 system. We believe that such significant performance gains are mainly due to efficient handling of worklists (single local worklist vs two separate worklists in UWOmp++; see Section 6), and being conservative in converting only the essential parts of the code to CPS form.
Note: we avoid showing a comparison with the OpenMP counterparts of these benchmarks as Aloor and Nandivada have already shown that UWOmp++ programs run faster than the plain OpenMP programs, and we show that UWOmp ğ‘ğ‘Ÿğ‘œ programs fare significantly better than their UWOmp++ counterparts. Evaluation of point-to-point synchronization. For the benchmark kernels 8-12, Figure summarizes the percentage improvement of the point-to-point variants of the codes compared to OpenMP, for varying number of threads, on both HW64 and HW40 systems. Figure summarizes the percentage improvement of the point-to-point variants of the codes compared to the all-to-all UWOmp ğ‘ğ‘Ÿğ‘œ versions, for varying #threads. We see a significant performance improvement obtained when using point-to-point synchronization routines over that of OpenMP. The percentage improvement varied between 6.8% to 86.5% on HW64, and between 6.9% to 84.8% on HW40 when compared with OpenMP. The percentage improvement varied between 27.3% to 82.4% on HW64, and between 6.4% to 82.9% on the HW40 system when compared with the all-to-all versions of UWOmp ğ‘ğ‘Ÿğ‘œ .
The main reason of this improvement is due to the lesser amount of communication (and faster execution) in point-to-point synchronization compared to all-to-all synchronization in OpenMP. For most of the kernels we see that the performance improvement reduces gradually with the increasing number of threads. This is mainly because the main overhead in all-to-all synchronization is the waiting time incurred by all the activities. As the number of threads increase, the overall waiting time gets amortized better and leads to a reduction in the overhead. Evaluation of reduction kernels. For the benchmark kernels 13-14, Figure shows the percentage improvement obtained using our proposed reduction scheme against the standard OpenMP benchmarks (using the OpenMP reduction clause wherever possible). We see that the proposed scheme performs significantly better. The percentage improvement varied between 26.1% to 52.4% on HW64, and between 31.4% to 82.7% on HW40, compared to OpenMP.
For reference, we also compared our generated codes using the techniques discussed in this paper (use parallel reduction operation) against that in which one of the activities ğ‘‹ 1 performs the reduction operation in serial. We have found that the parallel reduction operation clearly outperforms (31% to 78%) the serial one; the graphs for the same have been moved to the extended report . Evaluation of different schedules. We evaluate all kernels for different scheduling policies to demonstrate the importance of supporting diverse scheduling policies and the effectiveness of our dynamic and guided schedulers. Figure shows the percentage improvement of dynamic and guided scheduling compared to static scheduling; due to lack of space, we show this evaluation only for a fixed number of threads (set to the maximum available hardware cores in the system). We see that for kernels that only have all-to-all synchronization, the static schedule performed much better; we believe this is mainly due to our proposed optimization for all-to-all synchronization in the context of static-scheduling; see Section 6. In the case of kernels with point-to-point synchronization, since the set of tasks waiting for each other was not predictable, the dynamic/guided scheme performed better.
Further, we observe that for IA and HP kernels, the gains due to dynamic and guided schedules is less. We believe that it is due to the presence of all-to-all reduction operations in those kernels that seem to work better with static scheduling. For most kernels that do not use reduction operations, we find that the dynamic and guided policies work better.
Overall, for dynamic scheduling, the percentage improvement varied between âˆ’45% to +32% on the HW64 system, and between âˆ’43% to +44% on the HW40 system. Similarly, for guided scheduling, the percentage improvement varied between âˆ’39% to +31% on the HW64 system, and between âˆ’47% to +30% on the HW40 system. Such significant variance clearly attests to the importance of supporting different scheduling policies and the efficacy of our implemented schedulers.
Note: We compared our kernels with the baseline OpenMP kernels with dynamic or guided scheduling. We observed that the    (a) HW64 System. #threads = 64.  performance of our dynamic/guided scheduling scheme is comparable to that of the baseline (-2 to 2%); not much overhead. We skip the details due to lack of space.
M C M K P D P G E M V E R L C S L E L C R W F % Improvement
L C S M C M W F G E M V E R L E L C R J a c o b i1 D J a c o b i2 D S t