1 Introduction
The emergence of multi-core systems has brought forth many different parallel languages like X10 , Chapel , HJ , OpenMP , and so on to the mainstream. These languages provide means to express parallel logic conveniently. Besides supporting different ways of expressing parallelism, these languages support varied forms of synchronizations.
For example, OpenMP uses the efficient 'team of workers' model, where each worker (also interchangeably referred to as thread) is given a chunk of activities (for example, iterations of a parallel-for loop) to execute. An important facet of this model is that workers (and not activities) synchronize among themselves using barriers. However, certain computations (for example, stencil computations, graph analytics, and so on) are specified, arguably more conveniently, by expressing the synchronization among all the dependent activities. Languages like X10, HJ, and so on, support such notions of asynchronous activities and synchronization among the activities. Further, in contrast to global barriers (that perform all-to-all synchronization) among the parallel activities of a program, it may be more expressive and efficient (fewer number of communications) to synchronize only the inter-dependent activities. We refer to the latter as the point-to-point mode of synchronization.
We first use a motivating example to illustrate the expressiveness due to point-to-point synchronization and the scope of improved performance therein. Figure shows five different versions of the classical 1D Jacobian kernel (source ). Figure shows the kernel in OpenMP (source ). OpenMP prohibits the use of barrier statements inside the parallel-for-loop as the behaviour of the program can be unpredictable (may lead to incorrect output, correct output, or deadlock) ). In light of such a restriction, at the end of the parallel-for-loop, an implicit barrier is present, which allows all threads to synchronize after computing the respective average values ). The 'single' block performs the pointer swapping and incrementing the time-step variable t. The code snippet in Figure , shows the HJ version using all-to-all barriers among all the asynchronous activities. The next statement synchronizes all the activities before proceeding to the next phase. To support such all-to-all barriers in OpenMP, Aloor and Nandivada proposed UW-OpenMP that introduces the Unique Worker (UW) model in OpenMP, in which the programmer gets an impression that each iteration (a.k.a. activity) of the parallel-for-loop is run by a unique worker and thus the model allows all-to-all barriers to be specified among the activities. Aloor and Nandivada extended this idea to derive UWOmp++, and show that UWOmp++ codes are efficient and arguably more expressive compared to the OpenMP codes. Figure shows UWOmp++ version of the 1D Jacobian Kernel. However, such codes still suffer from multiple drawbacks, as discussed below.
In the code shown in Figure (and in Figure ), each activity 𝑋 i (corresponding to iteration i) is waiting for all remaining activities instead of only the activity 𝑋 1 waiting for all the other activities (to complete their computation), before swapping the pointers. Similarly, each activity waits for every other activity at the second barrier, even though each activity 𝑋 i (i ≠ 1) needs to wait only for 𝑋 1 . This leads to significant communication overheads.
To address such issues of communication overheads and improve the expressiveness, there have been many prior efforts to support point-to-point synchronization in task parallel languages like X10 , HJ , and so on. These languages use explicit synchronization objects (like Clocks and Phasers ) to realize the synchronization. Figure shows the HJ version of the 1D Jacobian kernel. The code snippet first allocates two phaser objects and registers the phaser objects with each activity. These phasers are used to perform only the required communication (signal or wait) among the activities. A similar computation can also be encoded using an array of phasers (one unique phaser per activity).   In the context of OpenMP, Shirako et al. present a promising approach to adapt HJ phasers to OpenMP. They allow activities to explicitly register/deregister themselves with phaser objects and these phaser objects are used to perform the synchronization among the registered activities. However, their design has multiple restrictions: (i) the phaser objects have to be explicitly allocated -leads to cumbersome code, (ii) the synchronization can only be in one direction, that is, from iteration with lower index to iteration with higher index -can be limit expressiveness; (iii) threads (not activities) block on wait operations -can limit parallelism and impact performance negatively; (iv) their scheme cannot work with dynamic/guided scheduling of OpenMP; and (v) the activities cannot perform reduction operations at the synchronization points. OpenMP 5.2 provides support for expressing dependencies between the iterations of parallel-for-loops with the doacross clause . Although this feature allows expressing some form of point to point synchronization, it also suffers from two of the restrictions ((ii) and (v)) discussed above.
In this paper, we address all these issues and propose a generic scheme to allow synchronization among the activities of each parallel-for-loop of OpenMP. We call our extension UWOmp 𝑝𝑟𝑜 . Figure shows a UWOmp 𝑝𝑟𝑜 version of the kernel shown in Figure . Here, the first all-to-all barrier of Figure has been replaced with two commands, where all activities (except the first activity) signal 𝑋 1 , and 𝑋 1 in turn waits for the signals from them. A convenient feature of UWOmp 𝑝𝑟𝑜 is that it supports conditional signal/wait commands. The first argument passed to the corresponding commands, evaluates to 1 (true) or 0 (false) and determines if the command should be executed by that activity or not. Further, the signal (wait) commands can signal to (wait for) multiple activities that are specified by a comma-separated list of iterations. Example: signal(1,i-1,i+1) sends a signal to 𝑋 i−1 and 𝑋 i+1 .
The second barrier of Figure is replaced by signalAll, followed by wait. The given condition in the signalAll command ensures that signalling is done only by 𝑋 1 to all the remaining activities. These activities (𝑋 i , i ≠ 1) in turn wait for that signal.
In contrast to X10 and HJ, in UWOmp 𝑝𝑟𝑜 , activities of a parallelfor-loop can synchronize among themselves without any need for the programmer to explicitly create (or pay the overheads of) clock/phaser objects. Further, the UWOmp 𝑝𝑟𝑜 code performing communication is arguably more readable than that of the HJ code involving multiple phaser objects performing point-to-point communication (for example, Figure vs. Figure ). An important aspect of our design is that we continue to take advantage of the efficient 'team of workers' model of OpenMP to derive high performance.
In addition, UWOmp 𝑝𝑟𝑜 optionally supports efficient reduction operations at the synchronization (wait) points, a feature not supported by languages like X10, HJ, or even OpenMP. Note: though OpenMP supports reduction operations in parallel-for-loops, the final reduced value is only available at the end of the parallel-region (and not immediately after the reduction operation).
UWOmp 𝑝𝑟𝑜 can help effectively and efficiently code wide classes of problems involving point-to-point synchronizations and reductions. Note: We do not claim that using point-to-point synchronization among the activities of parallel-for-loops is the only/best way to encode such computations. Instead, our proposed extension (common in modern languages like X10, HJ, and so on) provides additional ways to encode task parallelism, which is otherwise missing in OpenMP (and UWOmp++), while not missing out on the advantage of the efficient 'team of workers' model of OpenMP.
Our Contributions
• We propose UWOmp 𝑝𝑟𝑜 to allow point-to-point synchronization and reduction operations, among the activities of a parallel-for-loop. In contrast to UWOmp++, UWOmp 𝑝𝑟𝑜 supports all the scheduling policies defined in OpenMP.
• We present a scheme to compile UWOmp 𝑝𝑟𝑜 code to efficient OpenMP code by taking advantage of continuation-passing-style (CPS) to efficiently realize wait and continue operations.
• We present a runtime based on a novel communication subsystem to support efficient signal, wait, and reduction operations.
• To support fast reduction operations, we propose two reduction algorithms termed eager and lazy, to support efficient reduction operations among a subset of activities and all activities, respectively.
• We have implemented our scheme in the IMOP compiler framework and performed a thorough evaluation. We show that our generated code scales well and is highly performant.