<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoAgents: A Framework for Automatic Agent Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2024-04-29">29 Apr 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,119.87,178.57,68.92,8.96"><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
							<email>gy.chen@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,197.75,178.57,47.88,8.96"><forename type="first">Siwei</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,258.68,178.57,30.92,8.96"><forename type="first">Yu</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,302.66,178.57,42.35,8.96"><forename type="first">Ge</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,353.97,178.57,58.24,8.96"><forename type="first">Jaward</forename><surname>Sesay</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,421.18,178.57,64.48,8.96"><forename type="first">BÃ¶rje</forename><surname>Karlsson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,262.82,190.30,26.29,8.96"><forename type="first">Jie</forename><surname>Fu</surname></persName>
							<email>jiefu@ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName coords="1,301.73,190.30,43.46,8.96"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
							<email>ymshi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoAgents: A Framework for Automatic Agent Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-29">29 Apr 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">6FCC36C328E60E3C8C29B3E445976667</idno>
					<idno type="arXiv">arXiv:2309.17288v3[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-07-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multiagent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks. The repository of this project is available at https://github.com/Link-AGI/AutoAgents.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) have exhibited astounding capabilities <ref type="bibr" coords="1,405.39,545.69,15.89,8.64" target="#b25">[26,</ref><ref type="bibr" coords="1,424.81,545.69,12.50,8.64" target="#b21">22,</ref><ref type="bibr" coords="1,440.85,545.69,13.35,8.64" target="#b28">29]</ref> as versatile task-solving agents, endowed with a rich blend of knowledge and skills. Nevertheless, they still face difficulties <ref type="bibr" coords="1,152.52,567.51,15.73,8.64" target="#b25">[26,</ref><ref type="bibr" coords="1,170.74,567.51,12.43,8.64" target="#b21">22,</ref><ref type="bibr" coords="1,185.67,567.51,8.27,8.64" target="#b1">2]</ref> in tackling various tasks that require intensive knowledge and reasoning, such as avoiding hallucination <ref type="bibr" coords="1,207.47,578.42,15.13,8.64" target="#b18">[19]</ref>, employing slow-thinking strategies <ref type="bibr" coords="1,367.60,578.42,15.12,8.64" target="#b29">[30]</ref>, ensuring trustworthiness <ref type="bibr" coords="1,486.34,578.42,15.12,8.64" target="#b31">[32]</ref>, and in combining diverse domain knowledge and long-horizon planning. In contrast, humans often exploit the benefits of collaborative problem solving, which enables them to work together effectively to solve non-routine problems in diverse domains and enhance the quality and reliability of the solutions by distributing the workload among specialties and applying a diversity of perspectives and expertise <ref type="bibr" coords="1,146.31,632.97,15.77,8.64" target="#b20">[21,</ref><ref type="bibr" coords="1,164.57,632.97,12.45,8.64" target="#b26">27,</ref><ref type="bibr" coords="1,179.51,632.97,7.19,8.64" target="#b0">1]</ref>.</p><p>Inspired by collaborative problem solving, several recent works <ref type="bibr" coords="1,369.23,649.36,15.89,8.64" target="#b33">[34,</ref><ref type="bibr" coords="1,387.72,649.36,7.52,8.64" target="#b6">7,</ref><ref type="bibr" coords="1,397.84,649.36,12.50,8.64" target="#b16">17,</ref><ref type="bibr" coords="1,412.95,649.36,13.35,8.64" target="#b10">11]</ref> have improved the task-solving capabilities of LLMs by integrating multi-agent discussion. However, most of these multiagent systems depend on handcrafted or user-specified agents, with specific roles and necessitating human supervision, which often restricts the scope of collaborative applications. Moreover, manually Table <ref type="table" coords="2,143.71,80.12,3.88,8.64">1</ref>: Comparison of existing and proposed frameworks for LLM-based Agent framework.</p><p>To synthesize heterogeneous information from diverse domains is often a crucial requirement in creative industries and other real-world scenarios. We illustrate a concrete example of how AutoAgents tackles the challenging task of writing a novel about the awakening of artificial intelligence in Figure <ref type="figure" coords="2,498.42,481.17,3.66,8.64">1</ref>. The Story Planner and Researcher collaborate to devise the plot of the story with their respective expertise, while the Character Developer and Writer enrich the novel content through imagination based on the story. Moreover, we conduct quantitative experiments and case studies in complex tasks to demonstrate the effectiveness of AutoAgents. We also conduct a comprehensive analysis and demonstrate the importance of dynamic agents for handling complex tasks, the indispensability of self-refinement for proficient agents, and the effectiveness of collaborative conversation.</p><p>To summarize, this paper makes the following novel contributions: First, we propose AutoAgents, a novel framework that dynamically synthesizes and coordinates multiple expert agents to form customized AI teams for diverse tasks. Second, we conduct rigorous quantitative experiments on two challenging tasks and demonstrate that AutoAgents significantly improves both knowledge acquisition and reasoning ability in LLMs and outperforms other generated-agent frameworks. Third, we showcase AutoAgents' ability to adapt to complex tasks by applying it in various scenarios such as software development. Finally, we conduct a thorough investigation and reveal the importance of dynamic agents for accommodating complex tasks and the necessity of self-refinement for proficient agents, and the efficacy of collaborative conversation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>LLM-based Autonomous Agents. LLMs have been widely used as core controllers for autonomous agents that can accomplish specific objectives. Auto-GPT <ref type="bibr" coords="2,347.54,713.51,16.73,8.64" target="#b9">[10]</ref> is an early work that leverages an Figure <ref type="figure" coords="3,137.00,433.42,3.95,8.64">1</ref>: A schematic diagram of AutoAgents. The system takes the user input as a starting point and generates a set of specialized agents for novel writing, along with a corresponding execution plan. The agents collaboratively carry out the tasks according to the plan and produce the final novel. Meanwhile, an observer monitors the generation and execution of the Agents and the plan, ensuring the quality and coherence of the process.</p><p>LLM as an AI agent that can autonomously achieve a given goal with the help of several tools. However, Auto-GPT does not support multi-agent collaboration and can only work in isolation. One way to enhance the task-solving capabilities of LLMs is to assign different roles and responsibilities to multiple LLMs and let them coordinate their actions to achieve a common goal. For example, BabyAGI <ref type="bibr" coords="3,149.79,560.79,16.73,8.64" target="#b19">[20]</ref> is an AI-powered task management system with multiple LLM-based agents. One agent creates new tasks based on the previous task's objective and result, another agent prioritizes the task list, and another agent completes tasks. BabyAGI is a multi-agent system with a fixed order of agent communication. MetaGPT <ref type="bibr" coords="3,280.65,593.51,16.73,8.64" target="#b11">[12]</ref> is a multi-agent framework for assigning different roles to GPTs to form a collaborative software entity for complex tasks. It is a specialized LLMbased multi-agent framework for collaborative software development. Camel <ref type="bibr" coords="3,416.97,615.33,16.59,8.64" target="#b15">[16]</ref> is an LLM-based communicative agent framework that demonstrates how role-playing can be used to enable chat agents to communicate with each other for task completion. However, Camel does not support tool-using. Several recent works <ref type="bibr" coords="3,236.82,648.06,15.65,8.64" target="#b33">[34,</ref><ref type="bibr" coords="3,254.88,648.06,7.42,8.64" target="#b6">7,</ref><ref type="bibr" coords="3,264.70,648.06,12.40,8.64" target="#b16">17,</ref><ref type="bibr" coords="3,279.51,648.06,12.40,8.64" target="#b10">11,</ref><ref type="bibr" coords="3,294.32,648.06,12.40,8.64" target="#b30">31,</ref><ref type="bibr" coords="3,309.13,648.06,13.21,8.64" target="#b14">15]</ref> have enhanced the task-solving capabilities of LLMs by integrating multi-agent discussion. For instance, <ref type="bibr" coords="3,339.41,658.97,16.47,8.64" target="#b33">[34]</ref> proposes a multi-agent debate system that allows LLMs to argue for or against a given claim and generate a debate summary. <ref type="bibr" coords="3,453.19,669.88,11.48,8.64" target="#b6">[7]</ref> introduce a multi-agent dialogue system that enables LLMs to exchange information and opinions on a given topic and generate a dialogue report. AutoGen <ref type="bibr" coords="3,296.15,691.70,16.61,8.64" target="#b36">[37]</ref> is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. However, most of these multi-agent systems rely on handcrafted or user-specified agents with specific roles Agent Generalization. Several studies <ref type="bibr" coords="4,264.44,427.43,15.66,8.64" target="#b23">[24,</ref><ref type="bibr" coords="4,282.25,427.43,13.21,8.64" target="#b34">35]</ref> employ LLMs to generate agents for social simulacra and epidemic modeling, demonstrating how this technique can facilitate designers in assessing and improving their modeling designs prior to deploying them to real users. Likewise, ExpertPrompting <ref type="bibr" coords="4,123.08,460.16,16.52,8.64" target="#b37">[38]</ref> devised a method to generate diverse profiles of agents that can cooperate with human users to accomplish tasks with minimal supervision. However, this method still depends on a restricted set of predefined agents, and the generated agents vary only in their profiles. Recently, SSP <ref type="bibr" coords="4,487.27,481.98,16.73,8.64" target="#b33">[34]</ref> and AgentVerse <ref type="bibr" coords="4,174.29,492.89,11.66,8.64" target="#b4">[5]</ref> have proposed frameworks for automatically generating unlimited agents. SSP enables LLMs to generate agents for problem input by providing some agent samples, and has these agents solve the problem. AgentVerse generates the execution plan through the generated agents' discussions and adds evaluation strategies for cyclic execution. Unlike the previous two methods, AutoAgents places a heightened emphasis on the reliability of its generated agents and strategic plans, thereby enhancing task execution effect through the utilization of collaborative refinement actions and the integration of self-refinement actions , as illustrated in Table <ref type="table" coords="4,382.09,558.34,3.74,8.64">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Framework for Automatic Agent Generation</head><p>To enhance the effectiveness of autonomous multi-agent groups in accomplishing their goals, the process of AutoAgents consists of two critical stages: Drafting Stage and Execution Stage, as illustrated in Figure <ref type="figure" coords="4,192.78,658.97,3.81,8.64" target="#fig_0">2</ref>. The drafting stage synthesizes an agent team and an execution plan that are customized to the task by analyzing the input problem or task. The execution stage refines the plan by enabling inter-agent collaboration and feedback, and delivers the final result. The interagent collaboration is based on some principles of multi-agent cooperation, such as communication, coordination, and consensus. These principles help the agents to share information, align their actions, reach agreements, and adapt to the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Drafting Stage</head><p>Empirical evidence <ref type="bibr" coords="5,191.45,95.45,16.73,8.64" target="#b35">[36]</ref> suggests that diversity within human groups fosters diverse perspectives, which enhances the group's performance across various tasks. The drafting stage, which determines the composition of a multi-agent group, plays a crucial role in setting the upper limits of the group's capabilities. Therefore, it is imperative to generate the optimal agent team and execution plan that can maximize the group's potential.</p><p>Predominant methodologies <ref type="bibr" coords="5,227.74,155.48,15.89,8.64" target="#b9">[10,</ref><ref type="bibr" coords="5,247.61,155.48,12.50,8.64" target="#b11">12,</ref><ref type="bibr" coords="5,264.09,155.48,13.35,8.64" target="#b36">37]</ref> for assigning role descriptions to autonomous agents rely heavily on human intuition and prior knowledge, requiring manual assignment based on task understanding. Consistent with several parallel findings <ref type="bibr" coords="5,336.42,177.30,15.89,8.64" target="#b37">[38,</ref><ref type="bibr" coords="5,354.82,177.30,12.50,8.64" target="#b33">34,</ref><ref type="bibr" coords="5,369.83,177.30,7.27,8.64" target="#b4">5]</ref>, dynamically designing agents with different roles can significantly enhance their efficacy. However, the scalability and rationality of agent and plan generation are still unclear, especially in the face of various complex problem environments.</p><p>On the one hand, the generated agents should exhibit diversity to accommodate various tasks. On the other hand, the agent and the plan generation should adhere to certain principles, rendering their role allocation more rational. Therefore, we devise three artificially predefined agents to produce agent teams and execution plans, integrating artificial prior knowledge and the dynamic adaptation capability of LLMs to generate more sensible agent teams and execution plans. The three artificially predefined agents comprise Planner, Agent Observer, and Plan Observer:</p><p>â¢ Planner P generates and refines an agent team and an execution plan based on the content of the task. â¢ Agent Observer O agent provides suggestions on the rationality of the agent team members and their matching degree with the task. â¢ Plan Observer O plan provides suggestions on the rationality of the execution plan and its matching degree with the task and the agent team.</p><p>The Planner generates initial agent team members and a specific plan, and improves the agent team and execution plan based on continuous communication with the Agent Observer and Plan Observer.</p><p>Agent Generation. The Planner generates the agent team and facilitates its continuous improvement through reciprocal communication with the Agent Observer. To enable Planner to produce rational agents, we have devised a standard format for the essential elements of a single agent. For each agent A = {P, D, T, S}, the Planner needs to specify its prompt P, description D, toolset T, and suggestions S.</p><p>â¢ Prompt P provides a detailed and customized depiction of the expert identity for each specific agent, which comprises profile, goal, and constraints. Profile reflects the domain expertise of the role or job title. Goal indicates the primary responsibility or objective that the role aims to achieve. Constraints specify limitations or principles the role must adhere to when performing actions. â¢ Description D gives additional concrete identity to help establish a more comprehensive role, develop an execution plan, and inspect problems. â¢ Toolset T equips the Agent with tools that it can use, selected from a predefined set of tools.</p><p>The rationale for not using all the tools for each agent here is to prevent decision-making confusion caused by excessive tools. â¢ Suggestions S supplies some suggestions for each agent to execute the current task, including but not limited to a clear output, extraction of historical information, and suggestions for execution steps.</p><p>Based on the agent list {A 1 , A 2 , â¢ â¢ â¢ , A n } generated by Planner, the Agent Observer evaluates the quality and suitability of each agent. The Agent Observer first verifies whether every agent conforms to the aforementioned specifications and identifies any missing elements {P, description D, toolset T}. Secondly, the Agent Observer assesses the compatibility of each agent with the task, according to their description information and task content. Finally, the Agent Observer examines the agent list for any redundant or missing roles and eliminates or adds them accordingly.</p><p>After n rounds of bidirectional communication between the Planner and the Agent Observer, the optimal agent list for accomplishing the task is established. Given the vital role of the agent list in the  task execution, this framework employs a predefined agent and multiple rounds of iterative dialogue among multiple agents to finalize the agent list, thereby enhancing the stability and reliability of the execution phase.</p><p>Plan Generation. In parallel to agent generation, the Planner formulates the execution plan and promotes its progressive improvement through reciprocal communication with the Plan Observer. For a given task, the Planner delineates the specific steps {S 1 , S 2 , â¢ â¢ â¢ S n } to accomplish it in the execution plan P . Each step S i entails a clear identification of the agent A j responsible for it, as well as the input information and expected output required for it.</p><p>The Plan Observer subsequently validates the execution plan</p><formula xml:id="formula_0">P = {S 1 , S 2 , â¢ â¢ â¢ S n } according to the agent list {A 1 , A 2 , â¢ â¢ â¢ , A n }</formula><p>and the task content. It firstly ensures that each step has a corresponding agent and that the step content is coherent and concise. It secondly assesses whether all the steps are sufficient, whether the task can be accomplished, and whether there are any gaps that need to be filled. It finally provides feedback to the Planner, who further refines the execution plan accordingly. After n rounds of dialogue between the Planner and the Plan Observer, the ultimate execution plan for achieving the task is established.</p><p>Task Execution Actions. The Planner devises an execution plan that automatically assigns the requisite agents for diverse tasks. The execution plan comprises two actions of task execution: selfrefinement by a single agent and collaborative refinement by multiple agents, as shown in Figure <ref type="figure" coords="6,498.21,485.07,3.77,8.64" target="#fig_2">3</ref>. Self-refinement empowers an individual agent to augment its proficiency in accomplishing some specialized tasks. Collaborative refinement fosters knowledge sharing among multiple agents and achieves tasks requiring interdisciplinary expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Execution Stage</head><p>In the drafting phase, the framework generates an agent list and an execution plan based on the task requirements. Then, the framework creates corresponding roles and executes the plans in the execution environment <ref type="foot" coords="6,215.68,590.19,3.49,6.05" target="#foot_0">3</ref> . The communication and cooperation among multi-agent systems are essential for accomplishing the tasks effectively. This section elaborates on the communication among multiple agents, the task execution strategies, and the knowledge-sharing mechanisms.</p><p>Communication of Multiple Agent. The communication structures among agents have been investigated by many studies <ref type="bibr" coords="6,229.50,640.97,10.91,8.64" target="#b4">[5,</ref><ref type="bibr" coords="6,243.44,640.97,12.50,8.64" target="#b33">34,</ref><ref type="bibr" coords="6,258.99,640.97,12.50,8.64" target="#b24">25,</ref><ref type="bibr" coords="6,274.53,640.97,8.36,8.64" target="#b2">3]</ref> to examine their impact on task performance. In this framework, we adopt the vertical communication paradigm, which assigns different tasks to agents according to their roles. To facilitate the specific division of labor among the agents in the generated team, we introduce a predefined Action Observer as the team leader to coordinate the execution plan. Specifically, This mechanism of refinement and communication recurs until the Action Observer attains a unanimous agreement on the execution responses, or the process reaches its maximum iteration limit. For scenarios that demand iterative decision-making towards specific objectives, such as software development, vertical communication would be a preferable option.</p><p>Self-refinement Agent. Besides the inter-agent communication, the performance of a single agent also exerts a significant impact on the overall quality of feedback results. Hence, drawing on mechanisms such as AutoGPT <ref type="bibr" coords="7,233.58,451.60,16.65,8.64" target="#b9">[10]</ref> and ReAct <ref type="bibr" coords="7,297.92,451.60,15.34,8.64" target="#b39">[40]</ref>, we have devised a self-refinement mechanism for an individual agent.</p><p>For a single agent A, the action at step t is at a t = l t âª p t âª o t , where l t denotes the thought or the reasoning trace in the language space, which does not alter the external environment, and thus yields no observational feedback, p t represents the execution plan for task completion, o t comprises the completion steps and execution output for this time.</p><p>As illustrated in Figure <ref type="figure" coords="7,205.53,528.01,3.77,8.64" target="#fig_0">2</ref>, various types of useful thoughts can assist in devising a refinement plan. The execution plan enables the agent to anticipate the steps they need to undertake in the future, and the observational content of the execution result construction allows the agent to reevaluate and enhance the plan arrangement, thereby constructing more refined and complete actions. Through a cycle of self-continuous thinking, planning, execution, and feedback, a single agent can effectively execute and accomplish task content.</p><p>Collaborative Refinement Action. In the collaborative refinement action, the agents collaboratively refine and execute the tasks in a sequential manner. Each round of the collaboration involves a fixed order of turn-taking among the agents, who generate their responses based on the current observation. The chat history slot of each agent is updated by concatenating the previous utterances of the other agents. The collaboration terminates automatically when the agents reach a consensus or the maximum number of discussions is reached.</p><p>Knowledge Sharing Mechanism. AutoAgents also facilitates the sharing of execution results among various agents for improved communication and feedback. However, when the number of agents is large and a single agent has more self-iterations, it will generate more historical information. Due to the token limitation of LLM models, they often cannot encompass all information. Hence, this framework provides short-term memory, long-term memory, and dynamic memory.</p><p>Short-term memory is chiefly concentrated on a singular action, encompassing the gamut of intermediary notions, strategies, and outcomes that emerge during the self-refinement or collaborative refinement phases of an individual action. It is salient to note that these actions frequently culminate in a distilled summary of critical information, epitomizing the final phase of the refinement trajectory.</p><p>Long-term memory principally focuses on chronicling the historical trajectory of multifarious actions, predominantly documenting the executed results of each task along with the synthesis of vital feedback information. This aspect is imperative for evaluating the comprehensive extent of task completion.</p><p>Dynamic memory predominantly serves actions necessitating specialized attention. The Action Observer, having access to long-term memory archives, adeptly extracts ancillary information, dynamically tailoring it to the specific requirements of the action for task execution. This process significantly augments the efficiency of a single action in task fulfillment. O agent provides feedback on agent team.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>P refines agent team based on feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>O plan provides feedback on execution plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>P refines execution plan based on feedback. 9: until No feedback or reached the maximum iteration limit. </p><formula xml:id="formula_1">for {A i , â¢ â¢ â¢ , A j } do 18:</formula><p>Agent A m analyzes S k , M S and M D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>Agent A m plans the current step and executes this step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>The execution result is stored in M S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>until No step or reached the maximum iteration limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>The execution results of task S k are stored in M L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>24:</head><p>O action coordinates {S 1 , S 2 , â¢ â¢ â¢ S n } and monitors execution. 25: end for 26: return Execution results of final step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In order to demonstrate the capabilities and performance of AutoAgents in orchestrating autonomous agent groups to collaboratively accomplish tasks, we have performed extensive quantitative experiments on benchmark tasks and thorough case studies on more complex and realistic applications. In the quantitative analysis, we mainly present results for the Open-ended Question Answer task (detailed in Section 4.1) and the Trivia Creative Writing task (detailed in Section 4.2) to evaluate the framework effectiveness under distinct settings. The Case Studies, discussed in Section 4.4, illustrate the potential of a multi-agent group tackling intricate practical scenarios cooperatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>We conduct all experiments using the GPT-4 API <ref type="foot" coords="8,406.24,683.37,3.49,6.05" target="#foot_1">4</ref> and set the temperature to 0 to ensure reproducibility. The rationale behind this selection is the exceptional performance these models offer, providing more accurate and standardized output. Additionally, their accessibility and ease of use through APIs enable us to directly call and interact with the models during our research, significantly simplifying the process. The maximum number of discussions during the drafting phase is 3, and the maximum number of self-refinement by a single agent and collaborative refinement by multiple agents during the execution phase is 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Open-ended Question Answer</head><p>Task Description. Open-ended Question Answering is a crucial and challenging task in the domain of NLP and generative AI. It requires an AI system to produce coherent, elaborate, and human-like responses to questions that have no predetermined or fixed set of possible answers. <ref type="bibr" coords="9,447.73,184.97,16.70,8.64" target="#b40">[41]</ref> proposed MT-bench, a benchmark consisting of 80 high-quality collected open-ended questions from various categories such as common sense, counterfactual, coding, etc. We then utilize AutoAgents to produce collaborative answers based on multiple generated agents and compare them with the responses given by Vicuna-13B, ChatGPT, and GPT-4. Evaluation Metrics. To measure the quality of open-ended responses with minimal evaluation bias, we adopt FairEval <ref type="bibr" coords="9,189.76,345.62,16.73,8.64" target="#b32">[33]</ref> and HumanEval as the evaluation metrics for both the single agent and AutoAgents. FairEval incorporates several methods to mitigate the impact of various sources of bias, resulting in a better alignment with human judgment. For HumanEval, we enlist several volunteers to rate the responses from different models based on their helpfulness, reliability, accuracy, and level of detail.</p><p>Results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Trivia Creative Writing</head><p>Task Description. The Trivia Creative Writing task <ref type="bibr" coords="9,312.78,482.41,16.47,8.64" target="#b33">[34]</ref> challenges the capabilities of large language models to retrieve and integrate diverse information from their internal self-compressed knowledge. This task requires a model to craft a coherent story around a given topic while incorporating the answers to N trivia questions. We evaluate the models under two settings, N = 5 and N = 10, where a higher N entails more trivia questions and thus demands the model to exhibit more extensive domain knowledge. We constructed a benchmark consisting of 100 instances for each N , encompassing a total of 1000 trivia questions. Evaluation Metrics. Drawing on the approach of <ref type="bibr" coords="9,322.42,702.61,15.41,8.64" target="#b33">[34]</ref>, we adopt an automatic metric to identify factual errors and measure a model's capacity to integrate diverse domain knowledge. We conduct string matching with the veridical target answers for each question on the generated output. The target answers are supplied from the TriviaQA dataset <ref type="bibr" coords="10,389.46,86.39,15.42,8.64" target="#b13">[14]</ref>, and each question can have a list of answer variants. A match to any of the answer variants of a question is regarded as a correct mention. The metric score is calculated as Trivia Creative Writing Metric Score = # correct answer mentions/# trivia questions.</p><p>Results. Table <ref type="table" coords="10,167.40,135.50,4.88,8.64" target="#tab_4">3</ref> demonstrates the superior performance of AutoAgents in knowledge acquisition over the existing methods. Compared to the Standard method, which does not employ Agent Generation, AutoAgents achieves a remarkable 10% improvement across all experiments. Moreover, AutoAgents also surpasses SSP <ref type="bibr" coords="10,184.50,168.23,15.14,8.64" target="#b33">[34]</ref>, which utilizes agent generation but with a different approach. The enhanced performance of AutoAgents can be attributed to its elaborate methods of agent generation discussions and task execution including collaborative refinement and self-refinement. More examples are given in the Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Analysis</head><p>This section delves into the significance of key components within AutoAgents by separately analyzing the self-refinement action, collaborative refinement action, dynamic memory, and observers in the draft stage across 20 instances<ref type="foot" coords="10,231.89,265.32,3.49,6.05" target="#foot_2">5</ref> of the Trivia Creative Writing task and additional case studies.  <ref type="bibr" coords="10,168.21,363.70,16.60,8.64" target="#b38">[39]</ref> 66.0 -11.5% SPP-Profile <ref type="bibr" coords="10,197.00,374.61,16.60,8.64" target="#b33">[34]</ref> 74.0 -0.01% SPP <ref type="bibr" coords="10,167.11,385.51,16.60,8.64" target="#b33">[34]</ref> 84.4 +13.1% Collaborative discussion is crucial for rational agent generation and plan allocation. During the Drafting Stage, the Planner in AutoAgents engages in collaborative discussions with two Observers to determine the optimal list of agents and the execution plan. Figure <ref type="figure" coords="10,373.97,493.17,4.96,8.64" target="#fig_4">5</ref> illustrates the contrast between agent generation with and without collaborative discussion. In the absence of Observer feedback, the Planner tends to generate programmers exclusively to accomplish game development, neglecting the holistic process of game creation. With the input and coordination of the Observers, the Planner incorporates game design experts, UI design experts, and testing experts into the agent list. It is evident that the agent generation under collaborative discussions is more comprehensive and more aligned with the realistic scenarios of game development. This also corroborates the significance of collaborative discussions for agent generation and plan allocation, which will subsequently influence the execution outcomes. Concurrently, Table <ref type="table" coords="10,288.37,580.44,4.97,8.64" target="#tab_5">4</ref> elucidates that in the absence of observers, there is a marked 3% reduction in the overall performance of AutoAgents. This substantiates the imperative role of collaborative discussions in agent generation. AutoAgent markedly enhances the caliber of agent generation via collaborative discussions, a facet notably overlooked by other generative frameworks in their consideration of agent generation quality. The empirical data presented in Table <ref type="table" coords="10,499.12,624.08,4.88,8.64" target="#tab_2">2</ref> and 3 further accentuate the superiority of AutoAgents when juxtaposed against counterparts like AgentVerse and SPP.</p><p>Enhancing single-agent through self-refinement. Self-Refinement <ref type="bibr" coords="10,397.28,662.29,15.88,8.64" target="#b17">[18,</ref><ref type="bibr" coords="10,416.90,662.29,12.50,8.64" target="#b27">28,</ref><ref type="bibr" coords="10,433.14,662.29,7.52,8.64" target="#b8">9,</ref><ref type="bibr" coords="10,444.40,662.29,7.52,8.64" target="#b5">6,</ref><ref type="bibr" coords="10,455.66,662.29,12.50,8.64" target="#b12">13,</ref><ref type="bibr" coords="10,471.89,662.29,13.35,8.64" target="#b39">40]</ref> is a technique that enables LLMs to "converse" with themselves, evaluate their own generation, and iteratively improve their answers. Self-refinement has been shown to enhance the accuracy of LLMs' outputs in various domains <ref type="bibr" coords="10,214.03,695.01,15.65,8.64" target="#b17">[18,</ref><ref type="bibr" coords="10,231.71,695.01,12.40,8.64" target="#b27">28,</ref><ref type="bibr" coords="10,246.14,695.01,7.42,8.64" target="#b8">9,</ref><ref type="bibr" coords="10,255.58,695.01,7.42,8.64" target="#b5">6,</ref><ref type="bibr" coords="10,265.03,695.01,12.40,8.64" target="#b12">13,</ref><ref type="bibr" coords="10,279.46,695.01,11.74,8.64" target="#b39">40]</ref>. Although AutoAgents is a framework for multi-agent  collaboration, it also requires self-refinement agents to perform specialized roles for individual tasks.</p><p>As shown in the results in Table <ref type="table" coords="11,238.21,213.84,3.76,8.64" target="#tab_5">4</ref>, the performance of AutoAgents decreases by 3% in the absence of the self-refinement action. This observation corroborates the assertion that self-refinement is instrumental in augmenting proficiency in trivia creative writing tasks. Furthermore, the enhancement of single agents via self-refinement plays a pivotal role in fortifying the integrity of the overarching multi-agent framework.</p><p>Enhancing multi-agent collaboration through collaborative refinement action. For collaborative refinement, the process resembles the collaborative dialogue mentioned above, which involves integrating knowledge from different domains to accomplish tasks that demand cross-domain knowledge fusion. The results in Table <ref type="table" coords="11,219.35,306.59,4.97,8.64" target="#tab_5">4</ref> demonstrate the performance when collaborative refinement is absent. It's observable that compared to the scenario with AutoAgents, there is a decline of 2%. Since the necessity for multiple agents to collaborate on a single task is entirely dependent on the decision made by the agent in the drafting phase, not all problems necessarily involve tasks that require collaborative refinement. However, it's evident that when this principle is omitted from the prompt's design, there's a noticeable performance decrease.</p><p>Improve the effectiveness of actions by dynamic memory. Dynamic memory predominantly addresses the requisites of specialized agents. As shown in Figure <ref type="figure" coords="11,363.58,388.43,3.66,8.64" target="#fig_3">4</ref>, the Action Observer amalgamates pivotal data for forthcoming tasks, utilizing the historical action records archived in long-term memory. Table <ref type="table" coords="11,133.94,410.25,5.08,8.64" target="#tab_5">4</ref> elucidates a 1% diminution in the efficacy of AutoAgents bereft of dynamic memory. Quintessential insights derived from dynamic memory are assimilated into the prompt, thereby augmenting the comprehension of critical information and bolstering the operational proficiency of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>To demonstrate the applicability of AutoAgents to more sophisticated and realistic scenarios, we conduct a case study in the software engineering domain. Software engineering is a complex collaborative endeavor that involves diverse roles and responsibilities. From developers who create the underlying code, to UI designers who prioritize user experience, and software testers who ensure the software's quality, experts collaboratively work to enhance and refine the application, ensuring that it meets both functional and user-centric criteria.</p><p>As an illustration in Figure <ref type="figure" coords="11,219.01,560.90,3.80,8.64" target="#fig_5">6</ref>, a Tetris game has been developed by employing AutoAgents, which has generated various expert roles, such as game design expert, UI design expert, programmer, and debugging expert, to accomplish the game development task. The game design experts provide the game logic documents that specify the rules and mechanics of the game. The UI design experts design the UI components that create the visual interface of the game. The programmers implement the game design based on the aforementioned documents and use appropriate programming languages and tools. Finally, the debugging expert tests the game and debuges the program to ensure its functionality and quality. The game development process is based on the collaboration of multiple expert roles, with more elaborate documentation and programs, making it easier for users to comprehend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces AutoAgents, an innovative framework for automatically synthesizing collaborative specialized agents. AutoAgents mimics the collaborative process of human teams by decomposing tasks into drafting and execution phases and delegating different subtasks to different agents. Our experimental and empirical evaluation validates the advantages of AutoAgents, as it surpasses single agents and other groupings in various tasks that demand diverse skills. Furthermore, our case study in software development illustrates the versatility and potential benefits of our proposed framework. AutoAgents opens up new possibilities for enhancing the interaction and cooperation among agents and transforms the landscape of complex problem-solving. We envisage that its principles can be further generalized and refined to deal with a broader range of tasks, paving the way towards more useful assistive AI. Figure <ref type="figure" coords="15,180.72,437.39,3.88,8.64">7</ref>: An example of the self-refinement process of programmers' coding make sure the story they produce makes sense and is consistently improved upon. This also supports the idea that working together in this way is helpful when dealing with complicated tasks.</p><p>Dynamic agents enhance the adaptability of complex tasks. The ability to generate dynamic agents for various tasks is crucial for enhancing their adaptability to diverse scenarios. Figure <ref type="figure" coords="15,457.43,549.83,5.02,8.64" target="#fig_7">9</ref> illustrates the contrast between GPT4 and AutoAgents' responses to open-ended questions. Unlike GPT-4, AutoAgents can produce agents from three distinct domains, which can provide more elaborate answers. For the trivia creative writing task in Figure <ref type="figure" coords="15,326.61,582.56,10.16,8.64" target="#fig_8">10</ref> and 11, AutoAgents employs a four-step approach for task decomposition. Initially, it sources the answer to the given question using a domainspecific agent, followed by the construction of a narrative. Concurrently, the Language Expert Agent plays a pivotal role, conducting multiple checks to verify the coherence between the narrative and the question, thus guaranteeing the narrative's accuracy.</p><p>Furthermore, as illustrated in Figure <ref type="figure" coords="15,252.72,642.58,8.19,8.64" target="#fig_10">12</ref>, the Action Observer orchestrates the interaction among multiple generative agents. It provides a concise summary of essential information, proving instrumental in fostering collaboration between various intelligent agents. This coordination is key to ensuring the seamless flow of the task execution process. Collectively, these instances vividly showcase the adaptability and efficiency of our dynamic agent generation framework in handling complex tasks.</p><p>Conversely, the prompt employed by AutoAgents exhibits a more universal nature, signifying its capacity to acclimate to diverse tasks without necessitating bespoke customization. As Table 5   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework Application Agent Generalization by Multi-Agent Discussion</head><p>Prompt Generalization</p><p>Social Simulacra <ref type="bibr" coords="17,171.63,419.23,13.73,7.14" target="#b23">[24]</ref> Social Simulation Epidemic Modeling <ref type="bibr" coords="17,181.24,430.21,13.73,7.14" target="#b34">[35]</ref> Social Simulation SSP <ref type="bibr" coords="17,129.30,441.20,13.73,7.14" target="#b33">[34]</ref> General Autonomous Agents AgentVerse <ref type="bibr" coords="17,153.99,452.19,9.61,7.14" target="#b4">[5]</ref> General Autonomous Agents AutoAgents General Autonomous Agents delineates, both AgentVerse<ref type="foot" coords="17,219.12,496.38,3.49,6.05" target="#foot_3">6</ref> and SSP <ref type="foot" coords="17,260.19,496.38,3.49,6.05" target="#foot_4">7</ref> have implemented task-specific enhancements for varied task evaluations. In contrast, our methodology leverages a singular, unified prompt format to accommodate an array of tasks. The commendable efficacy in open-ended question-answer and trivia creative writing tasks further corroborates the wide-ranging applicability and versatility of prompt design within the AutoAgents framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Human Evaluation</head><p>In this section, we present the criteria for human evaluation. We instructed the volunteers, who are responsible for assessing the quality of different feedback, to adhere to these standards.</p><p>[ Text]Human Evaluation</p><p>We would like to request your feedback on the response to the user question displayed above. Please rate the helpfulness, relevance, accuracy, level of details of their responses.</p><p>Each response receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.</p><p>Output with the following format: Evaluation evidence: &lt;your evluation explanation here&gt; Score: &lt;score&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Discussion</head><p>Limitations. AutoAgents exhibit remarkable knowledge acquisition and adaptability in tackling complex tasks, but they are not flawless. One of the limitations is that they may still produce erroneous outcomes even with dynamic role generation. This could be ascribed to the rationality of role generation and planning arrangements. Although this framework employs collaborative discussions to enhance the quality of role generation and planning arrangements, it still necessitates a  more effective method for recruiting teams and devising plans, and further ameliorates the quality of role generation and planning arrangements.</p><p>Furthermore, the differences between different roles in this framework mainly hinge on variations in prompt and tool usage, but this does not accentuate the distinctions between different expert roles.</p><p>In the future, it is imperative to explore how to incorporate more expert knowledge and create more professional role agents, in order to improve the adaptability of professional agents to professional problems.</p><p>Currently, AutoAgents rely heavily on the powerful logical and textual capabilities of GPT-4, and their adaptability to some earlier LLMs is poor. In the future, it is essential to explore more reasonable prompts to improve the adaptability of AutoAgents to different LLMs.</p><p>Future Work. Cooperation among multiple agents requires dynamic adaptation and communication.</p><p>The initial plan generated by LLMs may not suffice to achieve the desired outcomes <ref type="bibr" coords="21,442.71,214.75,10.52,8.64" target="#b3">[4]</ref>, resulting in erroneous final output results. Hence, future multi-agent systems need to swiftly detect and rectify errors and adjust their plans dynamically to align with the desired outcomes.</p><p>The memory capacity of existing agents is limited by the number of tokens in LLMs. How to devise a high-quality memory mechanism that enables efficient retrieval and storage of memory by agents remains an open question.</p><p>The professional skills of the generated agents are effective, but they can be improved by retraining or other mechanisms. Alternatively, an Agent Bank can be established to enable the invocation of professional agents on demand. More professional agent construction is still worthy of exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Prompts</head><p>In this section, we present the prompts of five components in our framework: Planner, Plan Observer, Role Observer, Action Observer, and Custom Agent. These prompts are designed to elicit the desired behaviors and responses from the agents in different scenarios and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Planner</head><p>The prompt design principles outlined in the template focus on creating and utilizing specialized LLM-based agent roles to solve complex tasks and problems. Here's a summary of these principles:</p><p>1. Understanding and Breaking Down Tasks: The first step involves comprehensively understanding, analyzing, and deconstructing the given task or problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Utilization of Existing Expert Roles:</head><p>â¢ Fully leverage existing expert roles suited to the problem.</p><p>â¢ Ensure that these roles have cooperative or dependent relationships.</p><p>â¢ Output the details of selected roles in JSON format, including their original information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Creation of New Expert Roles:</head><p>â¢ Avoid duplication of functions in new roles.</p><p>â¢ New roles should have clear names, detailed descriptions, domain expertise, available tools, execution suggestions, and prompt templates. â¢ Ensure each new expert role has a distinct responsibility and domain of expertise.</p><p>â¢ Specify the goals, constraints, and toolset for each new role.</p><p>â¢ Provide execution suggestions and develop prompt templates for each new role.</p><p>â¢ Output details of new roles in JSON format, following a specific structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Creation of Detailed Execution Plan:</head><p>â¢ Develop a comprehensive plan with multiple steps addressing the problem.</p><p>â¢ Assign at least one expert role to each step, detailing their contributions and collaborations.</p><p>â¢ Provide detailed descriptions for each step, including expected outputs and inputs for subsequent steps.</p><p>â¢ Include a final independent step for a language expert to provide a detailed response to the user's question.</p><p>â¢ Present the execution plan as a numbered list, indicating the expert roles involved in each step.</p><p>This structured approach ensures a systematic and detailed resolution of tasks, leveraging the specialized expertise of various LLM agents.</p><p>[ Prompt]Planner Here is an example of a valid JSON blob: {{{{ "name": "ROLE NAME", "description": "ROLE DESCRIPTONS", "tools": ["ROLE TOOL"], "suggestions": "EXECUTION SUGGESTIONS", "prompt": "ROLE PROMPT", }}}} 4. Finally, based on the content of the problem/task and the expert roles, provide a detailed execution plan with the required steps to solve the problem. 4.1. The execution plan should consist of multiple steps that solve the problem progressively. Make the plan as detailed as possible to ensure the accuracy and completeness of the task. You need to make sure that the summary of all the steps can answer the question or complete the task. 4.2. Each step should assign at least one expert role to carry it out. If a step involves multiple expert roles, you need to specify the contributions of each expert role and how they collaborate to produce integrated results. 4.3. The description of each step should provide sufficient details and explain how the steps are connected to each other. 4.4. The description of each step must also include the expected output of that step and indicate what inputs are needed for the next step. The expected output of the current step and the required input for the next step must be consistent with each other. Sometimes, you may need to extract information or values before using them. Otherwise, the next step will lack the necessary input. 4.5. The final step should always be an independent step that says 'Language Expert: Based on the previous steps, please provide a helpful, relevant, accurate, and detailed response to the user's original question: XXX'. 4.6. Output the execution plan as a numbered list of steps. For each step, please begin with a list of the expert roles that are involved in performing it. the primary responsibility or objective that the expert role aims to achieve. 3.6. You should specify any limitations or principles that each new expert role must adhere to when performing actions. These are called constraints and they must be consistent with the problem requirements and the domain of expertise. 3.7. You should select the appropriate tools that each new expert role needs to use from the existing tool set. Each new expert role can have multiple tools or no tool at all, depending on their functions and needs. You should never create any new tool and only use the existing ones. 3.8. You should provide some helpful suggestions for each new expert role to execute the task effectively and efficiently. The suggestions should include but not limited to a clear output format, extraction of relevant information from previous steps, and guidance for execution steps. 3.9. You should create a prompt template for calling each new expert role according to its name, description, goal, constraints, tools and suggestions. A good prompt template should first explain the role it needs to play (name), its area of expertise (description), the primary responsibility or objective that it aims to achieve (goal), any limitations or principles that it must adhere to when performing actions (constraints), and some helpful suggestions for executing the task (suggestions). The prompt must follow this format: "You are [description], named [name]. Your goal is [goal], and your constraints are <ref type="bibr" coords="25,390.37,554.69,61.18,7.47">[constraints]</ref>. You could follow these execution suggestions: [suggestions].". 3.10. You should always have a language expert role who does not require any tools and is responsible for summarizing the results of all steps in natural language. 3.11. You should follow the JSON blob format for creating new expert roles. Specifically, The JSON of new expert roles should have a 'name' key (the expert role name), a 'description' key (the description of the expert role's expertise domain), a 'tools' key (with the name of the tools used by the expert role), a 'suggestions' key (some suggestions for each agent to execute the task), and a 'prompt' key (the prompt template required to call the expert role). Each JSON blob should only contain one expert role, and do NOT return a list of multiple expert roles. Here is an example of a valid JSON blob: {{{{ "name": "ROLE NAME", "description": "ROLE DESCRIPTONS", "tools": ["ROLE TOOL"], "suggestions": "EXECUTION SUGGESTIONS", </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Format example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Action Observer</head><p>The design principles for the Action Observer in this prompt focus on coordinating the efforts of various expert roles to address human questions or tasks effectively. Key aspects include:</p><p>1. Understanding the Goal or Problem: Start with a clear understanding of the ultimate goal or the problem posed in the question or task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Determining and Executing Next Steps:</head><p>â¢ Review the history of completed steps to understand the progress made so far.</p><p>â¢ Assess the unfinished steps and decide on the necessary actions to achieve the goal or solve the problem. â¢ If the next step is already outlined in the unfinished steps, output this selected step in the 'NextStep' section. â¢ If the next step is not in the unfinished steps, choose an appropriate expert role from the existing ones, indicate the expert role's name, and outline the steps it needs to complete in the 'NextStep' section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extracting and Utilizing Historical Information:</head><p>â¢ Extract relevant information from the history to assist in completing the next step.</p><p>â¢ Ensure not to alter the historical information and maintain its original form for the next step.</p><p>The final output must adhere to a specific format, maintaining clarity and consistency in the process. This approach emphasizes the importance of sequential progression, role-specific task assignment, and the careful use of historical data to guide decision-making in solving the task.</p><p>[ Prompt]Action Observer PROMPT_TEMPLATE = """ You are an expert role manager who is in charge of collecting the results of expert</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="4,108.00,318.69,396.00,9.03;4,108.00,329.60,396.00,9.03;4,108.00,340.90,396.00,8.64;4,108.00,351.80,360.94,8.64;4,108.00,72.00,396.01,238.95"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The execution process of AutoAgents. During the Drafting Stage, three predefined agents collaboratively determine the list of agents and the execution plan. During the Execution Stage, a predefined agent facilitates coordination and communication among the generated agent teams, and the individual generated agents enhance their execution efficiency through self-refinement.</figDesc><graphic coords="4,108.00,72.00,396.01,238.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="6,393.89,127.27,33.49,7.24;6,330.54,127.27,40.55,7.24;6,119.55,144.87,110.67,7.87;6,119.55,154.30,108.94,7.88;6,119.55,163.77,114.94,7.87;6,119.55,173.21,75.79,7.87;6,263.74,144.87,227.91,7.87;6,263.74,154.30,223.08,7.88;6,263.74,163.77,229.39,7.87;6,263.74,173.21,146.24,7.87;6,303.21,191.56,159.72,7.08;6,121.79,192.23,120.75,7.08"><head></head><label></label><figDesc>Write engaging and coherent chapters based on the outline and character profiles. This will form the main body of the novel. Task: The Story Planner collaborates with the Researcher to understand AI concepts and create a detailed outline for the novel. This includes a high-level overview of the story, a breakdown of the story into chapters, and a breakdown of each chapter into scenes. (b) Collaborative refinement by multiple agents (a) Self-refinement by a single agent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,108.00,211.94,396.00,9.03;6,108.00,222.85,396.00,9.03;6,108.00,234.15,396.35,8.64;6,108.00,245.06,38.31,8.64"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two types of actions for executing tasks: Self-refinement enables an individual agent to enhance its competence in performing some specialized tasks. Collaborative refinement facilitates knowledge exchange among multiple agents and accomplishes tasks that demand interdisciplinary expertise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="7,108.00,262.93,396.00,8.82;7,108.00,273.84,396.00,8.82;7,108.00,284.75,396.00,8.82;7,108.00,295.83,348.23,8.64;7,108.00,72.00,396.00,182.98"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Legend of Three Knowledge Sharing Mechanisms. (a) Long-term memory focuses on chronicling the historical trajectory of multiple actions. (b) Short-term memory records the history of the self-refinement or collaborative refinement phases of an individual action. (c) Dynamic memory serves actions necessitating specialized attention extracted from the long-term memory.</figDesc><graphic coords="7,108.00,72.00,396.00,182.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="11,108.00,157.24,396.24,8.64;11,108.00,167.97,234.89,8.82"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of whether there is a collaborative discussion in the Drafting Stage in the task that developing Python-based software for the Tetris game.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="12,108.00,256.39,396.00,8.82;12,107.69,267.30,171.39,8.59;12,108.00,72.00,396.01,176.44"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The illustration of an example process of software development. The task is to develop Python-based software for the Tetris game.</figDesc><graphic coords="12,108.00,72.00,396.01,176.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="16,132.96,683.79,346.09,8.64;16,147.60,98.46,316.80,577.20"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: An example of the collaborative refinement process of trivia creative writing.</figDesc><graphic coords="16,147.60,98.46,316.80,577.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,186.94,349.27,238.13,8.64;17,108.00,72.00,396.00,269.14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example of the Open-ended Question Answer.</figDesc><graphic coords="17,108.00,72.00,396.00,269.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="18,170.77,505.94,270.46,8.64;18,147.60,72.01,316.80,425.81"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (Page 1) An example of the Trivia Creative Writing task.</figDesc><graphic coords="18,147.60,72.01,316.80,425.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="19,170.77,620.17,270.46,8.64;19,147.60,162.06,316.80,449.98"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (Page 2) An example of the Trivia Creative Writing task.</figDesc><graphic coords="19,147.60,162.06,316.80,449.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="20,143.85,694.74,324.29,8.64;20,147.60,87.50,316.80,599.11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: An example of the coordination process for the Acton Observer agent.</figDesc><graphic coords="20,147.60,87.50,316.80,599.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="3,108.00,72.00,396.01,353.30"><head></head><label></label><figDesc></figDesc><graphic coords="3,108.00,72.00,396.01,353.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" coords="15,108.00,72.00,396.00,357.27"><head></head><label></label><figDesc></figDesc><graphic coords="15,108.00,72.00,396.00,357.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,108.50,365.19,307.34,85.17"><head>10 :</head><label>10</label><figDesc>Execution Stage: 11: Initialize Action Observer O action and long-term memory M L . 12: for {S 1 , S 2 , â¢ â¢ â¢ S n } do O action assign task S k and M D to corresponding agents {A i , â¢ â¢ â¢ , A j }.</figDesc><table coords="8,108.50,397.99,182.48,52.38"><row><cell>13:</cell><cell>O action generate dynamic memory M D .</cell></row><row><cell>15:</cell><cell>Initialize short-term memory M S .</cell></row><row><cell>16:</cell><cell>repeat</cell></row><row><cell>17:</cell><cell></cell></row></table><note>14:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,107.69,256.46,396.31,61.19"><head>Table 2 :</head><label>2</label><figDesc>Win Rate of AutoAgents over other models on Open-ended Question Answer, with FairEval<ref type="bibr" coords="9,144.40,267.37,16.60,8.64" target="#b32">[33]</ref> and HumanEval serving as evaluators.</figDesc><table coords="9,177.46,298.10,247.88,19.55"><row><cell>FairEval [33]</cell><cell>96.3%</cell><cell>96.3%</cell><cell>76.3%</cell></row><row><cell>HumanEval</cell><cell>75%</cell><cell>75%</cell><cell>62.5%</cell></row></table><note>Evaluator v.s. ChatGPT v.s. Vicuna-13B v.s. GPT-4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="9,108.00,405.64,396.00,41.36"><head></head><label></label><figDesc>Table 2 demonstrates that AutoAgents outperforms individual LLM models in both FairEval based on LLM and Human evaluations. AutoAgent can produce more comprehensive and nuanced answers to open questions by synthesizing multiple expert models. It can also provide more elaborate explanations and justifications for its answers. More examples are given in the Appendix A.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="9,107.69,575.39,396.30,110.08"><head>Table 3 :</head><label>3</label><figDesc>The results of Trivia Creative Writing task. â indicates the differences compared with Standard Prompting (first row).</figDesc><table coords="9,117.73,600.85,376.53,84.62"><row><cell>Methods</cell><cell cols="2">N (# trivia questions) = 5</cell><cell cols="2">N (# trivia questions ) = 10</cell></row><row><cell></cell><cell cols="2">Score (%) â (v.s Standard %)</cell><cell cols="2">Score (%) â (v.s Standard %)</cell></row><row><cell>Standard</cell><cell>74.6</cell><cell>0.0%</cell><cell>77.0</cell><cell>0.0%</cell></row><row><cell>CoT [39]</cell><cell>67.1</cell><cell>-10.0%</cell><cell>68.5</cell><cell>-11.1%</cell></row><row><cell>SPP-Profile [34]</cell><cell>79.1</cell><cell>+5.9%</cell><cell>83.0</cell><cell>+7.8%</cell></row><row><cell>SPP [34]</cell><cell>79.9</cell><cell>+7.1%</cell><cell>84.7</cell><cell>+10.0%</cell></row><row><cell>AutoAgents</cell><cell>82.0</cell><cell>+9.9%</cell><cell>85.3</cell><cell>+10.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="10,107.69,294.91,397.14,77.42"><head>Table 4 :</head><label>4</label><figDesc>The ablation studies of AutoAgents on 20 instances of Trivia Creative Writing task. â indicates the differences compared with Standard Prompting (first row).</figDesc><table coords="10,148.00,320.37,316.00,51.97"><row><cell>Methods</cell><cell cols="2">N (# trivia questions) = 5</cell></row><row><cell></cell><cell cols="2">Score (%) â (v.s Standard %)</cell></row><row><cell>Standard</cell><cell>74.6</cell><cell>0.0%</cell></row><row><cell>CoT</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="11,116.43,86.41,369.83,58.93"><head>We need to create a Programming Expert to write the game (a) w/o Collaborative Discussion Agent Observer Planner Planner We also need game experts to design games and UI experts to design game interfaces, â¦.. We need to create Game Design Expert, UI/UX Design Expert Programming Expert, Debugging Expert â¦â¦ (b) w/ Collaborative Discussion</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="17,115.83,373.11,380.04,8.64"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Comparison of existing and proposed frameworks for multi-agent generation methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="22,108.00,206.43,390.62,515.56"><head></head><label></label><figDesc>You are a manager and an expert-level ChatGPT prompt engineer with expertise in multiple fields. Your goal is to break down tasks by creating multiple LLM agents, assign them roles, analyze their dependencies, and provide a detailed execution plan. You should continuously improve the role list and plan based on the suggestions in the History section. Make full use of the existing expert roles to solve the problem. 2.2. Follow the requirements of the existing expert roles. Make sure to select the existing expert roles that have cooperative or dependent relationships. 2.3. You MUST output the details of the selected existing expert roles in JSON blob format. Specifically, the JSON of each selected existing expert role should include its original information. 3. According to the problem, existing expert roles and the toolset ({tools}), you will create additional expert roles that are needed to solve the problem. You should act as an expert-level ChatGPT prompt engineer and planner with expertise in multiple fields, so that you can better develop a problem-solving plan and provide the best answer. You should follow these principles when creating additional expert roles: 3.1. The newly created expert role should not have duplicate functions with any existing expert role. If there are duplicates, you do not need to create this role. 3.2. Each new expert role should include a name, a detailed description of their area of expertise, available tools, execution suggestions, and prompt templates. 3.3. Determine the number and domains of expertise of each new expert role based on the content of the problem. Please make sure each expert has a clear responsibility and do not let one expert do too many tasks. The description of their area of expertise should be detailed so that the role understands what they are capable of doing. 3.4. Determine the names of each new expert role based on their domains of expertise. The name should express the characteristics of expert roles. 3.5. Determine the goals of each new expert role based on their domains of expertise. The goal MUST indicate the primary responsibility or objective that the role aims to achieve. 3.6. Determine the constraints of each new expert role based on their domains of expertise. The constraints MUST specify limitations or principles that the role must adhere to when performing actions. 3.7. Determine the list of tools that each new expert needs to use based on the existing tool set. Each new expert role can have multiple tools or no tool at all. You should NEVER create any new tool and only use existing tools. 3.8. Provide some suggestions for each agent to execute the task, including but not limited to a clear output, extraction of historical information, and suggestions for execution steps. 3.9. Generate the prompt template required for calling each new expert role according to its name, description, goal, constraints, tools and suggestions. A good prompt template should first explain the role it needs to play (name), its area of expertise (description), the primary responsibility or objective that the role aims to achieve (goal), limitations or principles that the role must adhere to when performing actions (constraints), and suggestions for agent to execute the task (suggestions). The prompt MUST follow the following format "You are [description], named[name]. Your goal is [goal], and your constraints are[constraints]. You could follow these execution suggestions:[suggestions].". 3.10. You must add a language expert role who does not require any tools and is responsible for summarizing the results of all steps. 3.11. You MUST output the details of created new expert roles in JSON blob format. Specifically, The JSON of new expert roles should have a 'name' key (the expert role name), a 'description' key (the description of the expert role's expertise domain), a 'tools' key (with the name of the tools used by the expert role), a 'suggestions' key (some suggestions for each agent to execute the task), and a 'prompt' key (the prompt template required to call the expert role). Each JSON blob should only contain one expert role, and do NOT return a list of multiple expert roles.</figDesc><table coords="22,108.00,206.43,385.91,276.46"><row><cell>PROMPT_TEMPLATE = '''</cell></row><row><cell>-----</cell></row><row><cell># Question or Task</cell></row><row><cell>{context}</cell></row><row><cell># Existing Expert Roles</cell></row><row><cell>{existing_roles}</cell></row><row><cell># History</cell></row><row><cell>{history}</cell></row><row><cell># Steps</cell></row><row><cell>You will come up with solutions for any task or problem by following these steps:</cell></row><row><cell>1. You should first understand, analyze, and break down the human's problem/task.</cell></row><row><cell>2. According to the problem, existing expert roles and the toolset ({tools}), you</cell></row><row><cell>will select the existing expert roles that are needed to solve the problem. You</cell></row><row><cell>should act as an expert-level ChatGPT prompt engineer and planner with expertise</cell></row><row><cell>in multiple fields, so that you can better develop a problem-solving plan and</cell></row><row><cell>provide</cell></row><row><cell>the best answer. You should follow these principles when selecting existing expert</cell></row><row><cell>roles:</cell></row><row><cell>2.1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" coords="23,108.00,664.28,263.55,47.32"><head></head><label></label><figDesc>You should first understand, analyze, and break down the human's problem/task. 2. According to the problem, existing expert roles and the toolset ({tools}), you should check the selected expert roles. 2.1. You should make sure that the selected expert roles can help you solve the problem effectively and efficiently. 2.2. You should make sure that the selected expert roles meet the requirements of the problem and have cooperative or dependent relationships with each other. 2.3. You should make sure that the JSON blob of each selected expert role contains its original information, such as name, description, and requirements. 3. According to the problem, existing expert roles and the toolset ({tools}), you should check the new expert roles that you have created. 3.1. You should avoid creating any new expert role that has duplicate functions with any existing expert role. If there are duplicates, you should use the existing expert role instead. 3.2. You should include the following information for each new expert role: a name, a detailed description of their area of expertise, a list of tools that they need to use, some suggestions for executing the task, and a prompt template for calling them. 3.3. You should assign a clear and specific domain of expertise to each new expert role based on the content of the problem. You should not let one expert role do too many tasks or have vague responsibilities. The description of their area of expertise should be detailed enough to let them know what they are capable of doing. 3.4. You should give a meaningful and expressive name to each new expert role based on their domain of expertise. The name should reflect the characteristics and functions of the expert role. 3.5. You should state a clear and concise goal for each new expert role based on their domain of expertise. The goal must indicate</figDesc><table coords="23,108.00,664.28,263.55,47.32"><row><cell>steps:</cell></row><row><cell>1.</cell></row><row><cell>Your final output should ALWAYS in the following format:</cell></row><row><cell>{format_example}</cell></row><row><cell># Suggestions</cell></row><row><cell>{suggestions}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" coords="27,108.00,76.48,390.61,296.38"><head></head><label></label><figDesc>You will check the Execution Plan by following these steps: 1. You should first understand, analyze, and disassemble the human's problem. 2. You should check if the execution plan meets the following requirements: 2.1. The execution plan should consist of multiple steps that solve the problem progressively. Make the plan as detailed as possible to ensure the accuracy and completeness of the task. You need to make sure that the summary of all the steps can answer the question or complete the task. 2.2. Each step should assign at least one expert role to carry it out. If a step involves multiple expert roles, you need to specify the contributions of each expert role and how they collaborate to produce integrated results. 2.3. The description of each step should provide sufficient details and explain how the steps are connected to each other. 2.4. The description of each step must also include the expected output of that step and indicate what inputs are needed for the next step. The expected output of the current step and the required input for the next step must be consistent with each other. Sometimes, you may need to extract information or values before using them. Otherwise, the next step will lack the necessary input. 2.5. The final step should ALWAYS be an independent step that says 'Language Expert: Based on the previous steps, please respond to the user's original question: XXX'. 3. Output a summary of the inspection results above. If you find any errors or have any suggestions, please state them clearly in the Suggestions section. If there are no errors or suggestions, you MUST write 'No Suggestions' in the Suggestions section.</figDesc><table coords="27,108.00,325.55,263.55,47.32"><row><cell># Format example</cell></row><row><cell>Your final output should ALWAYS in the following format:</cell></row><row><cell>{format_example}</cell></row><row><cell>-----</cell></row><row><cell>'''</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">Execution environment of AutoAgents is built based on MetaGPT's environment and workspace<ref type="bibr" coords="6,472.17,714.16,13.74,7.77" target="#b11">[12]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The specific model version employed is "GPT-4-0613".</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">The last 20 samples from a dataset of 100 samples are used as test instances.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">https://github.com/OpenBMB/AgentVerse/tree/minecraft/agentverse/tasks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">https://github.com/MikeWangWZHL/Solo-Performance-Prompting/tree/main/prompts</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work was partially supported by the National Key R&amp;D Program of China (2022YFC2009600 and 2022YFC2009606) and the Postdoctoral Fellowship Program of CPSF under Grant Number GZB20230024.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More Examples</head><p>Enhancing single-agent through self-refinement. Figure <ref type="figure" coords="14,353.11,617.93,5.08,8.64">7</ref> depicts the self-refinement process of programmers' coding. They first write a pseudo code file and then generate the corresponding program files based on it. This refinement process significantly ensures the validity of the output file. Although AutoAgents is a framework for multi-agent collaboration, it also requires self-refinement agents to perform specialized roles for individual tasks.</p><p>Improving Teamwork in Multi-Agent Systems with Collaborative Refinement. In collaborative refinement, the method is similar to the team discussions we talked about earlier. This involves bringing together information from various fields to complete tasks that require blending knowledge from different areas. As illustrated in Figure <ref type="figure" coords="14,287.55,713.51,3.75,8.64">8</ref>, during this process, the two agents work together to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Agent Observer</head><p>The prompt's design principles for the Agent Observer are centered on evaluating and refining expert roles for problem-solving. Key aspects include:</p><p>1. Understanding and Analyzing the Task: Comprehensive analysis of the problem or task.</p><p>2. Evaluation of Selected Expert Roles: Ensuring selected roles are effective, meet the problem's requirements, and their information (name, description, requirements) is accurately represented in a JSON format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Review of Created Expert Roles:</head><p>â¢ Avoid creating roles with overlapping functions with existing ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Steps</head><p>You will check the selected roles list and created roles list by following these "prompt": "ROLE PROMPT", }}}} 3.12. You need to check if the tool contains other tools that are not in the tool ({tools}), and if they do, they should be removed. 4. Output a summary of the inspection results above. If you find any errors or have any suggestions, please state them clearly in the Suggestions section. If there are no errors or suggestions, you MUST write 'No Suggestions' in the Suggestions section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Format example</head><p>Your final output should ALWAYS in the following format: {format_example} -----'''</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Plan Observer</head><p>The design principles for the Plan Observer in this prompt focus on evaluating and improving an Execution Plan. The key elements include:</p><p>1. Understanding the Problem: Begin with a thorough understanding and analysis of the human's problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Reviewing the Execution Plan:</head><p>â¢ Ensure the plan contains multiple detailed steps that progressively solve the problem, with a summary that addresses the task or question.</p><p>â¢ Verify that each step assigns at least one expert role, detailing their contributions and collaboration for integrated results.</p><p>â¢ Provide sufficient details in each step, explaining how the steps interconnect.</p><p>â¢ Include in each step's description its expected output and the inputs needed for the next step, ensuring consistency and completeness.</p><p>â¢ Confirm that the final step involves a language expert responding to the original question. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Custom Agent</head><p>The design principles of this prompt are centered around guiding a role, presumably an AI agent, in efficiently completing tasks based on the results and responses of previous agents. The key aspects include:</p><p>1. Understanding Previous Results: Begin by analyzing the results of previous agents to grasp the context and progress of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Analysis and Breakdown:</head><p>Understand, analyze, and deconstruct the given task. Use available tools to assist in task completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Current Step Identification and Execution:</head><p>â¢ Examine completed steps and their outcomes to identify the current step that needs to be completed.</p><p>â¢ In the absence of completed steps, analyze the task, design a plan for the necessary steps, and accomplish the first one.</p><p>â¢ If steps have been completed, understand them to determine the next step to be completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tool Selection and Action Execution:</head><p>â¢ Choose the appropriate tool from the given list (tool) to complete the current step.</p><p>â¢ Follow specific format guidelines when using tools like 'Write File'.</p><p>â¢ Once all steps are completed, use the 'Final Output' action to summarize each step's outputs, ensuring the final output is detailed, comprehensive, and solves the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Maintaining Format Consistency:</head><p>Ensure that the final output adheres to the given format example, prioritizing helpfulness, relevance, accuracy, and detail.</p><p>This approach emphasizes systematic progression through tasks, leveraging tools and prior work, and producing comprehensive and detailed final outputs.</p><p>[ Prompt]Custom Agent PROMPT_TEMPLATE = ''' -----{role} Base on the following execution result of the previous agents and completed steps and their responses, complete the following tasks as best you can. Building upon the custom agent's framework, a critical aspect of the refinement process is the configuration of 'completed steps' in step 3.1. The specific procedural steps for self-refinement and collaborative refinement are outlined as follows:</p><p>Self-refinement Action: During its first execution, each custom agent omits the outlined step 3.1 and proceeds to complete the remaining steps. If the task's criteria are not met or the step limit has not been reached, the outcomes from this execution are incorporated as 'completed steps' in the prompt for the next iteration of the task. In subsequent executions, the custom agent will execute all steps, continuing this process until the task is deemed successfully completed.</p><p>Collaborative Refinement Action: This mirrors the self-refinement action, yet involves active collaboration between multiple agents. For instance, when agents A and B collaborate on a task, A initially bypasses step 3.1 in its first execution. Upon completion, B incorporates A's results as 'completed steps' and then executes all steps. In later cycles, A and B alternate their roles in the task, perpetuating this collaborative cycle until a joint conclusion is reached.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="12,129.58,488.16,374.42,8.82;12,129.58,499.07,179.30,8.82" xml:id="b0">
	<analytic>
		<title level="a" type="main">Achieving coordination in collaborative problem-solving groups</title>
		<author>
			<persName coords=""><forename type="first">Brigid</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of the learning sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="436" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.58,514.96,374.43,8.64;12,129.58,525.87,374.42,8.64;12,129.58,536.60,342.91,8.82" xml:id="b1">
	<monogr>
		<author>
			<persName coords=""><forename type="first">SÃ©bastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,129.58,552.49,375.66,8.64;12,129.58,563.40,376.17,8.64;12,129.58,574.13,168.15,8.82" xml:id="b2">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianxuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07201</idno>
		<title level="m">Chateval: Towards better llm-based evaluators through multi-agent debate</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,129.58,590.02,374.42,8.64;12,129.58,600.75,374.42,8.82;12,129.25,611.66,318.37,8.82" xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive discovering and merging for incremental novel class discovery</title>
		<author>
			<persName coords=""><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peixi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangru</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mengyue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="11276" to="11284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,129.58,627.55,375.67,8.64;12,129.00,638.46,375.00,8.64;12,129.58,649.19,374.92,8.82" xml:id="b4">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingwei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenfei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yaxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.10848</idno>
		<title level="m">Facilitating multi-agent collaboration and exploring emergent behaviors in agents</title>
				<imprint>
			<date type="published" when="2023">2023. 2, 4, 5, 6, 17</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,129.58,665.08,374.42,8.64;12,129.58,675.81,262.57,8.82" xml:id="b5">
	<monogr>
		<title level="m" type="main">Teaching large language models to self-debug</title>
		<author>
			<persName coords=""><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxwell</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathanael</forename><surname>SchÃ¤rli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05128</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,129.58,691.70,376.07,8.64;12,129.58,702.43,374.42,8.82;12,129.58,713.34,118.70,8.82" xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving factuality and reasoning in language models through multiagent debate</title>
		<author>
			<persName coords=""><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14325</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,75.48,374.42,8.64;13,129.22,86.21,376.52,8.82;13,129.58,97.30,4.98,8.64" xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving language model negotiation with self-play and in-context learning from ai feedback</title>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10142</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,111.53,374.42,8.64;13,129.58,122.44,374.42,8.64" xml:id="b8">
	<monogr>
		<title level="m" type="main">Critic: Large language models can self-correct with tool-interactive critiquing</title>
		<author>
			<persName coords=""><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,136.49,376.17,8.82;13,129.58,147.40,199.44,8.82" xml:id="b9">
	<monogr>
		<title level="m" type="main">Auto-gpt: An autonomous gpt-4 experiment</title>
		<author>
			<persName coords=""><forename type="first">Significant</forename><surname>Gravitas</surname></persName>
		</author>
		<ptr target="https://github.com/Significant-Gravitas/Auto-GPT" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,161.82,375.80,8.64;13,129.58,172.55,307.04,8.82" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weijian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingliu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12998</idno>
		<title level="m">Chatllm network: More brains, more intelligence</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,186.96,374.42,8.64;13,129.58,197.87,374.59,8.64;13,129.58,208.60,350.00,8.82" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sirui</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ceyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Ka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shing</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zijuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenyu</forename><surname>Ran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.00352</idno>
		<title level="m">Meta programming for multi-agent collaborative framework</title>
				<imprint>
			<date type="published" when="2006">2023. 2, 3, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,223.01,375.66,8.64;13,129.39,233.92,374.61,8.64;13,129.58,244.65,374.42,8.82" xml:id="b12">
	<monogr>
		<title level="m" type="main">Inner monologue: Embodied reasoning through planning with language models</title>
		<author>
			<persName coords=""><forename type="first">Wenlong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacky</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05608</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,259.07,374.42,8.64;13,129.58,269.80,374.42,8.82;13,128.97,280.70,375.03,8.82;13,129.58,291.79,180.15,8.64" xml:id="b13">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName coords=""><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,306.03,375.66,8.64;13,129.00,316.94,375.00,8.64;13,129.58,327.67,253.36,8.82" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Josifoski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lars</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saibo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><forename type="middle">Paul</forename><surname>Schnitzler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiheng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Debjit</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.01285</idno>
		<title level="m">Flows: Building blocks of reasoning and collaborating ai</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,342.08,374.42,8.64;13,129.58,352.99,374.42,8.64;13,129.58,363.72,211.76,8.82" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hasan</forename><surname>Abed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Al</forename><surname>Kader Hammoud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hani</forename><surname>Itani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitrii</forename><surname>Khizbullin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Camel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17760</idno>
		<title level="m">Communicative agents for&quot; mind&quot; exploration of large scale language model society</title>
				<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,378.13,375.66,8.64;13,129.58,389.04,374.42,8.64;13,129.58,399.77,292.10,8.82" xml:id="b16">
	<monogr>
		<title level="m" type="main">Encouraging divergent thinking in large language models through multi-agent debate</title>
		<author>
			<persName coords=""><forename type="first">Tian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenxiang</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.19118</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,414.18,374.42,8.64;13,129.22,425.09,374.78,8.64;13,129.22,435.82,253.07,8.82" xml:id="b17">
	<monogr>
		<title level="m" type="main">Self-refine: Iterative refinement with self-feedback</title>
		<author>
			<persName coords=""><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Skyler</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17651</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,450.24,374.42,8.64;13,129.58,460.97,374.42,8.82;13,128.97,471.88,375.03,8.82;13,129.58,482.96,131.17,8.64" xml:id="b18">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,497.20,374.42,8.64;13,129.58,507.93,376.08,8.82;13,129.58,518.84,344.86,8.82" xml:id="b19">
	<monogr>
		<title level="m" type="main">Task-driven autonomous agent utilizing gpt-4, pinecone, and langchain for diverse applications</title>
		<author>
			<persName coords=""><surname>Nakajima</surname></persName>
		</author>
		<ptr target="https://yoheinakajima.com/" />
		<imprint>
			<date type="published" when="2023-04">April 2023. 2023</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications</note>
</biblStruct>

<biblStruct coords="13,129.58,533.07,374.41,8.82;13,129.58,543.98,175.15,8.82" xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative problem solving</title>
		<author>
			<persName coords=""><forename type="first">Laurie</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nelson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Instructional-design theories and models</title>
				<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="241" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,558.40,162.11,8.64" xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,572.63,374.42,8.64;13,129.58,583.36,374.42,8.82;13,129.58,594.27,143.52,8.82" xml:id="b22">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.03442</idno>
		<title level="m">Generative agents: Interactive simulacra of human behavior</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,608.68,374.42,8.64;13,129.58,619.59,374.42,8.64;13,129.58,630.32,374.42,8.82;13,129.02,641.23,156.27,8.82" xml:id="b23">
	<analytic>
		<title level="a" type="main">Social simulacra: Creating populated prototypes for social computing systems</title>
		<author>
			<persName coords=""><forename type="first">Sung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lindsay</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carrie</forename><surname>Popowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Percy</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</title>
				<meeting>the 35th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="13,129.58,655.64,375.66,8.64;13,129.58,666.37,374.42,8.82;13,129.58,677.28,108.74,8.82" xml:id="b24">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.07924</idno>
		<title level="m">Communicative agents for software development</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="13,129.58,691.70,374.42,8.64;13,129.00,702.43,375.00,8.82;13,129.58,713.34,108.74,8.82" xml:id="b25">
	<monogr>
		<title level="m" type="main">Is chatgpt a general-purpose natural language processing task solver?</title>
		<author>
			<persName coords=""><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06476</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,75.48,376.08,8.64;14,129.58,86.21,375.66,8.82;14,128.83,97.30,30.98,8.64" xml:id="b26">
	<analytic>
		<title level="a" type="main">The construction of shared knowledge in collaborative problem solving</title>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Roschelle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephanie</forename><forename type="middle">D</forename><surname>Teasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer supported collaborative learning</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="69" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,129.58,112.62,374.42,8.64;14,129.58,123.35,323.91,8.82" xml:id="b27">
	<monogr>
		<title level="m" type="main">Reflexion: an autonomous agent with dynamic memory and self-reflection</title>
		<author>
			<persName coords=""><forename type="first">Noah</forename><surname>Shinn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beck</forename><surname>Labash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ashwin</forename><surname>Gopinath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11366</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,138.86,375.66,8.64;14,129.58,149.59,375.67,8.82;14,129.58,160.68,30.98,8.64" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yu</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guangyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daochen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiqi</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.15930</idno>
		<title level="m">Llasm: Large language and speech model</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,175.83,375.67,8.82;14,128.83,186.92,70.28,8.64" xml:id="b29">
	<analytic>
		<title level="a" type="main">The empirical case for two systems of reasoning</title>
		<author>
			<persName coords=""><surname>Steven A Sloman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,129.58,202.25,374.42,8.64;14,129.58,212.98,258.36,8.82" xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-agent collaboration: Harnessing the power of intelligent llm agents</title>
		<author>
			<persName coords=""><forename type="first">Yashar</forename><surname>Talebirad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amirhossein</forename><surname>Nadiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.03314</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,228.48,374.42,8.64;14,129.22,239.39,374.78,8.64;14,129.58,250.12,302.53,8.82" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Boxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hengzhi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chulin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mintong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chejian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zidi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ritik</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rylan</forename><surname>Schaeffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11698</idno>
		<title level="m">A comprehensive assessment of trustworthiness in gpt models</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,265.63,374.42,8.64;14,129.58,276.36,375.67,8.82;14,129.58,287.45,30.98,8.64" xml:id="b32">
	<monogr>
		<title level="m" type="main">Large language models are not fair evaluators</title>
		<author>
			<persName coords=""><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Binghuai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunbo</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17926</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,302.78,374.42,8.64;14,129.58,313.69,376.08,8.64;14,129.58,324.42,316.46,8.82" xml:id="b33">
	<monogr>
		<title level="m" type="main">Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration</title>
		<author>
			<persName coords=""><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoguang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenshan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05300</idno>
		<imprint>
			<date type="published" when="2009">2023. 1, 2, 3, 4, 5, 6, 9</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,339.92,376.08,8.64;14,129.58,350.65,355.34,8.82" xml:id="b34">
	<monogr>
		<title level="m" type="main">Epidemic modeling with generative agents</title>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niyousha</forename><surname>Hosseinichimeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aritra</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Navid</forename><surname>Ghaffarzadegan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04986</idno>
		<imprint>
			<date type="published" when="2023">2023. 2, 4, 17</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,366.16,374.42,8.64;14,129.58,376.89,359.53,8.82" xml:id="b35">
	<analytic>
		<title level="a" type="main">Collective intelligence and group performance</title>
		<author>
			<persName coords=""><forename type="first">Anita</forename><forename type="middle">Williams</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishani</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Malone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="420" to="424" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="14,129.58,392.40,375.67,8.64;14,129.58,403.31,374.42,8.64;14,129.58,414.04,338.79,8.82" xml:id="b36">
	<monogr>
		<title level="m" type="main">Autogen: Enabling next-gen llm applications via multi-agent conversation framework</title>
		<author>
			<persName coords=""><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gagan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jieyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiran</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erkang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Beibin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08155</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,429.55,374.42,8.64;14,129.58,440.28,374.42,8.82;14,129.58,451.19,163.45,8.82" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14688</idno>
		<title level="m">Expertprompting: Instructing large language models to be distinguished experts</title>
				<imprint>
			<date type="published" when="2005">2023. 2, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,466.69,374.67,8.64;14,129.58,477.42,374.42,8.82;14,129.58,488.33,158.46,8.82" xml:id="b38">
	<monogr>
		<title level="m" type="main">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10601</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,503.84,376.16,8.64;14,129.58,514.57,375.67,8.82;14,129.58,525.66,45.93,8.64" xml:id="b39">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Shunyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Izhak</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03629</idno>
		<title level="m">React: Synergizing reasoning and acting in language models</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="14,129.58,540.99,375.66,8.64;14,129.58,551.90,374.42,8.64;14,129.58,562.63,227.61,8.82" xml:id="b40">
	<monogr>
		<title level="m" type="main">Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05685</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
