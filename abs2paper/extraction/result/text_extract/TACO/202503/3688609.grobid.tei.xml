<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computing Machinery (ACM)</publisher>
				<availability status="unknown"><p>Copyright Association for Computing Machinery (ACM)</p>
				</availability>
				<date type="published" when="2025-03-21">2025-03-21</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,45.95,120.44,71.01,9.82"><forename type="first">Perry</forename><surname>Gibson</surname></persName>
							<email>p.gibson.2@research.gla.ac.uk</email>
							<idno type="ORCID">0000-0003-3370-0698</idno>
						</author>
						<author>
							<persName coords="1,45.77,146.34,52.43,9.82"><forename type="first">Jose</forename><surname>Cano</surname></persName>
							<email>jose.canoreyes@glasgow.ac.uk</email>
							<idno type="ORCID">0000-0002-2243-389X</idno>
						</author>
						<author>
							<persName coords="1,45.95,185.19,84.76,9.82"><forename type="first">Elliot</forename><surname>Crowley</surname></persName>
							<email>elliot.j.crowley@ed.ac.uk</email>
							<idno type="ORCID">0000-0001-5685-4724</idno>
						</author>
						<author>
							<persName coords="1,45.60,211.09,74.26,9.82"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
							<email>a.storkey@ed.ac.uk</email>
							<idno type="ORCID">0000-0002-8100-506X</idno>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>O’boyle</surname></persName>
							<idno type="ORCID">0000-0003-1619-5052</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country>United Kingdom of Great Britain and Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country>United Kingdom of Great Britain and Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>United Kingdom of Great Britain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>United Kingdom of Great Britain and Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>United Kingdom of Great Britain and North-ern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>United Kingdom of Great Britain and Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="laboratory">of Great Britain and Northern Ireland</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country>Scotland, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">School of Com-puting Science</orgName>
								<orgName type="department" key="dep2">of Great Britain and Northern Ireland</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="laboratory">of Great Britain and Northern Ireland</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="laboratory">of Great Britain and Northern Ireland</orgName>
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country>United Kingdom of Great Britain and Northern Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">The University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">of Great Britain and Northern Ireland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration</title>
					</analytic>
					<monogr>
						<title level="j" type="main">ACM Transactions on Architecture and Code Optimization</title>
						<title level="j" type="abbrev">ACM Trans. Archit. Code Optim.</title>
						<idno type="ISSN">1544-3566</idno>
						<idno type="eISSN">1544-3973</idno>
						<imprint>
							<publisher>Association for Computing Machinery (ACM)</publisher>
							<biblScope unit="volume">22</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="1" to="28"/>
							<date type="published" when="2025-03-21" />
						</imprint>
					</monogr>
					<idno type="MD5">954A64C8CAF2F248395C979C41605CEE</idno>
					<idno type="DOI">10.1145/3688609</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-07-22T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts:</term>
					<term>Computing methodologies → Machine learning</term>
					<term>Artificial intelligence</term>
					<term>• Software and its engineering → Compilers</term>
					<term>• General and reference → Empirical studies</term>
					<term>DNNs, Convolutional Neural Networks, Pruning, Quantization, Sparse Tensors, Auto-Scheduling, Auto-Tuning, DNNs, Tensor Compilers, Tensor Programs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Neural Networks (DNNs) are very computationally demanding, which presents a significant barrier to their deployment, especially on resource-constrained devices. Significant work from both the machine learning and computing systems communities has attempted to accelerate DNNs. However, the number of techniques available and the required domain knowledge for their exploration continue to grow, making design space exploration (DSE) increasingly difficult. To unify the perspectives from these two communities, this article introduces the Deep Learning Acceleration Stack (DLAS), a conceptual model for DNN deployment and acceleration. We adopt a six-layer representation that organizes and illustrates the key areas for DNN acceleration, from machine learning to software and computer architecture. We argue that the DLAS model balances simplicity and expressiveness, assisting practitioners from various domains in tackling co-design acceleration challenges. We demonstrate the interdependence of the DLAS layers, and thus the need for codesign, through an across-stack perturbation study, using a modified tensor compiler to generate experiments for combinations of a few parameters across the DLAS layers. Our perturbation study assesses the impact on inference time and accuracy when varying DLAS parameters across two datasets, seven popular DNN architectures, four compression techniques, three algorithmic primitives (with sparse and dense variants), untuned and auto-scheduled code generation, and four hardware platforms. The study observes significant changes in the relative performance of design choices with the introduction of new DLAS parameters (e.g., the fastest algorithmic primitive varies with the level of quantization). Given the strong evidence for the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have yielded rapid advances in deep learning, largely due to the unparalleled effectiveness of Deep Neural Networks (DNNs), such as Convolutional Neural Networks (CNNs) and Transformer architectures <ref type="bibr" coords="2,152.81,269.13,14.84,9.03" target="#b81">[81]</ref>, on a variety of difficult problems <ref type="bibr" coords="2,303.77,269.13,14.99,9.03" target="#b42">[42,</ref><ref type="bibr" coords="2,320.44,269.13,11.45,9.03" target="#b49">49,</ref><ref type="bibr" coords="2,333.58,269.13,11.26,9.03" target="#b87">87]</ref>. Despite increases in algorithmic efficiency <ref type="bibr" coords="2,128.54,281.09,14.83,9.03" target="#b35">[35]</ref>, the trend with DNN architectures is increased size and deployment costs as demands for more powerful and general solutions grows <ref type="bibr" coords="2,291.53,293.05,10.34,9.03" target="#b6">[7,</ref><ref type="bibr" coords="2,304.59,293.05,11.26,9.03" target="#b77">77]</ref>. As such, creative approaches are required to deploy DNNs on hardware with limited resources to enable a variety of emerging applications (e.g., autonomous driving <ref type="bibr" coords="2,217.72,316.95,14.83,9.03" target="#b75">[75]</ref>, collision avoidance for quadcopters <ref type="bibr" coords="2,383.70,316.95,10.13,9.03" target="#b1">[2]</ref>). However, often these optimization approaches come with limited benchmarks and few comparisons, and there may be a disconnect between machine learning-based and systems-based optimizations due to disparate communities with varying core competencies.</p><p>Even in a simplified view of the relevant components of deep learning deployment (e.g., machine learning, software, and hardware), it is evident that choices in one area can have consequences for the choices in others <ref type="bibr" coords="2,147.01,388.68,14.84,9.03" target="#b79">[79]</ref>. For example, constrained CPU hardware will require appropriately resource-conservative software and DNN models that fit limited memory and latency budgets. Alternatively, a new DNN architecture with a novel operation will need an optimized software kernel to execute it on hardware that can provide the required inference time. Without broad awareness of these interactions from practitioners, potential performance may be lost, since novel machine learning techniques may not be fully exploited by systems techniques <ref type="bibr" coords="2,328.94,448.46,10.36,9.03" target="#b3">[4,</ref><ref type="bibr" coords="2,341.51,448.46,11.26,9.03" target="#b28">28]</ref>, or machine learning practitioners may not be aware of under-utilized resources available on their hardware platforms.</p><p>In this work, we outline a high-level "stack-based" conceptual model of the relevant techniques pertaining to DNN acceleration and demonstrate how they are linked with a multi-level experimental analysis. Our goal is to enable a more comprehensive understanding of the performance available under different constraints of inference accuracy, execution time, memory space, and energy consumption. We introduce the Deep Learning Acceleration Stack (DLAS), which provides a model for both machine learning and systems researchers to reason about the impact of their performance optimizations. We propose DLAS as six layers, 1 as shown in Figure <ref type="figure" coords="2,395.69,544.10,3.41,9.03" target="#fig_1">1</ref>, covering parameters more relevant to machine learning (Datasets &amp; Problem Spaces, Models &amp; Neural Architectures, and Model Optimizations) and parameters more relevant to systems (Algorithms &amp; Data Formats, Systems Software, and Hardware). Each of the layers can be further decomposed into sub-layers, however, the intent of DLAS is to provide a starting point for reasoning about across-stack optimization and encourage co-design of accelerated DNN deployments. The core contributions of this work include: -We introduce the Deep Learning Acceleration Stack to reflect the different layers of optimization, from both machine learning and systems, that can be applied to run a DNN model more efficiently on a given target device. -We select parameters to vary at each layer of DLAS (two datasets, four models, three compression techniques, three algorithms, two compilation techniques, four hardware devices), which gives us a vertical slice of DLAS to explore, presenting results on the inference time and accuracy impacts of our variations. -We develop an experimental framework based on Apache TVM <ref type="bibr" coords="3,330.68,298.28,11.72,9.03" target="#b8">[9]</ref> to evaluate our parameters in a consistent environment, extending TVM where required and possible. -We explore across-stack interactions and make 13 key observations from our results, discussing their consequences and highlighting potential improvements that could be made from other works. The rest of the article is organized as follows: In Section 2, we motivate and describe DLAS. Section 3 gives the necessary background to understand the optimization techniques we explore across DLAS. For our evaluation, we describe the experimental setup in Section 4, and the results are presented in Section 5. In Section 6, we discuss the results of our evaluation as well as related work that could be exploited in further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Learning Acceleration Stack</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>The recent growth of deep learning has been partially facilitated by the computational power of high-end GPUs as well as improvements in algorithmic representations <ref type="bibr" coords="3,333.18,471.60,14.82,9.03" target="#b35">[35]</ref>. When combined with a tendency to focus narrowly on higher accuracies and the availability of large server-class GPUs, this has led to state-of-the-art DNN models to explode in size <ref type="bibr" coords="3,295.39,495.51,14.84,9.03" target="#b60">[60]</ref>. This presents a large barrier to deploying many modern machine learning applications on constrained devices.</p><p>Both machine learning researchers and systems engineers have proposed innovative solutions to overcome this barrier. However, these solutions are typically developed in isolation, meaning that machine learning practitioners may not explore the systems consequences of their approach and vice versa. For instance, sparsity is regarded by some in the machine learning community as a silver bullet for compressing models, whereas exploiting parallelism is generally seen as essential by system architects. Challenging these isolated preconceptions reveals that sparsity does not always excel at reducing the number of operations during inference, and parallelism does not necessarily bring the expected speedups. These observations are presented in greater detail in Section 6.</p><p>The goal of introducing DLAS as a conceptual model is to make it clearer to both machine learning and systems practitioners what the relevant contributors to performance for their DNN workloads are, allowing greater opportunities for co-design and co-optimization. This is not to 1:4 P. <ref type="bibr" coords="4,388.86,55.82,51.42,8.97">Gibson et al.</ref> advocate for machine learning experts to re-train as systems experts and vice versa. Rather, we aim to provide a framework of reasoning so practitioners can understand the context in which their area of expertise exists in and give a "checklist" of other relevant performance contributing factors to be aware of. By exposing the wide range of choices and highlighting the impact of across-stack interaction, we also hope to encourage better tooling so practitioners can more easily experiment with perturbations.</p><p>DLAS is an instantiation of the "systems stack, " whose layers have been chosen to highlight the most relevant components for deep learning acceleration. These goals are similar to other conceptual models such as the OSI model <ref type="bibr" coords="4,220.02,172.93,16.36,9.03" target="#b17">[17]</ref> and the LAMP stack for web applications, which present an abstraction that organizes and illustrates the critical areas of each system while allowing choice of concrete implementation.</p><p>Practically, by using DLAS as a conceptual model, multi-disciplinary teams may be able to align on a common language for accelerating their workloads, especially in constrained environments such as the edge. DLAS could help practitioners communicate how their techniques might interact with others and also act as a quick reference for new techniques that could be included in a given deployment to further accelerate their workloads, especially when a given acceleration strategy is giving diminishing returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Description of the Stack</head><p>We introduce the Deep Learning Acceleration Stack (DLAS), which spans from the machine learning domain all the way down to the hardware domain. Each layer can be tuned to optimize different goals (i.e., inference accuracy, execution time, memory footprint, power) or to yield further improvements in adjacent layers. However, for their potential to be fully realized, many optimizations are required to be implemented using techniques across several layers, i.e., co-design and co-optimization. DLAS contains the following six layers, with examples given in Figure <ref type="figure" coords="4,420.62,369.70,3.49,9.03" target="#fig_1">1</ref>: (3) Model Optimizations: approaches to reduce the size and costs of a DNN model (e.g., memory, inference time), while attempting to maintain the accuracy. (4) Algorithms &amp; Data Formats: DNN layers (e.g., convolutions) can be implemented using various algorithms, with myriad tradeoffs in space and time. Interlinked with algorithms are data formats, i.e., how data is laid out in memory. These choices can be consistent across a DNN model or vary per layer. (5) Systems Software: software used to run the DNNs, such as DNN frameworks, algorithmic implementations, supporting infrastructure, tensor compilers, and code-generators. (6) Hardware: devices the DNN is deployed on, from general purpose (e.g., CPUs, GPUs) to application-specific (e.g., FPGAs, NPUs, TPUs). It also includes hardware features, such as SIMD-units, cache behavior, and tensor cores.</p><p>Although we have delineated the layers of the stack, it is critical to highlight that design decisions made at each layer of DLAS can directly impact adjacent layers and those across the stack. In addition, a domain expert may divide a given layer into more detailed sub-layers, and increased co-design may blur the separation between layers. However, we believe this six-layer structure strikes a balance between descriptiveness and simplicity. The six layers of DLAS should be understandable by experts at opposite ends of the stack, e.g., machine learning experts can understand that hardware choices can be important but may not want to reason about the tradeoffs of different cache policies or ISA extensions.</p><p>In this article, we perform an across-stack perturbation of some parameters from each layer to determine their impact on inference and accuracy performance, and any interactions between parameters. In the future, practitioners will increasingly need to be aware of these across-stack interactions, as Moore's law scaling can no longer be relied upon by machine learning engineers <ref type="bibr" coords="5,423.03,137.07,14.82,9.03" target="#b34">[34]</ref>, and increased competition between hardware designers will require progressively more innovative workload-aware approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Case Study</head><p>In the remainder of this article, we demonstrate the value of DLAS with a case study, choosing a small subset of popular parameters at each layer and showing how they can influence each other. Note that even examining a small number of parameters can result in a large number of experiment variants, due to the combinatorial growth for each new parameter added-there are nearly 1,000 combinations in our study alone. Considering a wider range of parameters poses significant research challenges regarding efficient design space exploration (DSE), which is a broader problem the community continues to tackle from a number of directions <ref type="bibr" coords="5,376.69,271.23,10.36,9.03" target="#b2">[3,</ref><ref type="bibr" coords="5,389.54,271.23,6.83,9.03" target="#b5">6,</ref><ref type="bibr" coords="5,398.85,271.23,11.46,9.03" target="#b27">27,</ref><ref type="bibr" coords="5,412.80,271.23,11.25,9.03" target="#b78">78]</ref>.</p><p>We implement our case study using PyTorch and a modified version of Apache TVM. However, DLAS evaluations can be performed with any combination of software frameworks and techniques. Our case study highlights how an across-stack DLAS evaluation can be conducted with a narrow but deep investigation. We encourage readers to consider how the results could change if additional parameters were included, for example, Winograd convolution <ref type="bibr" coords="5,344.87,331.01,14.86,9.03" target="#b48">[48]</ref>, Transformers <ref type="bibr" coords="5,423.03,331.01,14.83,9.03" target="#b81">[81]</ref>, TPUs <ref type="bibr" coords="5,70.68,342.96,14.84,9.03" target="#b41">[41]</ref>, or some other acceleration technique of interest. However, including these additional parameters will not change the core purpose of the case study, namely, to highlight that DLAS can be a useful delineation for DNN acceleration research, and across-stack thinking will be increasingly important to unlock the next generation of acceleration techniques. Section 3 gives the necessary background to understand the details of our case study, Section 4 describes the experimental setup, and Sections 5 and 6 provide the results and discussion, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>In this section, we discuss the necessary background of the techniques in DLAS explored in our experiments. Note that providing a complete background of every technique in DLAS is beyond the scope of this article, as deep learning innovations are rapid and continuous, making any attempt at comprehensiveness quickly outdated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets &amp; Problem Spaces</head><p>We focus on two common image classification datasets, CIFAR-10 <ref type="bibr" coords="5,337.73,515.65,16.33,9.03" target="#b43">[43]</ref> and ImageNet <ref type="bibr" coords="5,423.45,515.65,14.84,9.03" target="#b19">[19]</ref>. CIFAR-10 images are 32×32 pixels across 10 classes, while ImageNet images are 224×224 pixels across 1,000 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models &amp; Neural Architectures</head><p>There is a wide range of DNN architectures available, with a variety of popular models for each. Convolutional neural networks (CNNs) are commonly leveraged for image classification tasks and are characterized by their use of convolutional layers. Other layers may include batch normalization and pooling layers, fully connected layers, and activation functions such as ReLU. The topology of these networks are generally deterministic, directed acyclic graphs. Some neural architectures may include skip-connections <ref type="bibr" coords="5,216.91,637.86,14.84,9.03" target="#b33">[33]</ref>, where activations from previous layers are reused in later layers, or depthwise separable convolutions <ref type="bibr" coords="6,259.84,212.49,14.85,9.03" target="#b68">[68]</ref>, which can reduce memory and computational requirements.</p><p>When training DNNs, we repeatedly present the full dataset (each round called an epoch) and assess ultimate performance against a separate test set. Algorithms like stochastic gradient descent (SGD) iteratively update the DNN weights to enhance accuracy between epochs. Hyperparameters like the learning rate (LR) influence weight adjustments each epoch, usually reducing as the network learns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Optimizations</head><p>It has been observed that DNN models are overparameterized, and similar accuracies can be achieved with smaller models <ref type="bibr" coords="6,169.32,337.55,14.84,9.03" target="#b24">[24]</ref>. As a result, a wide range of model optimization and compression techniques has been proposed. Figure <ref type="figure" coords="6,221.45,349.50,4.63,9.03" target="#fig_2">2</ref> shows some common compression techniques, with an uncompressed (or "dense") convolution shown in Figure <ref type="figure" coords="6,288.59,361.46,13.76,9.03" target="#fig_2">2(a)</ref>.</p><p>Pruning is a family of techniques that sets weights/parameters in a DNN to zero. This can reduce the computational or memory overheads of a given DNN model, with potential accuracy loss. There are two general types of parameter pruning: unstructured and structured, each of which can be used in either global or layer-wise configurations <ref type="bibr" coords="6,299.67,409.28,10.43,9.03" target="#b4">[5]</ref>. Unstructured pruning removes individual parameters, as seen in Figure <ref type="figure" coords="6,211.16,421.24,3.56,9.03" target="#fig_2">2</ref>(b), whereas structured pruning removes whole groups of parameters, such as blocks, or channels, as seen in Figure <ref type="figure" coords="6,288.58,433.20,13.49,9.03" target="#fig_2">2(c)</ref>. With global pruning, given some pruning target (e.g., 50% of parameters should be pruned), the pruning algorithm will find the best parameters to prune across the whole DNN model, meaning that some layers will be more or less pruned than others. For layer-wise pruning, we prune by a pre-defined amount per layer, e.g., 50% pruning. All types of pruning apply some scoring function to the weights to determine which are the least important and therefore are likely to have the lowest impact on accuracy if removed. A popular approach is "L1-pruning, " where we prune parameters that have the lowest absolute value, i.e., closest to zero. Other ranking approaches include gradient-based methods and Taylor series expansion.</p><p>Another compression technique is data-type quantization, which reduces the number of bits used to represent data, visualized in Figure <ref type="figure" coords="6,229.31,552.74,14.11,9.03" target="#fig_2">2(d)</ref>, where a continuous function is approximated with nine distinct values. Typically, DNNs are trained using float32, however, when they are deployed, we can reduce the precision. Common quantized data-types include float16 and int8, as well as emerging machine learning specific types such as bfloat16. For some types of data-type quantization, it may be necessary to add additional operations to the DNN, e.g., in many int8 DNNs, we need to store the partial-sums of MACs (multiply-accumulate operations) using up to 16 bits, and then quantize back into int8. Other layers, such as layer norm and softmax, may also require higher precision.</p><p>Note that for many model optimization techniques, including pruning and data-type quantization, it may be necessary to use some form of post-training fine-tuning or calibration to try and recover some of the lost accuracy. This can involve retraining the non-pruned parameters of the model in the case of pruning, or adjusting constants used in the rescaling operations in the case of data-type quantization, or some combination of the two.</p><p>From a systems perspective, many model optimization techniques do not necessarily provide speedups unless lower levels of the stack adequately support it; for example, hardware that can compute using data-types with fewer bits or algorithms that exploit the pruning to skip operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Algorithms &amp; Data Formats</head><p>The layers of a given DNN model can be implemented in a variety of ways, as long as they still provide the same output. However, the behavior of a given implementation is influenced by the size and shape of the data, the properties of the hardware we are running on, as well as the choices around how we exploit model optimization techniques.</p><p>For the CNN models we evaluate in our work, the most important component to optimize is the convolutional layers, since they are generally the most compute-and memory-intensive. Algorithms used in this work implementing convolutional layers include direct, GEMM, and spatial pack convolution. Direct convolution applies the convolution in a manner similar to the textbook definition "sliding-window" and does not reshape input data and weights. GEMM convolution reshapes input data into a 2D array, potentially replicating elements using a reshaping algorithm known as "im2col. " This means that the convolution can be computed as a matrix multiplication. Spatial pack convolution reshapes both in input data and weights, although the weights can be reshaped offline, with data packed into tiles that are ideally loaded once. Unlike GEMM convolution, the size of the reshaped input data is the same size, and tiling on inputs and weights is intended to exploit data reuse and SIMD vectorization. Simplified implementations of these three algorithms are given in the Appendix using the TVM tensor expression language.</p><p>Another important component of how we format data is the layout, which is how we order the data in memory, since memory may have a different structure to a given tensor (e.g., 1D memory but 4D tensors). For 2D arrays, common formats include "row-major" and "column-major" order, with the former meaning that data in the same row is contiguous in memory, and the latter meaning data in the same column is contiguous in memory. Similarly, for 4D data, which is more relevant to our image classification CNNs, two common formats are NCHW and NHWC, with N representing the batch size and C, H, and W representing the number of input channels, the input height, and input width, respectively. The algorithms in listings 1-3 are in the NCHW format, however, spatial pack (Listing 2) temporarily reshapes data to NCHWc, where c is an inner tile dimension to exploit vectorization.</p><p>To achieve savings from pruning, the Algorithms &amp; Data Formats layer is critical, since the chosen algorithm must support "sparsity, " i.e., exploiting the zeros generated by pruning to skip computation or reduce memory usage. The computational savings are enabled by the fact that, regardless of the value of an input, a multiplication by zero will have no impact on the final output; and the memory savings come from representing sequences of zeros in a more compressed format. In this work, we use the popular CSR (compressed sparse row) format, which represents 2D data using three arrays: (1) The non-zero elements of the parameters (data); (2) the original column index of the corresponding parameters (indices); and (3) the first non-zero elements in each row, as well as the final non-zero element (indptr). Other formats include BSR (block sparse row) and COO (coordinate list).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Systems Software</head><p>The systems software most commonly exposed to machine learning practitioners is the DNN framework. Common frameworks include PyTorch <ref type="bibr" coords="8,252.95,104.19,14.83,9.03" target="#b59">[59]</ref>, TensorFlow <ref type="bibr" coords="8,323.42,104.19,10.43,9.03" target="#b0">[1]</ref>, JAX <ref type="bibr" coords="8,358.19,104.19,14.84,9.03" target="#b25">[25]</ref>, and MXNet <ref type="bibr" coords="8,427.47,104.19,10.43,9.03" target="#b7">[8]</ref>, all of which are focused on DNN training. Other frameworks may focus exclusively on deployment, such as TensorFlow Lite and TensorRT <ref type="bibr" coords="8,201.28,128.10,14.84,9.03" target="#b15">[15]</ref>. These frameworks include utilities for defining, saving, and loading models; passing, processing, and inspecting data; and invoking and profiling training and inference. Underlying these DNN frameworks are the kernel libraries that execute the critical computations of the DNNs (e.g., the convolutional layers). For example, for Nvidia GPUs, many frameworks leverage the cuDNN library <ref type="bibr" coords="8,211.38,175.92,14.83,9.03" target="#b10">[11]</ref>, which is a collection of optimized CUDA kernels to run common DNN operations.</p><p>An alternative to using optimized vendor libraries is using a tensor compiler such as TVM <ref type="bibr" coords="8,428.57,199.83,11.71,9.03" target="#b8">[9]</ref> or IREE <ref type="bibr" coords="8,81.85,211.79,14.85,9.03" target="#b76">[76]</ref>. They generate code for a specific DNN and hardware backend, and when leveraged correctly can outperform vendor libraries, especially for operations that may be less popular or optimized. TVM uses a "compute schedule" programming paradigm, similar to Halide <ref type="bibr" coords="8,422.84,235.70,14.84,9.03" target="#b63">[63]</ref>, where a high-level description of the computation is complemented by "schedules. " Schedules are platform-specific transformations that are applied to the code for each algorithm to enable better performance. Examples of schedule language primitives include parallelism, loop unrolling/splitting/reordering, and vectorization.</p><p>Tensor compilers can be taken even further by tuning the code (i.e., schedule) for each layer, using tools such as AutoTVM <ref type="bibr" coords="8,170.04,307.43,16.36,9.03" target="#b9">[10]</ref> and Ansor <ref type="bibr" coords="8,234.98,307.43,14.84,9.03" target="#b90">[90]</ref>. Specifically, Ansor is an auto-scheduling system built on top of TVM that automatically searches for optimized schedules for a given DNN model on a given hardware platform. It leverages a genetic algorithm and learned cost model to iteratively explore schedule transformations. A tuned schedule alters a given operation to exploit how the data sizes and access patterns interact with the target hardware, e.g., changing the schedule of a DNN layer of a given size to better exploit the cache memory. This is contrasted to an untuned schedule, which is designed to be more generic, and does not exploit knowledge of how the hardware and a particular DNN layer interact.</p><p>Below the level of tensor compilers are programming paradigms and general-purpose compilers. LLVM <ref type="bibr" coords="8,74.25,415.03,16.36,9.03" target="#b47">[47]</ref> is a cross-platform compiler infrastructure that higher-level compilers such as TVM can lower their optimized code to, which is then converted to a binary. For hardware accelerators, systems such as CUDA <ref type="bibr" coords="8,145.51,438.94,16.36,9.03" target="#b57">[57]</ref> and OpenCL <ref type="bibr" coords="8,220.85,438.94,16.36,9.03" target="#b71">[71]</ref> provide a programming interface for GPUs, and more specialized accelerators may define their own libraries. Generally, when using accelerators, we still require CPU-side host code to manage accelerator calls and data transfers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Hardware</head><p>Hardware devices that DNN models commonly execute on include CPUs and GPUs, as well as more specialized accelerators. CPUs are generally complex independent processing cores, typically with one to several dozen cores on a single chip. GPUs are generally simpler interdependent processing cores, typically with dozens to several thousand cores on a single chip. Note that this comparison is a simplification, since these cores are not equivalent, and they may vary in speed, programmability, and other features.</p><p>Vector or SIMD instructions allow multiple data to be loaded or computed upon using a single instruction. For example, the Intel instruction MOVAPS loads four float32 values in a single instruction, which in theory represents a 4× speedup compared to loading them one-by-one. Practically, micro-architectural considerations means that this speedup may vary. Different architectures may support varying maximum SIMD length, e.g., Intel's AVX instructions support up to 256 bits, whereas Arm Neon supports up to 128 bits. Different hardware may also have varying support and levels of optimization for different data-types, e.g., a CPU may support float16 data-types but actually execute them as float32 instructions; whereas a GPU may have explicit float16 instructions that can be exploited.</p><p>Another important aspect of hardware is the memory system, with fast, small caches being close to computation and larger, slower memories higher up the hierarchy. As well as memory management within processor hardware, data transfers between CPU and an accelerator can also be a critical bottleneck to efficient processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Our experiments represent a vertical slice of DLAS, which demonstrates the design choices available and interactions that occur. Therefore, we do not optimize for every technique that may influence a given result. Additional techniques from the literature that could be used to further optimize performance, as well as discussion of the trends in our results, are given in Section 6. We develop our case study using Apache TVM (extending it where necessary), a state-of-the-art tensor compiler that is competitive with other optimized runtimes <ref type="bibr" coords="9,269.59,245.03,10.36,9.03" target="#b8">[9,</ref><ref type="bibr" coords="9,282.09,245.03,12.82,9.03" target="#b90">90]</ref> and has the advantage of being able to generate code for multiple targets (e.g., GPUs and CPUs of various architectures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models &amp; Neural Architectures</head><p>As highlighted in Section 3.1, we investigate two image classification datasets: CIFAR-10 and Ima-geNet. For CIFAR-10, we use model definitions from a PyTorch-based library <ref type="bibr" coords="9,358.39,305.17,14.84,9.03" target="#b44">[44]</ref>, which we train from scratch. We consider four architectures: ResNet18 <ref type="bibr" coords="9,273.50,317.12,14.83,9.03" target="#b33">[33]</ref>, MobileNet V1 <ref type="bibr" coords="9,353.79,317.12,16.36,9.03" target="#b37">[37]</ref> and V2 <ref type="bibr" coords="9,404.26,317.12,14.84,9.03" target="#b66">[66]</ref>, and VGG-16 <ref type="bibr" coords="9,80.39,329.07,14.84,9.03" target="#b69">[69]</ref>. ResNet18 and VGG-16 are larger models, and MobileNets V1 and V2 are designed to be more resource-efficient. To train the models, we used SGD to minimize the cross-entropy loss (averaged across all data items), which penalizes the network for making incorrect classifications. We used a 1cycle LR scheduler <ref type="bibr" coords="9,172.47,364.94,16.36,9.03" target="#b70">[70]</ref> with momentum 0.9, weight decay 5 × 10 −4 , and an initial LR of 5 × 10 −<ref type="foot" coords="9,85.22,374.86,4.00,6.85" target="#foot_1">2</ref> , trained for 200 epochs.</p><p>For our ImageNet models, we use pre-trained models from the TorchVision repository <ref type="bibr" coords="9,423.46,388.85,14.83,9.03" target="#b55">[55]</ref>. We consider four architectures: DenseNet161 <ref type="bibr" coords="9,236.71,400.81,14.83,9.03" target="#b39">[39]</ref>, EfficientNetB0 <ref type="bibr" coords="9,321.14,400.81,14.84,9.03" target="#b73">[73]</ref>, ResNet50 <ref type="bibr" coords="9,384.79,400.81,14.84,9.03" target="#b33">[33]</ref>, and Mo-bileNetV2 <ref type="bibr" coords="9,88.02,412.76,14.84,9.03" target="#b66">[66]</ref>. ResNet50 and DenseNet161 are larger models, and MobileNetV2 and EfficientNetB0 are designed to be more resource-efficient. These models are pre-trained with the training configurations described in the TorchVision documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Optimizations</head><p>We explore three approaches to compression: (1) global L1 unstructured pruning, which we call "weight pruning" (Figure <ref type="figure" coords="9,147.00,484.85,20.93,9.03" target="#fig_2">2(b));</ref> (2) global L1 structured pruning over convolutional channels, which we call "channel pruning" (Figure <ref type="figure" coords="9,189.74,496.82,10.00,9.03" target="#fig_2">2(c</ref>)); and (3) data-type quantization, exploring float16 and int8 quantization (Figure <ref type="figure" coords="9,152.44,508.77,13.74,9.03" target="#fig_2">2(d)</ref>). Our pruning techniques explore the impact of increasingly higher compression ratios, 2 thus for the evaluation of inference time and for each model and pruning technique, we select the pruning level that has the highest compression ratio before a significant accuracy drop (elbow point) occurs. To implement our pruning, we leverage PyTorch Lightning <ref type="bibr" coords="9,67.80,556.58,14.85,9.03" target="#b23">[23]</ref>, a wrapper of PyTorch that simplifies pruning. We apply pruning iteratively, starting with the pre-trained unpruned dense models. To reduce accuracy loss, at each pruning step, we apply fine-tuning. For weight pruning, we start by pruning at 50%, then increase in step sizes of 10%, additionally pruning at 95% and 99%. For channel pruning, we start by pruning at 5%, then increase in step sizes of 5%, additionally pruning at 99%. In total, each pruning technique gets the same number of fine-tuning epochs shared evenly in each pruning step. However, we perform channel pruning in a more fine-grained way to compensate for its more coarse-grained approach to removing weights. For our CIFAR-10 models, we use: 210 epochs of fine-tuning; an initial LR of 5 × 10 −2 ; and SGD with momentum 0.9, a weight decay 5 × 10 −4 , and the 1cycle LR scheduler <ref type="bibr" coords="10,423.27,101.20,14.84,9.03" target="#b70">[70]</ref>. For our ImageNet models, we use: 140 epochs of fine-tuning; an initial LR of 1 × 10 −3 ; and SGD with momentum 0.9, weight decay 5 × 10 −4 , and the cosine annealing LR scheduler <ref type="bibr" coords="10,385.40,125.11,14.84,9.03" target="#b52">[52]</ref>.</p><p>For data-type quantization, we use TVM's native conversion tool. To recover the accuracy from quantizing from float32 to int8, we use ONNXRuntime's <ref type="bibr" coords="10,313.71,149.03,16.36,9.03" target="#b20">[20]</ref> post-training quantization tool (with the default static quantization configuration) and the relevant validation dataset. For float16, as we show later in our results, there is no accuracy loss, thus, we do not perform any additional steps to recover lost accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithms &amp; Data Formats</head><p>We evaluate three algorithmic primitives for the convolutional layers: (1) direct, (2) GEMM, and (3) spatial pack convolution. We use both dense and sparse versions of these algorithms, which we implement or extend within TVM v0.8.0. The same high-level algorithm implementation is used for both CPU and GPU. All of our algorithms use the NCHW data layout, and for both weight and channel pruning, we use the CSR sparse data format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Systems Software</head><p>For each convolution algorithm, we implement a minimal TVM schedule that uses thread parallelism. However, since TVM's performance comes from optimized schedules, unoptimized algorithms may give an unrealistic indication of the best algorithm. Thus, rather than hand-optimize the schedules and risk introducing bias from inconsistent levels of optimization, we leverage the Ansor auto-scheduler <ref type="bibr" coords="10,135.16,352.27,16.36,9.03" target="#b90">[90]</ref> to generate optimized schedules for each DNN layer and algorithm. For CPU code, we use TVM's LLVM backend with AVX and Neon extensions for the Intel and Arm CPUs, respectively. For GPU code, we generate OpenCL and CUDA kernels for the Arm and Nvidia GPUs, respectively.</p><p>For our auto-scheduling (or "tuned") experiments, we allow Ansor to explore up-to 20,000 program variants, with early stopping permitted if no speedups have been observed after 1,000 variants. Auto-scheduling sparse computations is not fully supported by TVM. Thus, we employ an approach in TVM called "sparse sketch rules, " where we describe a starting point for the autoscheduler to begin schedule generation. This works for the CPU, however, TVM is unable to support auto-scheduled sparse computations on GPUs in the versions of TVM we have evaluated. This is because the auto-scheduler has two conflicting requirements: (1) cross-thread reduction, requiring partial sums that must be computed across GPU threads simultaneously; and (2) loops parallelized over threads must request a static number of threads. Both of these conditions cannot be satisfied, since the size of our reduction loop for our algorithms varies, depending on how many non-sparse elements there are in a given portion of the computation. Thus, we cannot tune pruned models on the GPU in our evaluation, which highlights a type of barrier faced by across-stack acceleration researchers, i.e., limited software support for a given combination of DLAS parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hardware</head><p>Table <ref type="table" coords="10,70.06,579.41,4.63,9.03" target="#tab_0">1</ref> shows the hardware platforms used in our experiments. For CPU experiments, we use an Intel i7 workstation machine and the HiKey 970 development board. The i7 has 6 cores, but due to hyper-threading, 12 threads are exposed. By default, TVM uses 1 thread per core, a default we follow in our experiments. The HiKey board has an Arm big.LITTLE architecture, meaning that it has 4 more powerful cores (A73@ 2.4 GHz) and 4 less powerful cores (A53@1.8 GHz). For our experiments, we use only the A73 (big) cores, which is the default for TVM. In principle, with 1:11 For the HiKey 970, we only use the A73 CPU.</p><p>appropriately configured load balancing between cores, using all cores could bring a performance improvement. However, this is outside the scope of this work, and as we will discuss in Section 6.6, exposes further across-stack considerations. For our GPU experiments, we leverage the GPU cores of the HiKey 970 and an Nvidia AGX Xavier devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Evaluation Methodology</head><p>On both CPU and GPU experiments, we ensure that devices are single-tenant and repeat inference experiments 150 times. We use TVM's time_evaluator function with a single input image (i.e., batch size 1). For auto-scheduling, we run our search across 20,000 program variants once per experiment and evaluate the optimized binary 150 times. We report the median inference time, disregarding an initial warm-up run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We split the results between CIFAR-10 (Section 5.1) and ImageNet (Section 5.2). We first analyze the accuracy impact of the optimization techniques and choose maximally compressed models for each technique that maintains accuracy. Then, we analyze the inference performance of these models using the algorithms and compilation configurations on the CPUs and GPUs. Section 6 gives a high-level discussion of the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">CIFAR-10</head><p>5.1.1 Accuracy. The models' accuracy with varying levels of compression can be seen in the first row of Figure <ref type="figure" coords="11,124.43,418.05,3.41,9.03" target="#fig_3">3</ref>. For the four models, Table <ref type="table" coords="11,246.30,418.05,4.63,9.03" target="#tab_1">2</ref> shows the baseline (dense) top-1 accuracy on CIFAR-10. We observe that, for weight pruning (Figure <ref type="figure" coords="11,277.62,430.01,13.45,9.03" target="#fig_3">3(a)</ref>), the accuracy is maintained for all models until 95% pruning, at which point all models see a drop in accuracy at 99%. However, the drop in accuracy for MobileNetV1 and V2 is higher, likely because they have fewer parameters.</p><p>We also observe this trend in channel pruning (Figure <ref type="figure" coords="11,275.39,465.87,3.46,9.03" target="#fig_3">3</ref>(b)), where VGG-16 and ResNet18 maintain their accuracy for longer compared to the MobileNets. However, for all models, the drop in accuracy is earlier compared to weight pruning, with the elbow appearing at 50% pruning for the MobileNet models and 80% for VGG-16 and ResNet18. We also observe that after the elbow the drop is higher, to around 10% accuracy or equivalent to random guessing.</p><p>For data-type quantization in Figure <ref type="figure" coords="11,203.86,525.64,3.37,9.03" target="#fig_3">3</ref>(c), we observe almost no change in accuracy for float16 across the four models. The output is not bit-wise identical, however, at most, this represents a 0.03% difference in top-1 accuracy. For uncalibrated int8 quantization, all models see a drop in accuracy, with MobileNets V1 and V2 seeing the highest drops, to 10.0% and 16.4%, respectively. However, with calibration, all models recover significantly, with MobileNetV1 losing the most accuracy at 1.7%. Table <ref type="table" coords="11,138.98,585.42,4.63,9.03" target="#tab_1">2</ref> shows the elbow points of accuracy we take for the inference experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Inference -CPU (Untuned).</head><p>The first two rows of Figure <ref type="figure" coords="11,307.84,603.32,4.63,9.03">4</ref> show the untuned performance of the CIFAR-10 models when running on the CPUs of the HiKey (4(a)-4(d)) and i7 (4(e)-4(h)) platforms, with varying compression strategies and convolutional primitives. The overall trends, including the fastest combination of parameters under different settings, are shown in Table <ref type="table" coords="11,435.17,639.19,3.41,9.03" target="#tab_2">3</ref>.  For the untuned baseline, we observe that the i7 and HiKey differ in fastest algorithm, with direct and GEMM for all models, respectively. However, in most cases across devices, GEMM algorithms are the fastest, with one exception for VGG-16 on the HiKey. In terms of compression techniques, int8 has the fastest overall time in three out of four cases on the i7, whereas weight pruning is the fastest on the Hikey in three out of four cases.</p><p>If we take the best baseline time for each model, then we can compute an expected speedup given the compression ratio of each model optimization technique. For example, on the HiKey for MobileNetV1, with a pruning rate of 95%, we could expect an ideal speedup of 20×. However, in this case, we only achieve a speedup of 2.6× for the best weight pruning algorithm (GEMM), i.e., 13.0% of the expected speedup. On average, for weight pruning, we achieve 11.5% and 21.8% of the expected speedup on the HiKey and i7, respectively.</p><p>For channel pruning, the elbow points for the models (see Table <ref type="table" coords="12,322.14,522.66,3.80,9.03" target="#tab_1">2</ref>) show that channel pruned models are less compressed than the weight pruning models, means that we would [expect] the latter to always be slower than the former. However, we observe several cases where a channel pruning model is faster, namely, for all VGG-16 variants, except for GEMM on the HiKey, which is slightly slower. On average, for channel pruning, we achieve 77.9% and 83.9% of the expected speedup on the HiKey and i7, respectively.</p><p>For float16, we observe a slowdown when compared to the baseline (float32) in every case. For int8, we generally see a speedup relative to the baseline, with some notable exceptions using spatial pack. In some cases, int8 gives the best time overall, as seen in the last column of Table <ref type="table" coords="12,419.35,618.30,3.41,9.03" target="#tab_2">3</ref>. On average, we achieve 33.6% and 157.2% of the expected speedup on the HiKey and i7, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Inference -CPU (Tuned).</head><p>The last two rows of Figure <ref type="figure" coords="13,311.63,418.02,4.63,9.03">4</ref> show the tuned performance of the CIFAR-10 models when running on the HiKey (4(i)-4(l)) and i7 (4(m)-4(p)) CPUs, with overall trends shown in Table <ref type="table" coords="13,172.27,441.93,3.41,9.03" target="#tab_2">3</ref>. Comparing to the untuned results in Section 5.1.2, we observe that tuning has created some significant differences in the relative performance of the experiments, beyond reducing the inference time significantly. For example, spatial pack is now predominantly the best algorithm for dense models on both CPUs. For weight pruning, direct is the best algorithm, and this combination is predominantly the fastest inference time across models and CPUs. On average, we achieve 9.5% and 6.9% of the expected speedup on the HiKey and i7, respectively, less than untuned but faster in absolute terms. For channel pruning, sparse direct is also the best algorithm, and we still observe that channel pruning is faster than weight pruning in most cases of VGG-16. On average, we achieve 39.8% and 26.6% of the expected speedup on the HiKey and i7, respectively. Also, float16 still gives a consistent slowdown, and these differences are exacerbated when compared to the untuned results. The relative speedups of int8 on the i7 have disappeared with tuning, with an average slowdown of 2.0×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Inference -GPU (Untuned).</head><p>The first two rows of Figure <ref type="figure" coords="13,307.94,591.37,4.63,9.03" target="#fig_5">5</ref> show the untuned performance of the CIFAR-10 models when running on the GPUs of the HiKey (5(a)-5(d)) and Xavier (5(e)-5(h)) devices, with overall trends shown in Table <ref type="table" coords="13,230.92,615.27,3.41,9.03" target="#tab_3">4</ref>. We note that the HiKey's inference time on the GPU is much higher than on the CPU, e.g., the dense direct MobileNetV1 is almost 7× slower on the GPU. For the pruned experiments, we see speedups most consistently using spatial pack, which is different from the CPU where we almost always saw a slowdown. In terms of expected speedup, for the HiKey and Xavier, respectively, for weight pruning, we achieve 7.4% and 2.2%, and for channel pruning, we achieve 41.1% and 26.0%. For float16, unlike the CPU, we observe speedups in several cases, however, this behavior is not consistent across models, algorithms, or devices. For example, on the HiKey using GEMM with MobileNetV1 (Figure <ref type="figure" coords="14,347.07,350.03,3.36,9.03" target="#fig_5">5</ref>(a)), float16 provides a slowdown, whereas, for other models on the HiKey, we observe a speedup (similar to the Xavier). On the Xavier, float16 provides a speedup in all cases except for direct convolution where we observe small slowdowns. On average, float16 achieves 51.9% and 49.4% of its potential speedup on the HiKey and Xavier, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Inference -GPU (Tuned).</head><p>The last two rows of Figure <ref type="figure" coords="14,304.04,419.07,4.63,9.03" target="#fig_5">5</ref> show the tuned performance of the CIFAR-10 models on GPUs, with overall trends shown in Table <ref type="table" coords="14,320.48,431.03,3.41,9.03" target="#tab_3">4</ref>. As noted in Section 4.4, we cannot provide tuned results for sparse models on the GPU. The HiKey GPU is still slower than the HiKey CPU (tuned), with the best dense result being 3.1× slower on average. The Xavier does not get any improvement when tuning; we discuss this issue in Section 6.5. For quantization, on the HiKey, taking the best result for each model, we achieve 58.9% and 44.4% of the expected speedup on average for float16 and int8, respectively; the Xavier achieves 49.0% and 28.4% of its expected speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ImageNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Accuracy.</head><p>For the four models, the baseline (dense) top-1 accuracy on ImageNet is shown in Table <ref type="table" coords="14,81.73,553.86,3.41,9.03" target="#tab_4">5</ref>. EfficientNetB0 has the highest accuracy, which may be surprising, given it has fewer parameters. However, EfficientNetB0 is more recent and thus exploits a number of newer machine learning techniques to improve its parameter and training efficiency.</p><p>The accuracy on ImageNet with varying levels of compression can be seen in the second row of Figure <ref type="figure" coords="14,73.65,601.69,3.41,9.03" target="#fig_3">3</ref>. We observe a similar trend to the CIFAR-10 models, namely, the smaller models (Efficient-NetB0 and MobileNetV2) lose their accuracy more quickly than the larger ones (DenseNet161 and ResNet50). We also observe that all models lose more accuracy earlier when compared to CIFAR-10 pruning. This suggests that the CIFAR-10 models are more overparameterized.  </p><formula xml:id="formula_0">MobileNetV2 Direct − − Direct Direct i8 i8 i8 i8+Direct ResNet18 Direct − − Direct Direct i8 i8 i8 i8+Direct VGG-16 Direct − − Direct Direct i8 i8 i8 i8+Direct MobileNetV1 Direct − − Direct Direct i8 i8 i8 i8+Direct</formula><p>The best inference times are shown in bold. Shortened names are used for brevity: WP (weight pruning), CP (channel pruning), i8 (int8), f16 (float16).  Fig. <ref type="figure" coords="16,61.54,412.59,3.07,8.07">6</ref>. Experiments comparing the compressed ImageNet models chosen from obvious elbows of accuracy, with varying algorithmic primitives, benchmarked on the i7 and HiKey CPU platforms, with and without auto-scheduling.</p><p>For data-type quantization, we observe a similar trend as CIFAR-10, namely, a negligible difference in accuracy for float16. For ResNet50, we see a large drop in accuracy for uncalibrated int8 quantization, recovering to around a 3.0% accuracy reduction. For MobileNetV2, we observe a huge drop in accuracy for the uncalibrated model, down to around 0.09%, recovering to around a 6.6% accuracy reduction. This drop was much higher than we expected, so we also tried importing the Keras <ref type="bibr" coords="16,86.45,513.14,16.36,9.03" target="#b11">[12]</ref> definition of MobileNetV2 and observed the same behavior.</p><p>For EfficientNetB0, we also observe a huge drop in accuracy to 0.08%, however, the recovery is much smaller than MobileNetV2's, reaching only 0.43% accuracy. This is due to architecture features of the model that make it less suitable for quantization, which we discuss in Section 6.2. For DenseNet161, we cannot run the int8 model in TVM due to an unsupported quantized operation. This excludes it from collection of uncalibrated accuracy and inference time results. However, when calibrated in ONNXRuntime, we reduce accuracy by 1.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Inference -CPU (Untuned).</head><p>The first two rows of Figure <ref type="figure" coords="16,307.67,603.15,4.63,9.03">6</ref> show the untuned performance of the ImageNet models when running on the i7 and HiKey CPUs, with overall trends shown in Table <ref type="table" coords="16,69.97,627.06,3.41,9.03" target="#tab_5">6</ref>. For dense models, we observe that in all cases on the HiKey GEMM gives the best performance, which matches its behavior as seen for CIFAR-10. For the i7, GEMM is fastest for the large  </p><p>models (ResNet50 and DenseNet161), however, direct is fastest for the small models (MobileNetV2 and EfficientNetB0); on CIFAR-10, direct was consistently the fastest on this CPU.</p><p>For weight pruning, we find that by taking the best-performing variants as before, we achieve 30.3% and 41.8% of the potential speedup for the HiKey and i7, respectively; significantly higher than CIFAR-10. For channel pruning, this is 60.2% and 84.2%, respectively, which is 16.7% less than CIFAR-10 for the HiKey and 0.3% more for the i7. For quantization, we see similar trends to CIFAR-10, namely, a slowdown using float16 and a speedup using int8. For int8, we achieve 25.0% and 73.0% of the expected speedup on the i7 and HiKey, respectively, lower than CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Inference -CPU (Tuned).</head><p>The last row of Figure <ref type="figure" coords="17,283.38,429.32,4.63,9.03">6</ref> shows the tuned performance of the ImageNet models when running on the i7 CPU. We note that tuning on the HiKey CPU (and GPU) was not practical, since the two variants we attempted took over 140 hours each, so we do not include any of the 57 variants required for each device. For the dense case on the i7, we see that spatial pack is consistently the best, matching the observed trends on tuned CIFAR-10. For the pruned models, we do not see any cases where pruned models are faster than a dense float32 implementation. This is contrasted with CIFAR-10, where we observe this in every case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Inference -GPU (Untuned).</head><p>On the Xavier in the dense case, spatial pack is the best algorithm for the larger models (ResNet50 and DenseNet161), and direct is the best for the smaller models (MobileNetV2 and EfficientNetB0). On the Hikey, for the smaller models, GEMM was the best, and for the larger models, direct was the best. However, on the HiKey, dense ResNet50 and DenseNet161 experiments using spatial pack crashed with the error CL_INVALID_WORK_GROUP_SIZE. This means that TVM is exceeding the number of supported work items (see the OpenCL specification for more details <ref type="bibr" coords="17,263.15,591.16,14.34,9.03" target="#b71">[71]</ref>). If we run auto-tuning, then TVM can configure the work group size, which could avoid this issue.</p><p>For the sparse experiments, spatial pack using weight pruning was consistently the best across both GPUs, however, only outperformed the baseline in one case, ResNet50 on the Xavier. On the Xavier, we found that sparse direct experiments did not halt, even allowing hours for a single run. GPU memory utilization was at its maximum, suggesting that some inefficiency in this algorithm/hardware combination. Again, auto-tuning may make this variant viable, however, as highlighted in Section 4.4, we cannot tune sparse models on GPUs.</p><p>For ImageNet, quantization was not consistently the best compression technique for the GPUs, unlike CIFAR-10. For several cases, weight pruning performed best. On the HiKey, using int8 for EfficientNet was the fastest approach, however, it should be noted that in this case the accuracy drop was prohibitively large, as discussed in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Inference -GPU (Tuned).</head><p>As discussed in Section 5.2.3, collecting tuned results for the HiKey GPU was not practical. In addition, we again observed no speedup on the Xavier when tuning, hence, we do not include the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Related Work</head><p>Our experiments show a large number of results, even from varying a small number of parameters across DLAS, which highlights a critical challenge: the huge design space for across-stack DNN acceleration. Other studies try to make more generalized claims of the impact of a specific parameter by exploring a significantly reduced design space, limiting the number of parameters explored at some layers of DLAS or keeping them fixed. For example, Hadidi et al. <ref type="bibr" coords="18,332.81,480.21,16.37,9.03" target="#b32">[32]</ref> do not explore the impact of compression techniques or algorithms. Similarly, the stack presented by VTA <ref type="bibr" coords="18,391.53,492.16,16.35,9.03" target="#b56">[56]</ref> focuses on a given compiler framework and accelerator, which is less general than DLAS.</p><p>Our results show that the interaction between different layers of DLAS can cause significant performance variations. This is our core observation: The number of design choices available for DNN acceleration continues to grow, and including additional parameters in a study can significantly change the performance optimization landscape. Some examples of this include:</p><p>-For MobileNetV2 on CIFAR-10, the fastest overall combination of algorithm and compression technique is weight pruning and GEMM (see Table <ref type="table" coords="18,288.09,578.94,3.27,9.03" target="#tab_2">3</ref>). This is true in the untuned cases of both the HiKey and i7 CPUs. However, adding in just one additional DLAS parameter (tuning) changes this, such that for the HiKey, weight pruning and direct convolution is the fastest, whereas for the i7, dense and spatial pack convolution is the fastest. This example shows that introducing just one new DLAS parameter can expose differences between other parameters (the hardware devices) not previously evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1:19</head><p>-Similarly, for ImageNet in the untuned case on the CPU, weight pruning is consistently the fastest compression technique on both devices. However, on the i7, for MobileNetV2 only, int8 quantization is the fastest option. -Finally, for CIFAR-10 on the GPUs, direct convolution is consistently the fastest algorithm in the untuned case, however, on the HiKey for VGG-16, spatial pack convolution is the fastest.</p><p>Overall, these examples show how multiple across-stack interactions can both unlock higher performance, but also present the challenge of how to efficiently explore the design space, i.e., choosing DLAS parameters that are likely to give high-performance improvements without significantly increasing the number of experiments required.</p><p>Identifying the across-stack problems relevant to DNN acceleration has been explored in other works. For example, Sze et al. <ref type="bibr" coords="19,169.71,205.39,16.37,9.03" target="#b72">[72]</ref> give a comprehensive overview of relevant DNN acceleration parameters, but do not simplify this overview into a concise structure, which is critical for efficient discussion and analysis. Accelerating DNNs is the combination of several NP-hard optimization problems, so, at best, researchers can create principled heuristics that try to give the highest speedups, while keeping the evaluation costs low. Our experiments used a modified tensor compiler (TVM) <ref type="bibr" coords="19,101.88,265.17,11.42,9.03" target="#b8">[9]</ref>, which helped reduce the cost of DSE by automatically generating code for different combinations of parameters. Techniques and software like this will be increasingly relevant as the number of design choices continues to grow <ref type="bibr" coords="19,254.75,289.08,14.83,9.03" target="#b26">[26]</ref>.</p><p>In Section 5, we observed many variations across our experiments with a number of non-trivial dynamics emerging. Our first observation is (I) MACs, accuracy, and inference time are not strongly correlated, along with 12 additional observations, (II)-(XIII). These observations are not intended to be definitive, and large changes in the results could be expected by bringing in new techniques from DLAS. However, the key point of this work is that across-stack interactions of machine learning and systems optimizations can be non-trivial, and bringing in additional features may significantly accelerate or impede a given technique. We refer the reader to other characterization works and surveys that highlight cutting-edge techniques from across DLAS <ref type="bibr" coords="19,314.76,384.72,10.38,9.03" target="#b4">[5,</ref><ref type="bibr" coords="19,327.64,384.72,11.45,9.03" target="#b32">32,</ref><ref type="bibr" coords="19,341.58,384.72,11.46,9.03" target="#b36">36,</ref><ref type="bibr" coords="19,355.53,384.72,11.45,9.03" target="#b50">50,</ref><ref type="bibr" coords="19,369.49,384.72,11.45,9.03" target="#b72">72,</ref><ref type="bibr" coords="19,383.44,384.72,11.25,9.03" target="#b84">84]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets &amp; Problem Spaces</head><p>Between datasets, models showed similar trends in terms of accuracy, with the accuracy losses due to compression being higher for ImageNet models. The computational requirements of models for each dataset varied, i.e., CIFAR-10 models have fewer MACs and parameters than ImageNet models (Tables <ref type="table" coords="19,77.47,462.60,28.81,9.03" target="#tab_4">2 and 5</ref>), which we would expect to have effects on the inference behavior. For instance, (II) tuned sparse CIFAR-10 models were faster than the dense baseline, but this was not the case for ImageNet models. Possible explanations include overheads in the sparse algorithm and data format choices, which could be exacerbated by the larger ImageNet models; and for tuning, both sets of models were given the same number of trials despite ImageNet models being larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Models &amp; Neural Architectures</head><p>As a related observation to (I), we note that (III) model size is not strongly correlated with accuracy, however, smaller models are more vulnerable to compression. For example, EfficientNetB0 has a higher baseline accuracy than both DenseNet161 and ResNet50, despite having at most 21% of the parameters. This can be understood as EfficientNetB0 being a more recent model that exploits novel architectural and training techniques to achieve better parameter efficiency. However, other works have shown that if we keep the model architecture the same, then more parameters generally means higher accuracy <ref type="bibr" coords="19,143.50,624.16,15.00,9.03" target="#b16">[16,</ref><ref type="bibr" coords="19,161.43,624.16,11.46,9.03" target="#b28">28,</ref><ref type="bibr" coords="19,175.82,624.16,11.26,9.03" target="#b73">73]</ref>. For CIFAR-10, we observed that ResNet18 and MobileNetV2 had higher baseline accuracies than VGG-16, despite both having fewer parameters. A similar explanation of using techniques such as residual blocks can explain this behavior. However, as (III) notes, models with fewer parameters were more vulnerable to compression.</p><p>Another related observation is that EfficientNetB0 had the highest accuracy drops for int8 quantization out of any model. We understand this to be due to EfficientNetB0's architecture not being amenable to post-training quantization. These issues were highlighted and corrected by EfficientNet-Lite <ref type="bibr" coords="20,115.07,137.07,14.85,9.03" target="#b51">[51]</ref>, which removes the squeeze-and-excitation networks and replaces the swish activation functions with ReLU6 activations <ref type="bibr" coords="20,226.62,149.03,14.83,9.03" target="#b37">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Optimizations</head><p>Overall, the best model optimization technique varied. (IV) Weight pruning tended to give better compression ratios and speedups than channel pruning, however, achieved less of its expected speedups. We also observed that (V) quantization's speedup varied by hardware platform and data-type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Pruning.</head><p>As observation (IV) notes, weight pruning was generally faster than channel pruning, however, the former achieved a lower proportion of its expected speedups. There were several cases where a less compressed channel pruning model was faster than a more compressed weight pruning model. However, this appears to come [from] algorithmic interactions, since taking the best variant across algorithms it was rare for channel pruning to outperform weight pruning. Overall, the relative performance of pruned models on the GPUs was worse than on the CPUs. This is because sparse computations are irregular and thus cannot easily take full advantage of the greater number of cores available in a GPU that dense computations can. However, some across-stack techniques can improve sparse performance on GPUs <ref type="bibr" coords="20,324.54,322.38,15.01,9.03" target="#b30">[30,</ref><ref type="bibr" coords="20,342.84,322.38,11.26,9.03" target="#b62">62]</ref>. Since we could not tune sparse computations on the GPUs, we cannot comment how well they would perform if the code further optimized. Other pruning techniques that we did not explore include layer-wise pruning and gradient-based methods, tradeoffs of which are discussed in Blalock et al. <ref type="bibr" coords="20,391.40,358.24,10.44,9.03" target="#b4">[5]</ref>. We only evaluated pruning techniques in terms of accuracy and compression, however, as other work has noted, pruning also comes with non-negligible costs due to retraining, which may be a bottleneck to DSE <ref type="bibr" coords="20,76.28,394.11,15.01,9.03" target="#b38">[38,</ref><ref type="bibr" coords="20,93.78,394.11,11.26,9.03" target="#b61">61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Data-type</head><p>Quantization. (VI) float16 had a negligible impact on accuracy, when calibrated int8 lost 1.8% and 22.2% for CIFAR-10 and ImageNet models on average, respectively. Excluding Ef-ficientNetB0, ImageNet models lost 3.8% of accuracy on average. With regards to inference time, as observation (V) notes for float16 on the CPUs, we observed a consistent slowdown when compared to float32. This is because the hardware runs float16 computation using software emulation. Although there are savings in memory footprint, the overhead involved in the emulation clearly outweighs these savings, with the differences exacerbated when tuning. For int8 on the CPUs, we generally observed a speedup, since the CPU ISAs can use SIMD instructions to compute int8 computations relatively efficiently.</p><p>In the untuned int8 case on the CPU, we observe some cases where we get higher than expected speedups; for instance, many of the GEMM and direct cases on the i7 in Figure <ref type="figure" coords="20,371.32,531.59,3.41,9.03">4</ref>. The highest of this is for VGG-16 with CIFAR-10 using GEMM, where we observe a speedup of over 9.2×. We hypothesize that using smaller data sizes is reducing cache usage, meaning that the algorithm has to fallback to slower caches less in the int8 case. However, this advantage is significantly reduced when we tune. We believe that Ansor is not fully exploiting the performance potential and search space of int8, since it was (1) initially designed with float32 computation in mind, and (2) int8 can require a more complex sequence of instructions to be generated to run efficiently, which Ansor does not appear to factor in.</p><p>For float16 on the GPUs, contrasting to the CPUs, we observed in general a reduction in inference time on both GPUs, as they have direct hardware support for float16 instructions. Like the 1:21 CPU, we also saw speedups with int8 models, in general, marginally higher than with float16. However, if we take the best times across all algorithms, we did not see any cases where float16 or int8 achieved close to an ideal speedup of 2× and 4× relative to float32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Algorithms &amp; Data Formats</head><p>We made several observations for algorithms and how they interacted with other layers of DLAS. In the dense case, when tuned, (VII) spatial-pack convolution is generally the best algorithm on the CPU (when tuned), and (VIII) direct convolution is generally the best algorithm on the GPU (when tuned). Observation (VIII) goes against conventional wisdom, where we would normally expect GEMM to be faster on the GPU. However, we should note that we are using a custom implementation of GEMM within TVM, rather than an optimized BLAS library. When the algorithm was not tuned, the best algorithm varied more, especially on the GPUs. For example, with ImageNet on the HiKey, GEMM was faster for smaller models, and direct was faster for larger models. In the sparse case, (IX) GEMM is generally the best algorithm on CPUs, and direct is generally the best on GPU. For quantized models, the best algorithm varied on the CPUs and HiKey GPU, whereas on the Xavier, direct was generally the fastest.</p><p>In the evaluation, we used the same convolution algorithm for all layers of a given model, but we could vary the algorithm used per-layer, which could bring significant performance speedups. However, as other work has noted, data format transformation overheads between layers need to be considered <ref type="bibr" coords="21,104.36,309.23,10.36,9.03" target="#b2">[3,</ref><ref type="bibr" coords="21,117.22,309.23,11.26,9.03" target="#b18">18]</ref>. We also did not explore all possible algorithms available, such as Winograd convolution <ref type="bibr" coords="21,96.45,321.18,14.84,9.03" target="#b48">[48]</ref>. However, we chose three common algorithms to keep the across-stack evaluation tractable. Other works have explored algorithmic tradeoffs in more detail <ref type="bibr" coords="21,345.77,333.13,15.00,9.03" target="#b13">[14,</ref><ref type="bibr" coords="21,363.26,333.13,11.45,9.03" target="#b22">22,</ref><ref type="bibr" coords="21,377.22,333.13,11.45,9.03" target="#b65">65,</ref><ref type="bibr" coords="21,391.16,333.13,11.26,9.03" target="#b85">85]</ref>.</p><p>As discussed in Section 6.3.1, (X) for the pruning techniques, we rarely observed the expected performance improvements when compared to the dense implementations. Partly this can be attributed to the inherent overheads of sparse data formats-CSR must store up to three values for every nonzero element. This, coupled with irregular data access patterns means that the sparse algorithms do not realize their full potential. CSR is not the only way to represent sparsity, and alternative data formats (and their complementary algorithms) may provide different tradeoffs <ref type="bibr" coords="21,393.96,404.86,15.01,9.03" target="#b12">[13,</ref><ref type="bibr" coords="21,412.25,404.86,11.46,9.03" target="#b31">31,</ref><ref type="bibr" coords="21,427.00,404.86,11.26,9.03" target="#b46">46]</ref>. For example, for channel pruning, we could store a list of the indices of pruned channels and store the non-pruned channel data in a dense format. This could allow us to leverage a more "dense-like" algorithm with an overhead for looking up pruned indices. Alternatively, we could use a format called block-sparse row (BSR), which is similar to CSR but represents blocks of sparse parameters, rather than individual weights. This could allow us to reduce the overheads of the sparse storage format, with greater savings with larger block sizes at the risk [of] higher accuracy loss. However, to fully exploit this, we would need to change the pruning method to prune blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Systems Software (XI) auto-tuned code dramatically accelerates inference time and can change the best algorithm or compression technique.</head><p>However, we observed no speedup tuning the Xavier. When testing with server-class Nvidia GPU platforms, we observed speedups using auto-scheduling and the same evaluation code. Our conclusion is that some aspect of the AGX Xavier's software stack was incompatible with Ansor, but we could not detect it. It is well known that auto-scheduling can provide significant speedups <ref type="bibr" coords="21,131.29,588.98,15.01,9.03" target="#b89">[89,</ref><ref type="bibr" coords="21,148.89,588.98,11.25,9.03" target="#b90">90]</ref>, but the search time required is non-negligible. We could not collect auto-scheduled results on the HiKey for the ImageNet models, since they took too long to tune (over 140 hours each). This highlights a key issue with auto-tuning, namely, the cost of search, especially on constrained devices. Approaches that significantly prune the search space <ref type="bibr" coords="21,406.37,624.84,16.37,9.03" target="#b78">[78]</ref> and techniques such as transfer-tuning <ref type="bibr" coords="21,189.40,636.80,16.37,9.03" target="#b27">[27]</ref> can reduce the search time.</p><p>As highlighted in Section 6.3.2, we observed that (XII) Pruned models saw a lower relative speedup when tuned. A more specialized sparse compiler, such as the emerging SparseTIR system <ref type="bibr" coords="22,397.74,89.25,14.83,9.03" target="#b88">[88]</ref>, could reduce the impact of these overheads. This was also observed for the quantized models, which may require similar optimization support.</p><p>We initially experienced an issue compiling int8 EfficientNet, as TVM assumed that in a multiplication operation only the left-hand operand would be pre-quantized. However, the structure of EfficientNet violated this assumption, which necessitated a bug-fix that we pushed upstream. This highlights that assumptions that systems software make about the properties of workloads may not always hold, especially when novel DNN architectures emerge.</p><p>All of the models were defined in PyTorch and evaluated in TVM, however, there are other DNN frameworks available, and the relative performance of different DNN frameworks has been well studied <ref type="bibr" coords="22,77.17,208.80,15.00,9.03" target="#b28">[28,</ref><ref type="bibr" coords="22,93.93,208.80,11.45,9.03" target="#b32">32,</ref><ref type="bibr" coords="22,107.16,208.80,11.26,9.03" target="#b54">54]</ref>. Other systems-software dimensions to consider are hand-tuned kernel libraries such as oneDNN <ref type="bibr" coords="22,118.62,220.76,14.84,9.03" target="#b58">[58]</ref>, cuDNN <ref type="bibr" coords="22,174.18,220.76,14.84,9.03" target="#b10">[11]</ref>, or other deep learning compilers such as TensorRT <ref type="bibr" coords="22,412.12,220.76,16.37,9.03" target="#b15">[15]</ref> or IREE <ref type="bibr" coords="22,67.97,232.71,14.84,9.03" target="#b76">[76]</ref>. Collage <ref type="bibr" coords="22,121.80,232.71,16.36,9.03" target="#b40">[40]</ref> can explore varying the backend for different subgraphs of the same DNN, which can bring significant performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Hardware</head><p>As expected, (XIII) the i7 CPU was generally faster than the HiKey CPU, and the Xavier GPU was generally faster than the HiKey GPU. Unfortunately, we did not see improvements when tuning on the Xavier GPU. Other aspects of the hardware that could be better utilized include the big.LITTLE architecture of the HiKey CPU (we only leveraged the big cores), hyper-threading on Intel CPU (we ran one thread per core), or leveraging both the CPU and GPU in parallel for the Xavier and HiKey. However, across-stack optimizations would be required to exploit these features properly <ref type="bibr" coords="22,409.46,343.73,15.00,9.03" target="#b53">[53,</ref><ref type="bibr" coords="22,426.83,343.73,11.25,9.03" target="#b83">83]</ref>. We also did not leverage the Xavier GPU's 64 tensor cores in addition to its 512 general purpose CUDA cores. TVM supports tensor cores but requires manual schedule re-design for each algorithm. MetaSchedule <ref type="bibr" coords="22,133.23,379.59,16.36,9.03" target="#b67">[67]</ref> can expand auto-scheduler search spaces to include hardware features such as tensor cores, which could ease [investigation of] this dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Evaluation Methodology</head><p>For the experiments, we kept the batch size as 1, took the median of 150 runs, disregarding the first warm-up run. Although this is a common deployment and evaluation scenario, it is important to be aware that this is not the only one, and experimental design should reflect which deployment case is being considered when evaluating models <ref type="bibr" coords="22,229.09,466.69,15.02,9.03" target="#b74">[74,</ref><ref type="bibr" coords="22,246.77,466.69,11.26,9.03" target="#b86">86]</ref>. For instance, for edge deployment, we may expect the batch size to be small, whereas on the cloud it may be large. Increased batch sizes mean increased memory requirements and inference latency, but also potentially higher throughput. For the use of 150 runs, disregarding the first run, there could be deployment scenarios where we are more interested in the performance of these initial warm-up runs, before the cache behavior becomes more regular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This article first motivates and introduces the Deep Learning Acceleration Stack (DLAS) and then presents a perturbation study with an exploration of the impact of varying a small number of parameters at each layer of the stack. In our study, we find a variety of across-stack interactions and scenarios where theoretical performance improvements were not achieved due to lack of full exploitation across the stack. Our work is not intended to propose solutions to all of these limitations, instead highlights some complexities that emerge in deep learning acceleration and presents a conceptual framework (DLAS) for practitioners to approach their studies in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1:23</head><p>We believe this can be achieved through closer collaboration across the layers of DLAS to enable more holistic co-design and co-optimization. Listing 2: Definition of spatial pack convolution using TVM's tensor expression language. 1 rc = te.reduce_axis((0, in_channel), name="rc") 2 ry = te.reduce_axis((0, kernel_h), name="ry") </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,61.31,189.67,363.82,8.07"><head>3 Fig. 1 .</head><label>31</label><figDesc>Fig. 1. Overview of DLAS, split between machine learning and systems techniques, with examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,55.62,389.45,384.67,9.08;4,70.18,401.45,188.78,9.03;4,55.62,413.36,384.88,9.08;4,70.18,425.37,81.31,9.03"><head>( 1 )</head><label>1</label><figDesc>Datasets &amp; Problem Spaces: the top-level of the stack defines the problem and/or environment that the machine learning solution must solve. (2) Models &amp; Neural Architectures: DNN models and families of architectures, as well as their training techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="6,45.77,133.66,395.88,8.07;6,45.77,144.62,394.51,8.07;6,45.77,155.58,394.74,8.07;6,45.77,166.54,394.51,8.07;6,45.77,177.50,394.51,8.07;6,45.77,188.46,225.37,8.07"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A visual representation of different Model Optimization techniques: (a) shows a slice of a typical DNN. Input on the left is computed with filters in the center to produce output on the right; (b) shows the same network slice with weight pruning applied. A subset of parameters in the filters is forced to zero (visually represented with black holes) producing sparse matrices; (c) shows the network slice with channel pruning applied onto the filters, where there are fewer channels; (d) shows data-type quantization, where the range of values that a given parameter of a DNN has been reduced.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="12,45.77,228.66,394.81,8.07;12,45.77,239.63,394.52,8.07;12,45.77,250.58,395.99,8.07;12,45.77,261.54,222.58,8.07;12,46.77,76.66,389.71,126.55"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Accuracy and compression tradeoffs for our CIFAR-10 (a-c) and ImageNet (d-f) models: (a/d) show the accuracy of each model after iterative weight pruning of the convolutional layers, and fine-tuning the model; (b/e) show a similar setup for channel pruning; and (c/f) show float16 and int8 quantization accuracy, with int8 accuracy shown before and after calibration.</figDesc><graphic coords="12,46.77,76.66,389.71,126.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="13,45.95,371.67,395.52,8.07;13,45.62,382.62,394.84,8.07;13,45.95,393.58,61.52,8.07;13,46.44,76.42,390.40,270.28"><head>13 Fig. 4 .</head><label>134</label><figDesc>Fig. 4. Experiments comparing the compressed CIFAR-10 models chosen from obvious elbows of accuracy, with varying algorithmic primitives, benchmarked on the i7 and HiKey CPU platforms, with and without auto-scheduling.</figDesc><graphic coords="13,46.44,76.42,390.40,270.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="15,45.95,372.66,395.52,8.07;15,45.62,383.63,394.85,8.07;15,45.95,394.58,61.52,8.07;15,46.95,76.36,389.20,270.76"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Experiments comparing the compressed CIFAR-10 models chosen from obvious elbows of accuracy, with varying algorithmic primitives, benchmarked on the HiKey and Xavier GPU platforms, with and without auto-scheduling.</figDesc><graphic coords="15,46.95,76.36,389.20,270.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="17,45.95,55.82,297.97,8.97;17,424.39,55.82,16.09,8.97"><head></head><label></label><figDesc>DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration 1:17</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="18,45.77,239.66,395.47,8.07;18,45.45,250.62,396.34,8.07;18,45.77,261.58,41.88,8.07;18,46.77,76.75,389.20,137.32"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Experiments comparing the compressed ImageNet models chosen from obvious elbows of accuracy, with varying algorithmic primitives, benchmarked on the HiKey and Xavier GPU platforms, without autoscheduling.</figDesc><graphic coords="18,46.77,76.75,389.20,137.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="23,48.15,144.18,2.78,5.42;23,66.27,143.85,132.10,5.47;23,48.15,151.95,2.78,5.42;23,66.27,151.63,159.16,5.47;23,48.15,159.72,2.78,5.42;23,76.44,159.39,128.70,5.48;23,48.15,167.50,2.78,5.42;23,89.98,167.17,311.61,5.47;23,48.15,175.26,2.78,5.42;23,76.44,174.94,6.77,5.47;23,48.15,183.04,2.78,5.42;23,66.27,182.71,3.39,5.47;23,48.15,190.81,2.78,5.42;23,66.27,190.48,104.99,5.47;23,48.15,198.57,2.78,5.42;23,66.27,198.25,348.86,5.48;23,70.64,222.54,345.14,9.03;23,48.15,265.53,2.78,5.42;23,66.27,265.21,172.74,5.47;23,48.15,273.30,2.78,5.42;23,66.27,272.97,348.83,5.47;23,48.15,281.08,2.78,5.42;23,66.27,280.75,135.49,5.47;23,48.15,288.84,2.78,5.42;23,76.44,288.52,284.52,5.48;23,48.15,296.62,2.78,5.42;23,66.27,296.29,3.39,5.47;23,48.15,304.39,2.78,5.42;23,66.27,304.06,159.17,5.47;23,48.15,312.16,2.78,5.42;23,66.27,311.84,169.33,5.47;23,48.15,319.93,2.78,5.42;23,66.27,319.60,165.95,5.47;23,48.15,327.70,2.78,5.42;23,66.27,327.37,128.71,5.47;23,45.37,335.47,5.56,5.42;23,66.27,335.15,81.26,5.47;23,45.37,343.24,5.56,5.42;23,76.44,342.91,284.51,5.48;23,45.37,351.01,5.56,5.42;23,89.98,350.69,16.92,5.47;23,45.37,358.78,5.56,5.42;23,103.54,358.46,331.92,5.47;23,45.37,366.56,5.56,5.42;23,89.98,366.23,325.17,5.47;23,45.37,374.32,5.56,5.42;23,76.44,374.00,6.77,5.47;23,45.37,382.09,5.56,5.42;23,66.27,381.77,3.39,5.47;23,45.37,389.87,5.56,5.42;23,66.27,389.54,118.55,5.47;23,45.37,397.63,5.56,5.42;23,76.44,397.31,277.73,5.48;23,45.37,405.41,5.56,5.42;23,66.27,405.08,3.39,5.47"><head>1 N 2 B 3 ( 5 ], 6 ) 7 k 8 C 1 # 5 ) 6 ic[ 13 n 14 ]</head><label>12356781561314</label><figDesc>, K, M = OC, (KH * KW * IC), (OH * OW) = te.compute( # perform im2col transformation batches, M, K), lambda n, m, k: data[ 4 n, (k // (KH * KW)) % IC, (k // KH) % KW + ((m // OW) * HSTR), (k % KW) + ((m % OW) * WSTR), = te.reduce_axis((0, K), "k") = te.compute((batches, OC, OH, OW), lambda b, c, h, w: te.sum(A[c, k] * B[b, h * OH + w, k], axis=k))Listing 1: Definition of GEMM convolution using TVM's tensor expression language. weight data is reshaped offline to 6-D with shape 2 # [num_filter_chunk, in_channel_chunk, filter_height, filter_width, in_channel_block, num_filter_block] 3 data = te.compute( # reshape data to 5-D 4 (n, ic_chunk, ih, iw, ic_bn), lambda bs, c, h, w, vc: data[bs, c * ic_bn + vc, h, w] = te.reduce_axis((0, in_channel), name="ic") 7 kh = te.reduce_axis((0, kernel_height), name="kh") 8 kw = te.reduce_axis((0, kernel_width), name="kw") 9 oshape = (n, oc_chunk * oc_bn, oh, ow) 10 packed_out = te.compute( 11 oshape, lambda n, oc_chunk, oh, ow, oc_block: te.sum( # compute to 5-D output volume 12 data, idxdiv(ic, ic_bn), oh * HSTR + kh * dilation_h, ow * WSTR + kw * dilation_w, idxmod(ic, ic_bn), * kernel[oc_chunk, idxdiv(ic, ic_bn), kh, kw, idxmod(ic, ic_bn), oc_block], axis=[ic, kh, kw], .compute( # reshape to NCHW 18 oshape, lambda n, c, h, w: packed_out[n, idxdiv(c, oc_bn), h, w, idxmod(c, oc_bn)]19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="23,48.15,487.90,2.78,5.42;23,66.27,487.58,152.40,5.47;23,48.15,495.68,2.78,5.42;23,66.27,495.35,206.59,5.47;23,48.15,503.45,2.78,5.42;23,76.44,503.12,101.59,5.48;23,48.15,511.21,2.78,5.42;23,89.98,510.89,277.72,5.47;23,48.15,518.99,2.78,5.42;23,89.98,518.66,149.03,5.47;23,48.15,526.76,2.78,5.42;23,76.44,526.43,6.77,5.47;23,48.15,534.53,2.78,5.42;23,66.27,534.21,3.39,5.47;23,73.54,558.49,339.35,9.03"><head>3 7 * 8 ), 9 ) 3 :</head><label>7893</label><figDesc>rx = te.reduce_axis((0, kernel_w), name="rx") 4 out = te.compute((batch, out_channel, out_height, out_width), 5 lambda nn, ff, yy, xx: te.sum( 6 data_pad[nn, rc, yy * stride_h + ry * dilation_h, xx * stride_w + rx * dilation_w] Filter[ff, rc, ry, rx], axis=[rc, ry, rx],Listing Definition of direct convolution using TVM's tensor expression language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="11,56.28,75.71,378.05,65.34"><head>Table 1 .</head><label>1</label><figDesc>Hardware Features of the Devices Used in the Experiments</figDesc><table coords="11,56.28,92.86,378.05,48.19"><row><cell>Device</cell><cell>CPU</cell><cell cols="2">L1 Cache (I+D) L2 (+L3) Cache</cell><cell>RAM</cell><cell>GPU</cell><cell>GPU API</cell></row><row><cell>Intel i7</cell><cell>Intel i7-8700 (6 cores) @ 3.2 GHz</cell><cell>192K + 192K</cell><cell>1.5M (+12M)</cell><cell>16 GB DDR3</cell><cell>-</cell><cell>-</cell></row><row><cell>HiKey 970</cell><cell>Arm Cortex-A73 (4 cores) @ 2.4 GHz Arm Cortex-A53 (4 cores) @ 1.8 GHz</cell><cell>256K + 256K 128K + 128K</cell><cell>2M shared 1M shared</cell><cell>6 GB LPDDR4</cell><cell>Mali-G72 (12 cores)</cell><cell>OpenCL</cell></row><row><cell cols="2">AGX Xavier Arm v8.2 Carmel (4 cores) @ 2.19 GHz</cell><cell>64K + 128K</cell><cell>2M (+ 4M)</cell><cell>16 GB LPDDR4x</cell><cell>Volta (512 cores)</cell><cell>CUDA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="12,57.51,284.85,375.72,89.21"><head>Table 2 .</head><label>2</label><figDesc>CIFAR-10 Models, Including Baseline Accuracy (Top1) and Our Chosen Compression Ratios and Corresponding Accuracies</figDesc><table coords="12,57.51,313.37,375.72,60.69"><row><cell>Model</cell><cell cols="2">Params MACs Top1</cell><cell cols="3">Model Optimization Accuracy (&amp; Compression Ratio) Weight Pruning Channel Pruning float16 int8</cell></row><row><cell>MobileNetV2</cell><cell>2.3M</cell><cell>98M 93.2%</cell><cell>92.7% (95%)</cell><cell>89.3% (50%)</cell><cell>93.2% (50%) 91.7% (75%)</cell></row><row><cell>ResNet18</cell><cell cols="2">11.2M 557M 94.3%</cell><cell>94.7% (95%)</cell><cell>89.0% (80%)</cell><cell>94.3% (50%) 93.2% (75%)</cell></row><row><cell>VGG-16</cell><cell cols="2">14.7M 314M 92.9%</cell><cell>93.1% (95%)</cell><cell>85.7% (80%)</cell><cell>92.9% (50%) 92.0% (75%)</cell></row><row><cell>MobileNetV1</cell><cell>3.2M</cell><cell>48M 91.3%</cell><cell>90.9% (95%)</cell><cell>86.9% (50%)</cell><cell>91.3% (50%) 89.6% (75%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,52.45,75.71,383.18,207.96"><head>Table 3 .</head><label>3</label><figDesc>Analysis of CIFAR-10 CPU Results for Varying Combinations of Parameters, Summarizing Figure4</figDesc><table coords="14,52.45,103.89,383.18,179.78"><row><cell cols="2">Platform Model</cell><cell>Dense</cell><cell>Fastest algorithm WP CP i8</cell><cell>f16</cell><cell cols="3">Fastest compression technique GEMM Direct Spatial (Pack)</cell><cell>Overall fastest</cell></row><row><cell></cell><cell cols="4">MobileNetV2 GEMM GEMM GEMM GEMM Direct</cell><cell>WP</cell><cell>WP</cell><cell>WP</cell><cell>WP+GEMM</cell></row><row><cell>HiKey</cell><cell>ResNet18</cell><cell cols="3">GEMM GEMM GEMM Direct Direct</cell><cell>WP</cell><cell>i8</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell>(untuned)</cell><cell>VGG-16</cell><cell cols="3">GEMM GEMM GEMM Direct Direct</cell><cell>WP</cell><cell>i8</cell><cell>Dense</cell><cell>i8+Direct</cell></row><row><cell></cell><cell cols="4">MobileNetV1 GEMM GEMM GEMM Direct Direct</cell><cell>WP</cell><cell>i8</cell><cell>i8</cell><cell>WP+GEMM</cell></row><row><cell></cell><cell cols="4">MobileNetV2 Direct GEMM GEMM GEMM GEMM</cell><cell>WP</cell><cell>i8</cell><cell>WP</cell><cell>WP+GEMM</cell></row><row><cell>i7</cell><cell>ResNet18</cell><cell cols="3">Direct GEMM GEMM GEMM Direct</cell><cell>i8</cell><cell>i8</cell><cell>Dense</cell><cell>i8+GEMM</cell></row><row><cell>(untuned)</cell><cell>VGG-16</cell><cell cols="3">Direct GEMM GEMM GEMM Direct</cell><cell>i8</cell><cell>i8</cell><cell>Dense</cell><cell>i8+GEMM</cell></row><row><cell></cell><cell cols="4">MobileNetV1 Direct GEMM GEMM GEMM GEMM</cell><cell>i8</cell><cell>i8</cell><cell>Dense</cell><cell>i8+GEMM</cell></row><row><cell></cell><cell cols="5">MobileNetV2 Spatial Direct Direct Direct Direct Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell>HiKey</cell><cell>ResNet18</cell><cell cols="4">Spatial Direct Direct Direct Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell>(tuned)</cell><cell>VGG-16</cell><cell cols="4">Direct Direct Direct Direct Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell></cell><cell cols="5">MobileNetV1 Spatial Direct Direct GEMM Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell></cell><cell cols="5">MobileNetV2 Spatial Direct Direct GEMM Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>Dense+Spatial</cell></row><row><cell>i7</cell><cell>ResNet18</cell><cell cols="4">Spatial Direct Direct Direct Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell>(tuned)</cell><cell>VGG-16</cell><cell cols="3">Spatial Direct Direct GEMM Direct</cell><cell>i8</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell></cell><cell cols="5">MobileNetV1 Spatial Direct Direct Direct Direct Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell cols="8">The best inference times are shown in bold. Shortened names are used for brevity: WP (weight pruning), CP (channel</cell></row><row><cell cols="3">pruning), i8 (int8), f16 (float16).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="15,56.67,415.63,377.41,180.08"><head>Table 4 .</head><label>4</label><figDesc>Analysis of CIFAR-10 GPU Results for Varying Combinations of Parameters, Summarizing Figure5</figDesc><table coords="15,56.67,443.86,377.41,151.86"><row><cell cols="2">Platform Model</cell><cell>Dense</cell><cell cols="3">Fastest algorithm WP CP i8</cell><cell>f16</cell><cell cols="3">Fastest compression technique GEMM Direct Spatial (Pack)</cell><cell>Overall fastest</cell></row><row><cell></cell><cell cols="6">MobileNetV2 GEMM Spatial Spatial GEMM Direct</cell><cell>f16</cell><cell>f16</cell><cell>WP</cell><cell>f16+Direct</cell></row><row><cell>HiKey</cell><cell>ResNet18</cell><cell cols="5">Spatial Spatial Spatial Direct GEMM</cell><cell>f16</cell><cell>i8</cell><cell>CP</cell><cell>i8+Direct</cell></row><row><cell>(untuned)</cell><cell>VGG-16</cell><cell cols="5">Spatial Spatial Spatial GEMM GEMM</cell><cell>i8</cell><cell>i8</cell><cell>CP</cell><cell>CP+Spatial</cell></row><row><cell></cell><cell cols="7">MobileNetV1 GEMM Spatial Spatial Direct Direct Dense</cell><cell>i8</cell><cell>WP</cell><cell>i8+Direct</cell></row><row><cell></cell><cell cols="6">MobileNetV2 Direct Direct Direct Direct Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell>Xavier</cell><cell>ResNet18</cell><cell cols="5">Direct Spatial Spatial Direct Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell>(untuned)</cell><cell>VGG-16</cell><cell cols="5">Direct Spatial Direct Direct Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell></cell><cell cols="6">MobileNetV1 Direct Direct Direct Direct Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell></cell><cell cols="2">MobileNetV2 Direct</cell><cell>−</cell><cell>−</cell><cell cols="2">Direct GEMM</cell><cell>f16</cell><cell>Dense</cell><cell>f16</cell><cell>f16+GEMM</cell></row><row><cell>HiKey</cell><cell>ResNet18</cell><cell>Direct</cell><cell>−</cell><cell>−</cell><cell cols="2">Spatial Direct</cell><cell>i8</cell><cell>f16</cell><cell>f16</cell><cell>f16+Direct</cell></row><row><cell>(tuned)</cell><cell>VGG-16</cell><cell>GEMM</cell><cell>−</cell><cell>−</cell><cell cols="3">Direct Spatial Dense</cell><cell>i8</cell><cell>f16</cell><cell>i8+Direct</cell></row><row><cell></cell><cell cols="2">MobileNetV1 Direct</cell><cell>−</cell><cell>−</cell><cell cols="2">Spatial Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Spatial</cell></row><row><cell>Xavier</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(tuned)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="16,57.44,75.71,375.83,88.50"><head>Table 5 .</head><label>5</label><figDesc>ImageNet Models, Including Baseline Accuracy (Top1), and Our Chosen Compression Ratios and Corresponding Accuracies</figDesc><table coords="16,57.44,104.20,375.83,60.00"><row><cell>Model</cell><cell cols="2">Params MACs Top1</cell><cell cols="3">Model Optimization Accuracy (&amp; Compression Ratio) Weight Pruning Channel Pruning float16 int8</cell></row><row><cell>MobileNetV2</cell><cell>3.5M</cell><cell>327M 71.9%</cell><cell>58.4% (80%)</cell><cell>49.9% (50%)</cell><cell>71.9% (50%) 65.3% (75%)</cell></row><row><cell>ResNet50</cell><cell>25.6M</cell><cell>4.1G 76.1%</cell><cell>67.3% (95%)</cell><cell>46.6 (80%)</cell><cell>76.1% (50%) 73.1% (75%)</cell></row><row><cell>DenseNet161</cell><cell>27.7M</cell><cell>7.8G 77.1%</cell><cell>74.3% (95%)</cell><cell>62.7 (80%)</cell><cell>77.1% (50%) 75.2% (75%)</cell></row><row><cell>EfficientNetB0</cell><cell>5.3M</cell><cell>415M 77.7%</cell><cell>65.3% (80%)</cell><cell>54.9 (50%)</cell><cell>77.6% (50%) 0.4% (75%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="17,52.62,75.71,383.18,235.89"><head>Table 6 .</head><label>6</label><figDesc>Analysis of ImageNet Results for Varying Combinations of Parameters, Summarizing Figures6 and 7</figDesc><table coords="17,52.62,103.85,383.18,207.75"><row><cell cols="2">Platform Model</cell><cell>Dense</cell><cell cols="2">Fastest algorithm WP CP i8</cell><cell>f16</cell><cell cols="3">Fastest compression technique GEMM Direct Spatial (Pack)</cell><cell>Overall fastest</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell cols="4">GEMM GEMM GEMM GEMM Direct</cell><cell>WP</cell><cell>i8</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell>HiKey CPU</cell><cell>ResNet50</cell><cell cols="4">GEMM GEMM GEMM GEMM GEMM</cell><cell>WP</cell><cell>i8</cell><cell>i8</cell><cell>WP+GEMM</cell></row><row><cell>(untuned)</cell><cell>DenseNet161</cell><cell cols="2">GEMM GEMM GEMM</cell><cell>−</cell><cell>GEMM</cell><cell>WP</cell><cell>WP</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell></cell><cell cols="5">EfficientNetB0 GEMM GEMM GEMM GEMM Direct</cell><cell>WP</cell><cell>i8</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell cols="4">Direct GEMM GEMM GEMM GEMM</cell><cell>i8</cell><cell>i8</cell><cell>Dense</cell><cell>i8+GEMM</cell></row><row><cell>i7</cell><cell>ResNet50</cell><cell cols="4">GEMM GEMM GEMM GEMM GEMM</cell><cell>WP</cell><cell>i8</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell>(untuned)</cell><cell>DenseNet161</cell><cell cols="2">GEMM GEMM GEMM</cell><cell>−</cell><cell>GEMM</cell><cell>WP</cell><cell>Dense</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell></cell><cell cols="5">EfficientNetB0 Direct GEMM GEMM GEMM GEMM</cell><cell>WP</cell><cell>Dense</cell><cell>Dense</cell><cell>WP+GEMM</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell cols="6">Spatial Direct Direct Direct Spatial Dense Dense</cell><cell>Dense</cell><cell>Dense+Spatial</cell></row><row><cell>i7</cell><cell>ResNet50</cell><cell cols="6">Spatial Direct Direct GEMM Spatial Dense Dense</cell><cell>Dense</cell><cell>Dense+Spatial</cell></row><row><cell>(tuned)</cell><cell>DenseNet161</cell><cell cols="2">Spatial Direct Direct</cell><cell>−</cell><cell cols="2">Spatial Dense</cell><cell>WP</cell><cell>Dense</cell><cell>WP+Direct</cell></row><row><cell></cell><cell cols="7">EfficientNetB0 Spatial Direct Direct GEMM Spatial Dense Dense</cell><cell>Dense</cell><cell>Dense+Spatial</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell cols="4">GEMM Spatial Direct Direct GEMM</cell><cell>f16</cell><cell>i8</cell><cell>i8</cell><cell>f16+GEMM</cell></row><row><cell>HiKey GPU</cell><cell>ResNet50</cell><cell cols="3">Direct Spatial Spatial Direct</cell><cell>Direct</cell><cell>WP</cell><cell>i8</cell><cell>WP</cell><cell>WP+Spatial</cell></row><row><cell>(untuned)</cell><cell>DenseNet161</cell><cell cols="2">Direct Spatial Spatial</cell><cell>−</cell><cell>GEMM</cell><cell>WP</cell><cell>WP</cell><cell>WP</cell><cell>WP+Spatial</cell></row><row><cell></cell><cell cols="5">EfficientNetB0 GEMM Spatial Spatial Direct GEMM</cell><cell>f16</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell cols="4">Direct Spatial Spatial Direct Direct</cell><cell>f16</cell><cell>f16</cell><cell>f16</cell><cell>f16+Direct</cell></row><row><cell>Xavier</cell><cell>ResNet50</cell><cell cols="4">Spatial Spatial Spatial Direct Direct</cell><cell>i8</cell><cell>i8</cell><cell>i8</cell><cell>i8+Direct</cell></row><row><cell>(untuned)</cell><cell>DenseNet161</cell><cell cols="2">Spatial Spatial Spatial</cell><cell>−</cell><cell>Direct</cell><cell>WP</cell><cell>f16</cell><cell>WP</cell><cell>WP+Spatial</cell></row><row><cell></cell><cell cols="5">EfficientNetB0 Direct Spatial Spatial Direct Direct</cell><cell>f16</cell><cell>f16</cell><cell>f16</cell><cell>f16+Direct</cell></row><row><cell cols="10">The best inference times are shown in bold. Shortened names are used for brevity: WP (weight pruning), CP (channel</cell></row><row><cell>pruning), i8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Trans. Arch. Code Optim., Vol. 22, No. 1, Article 1. Publication date: March 2025.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We define compression ratio as: (original size − compressed size)/original size × 100. ACM Trans. Arch. Code Optim., Vol. 22, No. 1, Article 1. Publication date: March 2025.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partially supported by the EU Project dAIEDGE (Grant Agreement Number 101120726) and the Innovate UK Horizon Europe Guarantee (Grant Agreement Number 10090788).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="23,63.02,600.12,378.78,7.22;23,63.02,610.08,378.70,7.22;23,63.02,620.04,378.69,7.22;23,63.02,629.96,378.25,7.26;23,63.02,639.93,286.37,7.26" xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName coords=""><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;16)</title>
				<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;16)<address><addrLine>USENIX Association, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,78.61,377.44,7.22;24,62.85,88.54,377.66,7.26;24,62.85,98.54,377.63,7.22" xml:id="b1">
	<analytic>
		<title level="a" type="main">Collision avoidance for quadrotors with a monocular camera</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><forename type="middle">M</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-23778-7_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-23778-7_14" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Symposium on Experimental Robotics</title>
				<editor>
			<persName><forename type="first">M</forename><forename type="middle">Ani</forename><surname>Hsieh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oussama</forename><surname>Khatib</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</editor>
		<meeting>the 14th International Symposium on Experimental Robotics<address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="195" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,108.50,378.80,7.22;24,62.85,118.42,377.74,7.26;24,62.61,128.43,200.90,7.22" xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal DNN primitive selection with partitioned boolean quadratic programming</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gregg</surname></persName>
		</author>
		<idno type="DOI">10.1145/3168805</idno>
		<ptr target="https://doi.org/10.1145/3168805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optimization (CGO&apos;18)</title>
				<meeting>the International Symposium on Code Generation and Optimization (CGO&apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="340" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,138.35,377.45,7.26;24,62.85,148.31,378.67,7.26;24,62.66,158.31,71.07,7.22" xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine learning systems are stuck in a rut</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3317550.3321441</idno>
		<ptr target="https://doi.org/10.1145/3317550.3321441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS&apos;19)</title>
				<meeting>the Workshop on Hot Topics in Operating Systems (HotOS&apos;19)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="177" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,168.28,377.45,7.22;24,62.85,178.20,378.66,7.26;24,62.85,188.20,216.06,7.22" xml:id="b4">
	<analytic>
		<title level="a" type="main">What iss the state of neural network pruning?</title>
		<author>
			<persName coords=""><forename type="first">Davis</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Javier</forename><forename type="middle">Gonzalez</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2020/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="129" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,198.16,377.44,7.22;24,62.85,208.09,372.08,7.26" xml:id="b5">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,218.09,377.60,7.22;24,62.85,228.05,226.87,7.22" xml:id="b6">
	<monogr>
		<title level="m" type="main">An Analysis of Deep Neural Network Models for Practical Applications</title>
		<author>
			<persName coords=""><forename type="first">Alfredo</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1605.07678</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1605.07678" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,238.02,377.43,7.22;24,62.85,247.98,378.65,7.22;24,62.85,257.90,264.46,7.26" xml:id="b7">
	<analytic>
		<title level="a" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Workshop on Machine Learning Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,267.90,378.31,7.22;24,62.61,277.87,377.70,7.22;24,62.85,287.79,378.25,7.26;24,62.85,297.75,172.20,7.26" xml:id="b8">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;18)</title>
				<meeting>the 13th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;18)<address><addrLine>USENIX Association, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,307.75,378.75,7.22;24,62.85,317.68,377.43,7.26;24,62.57,327.67,269.25,7.22" xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="DOI">10.5555/3327144.3327258</idno>
		<ptr target="https://doi.org/10.5555/3327144.3327258" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3393" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,337.63,378.80,7.22;24,62.85,347.56,299.68,7.26" xml:id="b10">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759[cs</idno>
		<title level="m">cuDNN: Efficient Primitives for Deep Learning</title>
				<imprint>
			<date type="published" when="2014-10">2014. Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,357.52,242.15,7.26" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/" />
		<title level="m">Keras. Retrieved</title>
				<imprint>
			<date type="published" when="2015-04-04">2015. April 4. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,367.52,377.73,7.22;24,62.85,377.44,378.38,7.26;24,62.59,387.45,135.82,7.22" xml:id="b12">
	<analytic>
		<title level="a" type="main">On the efficiency of sparse-tiled tensor graph processing for low memory usage</title>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Cipolletta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Calimera</surname></persName>
		</author>
		<idno type="DOI">10.1109/DAC18074.2021.9586154</idno>
		<ptr target="https://doi.org/10.1109/DAC18074.2021.9586154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th ACM/IEEE Design Automation Conference (DAC&apos;21)</title>
				<meeting>the 58th ACM/IEEE Design Automation Conference (DAC&apos;21)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,397.37,377.44,7.26;24,62.85,407.33,378.67,7.26" xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimizing computation in convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks (ICANN&apos;14)</title>
				<editor>
			<persName><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</editor>
		<meeting>the International Conference on Artificial Neural Networks (ICANN&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,417.33,332.05,7.22" xml:id="b14">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-319-11179-7_36</idno>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="281" to="290" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,427.30,378.68,7.22;24,62.85,437.26,64.29,7.22" xml:id="b15">
	<monogr>
		<title level="m" type="main">NVIDIA TensorRT: Programmable Inference Accelerator</title>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,447.18,377.44,7.26;24,62.85,457.14,377.43,7.26;24,62.85,467.15,171.75,7.22" xml:id="b16">
	<analytic>
		<title level="a" type="main">Moonshine: Distilling with cheap convolutions</title>
		<author>
			<persName coords=""><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gavin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2888" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,477.07,378.66,7.26;24,62.85,487.07,96.72,7.22" xml:id="b17">
	<analytic>
		<title level="a" type="main">The OSI reference model</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1109/PROC.1983.12775</idno>
		<ptr target="https://doi.org/10.1109/PROC.1983.12775" />
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1983">1983. 1983</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1334" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.84,497.03,377.43,7.22;24,62.85,506.96,378.67,7.26;24,62.66,516.96,198.96,7.22" xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to infer: RL-based search for DNN primitive selection on heterogeneous embedded systems</title>
		<author>
			<persName coords=""><forename type="first">Nuria</forename><surname>Miguel De Prado</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Pazos</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Benini</surname></persName>
		</author>
		<idno type="DOI">10.23919/DATE.2019.8714959</idno>
		<ptr target="https://doi.org/10.23919/DATE.2019.8714959" />
	</analytic>
	<monogr>
		<title level="m">2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE&apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1409" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,526.92,377.47,7.22;24,62.85,536.85,378.66,7.26;24,62.85,546.85,103.74,7.22" xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName coords=""><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,556.83,164.53,7.22" xml:id="b20">
	<monogr>
		<title level="m">ONNX Runtime developers</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>ONNX Runtime</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="24,62.85,566.79,377.43,7.22;24,62.85,576.70,377.45,7.26;24,62.85,586.67,378.77,7.26;24,62.85,596.67,372.76,7.22" xml:id="b21">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct coords="24,62.84,606.63,377.42,7.22;24,62.85,616.55,377.45,7.26;24,62.85,626.52,338.96,7.26" xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance-Energy Trade-Offs of deep learning convolution algorithms on ARM processors</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Manuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sergio</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Héctor</forename><surname>Barrachina</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adrián</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Antonio</forename><surname>Castelló</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Germán</forename><surname>Maciá</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrés</forename><forename type="middle">E</forename><surname>Fabregat</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Tomás</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11227-023-05050-4</idno>
		<ptr target="https://doi.org/10.1007/s11227-023-05050-4" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="9819" to="9836" />
			<date type="published" when="2023-06">2023. June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,78.61,378.66,7.22;25,63.02,88.58,25.95,7.22" xml:id="b23">
	<analytic>
		<title level="a" type="main">The PyTorch Lightning team</title>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3828935</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3828935" />
	</analytic>
	<monogr>
		<title level="m">PyTorch Lightning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,98.54,378.69,7.22;25,63.02,108.46,269.87,7.26" xml:id="b24">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;19)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,118.46,377.44,7.22;25,63.02,128.39,121.04,7.26" xml:id="b25">
	<analytic>
		<title level="a" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName coords=""><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,138.35,377.44,7.26;25,63.02,148.35,376.71,7.22" xml:id="b26">
	<analytic>
		<title level="a" type="main">Compiler-centric Across-stack Deep Learning Acceleration</title>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Gibson</surname></persName>
		</author>
		<idno type="DOI">10.5525/gla.thesis.83959</idno>
		<ptr target="https://doi.org/10.5525/gla.thesis.83959" />
	</analytic>
	<monogr>
		<title level="j">Ph. D. Dissertation. College of Science and Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>School of Computing Science, University of Glasgow</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,158.31,378.81,7.22;25,63.02,168.24,377.59,7.26;25,63.02,178.24,315.56,7.22" xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer-tuning: Reusing auto-schedules for efficient tensor program code generation</title>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<idno type="DOI">10.1145/3559009.3569682</idno>
		<ptr target="https://doi.org/10.1145/3559009.3569682" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Parallel Architectures and Compilation Techniques (PACT&apos;22)</title>
				<meeting>the Conference on Parallel Architectures and Compilation Techniques (PACT&apos;22)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,188.20,377.41,7.22;25,63.02,198.13,378.26,7.26;25,63.02,208.09,317.74,7.26" xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing grouped convolutions on edge devices</title>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Storkey</surname></persName>
		</author>
		<idno type="DOI">10.1109/ASAP49362.2020.00039</idno>
		<ptr target="https://doi.org/10.1109/ASAP49362.2020.00039" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Application-specific Systems, Architectures and Processors (ASAP&apos;20)</title>
				<meeting>the IEEE International Conference on Application-specific Systems, Architectures and Processors (ASAP&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,218.09,377.46,7.22;25,62.79,228.01,346.37,7.26" xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,238.02,377.44,7.22;25,63.02,247.98,377.45,7.22;25,63.02,257.90,378.39,7.26;25,62.77,267.90,121.65,7.22" xml:id="b30">
	<analytic>
		<title level="a" type="main">Accelerating sparse DNN Models without Hardware-support via tile-wise sparsity</title>
		<author>
			<persName coords=""><forename type="first">Cong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><forename type="middle">Yang</forename><surname>Hsueh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jingwen</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxian</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zehuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoying</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xipeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41405.2020.00020</idno>
		<ptr target="https://doi.org/10.1109/SC41405.2020.00020" />
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. 1-15</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.02,277.83,377.46,7.26;25,63.02,287.79,287.57,7.26" xml:id="b31">
	<analytic>
		<title level="a" type="main">Two fast algorithms for sparse matrices: Multiplication and permuted transposition</title>
		<author>
			<persName coords=""><forename type="first">Fred</forename><forename type="middle">G</forename><surname>Gustavson</surname></persName>
		</author>
		<idno type="DOI">10.1145/355791.355796</idno>
		<ptr target="https://doi.org/10.1145/355791.355796" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="250" to="269" />
			<date type="published" when="1978-09">1978. Sept. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,297.79,377.46,7.22;25,63.02,307.71,377.44,7.26;25,63.02,317.68,282.20,7.26" xml:id="b32">
	<analytic>
		<title level="a" type="main">Characterizing the deployment of deep neural networks on commercial edge devices</title>
		<author>
			<persName coords=""><forename type="first">Ramyad</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yilun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bahar</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISWC47752.2019.9041955</idno>
		<ptr target="https://doi.org/10.1109/IISWC47752.2019.9041955" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,327.67,377.44,7.22;25,63.02,337.59,378.67,7.26;25,63.02,347.60,85.20,7.22" xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,357.56,378.79,7.22;25,62.72,367.48,377.75,7.26;25,63.02,377.44,369.09,7.26" xml:id="b34">
	<analytic>
		<title level="a" type="main">A new golden age for computer architecture: Domain-specific hardware/software co-design, enhanced security, open instruction sets, and agile chip development</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2018.00011</idno>
		<ptr target="https://doi.org/10.1109/ISCA.2018.00011" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA&apos;18)</title>
				<meeting>the International Symposium on Computer Architecture (ISCA&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,387.41,377.43,7.26;25,62.78,397.41,37.76,7.22" xml:id="b35">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04305</idno>
		<title level="m">Measuring the algorithmic efficiency of neural networks</title>
				<imprint>
			<date type="published" when="2020-05">2020. May 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,407.33,377.97,7.26;25,63.02,417.33,25.95,7.22" xml:id="b36">
	<analytic>
		<title level="a" type="main">The hardware lottery</title>
		<author>
			<persName coords=""><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3467017</idno>
		<ptr target="https://doi.org/10.1145/3467017" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2021-11">2021. Nov. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,427.30,378.78,7.22;25,63.02,437.26,378.68,7.22;25,63.02,447.18,105.63,7.26" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017-04">2017. Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,457.14,378.24,7.26;25,63.02,467.11,228.14,7.26" xml:id="b38">
	<analytic>
		<title level="a" type="main">ICE-Pick: Iterative cost-efficient pruning for DNNs</title>
		<author>
			<persName coords=""><forename type="first">Wenhao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><surname>Cano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Compression: From Information Theory to Applications -Workshop @ ICML</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,477.11,377.42,7.22;25,63.02,487.03,334.75,7.26" xml:id="b39">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,497.03,378.81,7.22;25,63.01,506.96,377.44,7.26;25,63.01,516.92,378.32,7.26;25,63.01,526.92,160.15,7.22" xml:id="b40">
	<analytic>
		<title level="a" type="main">Collage: Seamless integration of deep learning backends with automatic placement</title>
		<author>
			<persName coords=""><forename type="first">Byungsoo</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peiyuan</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3559009.3569651</idno>
		<ptr target="https://doi.org/10.1145/3559009.3569651" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;22)</title>
				<meeting>the International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;22)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="517" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="25,63.01,536.89,377.40,7.22;25,63.01,546.85,378.82,7.22;25,63.01,556.81,378.79,7.22;25,63.01,566.77,378.31,7.22;25,62.74,576.73,377.74,7.22;25,63.01,586.69,377.44,7.22;25,63.01,596.65,378.78,7.22;25,63.01,606.61,378.34,7.22;25,63.01,616.58,377.48,7.22;25,62.78,626.54,378.88,7.22;25,63.02,636.48,377.45,7.26;26,45.55,55.82,16.09,8.97;26,378.84,55.82,61.43,8.97;26,62.85,78.57,378.67,7.26;26,62.85,88.81,145.29,6.44" xml:id="b41">
	<analytic>
		<title level="a" type="main">In-Datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName coords=""><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doug</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aaron</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alek</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Harshit</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steve</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Diemthu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhuyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alan</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gordon</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adriana</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maire</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kieran</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rahul</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ravi</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ray</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathy</forename><surname>Nix ; Amir Salek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emad</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jed</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mercedes</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horia</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erick</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Walter</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079856.3080246</idno>
		<ptr target="https://doi.org/10.1145/3079856.3080246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International 1:26 P. Gibson et al. Symposium on Computer Architecture (ISCA&apos;17)</title>
				<meeting>the 44th Annual International 1:26 P. Gibson et al. Symposium on Computer Architecture (ISCA&apos;17)<address><addrLine>Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross,; Toronto, ON, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,98.54,378.79,7.22;26,62.64,108.50,377.91,7.22;26,62.71,118.46,377.57,7.22;26,62.85,128.43,378.30,7.22;26,62.85,138.39,377.47,7.22;26,62.85,148.31,378.68,7.26;26,62.85,158.55,156.20,6.44" xml:id="b42">
	<analytic>
		<title level="a" type="main">Pushmeet Kohli, and Demis Hassabis. 2021. Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Augustin</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bernardino</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rishub</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stig</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ellen</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michal</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michalina</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tamas</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Berghammer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03819-2</idno>
		<ptr target="https://doi.org/10.1038/s41586-021-03819-2" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021-08">August 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,168.28,378.68,7.22;26,62.85,178.24,130.47,7.22" xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page">18268744</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,188.20,377.97,7.22;26,62.46,198.16,103.44,7.22" xml:id="b44">
	<monogr>
		<title level="m" type="main">PyTorch Lightning CIFAR10</title>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Kuangliu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gibson</surname></persName>
		</author>
		<ptr target="https://github.com/Wheest/pytorch-lightning-cifar" />
		<imprint>
			<date type="published" when="2023-03-15">2023. March 15. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,208.13,377.42,7.22;26,62.85,218.05,378.24,7.26;26,62.85,228.01,378.79,7.26;26,62.85,238.02,228.48,7.22" xml:id="b45">
	<analytic>
		<title level="a" type="main">MAERI: Enabling flexible dataflow mapping over DNN accelerators via reconfigurable interconnects</title>
		<author>
			<persName coords=""><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173162.3173176</idno>
		<ptr target="https://doi.org/10.1145/3173162.3173176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)</title>
				<meeting>the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;18)<address><addrLine>Williamsburg, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="461" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,247.94,378.30,7.26;26,62.85,257.90,258.52,7.26" xml:id="b46">
	<analytic>
		<title level="a" type="main">Evaluation criteria for sparse matrix storage formats</title>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Langr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavel</forename><surname>Tvrdík</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2015.2401575</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2015.2401575" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="2016-02">2016. Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,267.86,378.33,7.26;26,62.85,277.87,180.68,7.22" xml:id="b47">
	<monogr>
		<title level="m" type="main">LLVM: An Infrastructure for Multi-stage Optimization</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ph. D. Dissertation. Computer Science Dept., University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,287.79,377.45,7.26;26,62.85,297.75,378.67,7.26;26,62.66,307.75,11.12,7.22" xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.435</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.435" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,317.68,379.45,7.26;26,62.85,327.67,115.40,7.22" xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName coords=""><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
		<ptr target="https://doi.org/10.1038/nature14539" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">2015. May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,337.63,377.45,7.22;26,62.85,347.56,377.44,7.26;26,62.85,357.52,275.21,7.26" xml:id="b50">
	<analytic>
		<title level="a" type="main">The deep learning compiler: A comprehensive survey</title>
		<author>
			<persName coords=""><forename type="first">Mingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qingxiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xin</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongzhi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guangwen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Depei</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2020.3030548</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2020.3030548" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="708" to="727" />
			<date type="published" when="2021-03">2021. March 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,367.48,378.67,7.26;26,62.85,377.48,279.22,7.22" xml:id="b51">
	<monogr>
		<title level="m" type="main">Higher Accuracy on Vision Models with EfficientNet-Lite</title>
		<author>
			<persName coords=""><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html" />
		<imprint>
			<date type="published" when="2020-03-14">2020. March 14. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,387.41,377.45,7.26;26,62.85,397.37,202.54,7.26" xml:id="b52">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;17)</title>
				<meeting>the International Conference on Learning Representations (ICLR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,407.37,378.80,7.22;26,62.85,417.29,378.23,7.26;26,62.85,427.26,226.06,7.26" xml:id="b53">
	<analytic>
		<title level="a" type="main">Accelerating deep neural networks on low power heterogeneous architectures</title>
		<author>
			<persName coords=""><forename type="first">Manolis</forename><surname>Loukadakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
		<ptr target="http://eprints.gla.ac.uk/183819/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Programmability and Architectures for Heterogeneous Multicores (MULTIPROG&apos;18</title>
				<meeting>the Workshop on Programmability and Architectures for Heterogeneous Multicores (MULTIPROG&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.84,437.26,377.41,7.22;26,62.85,447.18,280.84,7.26" xml:id="b54">
	<analytic>
		<title level="a" type="main">Assessing robustness of image recognition models to changes in the computational environment</title>
		<author>
			<persName coords=""><forename type="first">Nikolaos</forename><surname>Louloudakis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jose</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajitha</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS ML Safety Workshop</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,457.18,377.47,7.22;26,62.66,467.15,155.87,7.22" xml:id="b55">
	<monogr>
		<title level="m" type="main">TorchVision: PyTorch&apos;s Computer Vision Library</title>
		<ptr target="https://github.com/pytorch/vision" />
		<imprint>
			<date type="published" when="2016-03-15">2016. March 15</date>
		</imprint>
	</monogr>
	<note>TorchVision maintainers and contributors</note>
</biblStruct>

<biblStruct coords="26,62.85,477.11,377.44,7.22;26,62.85,487.07,377.44,7.22;26,62.85,497.03,174.76,7.22" xml:id="b56">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Josh</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04188</idno>
		<title level="m">A hardware-software blueprint for flexible deep learning specialization</title>
				<imprint>
			<date type="published" when="2019-04">2019. April 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="26,62.85,507.00,377.43,7.22;26,62.85,516.92,378.33,7.26;26,62.66,526.92,170.21,7.22" xml:id="b57">
	<monogr>
		<title level="m" type="main">Scalable parallel programming with CUDA: Is CUDA the parallel programming model that application developers have been waiting for? Queue</title>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
		<idno type="DOI">10.1145/1365490.1365500</idno>
		<ptr target="https://doi.org/10.1145/1365490.1365500" />
		<imprint>
			<date type="published" when="2008-03">2008. Mar. 2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="40" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,536.85,318.80,7.26" xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Intel</surname></persName>
		</author>
		<ptr target="https://github.com/oneapi-src/oneDNN" />
		<imprint>
			<date type="published" when="2020-06-19">2020. June 19. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,546.85,377.44,7.22;26,62.85,556.81,378.33,7.22;26,62.57,566.77,377.72,7.22;26,62.85,576.69,377.44,7.26;26,62.85,586.65,284.00,7.26" xml:id="b59">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, High-Performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
				<meeting>the 33rd International Conference on Neural Information Processing Systems<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,596.65,377.45,7.22;26,62.61,606.61,378.20,7.22;26,62.85,616.60,54.39,7.22" xml:id="b60">
	<monogr>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2104.10350</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2104.10350" />
		<title level="m">Carbon Emissions and Large Neural Network Training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="26,62.85,626.52,377.45,7.26;26,62.85,636.48,262.21,7.26" xml:id="b61">
	<analytic>
		<title level="a" type="main">Taxonomy of saliency metrics for channel pruning</title>
		<author>
			<persName coords=""><forename type="first">Kaveena</forename><surname>Persand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gregg</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2021.3108545</idno>
		<ptr target="https://doi.org/10.1109/ACCESS.2021.3108545" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021-08">2021. August 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,78.61,378.31,7.22;27,63.02,88.58,378.66,7.22;27,63.02,98.50,377.98,7.26;27,63.02,108.50,83.88,7.22" xml:id="b62">
	<analytic>
		<title level="a" type="main">Performance aware convolutional neural network channel pruning for embedded GPUs</title>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kuba</forename><surname>Kaszyk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISWC47752.2019.9042000</idno>
		<ptr target="https://doi.org/10.1109/IISWC47752.2019.9042000" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Workload Characterization (IISWC)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,118.46,378.78,7.22;27,63.02,128.43,378.68,7.22;27,63.02,138.35,317.41,7.26" xml:id="b63">
	<analytic>
		<title level="a" type="main">Halide: Decoupling algorithms from schedules for high-performance image processing</title>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
		<idno type="DOI">10.1145/3150211</idno>
		<ptr target="https://doi.org/10.1145/3150211" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2017-12">2017. December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,148.35,377.43,7.22;27,63.02,158.27,377.44,7.26;27,63.02,168.24,83.75,7.26" xml:id="b64">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName coords=""><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,178.24,378.80,7.22;27,63.02,188.16,377.43,7.26;27,63.02,198.13,248.60,7.26" xml:id="b65">
	<analytic>
		<title level="a" type="main">Optimising convolutional neural networks inference on lowpowered GPUs</title>
		<author>
			<persName coords=""><forename type="first">Simon</forename><surname>Rovder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
		<ptr target="https://eprints.gla.ac.uk/183820/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Programmability and Architectures for Heterogeneous Multicores (MULTIPROG&apos;19)</title>
				<meeting>the International Workshop on Programmability and Architectures for Heterogeneous Multicores (MULTIPROG&apos;19)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,208.13,378.78,7.22;27,63.02,218.05,377.45,7.26;27,62.74,228.05,251.59,7.22" xml:id="b66">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00474</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00474" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,238.02,377.65,7.22;27,63.02,247.94,377.45,7.26;27,63.02,257.90,357.95,7.26" xml:id="b67">
	<analytic>
		<title level="a" type="main">Tensor program optimization with probabilistic programs</title>
		<author>
			<persName coords=""><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bohan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wuwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masahiro</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cody</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nyCr6-0hinG" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2022-03-16">2022. March 16. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,267.86,377.44,7.26" xml:id="b68">
	<analytic>
		<title level="a" type="main">Rigid-motion Scattering for Image Classification</title>
		<author>
			<persName coords=""><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph. D. Dissertation. Ecole Polytechnique</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CMAP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,277.87,378.67,7.22;27,63.02,287.79,90.86,7.26" xml:id="b69">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014-09">2014. Sept. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,297.79,377.47,7.22;27,63.02,307.75,204.13,7.22" xml:id="b70">
	<monogr>
		<title level="m" type="main">Super-convergence: Very Fast Training of Neural Networks Using Large Learning Rates</title>
		<author>
			<persName coords=""><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholay</forename><surname>Topin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1708.07120</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1708.07120" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,317.72,377.43,7.22;27,63.02,327.63,343.11,7.26" xml:id="b71">
	<analytic>
		<title level="a" type="main">OpenCL: A parallel programming standard for heterogeneous computing systems</title>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">E</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Gohara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guochun</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2010.69</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2010.69" />
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2010-05">2010. May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,337.59,378.68,7.26;27,63.02,347.60,290.42,7.22" xml:id="b72">
	<monogr>
		<title level="m" type="main">Efficient Processing of Deep Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-01766-7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-01766-7" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,357.52,378.25,7.26;27,63.02,367.48,292.15,7.26" xml:id="b73">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML&apos;19)</title>
				<meeting>the International Conference on Machine Learning (ICML&apos;19)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.01,377.48,377.44,7.22;27,62.72,387.41,377.74,7.26;27,63.02,397.37,288.95,7.26" xml:id="b74">
	<analytic>
		<title level="a" type="main">Optimizing Google&apos;s warehouse scale computers: The NUMA experience</title>
		<author>
			<persName coords=""><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eric</forename><surname>Tune</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2013.6522318</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2013.6522318" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,407.37,377.43,7.22;27,63.02,417.29,378.67,7.26;27,62.84,427.30,187.28,7.22" xml:id="b75">
	<analytic>
		<title level="a" type="main">MultiNet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName coords=""><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marius</forename><surname>Zöllner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1109/IVS.2018.8500504</idno>
		<ptr target="https://doi.org/10.1109/IVS.2018.8500504" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Vehicles Symposium (IV&apos;18)</title>
				<meeting>the IEEE Intelligent Vehicles Symposium (IV&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,437.26,254.68,7.22" xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName coords=""><forename type="first">Iree</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Authors</surname></persName>
		</author>
		<ptr target="https://github.com/openxla/iree" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,447.22,377.42,7.22;27,63.02,457.19,184.63,7.22" xml:id="b77">
	<monogr>
		<title level="m" type="main">The Computational Limits of Deep Learning</title>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristjan</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keeheon</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><forename type="middle">F</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Manso</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2007.05558</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2007.05558" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,467.15,378.35,7.22;27,63.02,477.07,377.45,7.26;27,63.02,487.03,280.12,7.26" xml:id="b78">
	<analytic>
		<title level="a" type="main">Autotuning convolutions is easier than you think</title>
		<author>
			<persName coords=""><forename type="first">Nicolas</forename><surname>Tollenaere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillaume</forename><surname>Iooss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stéphane</forename><surname>Pouget</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugo</forename><surname>Brunie</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Guillon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabrice</forename><surname>Rastello</surname></persName>
		</author>
		<idno type="DOI">10.1145/3570641</idno>
		<ptr target="https://doi.org/10.1145/3570641" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2023-03">2023. March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,497.04,377.46,7.22;27,63.02,506.96,378.23,7.26;27,63.02,516.92,248.85,7.26" xml:id="b79">
	<analytic>
		<title level="a" type="main">Characterising across-stack optimisations for deep convolutional neural networks</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">V</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="DOI">10.1109/IISWC.2018.8573503</idno>
		<ptr target="https://doi.org/10.1109/IISWC.2018.8573503" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Workload Characterization (IISWC&apos;18)</title>
				<meeting>the IEEE International Symposium on Workload Characterization (IISWC&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,526.92,377.44,7.22;27,63.02,536.85,228.04,7.26" xml:id="b80">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">José</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10460</idno>
		<title level="m">Distilling with performance enhanced students</title>
				<imprint>
			<date type="published" when="2018-10">2018. Oct. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct coords="27,63.01,546.85,377.45,7.22;27,63.02,556.79,378.32,7.26;27,63.02,566.79,231.37,7.22" xml:id="b81">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.5555/3295222.3295349</idno>
		<ptr target="https://doi.org/10.5555/3295222.3295349" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,576.74,377.66,7.22;27,63.02,586.67,377.45,7.26;27,63.02,596.63,90.84,7.26" xml:id="b82">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
				<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="27,63.02,606.63,378.78,7.22;27,63.02,616.55,377.47,7.26;27,63.02,626.52,378.70,7.26" xml:id="b83">
	<analytic>
		<title level="a" type="main">Highthroughput CNN inference on embedded ARM Big. LITTLE Multicore Processors</title>
		<author>
			<persName coords=""><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gayathri</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neeraj</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2019.2944584</idno>
		<ptr target="https://doi.org/10.1109/TCAD.2019" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2254" to="2267" />
			<date type="published" when="2020-10">2020. October 2020</date>
		</imprint>
	</monogr>
	<note>Anuj Pathania, and Tulika Mitra</note>
</biblStruct>

<biblStruct coords="28,62.85,78.61,377.43,7.22;28,62.85,88.54,378.67,7.26;28,62.85,98.78,161.48,6.44" xml:id="b84">
	<analytic>
		<title level="a" type="main">Convergence of edge computing and deep learning: A comprehensive survey</title>
		<author>
			<persName coords=""><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiwen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><forename type="middle">M</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dusit</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xueqiang</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/COMST.2020.2970550</idno>
		<ptr target="https://doi.org/10.1109/COMST.2020.2970550" />
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys Tutorials</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.85,108.50,377.43,7.22;28,62.85,118.42,378.23,7.26;28,62.85,128.39,378.29,7.26;28,62.85,138.39,70.38,7.22" xml:id="b85">
	<analytic>
		<title level="a" type="main">TASO: Time and space optimization for memory-constrained DNN inference</title>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valentin</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><forename type="middle">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gregg</surname></persName>
		</author>
		<idno type="DOI">10.1109/SBAC-PAD49847.2020.00036</idno>
		<ptr target="https://doi.org/10.1109/SBAC-PAD49847.2020.00036" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD&apos;20)</title>
				<meeting>the International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD&apos;20)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.84,148.35,377.46,7.22;28,62.85,158.31,378.33,7.22;28,62.57,168.28,377.71,7.22;28,62.85,178.20,377.45,7.26;28,62.85,188.16,378.39,7.26;28,62.58,198.16,114.90,7.22" xml:id="b86">
	<analytic>
		<title level="a" type="main">Machine learning at facebook: Understanding inference at the edge</title>
		<author>
			<persName coords=""><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Douglas</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sy</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marat</forename><surname>Dukhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eldad</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bill</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tommer</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lin</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joe</forename><surname>Spisak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bram</forename><surname>Wasti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ran</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/HPCA.2019.00048</idno>
		<ptr target="https://doi.org/10.1109/HPCA.2019.00048" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.84,208.13,377.45,7.22;28,62.85,218.05,377.44,7.26;28,62.85,228.01,378.39,7.26;28,62.58,238.02,140.67,7.22" xml:id="b87">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entity-aware Self-attention</title>
		<author>
			<persName coords=""><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.523" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.84,247.98,377.61,7.22;28,62.85,257.90,377.45,7.26;28,62.85,267.86,378.38,7.26;28,62.58,277.87,110.88,7.22" xml:id="b88">
	<analytic>
		<title level="a" type="main">SparseTIR: Composable abstractions for sparse compilation in deep learning</title>
		<author>
			<persName coords=""><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<idno type="DOI">10.1145/3582016.3582047</idno>
		<ptr target="https://doi.org/10.1145/3582016.3582047" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;23)</title>
				<meeting>the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;23)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="660" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.84,287.83,377.43,7.22;28,62.84,297.75,377.43,7.26;28,62.84,307.71,377.95,7.26;28,62.85,317.72,216.33,7.22" xml:id="b89">
	<analytic>
		<title level="a" type="main">DietCode: Automatic optimization for dynamic tensor programs</title>
		<author>
			<persName coords=""><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joshua</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<ptr target="https://proceedings.mlsys.org/paper/2022/hash/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022-04">2022. April 2022. July 21</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="848" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="28,62.85,327.67,377.46,7.22;28,62.85,337.63,377.42,7.22;28,62.85,347.56,378.68,7.26;28,62.85,357.56,317.45,7.22;28,45.77,376.24,263.80,8.12" xml:id="b90">
	<analytic>
		<title level="a" type="main">Ansor: Generating High-Performance tensor programs for deep learning</title>
		<author>
			<persName coords=""><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi20/presentation/zheng" />
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<date type="published" when="2020-11-14">2020. December 26. 2020. 14 November 2023. 2024. 19 July 2024</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
	<note>revised 20 June</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
