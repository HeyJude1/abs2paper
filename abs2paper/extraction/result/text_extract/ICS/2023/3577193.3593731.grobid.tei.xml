<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Additive Modifications in LU Factorization Instead of Pivoting</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2023-06-21">2023-06-21</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Neil</forename><surname>Lindquist</surname></persName>
							<idno type="ORCID">0000-0001-9404-3121</idno>
						</author>
						<author>
							<persName><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
							<idno type="ORCID">0000-0002-0089-6965</idno>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
							<idno type="ORCID">0000-0003-3247-1782</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Computing Laboratory University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Computing</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Additive Modifications in LU Factorization Instead of Pivoting</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 37th International Conference on Supercomputing</title>
						<meeting>the 37th International Conference on Supercomputing						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="page" from="14" to="24"/>
							<date type="published" when="2023-06-21" />
						</imprint>
					</monogr>
					<idno type="MD5">FB7B1DAA881D016075E0490A5D59C390</idno>
					<idno type="DOI">10.1145/3577193.3593731</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-07-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LU factorization</term>
					<term>linear algebra</term>
					<term>communication avoidance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Direct solvers for dense systems of linear equations commonly use partial pivoting to ensure numerical stability. However, pivoting can introduce significant performance overheads, such as synchronization and data movement, particularly on distributed systems. To improve the performance of these solvers, we present an alternative to pivoting in which numerical stability is obtained through additive updates. We implemented this approach using SLATE, a GPU-accelerated numerical linear algebra library, and evaluated it on the Summit supercomputer. Our approach provides better performance (up to 5-fold speedup) than Gaussian elimination with partial pivoting for comparable accuracy on most of the tested matrices. It also provides better accuracy (up to 15 more digits) than Gaussian elimination with no pivoting for comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Mathematics of computing â†’ Mathematical software performance; Computations on matrices; â€¢ Computing methodologies â†’ Distributed algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Solving large, dense, non-symmetric systems of linear equations is a key step in many applications <ref type="bibr" coords="1,160.01,557.98,9.23,12.39" target="#b4">[5,</ref><ref type="bibr" coords="1,170.82,557.98,10.05,12.39" target="#b19">20]</ref>. Gaussian elimination with partial pivoting (GEPP) is commonly used to solve these systems and provides robust numerical accuracy for almost all classes of matrices. However, partial pivoting can introduce significant overheads, including synchronizations, latency-bound pivot searches, and exchanging rows in memory. The row exchanges also must be interleaved with the Schur-complement updates, reducing the available parallelism. Finally, these overheads are made worse by the growing performance gap between arithmetic and data movement <ref type="bibr" coords="1,542.92,240.92,13.33,12.39" target="#b17">[18]</ref>. Gaussian elimination with no pivoting (GENP) can achieve significantly higher performance by avoiding these overheads but cannot accurately solve many types of systems <ref type="bibr" coords="1,469.72,273.80,13.50,12.39" target="#b27">[28]</ref>. We consider an alternative to pivoting based on low rank, additive modifications of the diagonal submatrices: we call it block elimination with additive modifications (BEAM). The goal of this approach is to incur lower overheads than GEPP while providing better numerical stability than GENP. Related modifications have been previously considered for sparse matrices <ref type="bibr" coords="1,390.16,339.55,13.56,12.39" target="#b20">[21,</ref><ref type="bibr" coords="1,405.97,339.55,10.17,12.39" target="#b25">26]</ref>. But, we consider dense matrices, and our proposed algorithm modifies entire blocks of the matrix instead of targeting just individual elements.</p><p>To motivate the additive strategy, consider GENP. Thus, small ğ´[ğ‘˜, ğ‘˜] entries can result in significant element growth, which in turn can lead to a large backward error <ref type="bibr" coords="1,504.85,426.87,13.49,12.39" target="#b16">[17]</ref>. Diagonal blocks with small singular values behave analogously. To prevent this growth, we propose monitoring the singular values of the diagonal blocks and modifying those having values below a predefined tolerance. These modifications give rise to a perturbed system with better numerical properties than the original one. The perturbation can then be corrected collectively with the Woodbury formula<ref type="foot" coords="1,346.67,502.23,3.38,10.05" target="#foot_0">1</ref>  <ref type="bibr" coords="1,352.80,503.58,13.46,12.39" target="#b15">[16,</ref><ref type="bibr" coords="1,368.52,503.58,11.51,12.39" target="#b28">29]</ref> or iterative refinement. Consequently, our work makes the following contributions:</p><p>â€¢ We propose a general scheme for additive modifications in the LU factorization of dense systems.</p><p>â€¢ We propose choosing additive modifications in batches based on diagonal blocks of the matrix instead of individual entries.</p><p>â€¢ We prove practical bounds on key condition numbers that determine the overall numerical stability of our method.</p><p>â€¢ We test the accuracy and performance for essential matrix types at scale on the Summit supercomputer with multiple GPU accelerators per node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Using element-wise modifications instead of pivoting was first suggested by Stewart for sparse matrices <ref type="bibr" coords="1,452.18,662.24,13.23,12.39" target="#b25">[26]</ref>. However, preserving the sparsity limited the modifications to only a single diagonal element at a time. Additionally, the adjustments were combined into the factorization in a recursive manner that limits the available parallelism. Unfortunately, this approach has seen limited use outside the optimization community <ref type="bibr" coords="2,146.26,127.40,13.32,12.39" target="#b30">[31]</ref>. There are a few areas where BEAM differs from past efforts. First, our new algorithm enhances dense LU factorization which is significantly more compute-intensive than the sparse equivalent. Furthermore, dense problems are unconstrained by sparsity issues. Next, the modifications are chosen based on entire diagonal blocks instead of individual diagonal elements, giving us a greater opportunity for exploiting non-local numerical properties. Finally, all modifications are corrected simultaneously at the end of the factorization. This increases the arithmetic intensity when applying the Woodbury formula, which improves the performance on modern hardware. An interesting variant of the additive approach is to correct the perturbations by adding extra rows and columns to the matrix <ref type="bibr" coords="2,282.93,258.90,9.37,12.39" target="#b2">[3]</ref>. That is, the system ğ´ğ‘¥ = ğ‘ is replaced with</p><formula xml:id="formula_0">ğ´ + ğ¹ 1 ğ¹ 2 ğ¹ 1 ğ¹ 2 ğ¼ ğ‘¥ ğ‘¦ = ğ‘ 0</formula><p>where ğ¹ 1 ğ¹ 2 modifies the appropriate diagonal elements. This variant is closely related to the derivation of the Woodbury formula via the Schur complement <ref type="bibr" coords="2,155.09,335.62,13.50,12.39" target="#b15">[16]</ref>. Unfortunately, this idea has not been explored beyond avoiding fill-in for sparse, symmetric positive definite matrices with a few dense rows. We used the Woodbury formula to apply the correction instead of this approach to simplify matrix allocation. A final approach is to apply modifications without directly correcting them, relying instead on iterative refinement <ref type="bibr" coords="2,252.23,401.37,13.50,12.39" target="#b20">[21]</ref>. While iterative refinement based on stationary schemes can recover minor errors, large errors can slow or even prevent convergence, especially for ill-conditioned matrices. Thus, there is a trade-off in the factorization's backward error between the direct perturbations to the matrix and the error induced by element growth. Using the Woodbury formula directly addresses the perturbations, which helps alleviate this conflict. Therefore, we tested the additive approach both with and without the Woodbury formula. As before, the prior work is related to ours, but our approach chooses modifications based on diagonal blocks and is applied to dense matrices unencumbered by sparsity considerations.</p><p>Beside additive modifications, there have been various other approaches to remove or reduce the cost of pivoting. One such strategy is to replace pivoting with randomized preprocessing, which allows using an optimized GENP code for the factorization at the cost of preprocessing the matrix <ref type="bibr" coords="2,148.18,576.71,13.56,12.39" target="#b23">[24,</ref><ref type="bibr" coords="2,163.98,576.71,10.16,12.39" target="#b24">25]</ref>. Recent work has demonstrated significant speedups for many types of matrices; unfortunately, these approaches can fail in some cases <ref type="bibr" coords="2,197.84,598.63,13.25,12.39" target="#b22">[23]</ref>. Thus, it is valuable to investigate algorithms that may be successful for linear systems where randomization is insufficient. Furthermore, using BEAM in combination with randomized preprocessing is likely to be more robust than either of the methods separately.</p><p>Another pivoting-replacement strategy is a hybrid of the LU and QR factorizations <ref type="bibr" coords="2,136.57,664.38,9.49,12.39" target="#b8">[9]</ref>. This algorithm attempts to factor each block column with GENP. It then tests the stability and, if unstable, re-factors the block column with the unequivocally stable QR factorization. Thus, in the best case, the factorization progresses as per GENP but provides more robust behavior when GENP struggles. Unfortunately, for task-parallel implementations testing each block column for stability and the occasional QR factorization results in similar parallel dependencies to GEPP, which reduces the available parallelism. This hybrid method and our algorithm are similar in their optimism about GENP, but they differ in the mechanism by which they recover stability in problematic cases.</p><p>Finally, there have also been efforts to reduce the cost of pivoting without completely removing it. The most well-known approach is tournament pivoting, which computes pivots block-wise to avoid synchronizing for each column <ref type="bibr" coords="2,473.80,193.15,13.49,12.39" target="#b13">[14]</ref>. Another strategy is to relabel the rows without exchanging them in memory, although swapping rows may still be needed for load balancing <ref type="bibr" coords="2,542.98,215.07,13.28,12.39" target="#b10">[11]</ref>. The recent COğ‘›ğ‘“ LUX algorithm goes further by combining these strategies <ref type="bibr" coords="2,354.32,236.99,13.23,12.39" target="#b18">[19]</ref>. Finally, a recent proposal, threshold pivoting, tries to reduce data movement by allowing the selection of pivots smaller than the column's maximum <ref type="bibr" coords="2,423.42,258.90,13.22,12.39" target="#b21">[22]</ref>. Unfortunately, these approaches still incur significant pivoting overheads and reduce the available parallelism since the exchanged rows are unknown until runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADDITIVE MODIFICATIONS ALGORITHM</head><p>The core idea of our approach is to apply additive modifications during the factorization when small entries occur on the diagonal instead of exchanging rows. A straightforward way to do this is to perform the classic non-pivoted LU factorization and modify diagonal entries whenever they dip below a preset tolerance. However, this results in a myopic view of the matrix; the issues with one diagonal element can often be fixed using just the next row, for example in a matrix where the leading 2-by-2 diagonal submatrix is 0 1 1 0 . Thus, we use a block LU factorization where the diagonal blocks are factored with the singular value decomposition (SVD). Then, we modify the singular values that are too small. However, the SVD requires significantly more computation than an LU decomposition: 21ğ‘› 3 operations instead of 2/3ğ‘› 3 -a 30Ã— difference <ref type="bibr" coords="2,501.33,449.05,13.23,12.39" target="#b11">[12]</ref>. This limits the block size, ğ‘› ğ‘ , that can be used without introducing significant overhead. Other rank-revealing factorizations, such as QR with column pivoting, are cheaper; however, the SVD is a more robust factorization, which helps us focus on the effects of the overall block-wise factorization and the additive modifications. Note that by using the SVD in this way, the final decomposition is not a regular LU factorization (unless ğ‘› ğ‘ = 1) but a decomposition into lower and upper block-triangular matrices.</p><p>Additive modifications commute naturally with the preceding Schur complement updates of the block LU factorization via the commutativity of matrix addition. Hence, the modified LU factors are equivalent to the factors produced by applying all modifications before beginning the factorization (if we ignore the effects of numerical round-off). This is analogous to representing row pivoting as pre-multiplication by a permutation matrix ğ‘ƒ: ğ´ â‰¡ ğ‘ƒğ´. Thus, our proposed method factorizes ğ´ into</p><formula xml:id="formula_1">ğ¿ ğ‘… = ğ´ â‰¡ ğ´ + ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ (1)</formula><p>where ğ¿ and ğ‘… are lower and upper block-triangular matrices, respectively, while ğ‘€ Î£ is a diagonal matrix containing the modifications. Note that we denote the upper block-triangular factor as ğ‘… ("right") instead of the usual ğ‘ˆ ("upper") to avoid confusion with the ğ‘ˆ factor of the SVD. The columns of ğ‘€ ğ‘ˆ and ğ‘€ ğ‘‰ are the left and right singular vectors corresponding to the modifications in ğ‘€ Î£ and padded with zeros to match the size of ğ´. Thus, ğ‘€</p><formula xml:id="formula_2">MV M T L MÎ£ MU A + â‰¡ Ã— R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘ˆ and ğ‘€</head><p>ğ‘‰ are tall-and-narrow matrices whose columns are a subset of a block-diagonal matrix. Figure <ref type="figure" coords="3,185.25,209.21,4.25,12.39">1</ref> visualizes these sub-matrix structures.</p><p>Because of the perturbations, the factored matrix ğ´ âˆ’1 often only provides the solution to a nearby system, so a correction is needed to obtain the solution to the original system. We considered two approaches for this correction: iterative refinement and the Woodbury formula. While the former has a well established formulation, the latter can take various forms. The most general form of the Woodbury formula is</p><formula xml:id="formula_3">(ğ´ âˆ’ ğµğ¶ğ·) âˆ’1 = ğ´ âˆ’1 + ğ´ âˆ’1 ğµ(ğ¶ âˆ’1 âˆ’ ğ·ğ´ âˆ’1 ğµ) âˆ’1 ğ·ğ´ âˆ’1 ,<label>(2)</label></formula><p>although a simplified form is often used where ğ¶ â‰¡ ğ¼ <ref type="bibr" coords="3,259.88,328.24,13.49,12.39" target="#b15">[16]</ref>. The term ğ¶ âˆ’1 âˆ’ ğ·ğ´ âˆ’1 ğµ is called a capacitance matrix, and its inverse is the centerpiece of the Woodbury formula. Here, we formulate the correction as</p><formula xml:id="formula_4">(ğ´âˆ’ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ) âˆ’1 = ğ´ âˆ’1 +ğ´ âˆ’1 ğ‘€ ğ‘ˆ (ğ¼ âˆ’ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 ğ‘€ ğ‘ˆ ) âˆ’1 ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 instead of the more obvious (ğ´ âˆ’ ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ) âˆ’1 = ğ´ âˆ’1 +ğ´ âˆ’1 ğ‘€ ğ‘ˆ (ğ‘€ âˆ’1 Î£ âˆ’ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 ğ‘€ ğ‘ˆ ) âˆ’1 ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1</formula><p>to avoid the need to invert the possibly ill-conditioned ğ‘€ Î£ and to improve the conditioning of the entire capacitance matrix. We discuss these numerical properties further in Section 4.</p><p>We outline our approach, abbreviated BEAM, in Algorithm 1. While we describe the algorithm with a fixed block size, ğ‘› ğ‘ , it can easily be extended to a variable block size. In lines 6-17, we decompose the diagonal block and apply any necessary modifications. Then, lines 18-21 proceed as per a regular blocked, non-pivoted LU factorization. Finally, we compute and factor the capacitance matrix if the Woodbury formula is needed. While computing the capacitance matrix, we form and save the C ğ‘… and C ğ¿ matrices; this reduces memory accesses at the cost of a slight increase in storage unless there are numerous modifications. In spite of the factored capacitance matrix being denoted C âˆ’1 , the inverse should not be formed explicitly; instead, the factored form is preferable for numerical accuracy. We factor this second matrix with GEPP, but other methods could also work. The solve step simply applies the block-triangular factors and possibly the Woodbury formula.</p><p>A key advantage of Algorithm 1 comes from the fact that it has the high-level structure of a non-pivoted, block LU. Such structure provides more parallelism than partial pivoting because the panel of ğ¿ and panel of ğ‘… can be updated simultaneously <ref type="bibr" coords="3,222.09,653.43,9.24,12.39" target="#b7">[8,</ref><ref type="bibr" coords="3,232.86,653.43,10.05,12.39" target="#b22">23]</ref>. Furthermore, it allows the trailing matrix update from one iteration to overlap with the panel updates from a subsequent iteration.</p><p>To outperform GEPP, the overhead introduced by this method must be lower than that of pivoting. To that end, we count the Algorithm 1 BEAM algorithm's factor and solve steps. Subscripts for ğ´, ğ¿, ğ‘… denote submatrices in terms of matrix blocks, and ğ‘› ğ‘ denotes the block size. </p><formula xml:id="formula_5">ğ´ (0) â† ğ´ 5:</formula><p>for ğ‘˜ = 1 : ğ‘› ğ‘¡ do 6:</p><p>ğ‘ˆ ğ‘˜ , Î£ ğ‘˜ , ğ‘‰ ğ‘‡ ğ‘˜ â† SVD(ğ´</p><formula xml:id="formula_6">(ğ‘˜ âˆ’1)</formula><p>ğ‘˜,ğ‘˜ )</p><p>7:</p><p>for ğ‘– = 1 :</p><formula xml:id="formula_7">ğ‘› ğ‘ do 8: if Î£ ğ‘˜ [ğ‘–] â‰¤ ğœ then âŠ² is ğœ ğ‘– below tolerance ğœ 9: ğ‘š â† ğ‘š + 1 âŠ² Record modification 10: ğ‘€ Î£ [ğ‘š, ğ‘š] â† ğœ âˆ’ Î£ ğ‘˜ [ğ‘–] 11: ğ‘€ ğ‘ˆ [:, ğ‘š] â† [0, ğ‘ˆ ğ‘˜ [:, ğ‘–] ğ‘‡ , 0] ğ‘‡ 12: ğ‘€ ğ‘‰ [:, ğ‘š] â† [0, ğ‘‰ ğ‘˜ [:, ğ‘–] ğ‘‡ , 0] ğ‘‡ 13:</formula><p>Î£ ğ‘˜ [ğ‘–] â† ğœ âŠ² Apply modification  if ğ‘š &gt; 0 and using Woodbury formula then 24:</p><formula xml:id="formula_8">ğ¿ I,ğ‘˜ â† ğ´ I,ğ‘˜ ğ‘… âˆ’1 ğ‘˜,ğ‘˜ 20: ğ‘… ğ‘˜,I â† ğ¿ âˆ’1 ğ‘˜,ğ‘˜ ğ´ ğ‘˜,I<label>21</label></formula><formula xml:id="formula_9">C ğ‘… â† ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ğ‘… âˆ’1 25: C ğ¿ â† ğ¿ âˆ’1 ğ‘€ ğ‘ˆ 26: C â† ğ¼ âˆ’ C ğ‘… C ğ¿ 27: C âˆ’1 â† FACTOR(C)</formula><p>âŠ² Using, e.g., GEPP </p><formula xml:id="formula_10">ğ‘¥ â† ğ¿ âˆ’1 ğ‘ 32:</formula><p>if ğ‘š &gt; 0 and using Woodbury formula then 33:</p><formula xml:id="formula_11">ğ‘¥ â† (ğ¼ + C ğ¿ C âˆ’1 C ğ‘… )ğ‘¥ 34: end if 35:</formula><p>ğ‘¥ â† ğ‘… âˆ’1 ğ‘¥ 36: end procedure number of arithmetic operations used in the modifications and Woodbury formula. Let ğ‘› be the size of the system, ğ‘š be the rank of the Woodbury correction, ğ‘› ğ‘ be the size of the diagonal blocks, and â„“ ğ‘Ÿâ„ğ‘  be the number of right-hand sides. (If the Woodbury formula is not applied, ğ‘š = 0.) Because the factors' diagonal blocks are full instead of triangular, computing the Schur complement takes an extra ğ‘› 2 ğ‘› ğ‘ + O (ğ‘›ğ‘› 2 ğ‘ ) FLOP. Thus, BEAM without the Woodbury correction takes</p><formula xml:id="formula_12">2 3 ğ‘› 3 + 2ğ‘› 2 â„“ ğ‘Ÿâ„ğ‘  + ğ‘› 2 ğ‘› ğ‘ + O (ğ‘›ğ‘› 2</formula><p>ğ‘ + ğ‘›ğ‘› ğ‘ â„“ ğ‘Ÿâ„ğ‘  ) FLOP. Next, building and factoring the capacitance matrix (via GEPP) takes 2ğ‘› 2 ğ‘š + 2ğ‘›ğ‘š 2 + 2 3 ğ‘š 3 + O (ğ‘›ğ‘š) FLOP. Finally, the Woodbury formula requires two triangular solves and two matrix multiplies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>So, the Woodbury formula adds an extra</head><formula xml:id="formula_13">2ğ‘› 2 ğ‘š + 2ğ‘›ğ‘š 2 + 2 3 ğ‘š 3 + 4ğ‘›ğ‘šâ„“ ğ‘Ÿâ„ğ‘  + 2ğ‘š 2 â„“ ğ‘Ÿâ„ğ‘  + O (ğ‘› 2 + ğ‘›â„“ ğ‘Ÿâ„ğ‘  ) FLOP.</formula><p>Hence, if ğ‘› ğ‘ , ğ‘š â‰ª ğ‘›, the arithmetic overhead compared to GENP should be negligible. While this does not measure the cost of data movement, most of the added computations have high data locality, especially compared to pivoting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>Because BEAM inverts ğ´ instead of ğ´, it is crucial to understand the errors that arise when solving ğ´ğ‘¥ = ğ‘ in finite-precision. First, the additive modifications guarantee that ğ´ and its block principal leading submatrices are non-singular, a prerequisite for the success of non-pivoted block-LU factorizations. Thus, this algorithm (without the Woodbury formula) computes a solution, ğ‘¥, that satisfies a nearby system,</p><formula xml:id="formula_14">( ğ´ + Î” ğ´) ğ‘¥ = (ğ‘ + Î”ğ‘),<label>(3)</label></formula><p>with</p><formula xml:id="formula_15">âˆ¥Î” ğ´âˆ¥ 2 â‰¤ ğœ‚ 2 ( ğ‘¥) âˆ¥ ğ´âˆ¥ 2 and âˆ¥Î”ğ‘ âˆ¥ 2 â‰¤ ğœ‚ 2 ( ğ‘¥) âˆ¥ğ‘ âˆ¥ 2 ;<label>(4)</label></formula><p>that is, ğœ‚ 2 ( ğ‘¥) is the normwise backward error for the spectral norm. Then, combining (3) with the definition of ğ´ from ( <ref type="formula" coords="4,241.06,321.99,3.17,12.39">1</ref>) gives</p><formula xml:id="formula_16">(ğ´ + ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ + Î” ğ´) ğ‘¥ = (ğ‘ + Î”ğ‘).</formula><p>Thus, the backward error of ğ‘¥ for the original system ğ´ğ‘¥ = ğ‘ is</p><formula xml:id="formula_17">ğœ‚ 2 ( ğ‘¥) â‰¤ max âˆ¥ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ + Î” ğ´âˆ¥ 2 âˆ¥ğ´âˆ¥ 2 , âˆ¥Î”ğ‘ âˆ¥ 2 âˆ¥ğ‘ âˆ¥ 2 â‰¤ Ï„ + ğœ‚ 2 ( ğ‘¥).<label>(5)</label></formula><p>Hence, the forward error of ğ‘¥ can be bounded with only Ï„, the backward stability of the block factorization, and the condition number of ğ´. Importantly, ğ´ need not be well conditioned. Furthermore, the convergence of iterative refinement is also ensured when those three values are sufficiently small <ref type="bibr" coords="4,183.39,447.94,9.52,12.39" target="#b3">[4]</ref>. Note that (5) implies that Ï„ may directly contribute to the backward error when there are modifications but the Woodbury formula is not applied.</p><p>Unfortunately, an ill-conditioned ğ´ can still be problematic when using the Woodbury formula because its forward error directly perturbs the capacitance matrix and, thus, the correction. To help understand this condition number, we provide the following theorem, which states that if the tolerance, Ï„, is small relative to the reciprocal condition number of ğ´, the conditioning of ğ´ will be close to that of ğ´. Interestingly, the condition of this theorem ( Ï„ğœ… 2 (ğ´) â‰ª 1) appears related to the requirement implied by <ref type="bibr" coords="4,225.23,558.54,9.51,12.39" target="#b4">(5)</ref> for the solution to have any digits of accuracy ( Ï„ğœ… 2 (ğ´) + ğœ‚ 2 ğœ… 2 (ğ´) â‰ª 1). </p><formula xml:id="formula_18">= Ï„ âˆ¥ğ´âˆ¥ 2 . If Ï„ğœ… 2 (ğ´) &lt; 1, then ğœ… 2 ( ğ´) â‰¤ ğœ 1 (ğ´) + ğœ ğœ ğ‘› (ğ´) âˆ’ ğœ = ğœ… 2 (ğ´) 1 + Ï„ 1 âˆ’ Ï„ğœ… 2 (ğ´)</formula><p>where ğœ 1 (ğ´) and ğœ ğ‘› (ğ´) denote the largest and smallest singular values, respectively, and ğœ… 2 (ğ´) = ğœ 1 (ğ´)/ğœ ğ‘› (ğ´).</p><p>Proof. By the triangle inequality,</p><formula xml:id="formula_19">ğœ 1 ( ğ´) â‰¤ ğœ 1 (ğ´) + ğœ and ğœ ğ‘› ( ğ´) â‰¥ ğœ ğ‘› (ğ´) âˆ’ ğœ . Suppose Ï„ğœ… 2 (ğ´) &lt; 1, which implies ğœ ğ‘› (ğ´) âˆ’ ğœ &gt; 0. Hence, ğœ… 2 ( ğ´) â‰¤ ğœ 1 (ğ´) + ğœ ğœ ğ‘› (ğ´) âˆ’ ğœ = ğœ 1 (ğ´) + Ï„ğœ 1 (ğ´) ğœ ğ‘› (ğ´) âˆ’ Ï„ğœ 1 (ğ´) = ğœ… 2 (ğ´) 1 + Ï„ 1 âˆ’ Ï„ğœ… 2 (ğ´)</formula><p>. â–¡</p><p>When applying the Woodbury formula, we must also invert the capacitance matrix as per <ref type="bibr" coords="4,416.24,142.06,8.64,12.39" target="#b1">(2)</ref>. Thus, its condition number is also crucial in the analysis of this method. We start by generalizing a lemma of Yip <ref type="bibr" coords="4,368.86,163.98,14.72,12.39" target="#b29">[30]</ref> to the full version of the Woodbury formula. Lemma 4.2. Let âˆ¥â€¢âˆ¥ ğ‘ be any sub-multiplicative matrix norm. We denote the condition number with respect to the Moore-Penrose pseudoinverse by ğœ… + ğ‘ (ğ´) = âˆ¥ğ´âˆ¥ ğ‘ âˆ¥ğ´ + âˆ¥ ğ‘ . Suppose ğ´ = ğ´ + ğ‘ˆ Î£ğ‘‰ ğ‘‡ is nonsingular with ğ‘ˆ , ğ‘‰ having full column rank and Î£ being nonsingular. Then,</p><formula xml:id="formula_20">ğœ… ğ‘ (Î£ âˆ’1 âˆ’ ğ‘‰ ğ‘‡ ğ´ âˆ’1 ğ‘ˆ ) â‰¤ min ğœ… + ğ‘ (ğ‘ˆ ) 2 , ğœ… + ğ‘ (ğ‘‰ ğ‘‡ ) 2 ğœ… ğ‘ (Î£)ğœ… ğ‘ (ğ´ ğ´ âˆ’1 ) â‰¤ min ğœ… + ğ‘ (ğ‘ˆ ) 2 , ğœ… + ğ‘ (ğ‘‰ ğ‘‡ ) 2 ğœ… ğ‘ (Î£)ğœ… ğ‘ (ğ´)ğœ… ğ‘ ( ğ´).</formula><p>Proof. Because we must bound the norm of the capacitance matrix, we start by rewriting its expression. Multiplying ğ´âˆ’ğ‘ˆ Î£ğ‘‰ ğ‘‡ = ğ´ on the left by Î£ âˆ’1 ğ‘ˆ + and on the right by ğ´ âˆ’1 ğ‘ˆ gives</p><formula xml:id="formula_21">(Î£ âˆ’1 âˆ’ ğ‘‰ ğ‘‡ ğ´ âˆ’1 ğ‘ˆ ) = Î£ âˆ’1 ğ‘ˆ + ğ´ ğ´ âˆ’1 ğ‘ˆ .<label>(6)</label></formula><p>We next seek a similar expression for its inverse. Note that,</p><formula xml:id="formula_22">ğ´ ğ´ âˆ’1 ğ‘ˆ = ( ğ´ âˆ’ ğ‘ˆ Î£ğ‘‰ ğ‘‡ ) ğ´ âˆ’1 ğ‘ˆ = ğ‘ˆ âˆ’ ğ‘ˆ Î£ğ‘‰ ğ‘‡ ğ´ âˆ’1 ğ‘ˆ .</formula><p>So, the columns of ğ´ ğ´ âˆ’1 ğ‘ˆ are within the column space of ğ‘ˆ . Since ğ‘ˆğ‘ˆ + is an orthogonal projector onto that space <ref type="bibr" coords="4,505.98,396.73,13.50,12.39" target="#b12">[13]</ref>, we have (ğ‘ˆğ‘ˆ + )ğ´ ğ´ âˆ’1 ğ‘ˆ = ğ´ ğ´ âˆ’1 ğ‘ˆ . Using this, we can verify that</p><formula xml:id="formula_23">(ğ‘ˆ + ğ´ğ´ âˆ’1 ğ‘ˆ Î£)(Î£ âˆ’1 ğ‘ˆ + ğ´ ğ´ âˆ’1 ğ‘ˆ ) = ğ¼ .</formula><p>Combining this with <ref type="bibr" coords="4,395.92,443.87,9.51,12.39" target="#b5">(6)</ref> gives the desired inverse:</p><formula xml:id="formula_24">(Î£ âˆ’1 âˆ’ ğ‘‰ ğ‘‡ ğ´ âˆ’1 ğ‘ˆ ) âˆ’1 = ğ‘ˆ + ğ´ğ´ âˆ’1 ğ‘ˆ Î£.</formula><p>Hence, the condition number can be bounded as</p><formula xml:id="formula_25">ğœ… ğ‘ (Î£ âˆ’1 âˆ’ ğ‘‰ ğ‘‡ ğ´ âˆ’1 ğ‘ˆ ) = âˆ¥Î£ âˆ’1 ğ‘ˆ + ğ´ ğ´ âˆ’1 ğ‘ˆ âˆ¥ ğ‘ âˆ¥ğ‘ˆ + ğ´ğ´ âˆ’1 ğ‘ˆ Î£âˆ¥ ğ‘ â‰¤ ğœ… + ğ‘ (ğ‘ˆ ) 2 ğœ… ğ‘ (Î£)âˆ¥ğ´ ğ´ âˆ’1 âˆ¥ ğ‘ âˆ¥ ğ´ğ´ âˆ’1 âˆ¥ ğ‘ . A similar argument shows that ğœ… ğ‘ (Î£ âˆ’ ğ‘ˆ ğ´ âˆ’1 ğ‘‰ ğ‘‡ ) â‰¤ ğœ… + ğ‘ (ğ‘‰ ğ‘‡ ) 2 ğœ… ğ‘ (Î£)âˆ¥ğ´ ğ´ âˆ’1 âˆ¥ ğ‘ âˆ¥ ğ´ğ´ âˆ’1 âˆ¥ ğ‘ . â–¡</formula><p>Using this lemma, the condition number for the capacitance matrix in the obvious form of the Woodbury formula is bounded by</p><formula xml:id="formula_26">ğœ… 2 (ğ‘€ âˆ’1 Î£ âˆ’ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 ğ‘€ ğ‘ˆ ) â‰¤ ğœ… 2 (ğ‘€ Î£ )ğœ… 2 (ğ´ ğ´ âˆ’1 )</formula><p>. As mentioned in Section 3, we instead formulate the Woodbury correction to get a tighter bound on the condition number:</p><formula xml:id="formula_27">ğœ… 2 (ğ¼ âˆ’ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 ğ‘€ ğ‘ˆ ) â‰¤ ğœ… 2 (ğ´ ğ´ âˆ’1 )</formula><p>. The conditioning of this latter matrix can be further improved, particularly for the 2-norm. The following theorem shows that as long as neither ğ´ nor ğ´ are ill-conditioned, the capacitance matrix will have an excellent condition number. </p><formula xml:id="formula_28">âˆ¥ğ‘€ Î£ âˆ¥ 2 = ğœ. Additionally, let C = ğ¼ âˆ’ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ ğ´ âˆ’1 ğ‘€ ğ‘ˆ . Then, ğœ… 2 (C) â‰¤ (1 + ğœ âˆ¥ ğ´ âˆ’1 âˆ¥ 2 ) (1 + ğœ âˆ¥ğ´ âˆ’1 âˆ¥ 2 ).</formula><p>If ğœ = Ï„ âˆ¥ğ´âˆ¥ 2 with Ï„ &lt; 1, then we can simplify the bound to</p><formula xml:id="formula_29">ğœ… 2 (C) â‰¤ (1 + Ï„ 1âˆ’ Ï„ ğœ… 2 ( ğ´)) (1 + Ï„ğœ… 2 (ğ´)).</formula><p>Proof. With ğ‘ˆ = ğ‘€ ğ‘ˆ , Î£ = ğ¼ , and</p><formula xml:id="formula_30">ğ‘‰ ğ‘‡ = ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ , Lemma 4.2 gives the bound ğœ… 2 (C) â‰¤ âˆ¥ğ´ ğ´ âˆ’1 âˆ¥ 2 âˆ¥ ğ´ğ´ âˆ’1 âˆ¥ 2 . Since ğ´ = ğ´ âˆ’ ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ and âˆ¥ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ âˆ¥ 2 = ğœ, a little algebra shows that ğœ… 2 (C) â‰¤ (1 + ğœ âˆ¥ ğ´ âˆ’1 âˆ¥ 2 ) (1 + ğœ âˆ¥ğ´ âˆ’1 âˆ¥ 2 ). Suppose ğœ = Ï„ âˆ¥ğ´âˆ¥ 2 and Ï„ &lt; 1. Then, âˆ¥ğ´âˆ¥ 2 = âˆ¥ ğ´ âˆ’ ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ âˆ¥ 2 â‰¤ âˆ¥ ğ´âˆ¥ 2 + Ï„ âˆ¥ğ´âˆ¥ 2 , and so âˆ¥ğ´âˆ¥ 2 â‰¤ (1 âˆ’ Ï„) âˆ’1 âˆ¥ ğ´âˆ¥ 2 . Therefore, ğœ… 2 (C) â‰¤ (1 + Ï„ âˆ¥ğ´âˆ¥ 2 âˆ¥ ğ´ âˆ’1 âˆ¥ 2 ) (1 + Ï„ âˆ¥ğ´âˆ¥ 2 âˆ¥ğ´ âˆ’1 âˆ¥ 2 ) â‰¤ (1 + Ï„ 1âˆ’ Ï„ ğœ… 2 ( ğ´)) (1 + Ï„ğœ… 2 (ğ´)).</formula><p>â–¡ After the backward error bound in <ref type="bibr" coords="5,192.09,315.48,9.51,12.39" target="#b4">(5)</ref> and the theorems on key condition numbers, one major concern remains: how backward stable is the factorization of ğ´? To our knowledge, no analysis exists for block LU that would apply to Algorithm 1. The closest is by Demmel, Higham, and Schreiber <ref type="bibr" coords="5,197.90,359.71,9.52,12.39" target="#b6">[7]</ref>, but the block LU they analyzed differs from our factorization in two primary ways. First, the diagonal blocks are factored with GEPP instead of the SVD. Second, the diagonal blocks of the lower block-triangular factor are identity matrices instead of singular vectors. Under reasonable assumptions, they proved that it computes a solution, ğ‘¥, to ğ´ğ‘¥ = ğ‘ such that</p><formula xml:id="formula_31">(ğ´ + Î”ğ´) ğ‘¥ ğ‘, âˆ¥ ğ‘¥ âˆ¥ max â‰¤ ğ‘ (ğ‘›)ğ‘¢ (âˆ¥ğ´âˆ¥ max + âˆ¥ ğ¿âˆ¥ max âˆ¥ ğ‘ˆ âˆ¥ max )</formula><p>where ğ¿ and ğ‘ˆ are the computed block-triangular factors and ğ‘ (ğ‘›) is a constant dependent on ğ‘›. Thus, we expect the method to be backward stable when âˆ¥ ğ¿âˆ¥ max âˆ¥ ğ‘ˆ âˆ¥ max /âˆ¥ ğ´âˆ¥ max is small. For the general case, Demmel et al. proved that this ratio is at most</p><formula xml:id="formula_32">âˆ¥ ğ¿âˆ¥ max âˆ¥ ğ‘ˆ âˆ¥ max âˆ¥ğ´âˆ¥ max â‰¤ ğ‘›ğœŒ 3 NP ğœ… max (ğ´)<label>(7)</label></formula><p>where ğœŒ NP is the growth factor of ğ´ for GENP (i.e., the magnitude of the largest element that occurs in any Schur complement). While the previous analysis cannot guarantee the backward stability of BEAM's factorization, the similarity between the factorizations suggests such stability is likely. Furthermore, we expect a stronger version of ( <ref type="formula" coords="5,98.85,585.47,3.17,12.39" target="#formula_32">7</ref>) can be proven for BEAM since the norms of the inverses of the diagonal blocks are bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>To investigate the feasibility of this approach in terms of numerical stability, scalability, and performance, we implemented it in the Software for Linear Algebra Targeting Exascale (SLATE) library and evaluated it on the Summit supercomputer. SLATE is a dense linear algebra library that targets distributed-memory, GPU-accelerated systems <ref type="bibr" coords="5,85.07,686.30,13.36,12.39" target="#b9">[10]</ref>. Our code and results are available at https://doi.org/ 10.6084/m9.figshare.21982472.</p><p>Our implementation follows Algorithm 1 and uses a high-level structure based on SLATE's GENP routine <ref type="bibr" coords="5,472.59,94.52,13.27,12.39" target="#b22">[23]</ref>. However, we separated BEAM's algorithmic block size from the distribution tile size (the former being smaller than the latter). For simplicity, our code does not support an algorithmic block to be split across multiple tiles of SLATE and will truncate the last block in a tile to fit. But all our experiments align the block and tile sizes so that truncation only happens in the last tile. After the factorization is complete, the capacitance matrix is built and factored. While our theory defines ğœ in terms of âˆ¥ğ´âˆ¥ 2 , this is expensive to compute in practice. So, our experiments instead used the Frobenius norm, ğœ = Ï„ âˆ¥ğ´âˆ¥ ğ¹ , which is closely related.</p><p>For performance purposes, we implemented a GPU routine for batched, block-triangular solves, using a recursive formulation similar to the MAGMA <ref type="bibr" coords="5,390.10,236.99,10.42,12.39" target="#b0">[1]</ref> and KBLAS <ref type="bibr" coords="5,446.16,236.99,10.43,12.39" target="#b5">[6]</ref> libraries. Because the diagonal blocks come from the SVD, these inverses can be realized by a matrix multiplication and sometimes a diagonal scaling. While cuBLAS's batched GEMM routine was effective for the trailingmatrix updates, its performance was lacking for small block sizes due to the subsequent copy or scale operation. For such cases, we implemented a custom routine that combined the multiplication and the copy to improve cache reuse and avoid extra kernel launch overheads. To reduce the effort in performance tuning, we used part of MAGMA's matrix-multiplication routine in our kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We ran our experiments on eight nodes of the Summit supercomputer at Oak Ridge National Laboratory. This machine is based on IBM Power System AC922 nodes, each containing two 22-core IBM POWER9 CPUs and six NVIDIA Volta V100 GPUs. One core of each socket is reserved for the OS (Red Hat Enterprise Linux version 8.2). Most of the computational power comes from the GPUs, each providing 7.8 TFLOP/s, 16 GiB HBM2, and 900 GB/s memory bandwidth. Each CPU provides 540 GFLOP/s, 256 GiB DDR4 memory, and 170 GB/s memory bandwidth. NVIDIA NVLink provides a bidirectional 50 GB/s between components in a socket. A dual-rail EDR InfiniBand network with a non-blocking fat-tree topology connects the nodes.</p><p>We used a modified version of SLATE's test harness for our experiments. The tester was compiled with GCC 9.1.0, CUDA 11.0.3, IBM Spectrum MPI 10.4.0.3, IBM ESSL 6.1.0, Netlib LAPACK 3.8.0, and Netlib ScaLAPACK 2.1.0. We set the smt1 flag and started MPI with jsrun -n 16 -a 1 -c 21 -g 3 -b packed:21 -d packed which allocates 16 processes, each bound to a single socket and its GPUs. For all experiments, we configured the tester with --origin h --target d --ref n --grid 4x4 --panel-threads 20 --seed 1 --seedB 2 --matrixB randn --nrhs 1. We also set the --matrix, --dim, and --check flags as appropriate for the experiment. For GEPP, we also set --nb 768 --ib 64 --lookahead 1. For GENP and BEAM, we set --nb 512 --lookahead 2 --ib 64, except for the experiment described in Section 5.3 which changed the last argument as appropriate for BEAM. This configuration gives a 2d block-cyclic distribution with a 4 Ã— 4 process grid and blocks of size 512 or 768, as indicated by --nb. Note that SLATE's ib parameter corresponds to the ğ‘› ğ‘ value discussed in this paper; SLATE's nb corresponds to the larger blocks used to distribute the matrix. All tests were preceded by extra tests of size ğ‘› = 5000 (with the otherwise identical configuration) to ensure that our results were not influenced by initialization costs. To measure the effects of system noise, we ran each performance test three times and computed the mean and 95% confidence interval. Except for svd_geo, the error and number of modifications were the same between the different runs. Due to minor non-determinism in SLATE's QR factorization, there is slight variability between runs in the entries of svd_geo. However, this variability is small and does not affect our conclusions or analysis, so we just present the error values and number of modifications from the first run.</p><p>To understand how our BEAM algorithm behaves across various linear systems, we used seven random and eight structured matrices in our tests. Table <ref type="table" coords="6,123.13,469.04,4.25,12.39" target="#tab_4">1</ref> describes these matrices. We choose a righthand side with each element randomly taken from the normal distribution. The matrix generator was always seeded with 1 for the matrices and with 2 for the right-hand sides so that the test problems can be reproduced.</p><p>Accuracy was measured with the infinity-norm backward error:</p><formula xml:id="formula_33">ğœ‚ âˆ (ğ‘¥) = âˆ¥ğ‘ âˆ’ ğ´ğ‘¥ âˆ¥ âˆ âˆ¥ğ´âˆ¥ âˆ âˆ¥ğ‘¥ âˆ¥ âˆ + âˆ¥ğ‘ âˆ¥ âˆ . (<label>8</label></formula><formula xml:id="formula_34">)</formula><p>Correspondingly, in experiments with iterative refinement, the refinement was terminated when this error was less than or equal to âˆš ğ‘› times the unit roundoff (âˆ¼3.5 Ã— 10 âˆ’14 when ğ‘› = 10 5 ) or after 30 iterations. We selected this criterion based on the accuracy of GEPP (see Table <ref type="table" coords="6,115.30,614.36,2.94,12.39" target="#tab_5">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline Accuracy and Performance Experiments</head><p>First, Table <ref type="table" coords="6,97.31,664.38,4.25,12.39" target="#tab_5">2</ref> compares the accuracy of BEAM against GEPP and GENP for varying values of tolerance Ï„. The matrices were of size 10 5 , with a blocking factor of 64 for BEAM. The reported error is the infinity-norm backward error of <ref type="bibr" coords="6,183.87,697.26,8.59,12.39" target="#b7">(8)</ref>. Most importantly, the error of BEAM with the Woodbury correction is smaller than or approximately equal to that of GENP for all but one case (orthog with Ï„ = 10 âˆ’10 ). Furthermore, BEAM with Woodbury correction has a significantly smaller error than GENP for most matrices and only incurs NaN values for one matrix, zielkeNS. (Those NaN values resulting from growth-induced overflow.) These results demonstrate the ability of our approach to provide better numerical stability than GENP. Moreover, the error was smaller than 10 âˆ’10 for many of the matrices. This implies that the iterative refinement should often converge quickly to double-precision accuracy <ref type="bibr" coords="6,487.92,182.19,9.30,12.39" target="#b3">[4]</ref>. While Ï„ = 10 âˆ’6 leads to modifications for most matrices, only five of the fifteen matrices required more than ten modifications when Ï„ â‰¤ 10 âˆ’8 (one of which was accurately solved without any modifications when Ï„ = 10 âˆ’10 ). This indicates that the proposed approach is likely effective for a large class of matrices. Additionally, many linear systems saw a significant improvement in accuracy compared to GENP, even without modification. Thus, even just applying an SVD factorization to invert the diagonal blocks increases the stability of a non-pivoted factorization.</p><p>The results become more nuanced when considering BEAM without Woodbury correction. For Ï„ = 10 âˆ’10 , the uncorrected solver behaved similarly to the corrected one. For larger tolerances, however, there is a significant gap between the two, particularly for Ï„ = 10 âˆ’6 . This indicates that the perturbation of the uncorrected modification becomes the dominant source of error when Ï„ â‰³ 10 âˆ’8 . This aligns with both the Ï„ term in the normwise backward error bound of ( <ref type="formula" coords="6,356.05,368.49,3.17,12.39" target="#formula_17">5</ref>) and the recommended tolerance when applying scalar updates without correction <ref type="bibr" coords="6,419.91,379.45,13.39,12.39" target="#b20">[21]</ref>. In contrast, when the Woodbury correction was applied, increasing the tolerances always saw similar or better accuracies. This suggests that the error in the corrected case comes from the presence of small diagonal singular values and the resulting growth and not from applying the modifications or the Woodbury correction process.</p><p>Table <ref type="table" coords="6,349.31,445.21,4.09,12.39">3</ref> augments Table <ref type="table" coords="6,413.68,445.21,4.09,12.39" target="#tab_5">2</ref> by showing the time to solve the linear systems of equations (again with ğ‘› = 10 5 ). Up to 30 steps of iterative refinement were used for BEAM but not for GEPP or GENP. To clarify where iterative refinement was unsuccessful, we marked the cases which failed to achieve convergence criterion for iterative refinement (ğœ‚ âˆ (ğ‘¥) â‰² 3.5 Ã— 10 âˆ’14 ). Furthermore, we provide the number of refinement iterations, with 30 being the limit.</p><p>First, note that by using iterative refinement, our approach achieved an error of less than 2 âˆ’53 âˆš ğ‘› for almost all cases. As above, zielkeNS's failure involved excessive growth generating NaN values. For the remaining failures, BEAM produced a non-NaN solution, but iterative refinement failed to converge to full accuracy. These cases included svd_geo, chebspec, fiedler, and riemann with larger tolerances (and many modifications) but without Woodbury correction. These matrices are all ill-conditioned, which limits the ability of iterative refinement to converge when the inner solution is only moderately accurate <ref type="bibr" coords="6,495.54,620.55,9.52,12.39" target="#b3">[4]</ref>. For example, ğœ… âˆ (fiedler) = 2ğ‘›(ğ‘› âˆ’ 1) â‰ˆ 2 Ã— 10 10 [27, pg. 159], so iterative refinement can only be expected to converge to full accuracy when ğœ‚ âˆ (ğ‘¥) â‰² 5 Ã— 10 âˆ’11 . Furthermore, this further supports the implication of both (5) and Theorem 4.1 that Ï„ should be chosen such that Ï„ğœ… 2 (ğ´) â‰ª 1. Interestingly, these systems were successfully solved when using the Woodbury formula, despite the dire implication of Theorems 4.1 and 4.3 that Ï„ğœ… 2 (ğ´) â‰² 1 can lead to a large forward </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of tolerance</head><p>We next investigated the tradeoff in performance and accuracy for a larger variety of tolerance values and blocking sizes on select matrices without iterative refinement. Table <ref type="table" coords="8,221.55,119.18,4.25,12.39" target="#tab_6">4</ref> shows the results. Matching Tables <ref type="table" coords="8,119.41,130.14,27.40,12.39" target="#tab_5">2 and 3</ref>, the matrix sizes are all ğ‘› = 10 5 . Furthermore, the number of modifications and error in Table <ref type="table" coords="8,276.17,141.10,4.25,12.39" target="#tab_6">4</ref> for Ï„ = 10 âˆ’6 , 10 âˆ’8 , 10 âˆ’10 correspond to the values in Table <ref type="table" coords="8,254.16,152.06,3.09,12.39" target="#tab_5">2</ref>; however, the run times differ from Table <ref type="table" coords="8,171.10,163.01,4.25,12.39">3</ref> due to the omission of iterative refinement. While GEPP and GENP do not have the algorithmic blocking parameter ğ‘› ğ‘ of BEAM, they implement cache-blocking with a similar structure and a block size of 64.</p><p>Both rand_dominant and rand matrix types saw BEAM significantly outperform GEPP for all configurations. However, as mentioned earlier, BEAM performed worse than GENP, even when no corrections were applied. Moreover, smaller block sizes performed slightly better, likely due to increased arithmetic for the SVDs and block-triangular solves. For rand_dominant, all configurations resulted in no modifications and the same accuracy. For rand, on the other hand, increasing the tolerance above 10 âˆ’10 increased the accuracy when the Woodbury correction was applied but decreased the accuracy when it was not, with the number of modifications increasing in both cases. Given the number of modifications introduced when Ï„ â‰¤ 10 âˆ’6 , a tolerance of 10 âˆ’8 or 10 âˆ’10 is a better choice, particularly when not applying the Woodbury correction. Finally, increasing the block size reduced the number of modifications and increased the accuracy in all but one case.</p><p>The structured matrices provided more interesting results. As in the previous tables, BEAM applied numerous modifications to the orthog matrix for all of the tested configurations. Increasing the blocking factor helped the accuracy, although it also increased the number of modifications. The best tradeoff between performance and accuracy seems to be for tolerances of 10 âˆ’6 or 10 âˆ’8 (depending on the block size) without the Woodbury correction. Unexpectedly, a smaller block size led to fewer modifications; we suspect this is due to element growth in the later diagonals. For zielkeNS, only Ï„ = 10 âˆ’4 without the Woodbury formula produced a non-NaN solution. On the other hand, the block size had limited effect on the performance or the accuracy. For Ï„ = 10 âˆ’4 , all three block factor sizes resulted in the modification of about 95% of the diagonal singular values, whereas for smaller tolerance values, the number of modifications was just slightly larger than the number of blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scaling Results</head><p>Finally, we tested the performance of the different solvers as the size, ğ‘›, varies for the rand_dominant, rand, and orthog matrices. BEAM achieved speedups ranging from 4Ã— to almost 5Ã— for the three matrices compared to GEPP applied to rand. BEAM was configured with a blocking factor size of ğ‘› ğ‘ = 64 and a tolerance of Ï„ = 10 âˆ’8 . BEAM with iterative refinement ran out of GPU memory for ğ‘› = 250 000 due to the extra copy of the system matrix. Note that a diagonally dominant matrix, such as the rand_dominant, is the best-case scenario for the performance of GEPP because the selected pivots already reside on the diagonal and the memory traffic of exchanging rows is avoided (though we still perform the pivot search for each column). For the large matrices, BEAM reached 80% of GENP's performance for rand_dominant and rand, as the 0 50,000 100,000 150,000 200,000 250,000 300,000 0 former required no modifications and the latter required just a few modifications. Without the Woodbury correction, BEAM also performed similarly on orthog. However, with the correction, the performance dropped to approximately that of the best case for GEPP. Adding iterative refinement slightly reduced the overall performance, but BEAM still outperformed the best-case scenario of GEPP by 84% to 162% on the rand_dominant and rand matrices. Without the Woodbury formula, BEAM performed almost as well on orthog as on rand with speedups of 70% to 144%. With the Woodbury formula, BEAM performed in the range of GEPP, between 40% and 112% faster than the GEPP's performance on rand (which is close to its performance on orthog, as per Table <ref type="table" coords="8,526.29,664.38,2.88,12.39">3</ref>). These speedups for orthog are particularly promising because most approaches struggle to accurately outperform GEPP on this matrix, especially for large sizes <ref type="bibr" coords="8,409.09,697.26,9.33,12.39" target="#b7">[8,</ref><ref type="bibr" coords="8,420.66,697.26,7.64,12.39" target="#b21">[22]</ref><ref type="bibr" coords="8,428.30,697.26,3.82,12.39" target="#b22">[23]</ref><ref type="bibr" coords="8,432.13,697.26,11.47,12.39" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>We proposed using additive modifications in Gaussian elimination for dense matrices to avoid the performance overheads associated with partial pivoting while retaining the numerical stability achieved in classic implementations of LU factorization. As a result, we recorded speedups reaching as high as 5Ã— against the GEPP implementation on a GPU-accelerated supercomputer. Our method of additive modifications for dense matrices (unlike single-element modifications occasionally used for sparse matrices) factors the diagonal blocks with the SVD to reduce the number of modifications required. We experimentally established that BEAM provides better performance for comparable accuracy to GEPP and better accuracy for comparable performance to GENP for the majority of our test matrices. Furthermore, by testing this approach both with and without the Woodbury formula, we find that (when using iterative refinement) omitting the Woodbury correction often provides a better time-to-solution than when applying the correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parameter Selection</head><p>The success of BEAM depends heavily on both the tolerance and whether to apply the Woodbury formula. The block size also matters but, based on Table <ref type="table" coords="10,123.26,324.15,3.01,12.39" target="#tab_6">4</ref>, to a lesser extent; we suggest starting with the size of cache-blocking for non-pivoted LU or slightly larger. Below, we analyze in detail considerations for choosing the threshold and whether to use the Woodbury formula. But as a starting point, we suggest Ï„ = min(0.5ğœ… 2 (ğ´), 10 âˆ’8 ) and no Woodbury formula. For most applications, however, various configurations should be tested on the linear systems produced by representative domain problems. For the Woodbury formula, recall that in Table <ref type="table" coords="10,229.59,400.86,4.09,12.39">3</ref> and Figure <ref type="figure" coords="10,274.87,400.86,3.01,12.39" target="#fig_4">2</ref>, the corrected solver never outperformed the corresponding uncorrected one. However, a few ill-conditioned cases failed to converge without the Woodbury formula, while all cases converged when using the Woodbury formula (except for zielkeNS, which overflowed for both). Furthermore, as noted before, Table <ref type="table" coords="10,205.09,455.66,4.09,12.39">3</ref> indicates that using the Woodbury formula can enable convergence when Ï„ğœ… 2 (ğ´) â‰¥ 1. Thus, the Woodbury formula appears to be preferable for ill-conditioned matrices. And, iterative refinement already measures the quality of the factorization. So, we suggest initially skipping the Woodbury formula. Then if iterative refinement fails to converge within, e.g., five iterations, use the Woodbury formula in subsequent iterations.</p><p>For selecting Ï„, we first wish to draw attention to the importance of the inequality Ï„ğœ… 2 (ğ´) â‰ª 1. For BEAM without a Woodbury correction, <ref type="bibr" coords="10,83.72,554.29,9.51,12.39" target="#b4">(5)</ref> implies that this inequality is a prerequisite to proving that the solution has at least one digit of accuracy and that iterative refinement can converge to full backward accuracy. For BEAM with a Woodbury correction, this is necessary to show that ğ´ is well conditioned using Theorem 4.1. Finally, our experimental results demonstrated that violating this inequality can lead to a failure of BEAM without the Woodbury formula. Although, our experimental results also failed to show a similar result when the Woodbury formula was applied. Thus, there may be a subtle interaction between the perturbations and the resulting Woodbury correction that leads to better stability than the existing analysis suggests.</p><p>Beyond ensuring Ï„ğœ… 2 (ğ´) â‰ª 1, there are a few relative concerns in the selection of Ï„. First, consider the omission of the Woodbury correction. Recall that in Table <ref type="table" coords="10,165.89,697.26,3.01,12.39">3</ref>, the smallest tolerance (i.e., 10 âˆ’10 ) outperformed the largest tolerance (i.e., 10 âˆ’6 ) in all but one case. Furthermore, the added perturbations become overwhelmed by the roundoff perturbations when 10 âˆ’10 â‰² Ï„ â‰² 10 âˆ’8 (depending on the matrix). Thus, a small tolerance, such as the square root of unit roundoff, is preferable. Next, consider the inclusion of the Woodbury correction. Here the number of modifications becomes relevant in addition to their magnitude. Unfortunately, we know of no way to determine a priori the number of modifications that will result from a given tolerance. However, Table <ref type="table" coords="10,502.77,171.23,4.18,12.39">3</ref> suggests that, like in the uncorrected case, smaller tolerances usually result in better performance. Thus, we recommend starting with a similar tolerance to the uncorrected case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Future Research Directions</head><p>Because of the novelty of this approach, there are several areas in which it can be improved. First, the theoretical analysis could be significantly extended, particularly for growth factors and numerical errors in both the worst and the average cases. Second, the experimental accuracy results indicate that matrices with many modifications are sensitive to a small tolerance. Thus, it would be beneficial to explore using a more dynamic policy that increases the tolerance if too many modifications are made. Relatedly, the overhead of applying correction is only significant when many modifications are applied. Thus, a scheme where only a subset of modifications (e.g., the largest twenty) are corrected may better exploit the tradeoffs between BEAM with and without the Woodbury correction. Next, the SVD will be expensive for large block sizes. Replacing it with a cheaper factorization, such as pivoted QR, may allow for increasing the block size without impacting performance. Finally, some previous efforts involving element-wise modifications have included a permutation to help place large elements on the diagonal before starting the factorization <ref type="bibr" coords="10,470.20,434.25,13.37,12.39" target="#b20">[21]</ref>. Such permutations may improve the reliability of BEAM (and other pivot-avoiding methods) since many of the troublesome matrices have leading blocks with small norms.</p><p>Another area of potential future work is in applying BEAM to matrices with exploitable structure since it provides numerical stability without destroying the block structure. First, solving symmetric-indefinite matrices requires either an LU factorization (which cannot exploit symmetry) or symmetric pivoting (which incurs complicated data-access patterns that are prohibitive on modern memory hierarchies). So, BEAM could be used to exploit the matrix's symmetry while avoiding the complexity of symmetric pivoting. This would likely take the form of a block LDLT factorization, with a symmetric-eigenvalue decomposition replacing BEAM's SVD. Second, while the approach of element-wise modifications has been previously explored for sparse matrices <ref type="bibr" coords="10,529.78,598.63,13.59,12.39" target="#b20">[21,</ref><ref type="bibr" coords="10,545.61,598.63,10.19,12.39" target="#b25">26]</ref>, our proposal of choosing modifications based on diagonal blocks has not. This would be particularly useful for block-sparse matrices or multi-frontal methods where the diagonal blocks are already dense and no extra fill-in would be incurred. Finally, there are several matrix formats designed to exploit low-rank structures within dense matrices <ref type="bibr" coords="10,375.60,664.38,9.44,12.39" target="#b1">[2,</ref><ref type="bibr" coords="10,387.33,664.38,10.21,12.39" target="#b14">15]</ref>. Unfortunately, pivoting is quite restricted in such formats, which limits the matrices to which they can be safely applied. Thus, BEAM may improve the numerical stability of factorizations using such bespoke formats.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,510.18,372.43,48.01,12.39;1,317.60,383.39,240.84,12.39;1,329.50,402.86,120.84,9.18;1,451.17,401.06,8.30,6.25;1,461.20,402.86,85.04,9.18"><head></head><label></label><figDesc>Factoring the ğ‘˜th diagonal, ğ´[ğ‘˜, ğ‘˜], updates each ğ´[ğ‘–, ğ‘—] in the trailing matrix by ğ´[ğ‘–, ğ‘—] â† ğ´[ğ‘–, ğ‘—] âˆ’ ğ´[ğ‘–, ğ‘˜] ğ´[ğ‘˜, ğ‘˜] âˆ’1 ğ´[ğ‘˜, ğ‘—] (ğ‘˜ &lt; ğ‘–, ğ‘— â‰¤ ğ‘›).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,66.52,140.08,35.10,7.70;3,103.87,135.62,18.03,12.39;3,124.13,140.08,157.18,7.70"><head>Figure 1 : 3 Ã— 3</head><label>133</label><figDesc>Figure 1: 3 Ã— 3 block structure of BEAM factorization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,63.76,83.56,50.79,12.39;5,119.04,86.60,71.65,9.18;5,190.30,94.01,4.97,3.24;5,196.92,86.60,7.21,9.18;5,204.31,91.60,4.17,7.27;5,208.97,86.60,7.21,9.18;5,216.31,87.23,4.20,3.24;5,215.75,94.05,4.63,3.24;5,224.67,86.60,31.25,9.18;5,255.53,94.01,4.97,3.24;5,264.81,86.60,23.34,9.18;5,287.72,94.01,4.63,3.24;5,53.80,98.74,134.33,8.02"><head>Theorem 4 . 3 .</head><label>43</label><figDesc>Suppose ğ´ = ğ´ + ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ where ğ‘€ ğ‘ˆ and ğ‘€ ğ‘‰ each have orthonormal columns, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="8,317.96,447.11,240.24,7.70;8,317.61,458.25,172.47,7.70;8,493.17,452.17,3.38,10.05;8,493.17,456.83,8.86,12.01;8,502.34,452.45,12.05,13.74;8,514.69,455.49,11.68,6.25;8,528.75,458.25,29.45,7.70;8,317.95,469.21,240.24,7.70;8,317.96,480.17,8.15,7.70;8,328.35,475.71,35.39,12.39"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Performance of BEAM for three matrices compared with GEPP and GENP. The y-axis is equal to2  3 ğ‘› 3 10 âˆ’12 divided by the time in seconds; for GEPP and GENP, this is equivalent to TFLOP/s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,323.83,120.15,234.37,43.07"><head></head><label></label><figDesc>ğ‘› ğ‘¡ â† ğ‘›/ğ‘› ğ‘ âŠ² number of blocks in ğ´</figDesc><table coords="3,323.83,139.83,234.37,23.39"><row><cell>3:</cell><cell>ğ‘š â† 0</cell><cell>âŠ² number of modifications applied</cell></row><row><cell>4:</cell><cell></cell><cell></cell></row></table><note>1: procedure FactorBEAM(ğ´, ğœ) 2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="4,53.80,587.24,240.25,34.14"><head></head><label></label><figDesc>Theorem 4.1. Let ğ´ = ğ´ + ğ‘€ ğ‘ˆ ğ‘€ Î£ ğ‘€ ğ‘‡ ğ‘‰ where ğ‘€ ğ‘ˆ and ğ‘€ ğ‘‰ each have orthonormal columns, and ğ‘€ Î£ is a diagonal matrix with positive entries of at most ğœ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="6,60.60,85.73,228.15,213.95"><head>Table 1 :</head><label>1</label><figDesc>Tested Matrices</figDesc><table coords="6,60.60,107.43,228.15,192.25"><row><cell>Name</cell><cell>Description</cell></row><row><cell cols="2">rand rands randn randb randr rand_dominant rand plus ğ‘›ğ¼ Random elements uniform on [0, 1] Random elements uniform on [âˆ’1, 1] Random elements normally distributed Random elements of 0 or 1 Random elements of -1 or 1 svd_geo Random matrix with singular values geomet-rically spaced from 10 âˆ’8 to 1 chebspec From MATLAB's gallery function circul From MATLAB's gallery function fiedler From MATLAB's gallery function kms From MATLAB's gallery function orthog From MATLAB's gallery function riemann From MATLAB's gallery function ris From MATLAB's gallery function zielkeNS Zielke's nonsymmetric matrix (ğ‘ = 1) [32]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="7,114.07,85.73,434.21,46.20"><head>Table 2 :</head><label>2</label><figDesc>Accuracy Comparison of BEAM without Iterative Refinement Versus GEPP and GENP.</figDesc><table coords="7,132.92,104.19,415.37,27.73"><row><cell></cell><cell></cell><cell></cell><cell>Ï„ = 10 âˆ’6</cell><cell></cell><cell></cell><cell>Ï„ = 10 âˆ’8</cell><cell></cell><cell></cell><cell>Ï„ = 10 âˆ’10</cell><cell></cell></row><row><cell>GEPP</cell><cell>GENP</cell><cell>#</cell><cell>Corr.</cell><cell>Uncorr.</cell><cell>#</cell><cell>Corr.</cell><cell>Uncorr.</cell><cell>#</cell><cell>Corr.</cell><cell>Uncorr.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="9,53.50,81.33,504.47,606.65"><head>Table 4 :</head><label>4</label><figDesc>Tradeoffs between performance and accuracy on select matrices for tolerance values of {10 âˆ’4 , 10 âˆ’6 , 10 âˆ’8 , 10 âˆ’10 , 10 âˆ’12 } and block sizes of {32, 64, 128} without iterative refinement.</figDesc><table coords="9,74.83,115.20,455.67,572.78"><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘› ğ‘ = 32</cell><cell></cell><cell>ğ‘› ğ‘ = 64</cell><cell></cell><cell>ğ‘› ğ‘ = 128</cell></row><row><cell></cell><cell></cell><cell cols="3">Corr. # Mods. Time (s)</cell><cell>Error</cell><cell># Mods. Time (s)</cell><cell>Error</cell><cell># Mods. Time (s)</cell><cell>Error</cell></row><row><cell>rand_dominant</cell><cell>GEPP GENP Ï„ = 10 âˆ’4 Ï„ = 10 âˆ’6 Ï„ = 10 âˆ’8 Ï„ = 10 âˆ’10 Ï„ = 10 âˆ’12</cell><cell>--Y N Y N Y N Y N Y N</cell><cell>--0 0 0 0 0</cell><cell cols="2">--9.8Â±0.2 1Ã—10 âˆ’14 --9.6Â±0.1 1Ã—10 âˆ’14 9.5Â±0.4 1Ã—10 âˆ’14 9.5Â±0.2 1Ã—10 âˆ’14 9.5Â±0.2 1Ã—10 âˆ’14 9.5Â±0.3 1Ã—10 âˆ’14 9.6Â±0.3 1Ã—10 âˆ’14 9.4Â±0.2 1Ã—10 âˆ’14 9.4Â±0.4 1Ã—10 âˆ’14 9.5Â±0.2 1Ã—10 âˆ’14</cell><cell cols="2">-25.1Â±0.5 2Ã—10 âˆ’14 -6.7Â±0.4 1Ã—10 âˆ’14 0 9.8Â±0.1 1Ã—10 âˆ’14 9.6Â±0.3 1Ã—10 âˆ’14 0 9.7Â±0.1 1Ã—10 âˆ’14 9.7Â±0.4 1Ã—10 âˆ’14 0 9.8Â±0.1 1Ã—10 âˆ’14 9.7Â±0.1 1Ã—10 âˆ’14 0 9.7Â±0.4 1Ã—10 âˆ’14 9.6Â±0.2 1Ã—10 âˆ’14 0 9.7Â±0.2 1Ã—10 âˆ’14 9.8Â±0.2 1Ã—10 âˆ’14</cell><cell>--0 0 0 0 0</cell><cell>--10.9Â±0.3 1Ã—10 âˆ’14 --10.8Â±0.4 1Ã—10 âˆ’14 10.9Â±0.2 1Ã—10 âˆ’14 10.8Â±0.1 1Ã—10 âˆ’14 10.9Â±0.2 1Ã—10 âˆ’14 10.9Â±0.2 1Ã—10 âˆ’14 11.0Â±0.3 1Ã—10 âˆ’14 10.8Â±0.1 1Ã—10 âˆ’14 10.8Â±0.2 1Ã—10 âˆ’14 10.8Â±0.2 1Ã—10 âˆ’14</cell></row><row><cell>rand</cell><cell>GEPP GENP Ï„ = 10 âˆ’4 Ï„ = 10 âˆ’6 Ï„ = 10 âˆ’8 Ï„ = 10 âˆ’10 Ï„ = 10 âˆ’12</cell><cell>--Y N Y N Y N Y N Y N</cell><cell>--8 798 138 2 0 0</cell><cell cols="2">--24.5Â±1.2 4Ã—10 âˆ’14 --9.5Â±0.2 6Ã—10 âˆ’5 13.0Â±0.3 5Ã—10 âˆ’13 9.5Â±0.2 2Ã—10 âˆ’7 10.1Â±0.2 3Ã—10 âˆ’12 9.5Â±0.3 1Ã—10 âˆ’9 9.6Â±0.5 7Ã—10 âˆ’11 9.5Â±0.2 7Ã—10 âˆ’11 9.4Â±0.3 7Ã—10 âˆ’11 9.5Â±0.3 7Ã—10 âˆ’11</cell><cell cols="2">-50.0Â±0.6 2Ã—10 âˆ’14 -6.7Â±0.4 3Ã—10 âˆ’9 8 397 23.7Â±0.9 2Ã—10 âˆ’14 9.7Â±0.3 5Ã—10 âˆ’5 126 12.9Â±0.3 2Ã—10 âˆ’13 9.7Â±0.3 2Ã—10 âˆ’7 2 10.2Â±0.1 2Ã—10 âˆ’12 9.7Â±0.5 4Ã—10 âˆ’11 0 9.8Â±0.2 5Ã—10 âˆ’12 9.8Â±0.2 5Ã—10 âˆ’12 0 9.7Â±0.2 5Ã—10 âˆ’12 9.7Â±0.3 5Ã—10 âˆ’12</cell><cell>--8 206 122 1 0 0</cell><cell>--24.7Â±0.8 9Ã—10 âˆ’15 --10.9Â±0.2 4Ã—10 âˆ’5 14.0Â±0.1 1Ã—10 âˆ’13 10.7Â±0.3 2Ã—10 âˆ’7 11.2Â±0.2 1Ã—10 âˆ’12 10.8Â±0.2 2Ã—10 âˆ’10 10.7Â±0.4 1Ã—10 âˆ’12 10.8Â±0.2 1Ã—10 âˆ’12 10.7Â±0.1 1Ã—10 âˆ’12 10.7Â±0.2 1Ã—10 âˆ’12</cell></row><row><cell>orthog</cell><cell>GEPP GENP Ï„ = 10 âˆ’4 Ï„ = 10 âˆ’6 Ï„ = 10 âˆ’8 Ï„ = 10 âˆ’10 Ï„ = 10 âˆ’12</cell><cell>--Y N Y N Y N Y N Y N</cell><cell>--48 493 44 980 1 239 987 846</cell><cell cols="2">--43.6Â±2.3 6Ã—10 âˆ’8 --9.6Â±0.4 1Ã—10 âˆ’4 41.8Â±2.0 1Ã—10 âˆ’6 9.6Â±0.2 1Ã—10 âˆ’6 18.8Â±0.3 4Ã—10 âˆ’4 9.5Â±0.2 4Ã—10 âˆ’4 18.1Â±0.4 4Ã—10 âˆ’4 9.5Â±0.1 5Ã—10 âˆ’4 18.0Â±0.7 5Ã—10 âˆ’4 9.5Â±0.2 1Ã—10 âˆ’4</cell><cell cols="2">-50.2Â±1.2 3Ã—10 âˆ’15 -6.6Â±0.3 5Ã—10 âˆ’5 49 065 44.0Â±2.5 5Ã—10 âˆ’11 9.8Â±0.1 1Ã—10 âˆ’4 47 216 43.1Â±1.7 4Ã—10 âˆ’7 9.7Â±0.2 1Ã—10 âˆ’6 21 420 29.6Â±1.1 2Ã—10 âˆ’5 9.7Â±0.3 2Ã—10 âˆ’5 1 022 18.2Â±0.2 6Ã—10 âˆ’4 9.7Â±0.2 3Ã—10 âˆ’4 873 18.2Â±0.3 2Ã—10 âˆ’4 9.6Â±0.6 3Ã—10 âˆ’4</cell><cell>--49 314 48 470 46 159 1 121 892</cell><cell>--45.3Â±1.7 2Ã—10 âˆ’11 --10.5Â±0.3 1Ã—10 âˆ’4 44.8Â±2.0 8Ã—10 âˆ’8 10.7Â±0.3 1Ã—10 âˆ’6 43.5Â±2.2 2Ã—10 âˆ’7 10.7Â±0.3 2Ã—10 âˆ’7 19.9Â±0.4 4Ã—10 âˆ’4 10.6Â±0.0 5Ã—10 âˆ’4 19.2Â±0.5 5Ã—10 âˆ’4 10.5Â±0.2 5Ã—10 âˆ’4</cell></row><row><cell>zielkeNS</cell><cell>GEPP GENP Ï„ = 10 âˆ’4 Ï„ = 10 âˆ’6 Ï„ = 10 âˆ’8 Ï„ = 10 âˆ’10 Ï„ = 10 âˆ’12</cell><cell>--Y N Y N Y N Y N Y N</cell><cell>--96 875 3 156 3 156 3 156 3 156</cell><cell cols="2">--89.4Â±3.7 9.3Â±0.5 7Ã—10 âˆ’5 --NaN 21.2Â±0.9 NaN 9.3Â±0.2 NaN 21.0Â±0.9 NaN 9.3Â±0.4 NaN 20.9Â±0.7 NaN 9.4Â±0.5 NaN 20.9Â±0.5 NaN 9.3Â±0.6 NaN</cell><cell cols="2">-39.5Â±0.3 2Ã—10 âˆ’19 -6.7Â±0.3 NaN 95 313 89.1Â±4.2 NaN 9.7Â±0.2 7Ã—10 âˆ’5 1 594 19.4Â±0.7 NaN 9.6Â±0.1 NaN 1 594 19.5Â±0.7 NaN 9.5Â±0.3 NaN 1 594 19.4Â±0.4 NaN 9.6Â±0.1 NaN 1 594 19.5Â±0.7 NaN 9.5Â±0.2 NaN</cell><cell>--95 313 813 813 813 813</cell><cell>--90.3Â±1.8 10.8Â±0.2 7Ã—10 âˆ’5 --NaN 19.1Â±0.3 NaN 10.7Â±0.3 NaN 19.1Â±0.3 NaN 10.7Â±0.3 NaN 19.2Â±0.0 NaN 10.6Â±0.2 NaN 19.2Â±0.3 NaN 10.6Â±0.3 NaN</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This formula has a variety of names, including the Bartlett-Sherman-Morrison-Woodbury formula and the Sherman-Morrison-Woodbury formula. Hagar's expository paper provides a history of the formula and its repeated discovery<ref type="bibr" coords="1,507.73,699.50,10.39,9.64" target="#b15">[16]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This material is based upon work supported by the National Science Foundation Office of Advanced Cyberinfrastructure under Grant No. 2004541. This research was also supported by the Exascale Computing Project, a collaborative effort of the U.S. Department of Energy Office of Science and the National Nuclear Security Administration. Additionally, this research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>circul 31.4Â±0.9 6.5Â±0.9 0 10.3Â±0.9 3 10.5Â±0.9 0 9.8Â±0.9 0 9.8Â±0.9 0 9.8Â±0.9 0 9.9Â±0.9 fiedler 39.1Â±0.6 6.6Â±0. error. The only other cases with a high iteration count were orthog and Ï„ = 10 âˆ’10 , with and without Woodbury correction. While this matrix is orthogonal and perfectly conditioned, the factorization is of very low quality, almost certainly due to a large growth factor. BEAM outperformed GEPP in all cases except fiedler and ris with many modifications and the Woodbury formula. Furthermore, most cases show a large speedup, particularly when the Woodbury formula was not applied. (Although, for cases that failed to converge, the speedup is, of course, a moot point.) Unfortunately, BEAM had, at best, about two-thirds the performance of GENP. The block factorization seems to be the predominant source of errors, with iterative refinement only adding a significant overhead when many iterations are applied (cf. Section 5.3 and Table <ref type="table" coords="7,490.52,583.59,2.94,12.39">4</ref>).</p><p>Interestingly, in cases for which it converged, BEAM without Woodbury correction outperformed the corrected version in all cases. Furthermore, when Ï„ = 10 âˆ’10 , BEAM without Woodbury correction converged in all but the zielkeNS case. Combining this observation with (5) and Table <ref type="table" coords="7,433.09,638.38,4.22,12.39">2</ref> suggests that the Woodbury formula is unnecessary for small tolerances. Furthermore, comparing the Ï„ = 10 âˆ’6 and Ï„ = 10 âˆ’10 columns of Table <ref type="table" coords="7,489.33,660.30,4.25,12.39">3</ref> shows that in all but one case, smaller tolerances give similar or better performance than larger tolerances. The one exception is orthog without the Woodbury formula, likely due to the excessive growth.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="11,69.23,219.05,225.89,9.64;11,69.23,227.02,226.00,9.64;11,69.23,234.99,224.82,9.64;11,68.81,242.96,226.31,9.64;11,69.23,250.93,113.15,9.64" xml:id="b0">
	<analytic>
		<title level="a" type="main">Novel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs</title>
		<author>
			<persName coords=""><forename type="first">Ahmad</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azzam</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<idno type="DOI">10.1145/3079079.3079103</idno>
		<ptr target="https://doi.org/10.1145/3079079.3079103" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Supercomputing (ICS &apos;17)</title>
				<meeting>the International Conference on Supercomputing (ICS &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,258.90,225.89,9.64;11,69.23,266.87,225.64,9.64;11,69.03,274.84,225.02,9.64;11,69.07,282.81,194.86,9.64" xml:id="b1">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Flat and Hierarchical Low-Rank Matrix Formats: The Multilevel Block Low-Rank Format</title>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><forename type="middle">R</forename><surname>Amestoy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alfredo</forename><surname>Buttari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Yves L'</forename><surname>Excellent</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Theo</forename><forename type="middle">A</forename><surname>Mary</surname></persName>
		</author>
		<idno type="DOI">10.1137/18M1182760</idno>
		<ptr target="https://doi.org/10.1137/18M1182760" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="A1414" to="A1442" />
			<date type="published" when="2019-01">2019. Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,290.78,224.81,9.64;11,69.23,298.75,225.57,9.64;11,69.23,306.72,224.97,9.64" xml:id="b2">
	<analytic>
		<title level="a" type="main">A Modified Schur-complement Method for Handling Dense Columns in Interior-Point Methods for Linear Programming</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Knud</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Andersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/232826.232937</idno>
		<ptr target="https://doi.org/10.1145/232826.232937" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="348" to="356" />
			<date type="published" when="1996-09">1996. Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,314.69,224.95,9.64;11,69.23,322.67,224.81,9.64;11,69.23,330.64,221.31,9.64" xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerating the Solution of Linear Systems by Iterative Refinement in Three Precisions</title>
		<author>
			<persName coords=""><forename type="first">Erin</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<idno type="DOI">10.1137/17M1140819</idno>
		<ptr target="https://doi.org/10.1137/17M1140819" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="A817" to="A847" />
			<date type="published" when="2018-01">2018. Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,338.61,225.89,9.64;11,69.23,346.58,224.81,9.64;11,68.99,354.55,225.82,9.64;11,69.23,362.52,24.81,9.64" xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrated Simulation and Analysis of Super Large Slotted Waveguide Array</title>
		<author>
			<persName coords=""><forename type="first">Chang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shugang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhongchao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xunwang</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1137/17M1140819</idno>
	</analytic>
	<monogr>
		<title level="j">Applied Computational Electromagnetics Society Journal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="813" to="820" />
			<date type="published" when="2020-07">2020. July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,370.49,224.81,9.64;11,69.23,378.46,225.57,9.64;11,69.23,386.43,200.96,9.64" xml:id="b5">
	<analytic>
		<title level="a" type="main">Batched Triangular Dense Linear Algebra Kernels for Very Small Matrix Sizes on GPUs</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Charara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Keyes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hatem</forename><surname>Ltaief</surname></persName>
		</author>
		<idno type="DOI">10.1145/3267101</idno>
		<ptr target="https://doi.org/10.1145/3267101" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2019-05">2019. May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,394.40,224.99,9.64;11,69.23,402.37,225.58,9.64;11,69.07,410.34,139.03,9.64" xml:id="b6">
	<analytic>
		<title level="a" type="main">Stability of Block LU Factorization</title>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
		<idno type="DOI">10.1002/nla.1680020208</idno>
		<ptr target="https://doi.org/10.1002/nla.1680020208" />
	</analytic>
	<monogr>
		<title level="j">Numerical Linear Algebra with Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="173" to="190" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,418.31,225.59,9.64;11,69.23,426.28,224.81,9.64;11,69.23,434.25,225.27,9.64;11,69.23,442.22,224.81,9.64" xml:id="b7">
	<analytic>
		<title level="a" type="main">A Survey of Recent Developments in Parallel Implementations of Gaussian Elimination</title>
		<author>
			<persName coords=""><forename type="first">Simplice</forename><surname>Donfack</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathieu</forename><surname>Faverge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakub</forename><surname>Kurzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ichitaro</forename><surname>Yamazaki</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpe.3306</idno>
		<ptr target="https://doi.org/10.1002/cpe.3306" />
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1292" to="1309" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,450.19,225.59,9.64;11,69.23,458.16,224.81,9.64;11,69.23,466.13,225.57,9.64;11,69.23,474.10,167.70,9.64" xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixing LU and QR Factorization Algorithms to Design High-Performance Dense Linear Algebra Solvers</title>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Mathieu Faverge</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julien</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bradley</forename><surname>Langou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yves</forename><surname>Lowery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dongarra</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpdc.2015.06.007</idno>
		<ptr target="https://doi.org/10.1016/j.jpdc.2015.06.007" />
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="32" to="46" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,482.07,225.89,9.64;11,69.23,490.04,225.89,9.64;11,69.23,498.01,225.58,9.64;11,69.23,505.98,225.59,9.64;11,69.23,513.95,185.83,9.64" xml:id="b9">
	<analytic>
		<title level="a" type="main">SLATE: Design of a Modern Distributed and Accelerated Linear Algebra Library</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakub</forename><surname>Kurzak</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Charara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Asim</forename><surname>Yarkhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<idno type="DOI">10.1145/3295500.3356223</idno>
		<ptr target="https://doi.org/10.1145/3295500.3356223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;19)</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;19)<address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,521.92,224.82,9.64;11,69.23,529.89,225.57,9.64;11,69.23,537.86,163.99,9.64" xml:id="b10">
	<analytic>
		<title level="a" type="main">LU Factorization Algorithms on Distributed-Memory Multiprocessor Architectures</title>
		<author>
			<persName coords=""><forename type="first">George</forename><forename type="middle">A</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><forename type="middle">H</forename><surname>Romine</surname></persName>
		</author>
		<idno type="DOI">10.1137/0909042</idno>
		<ptr target="https://doi.org/10.1137/0909042" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="649" />
			<date type="published" when="1988-07">1988. July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,545.83,225.88,9.64;11,69.03,553.80,162.74,9.64" xml:id="b11">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Gene</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Chales</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Van Loan</surname></persName>
		</author>
		<idno type="DOI">10.1137/0909042</idno>
		<title level="m">Matrix Computations</title>
				<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The John Hopkins University Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>fourth ed.</note>
</biblStruct>

<biblStruct coords="11,69.23,561.77,225.89,9.64;11,69.23,569.74,194.12,9.64" xml:id="b12">
	<analytic>
		<title level="a" type="main">Note on the Generalized Inverse of a Matrix Product</title>
		<author>
			<persName coords=""><forename type="first">T</forename><forename type="middle">N E</forename><surname>Greville</surname></persName>
		</author>
		<idno type="DOI">10.1137/1008107</idno>
		<ptr target="https://doi.org/10.1137/1008107" />
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="518" to="521" />
			<date type="published" when="1966-10">1966. Oct. 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,85.80,224.81,9.64;11,333.39,93.77,225.58,9.64;11,333.23,101.74,131.93,9.64" xml:id="b13">
	<analytic>
		<title level="a" type="main">CALU: A Communication Optimal LU Factorization Algorithm</title>
		<author>
			<persName coords=""><forename type="first">Laura</forename><surname>Grigori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hua</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.1137/100788926</idno>
		<ptr target="https://doi.org/10.1137/100788926" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1317" to="1350" />
			<date type="published" when="2011-10">2011. Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,109.71,225.89,9.64;11,333.39,117.68,212.82,9.64" xml:id="b14">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Hackbusch</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-47324-5_1</idno>
		<title level="m">Hierarchical Matrices: Algorithms and Analysis</title>
				<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,125.65,224.81,9.64;11,333.23,133.62,137.28,9.64" xml:id="b15">
	<analytic>
		<title level="a" type="main">Updating the Inverse of a Matrix</title>
		<author>
			<persName coords=""><forename type="first">William</forename><forename type="middle">W</forename><surname>Hager</surname></persName>
		</author>
		<idno type="DOI">10.1137/1031049</idno>
		<ptr target="https://doi.org/10.1137/1031049" />
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="239" />
			<date type="published" when="1989-06">1989. June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,141.59,224.81,9.64;11,333.39,149.56,225.89,9.64;11,333.39,157.53,113.15,9.64" xml:id="b16">
	<analytic>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Higham</surname></persName>
		</author>
		<idno type="DOI">10.1137/1.9780898718027</idno>
		<ptr target="https://doi.org/10.1137/1.9780898718027" />
	</analytic>
	<monogr>
		<title level="m">Accuracy and Stability of Numerical Algorithms</title>
				<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct coords="11,333.39,165.50,224.81,9.64;11,333.39,173.47,224.82,9.64;11,333.18,181.44,225.02,9.64;11,333.39,189.41,224.82,9.64;11,333.39,197.38,224.81,9.64" xml:id="b17">
	<analytic>
		<title level="a" type="main">An Analysis of System Balance and Architectural Trends Based on Top500 Supercomputers</title>
		<author>
			<persName coords=""><forename type="first">Awais</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyogi</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sudharshan</forename><forename type="middle">S</forename><surname>Vazhkudai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3432261.3432263</idno>
		<ptr target="https://doi.org/10.1145/3432261.3432263" />
	</analytic>
	<monogr>
		<title level="m">The International Conference on High Performance Computing in Asia-Pacific Region (HPC Asia 2021)</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,203.71,225.99,9.64;11,333.39,211.68,224.81,9.64;11,333.39,219.65,224.81,9.64;11,333.39,227.62,225.89,9.64;11,333.39,235.59,225.58,9.64;11,333.39,243.56,225.59,9.64;11,333.39,251.53,192.97,9.64" xml:id="b18">
	<analytic>
		<title level="a" type="main">On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal Matrix Factorizations</title>
		<author>
			<persName coords=""><forename type="first">Grzegorz</forename><surname>Kwasniewski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marko</forename><surname>Kabic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandros</forename><forename type="middle">Nikolaos</forename><surname>Ziogas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jens</forename><forename type="middle">Eirik</forename><surname>Saethre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">AndrÃ©</forename><surname>Gaillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timo</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anton</forename><surname>Kozhevnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joost</forename><surname>Vandevondele</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458817.3476167</idno>
		<ptr target="https://doi.org/10.1145/3458817.3476167" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;21)</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,259.50,224.81,9.64;11,333.39,267.47,224.81,9.64;11,333.06,275.44,225.91,9.64;11,333.15,283.41,225.02,9.64" xml:id="b19">
	<analytic>
		<title level="a" type="main">AORSA Full Wave Calculations of Helicon Waves in DIII-D and ITER</title>
		<author>
			<persName coords=""><forename type="first">Cornwall</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nicola</forename><surname>Bertelli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lee</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><forename type="middle">L</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Masanori</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jin</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">I</forename><surname>Pinsker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ron</forename><surname>Prater</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-4326/aab96d</idno>
		<ptr target="https://doi.org/10.1088/1741-4326/aab96d" />
	</analytic>
	<monogr>
		<title level="j">Nuclear Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">066004</biblScope>
			<date type="published" when="2018-04">2018. April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,291.38,224.81,9.64;11,333.39,299.35,224.81,9.64;11,333.39,307.32,225.64,9.64;11,333.17,315.29,90.21,9.64" xml:id="b20">
	<analytic>
		<title level="a" type="main">Making Sparse Gaussian Elimination Scalable by Static Pivoting</title>
		<author>
			<persName coords=""><forename type="first">Xiaoye</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.1998.10030</idno>
		<ptr target="https://doi.org/10.1109/SC.1998.10030" />
	</analytic>
	<monogr>
		<title level="m">SC &apos;98: Proceedings of the 1998 ACM/IEEE Conference on Supercomputing</title>
				<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="34" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,323.26,224.82,9.64;11,333.39,331.23,224.81,9.64;11,333.39,342.44,225.89,6.23;11,333.39,347.17,225.27,9.64;11,333.39,355.14,69.80,9.64" xml:id="b21">
	<analytic>
		<title level="a" type="main">Threshold Pivoting for Dense LU Factorization</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ScalAH56622.2022.00010</idno>
		<ptr target="https://doi.org/10.1109/ScalAH56622.2022.00010" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Workshop on Latest Advances in Scalable Algorithms for Large-Scale Heterogeneous Systems</title>
		<imprint>
			<biblScope unit="page" from="34" to="42" />
			<date type="published" when="2022">2022. 2022</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,363.11,226.00,9.64;11,333.39,371.08,224.81,9.64;11,333.39,382.29,224.82,6.23;11,332.97,387.02,226.32,9.64;11,333.39,394.99,30.72,9.64" xml:id="b22">
	<analytic>
		<title level="a" type="main">Replacing Pivoting in Distributed Gaussian Elimination with Randomized Techniques</title>
		<author>
			<persName coords=""><forename type="first">Neil</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Piotr</forename><surname>Luszczek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ScalA51936.2020.00010</idno>
		<ptr target="https://doi.org/10.1109/ScalA51936.2020.00010" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems</title>
		<imprint>
			<biblScope unit="page" from="35" to="43" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,402.96,224.81,9.64;11,333.39,410.93,225.89,9.64;11,333.23,418.90,60.66,9.64" xml:id="b23">
	<analytic>
		<title level="a" type="main">Numerically Safe Gaussian Elimination with No Pivoting</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.laa.2017.04.007</idno>
		<ptr target="https://doi.org/10.1016/j.laa.2017.04.007" />
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">527</biblScope>
			<biblScope unit="page" from="349" to="383" />
			<date type="published" when="2017-08">2017. Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,426.87,224.81,9.64;11,333.39,434.84,224.81,9.64;11,333.39,442.81,150.91,9.64" xml:id="b24">
	<analytic>
		<title level="a" type="main">Random Butterfly Transformations with Applications in Computational Linear Algebra</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">Stott</forename><surname>Parker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.laa.2017.04.007</idno>
		<idno>CSD-950023</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Science Department</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="11,333.39,450.78,225.89,9.64;11,333.39,458.75,225.50,9.64;11,333.39,466.72,28.80,9.64" xml:id="b25">
	<analytic>
		<title level="a" type="main">Modifying Pivot Elements in Gaussian Elimination</title>
		<author>
			<persName coords=""><forename type="first">Gilbert</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1090/S0025-5718-1974-0343559-8</idno>
		<idno>S0025-5718-1974- 0343559-8</idno>
		<ptr target="https://doi.org/10.1090/" />
	</analytic>
	<monogr>
		<title level="j">Math. Comp</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="537" to="542" />
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,474.69,225.63,9.64;11,333.17,482.66,103.65,9.64" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Todd</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-0348-7286-7</idno>
		<title level="m">Basic Numerical Mathematics. BirkhÃ¤user, Basel</title>
				<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,490.63,224.81,9.64;11,333.39,498.60,225.88,9.64;11,333.39,506.57,88.91,9.64" xml:id="b27">
	<analytic>
		<title level="a" type="main">Average-Case Stability of Gaussian Elimination</title>
		<author>
			<persName coords=""><forename type="first">Lloyd</forename><forename type="middle">N</forename><surname>Trefethen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
		<idno type="DOI">10.1137/0611023</idno>
		<ptr target="https://doi.org/10.1137/0611023" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="360" />
			<date type="published" when="1990-07">1990. July 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,514.54,225.59,9.64;11,333.16,522.51,140.85,9.64" xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">Max</forename><forename type="middle">A</forename><surname>Woodbury</surname></persName>
		</author>
		<idno type="DOI">10.1137/0611023</idno>
	</analytic>
	<monogr>
		<title level="j">Inverting Modified Matrices. Memorandum Report</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="1950">1950</date>
		</imprint>
		<respStmt>
			<orgName>Statistical Research Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,530.48,224.81,9.64;11,333.39,538.45,225.57,9.64;11,333.39,546.42,192.73,9.64" xml:id="b29">
	<analytic>
		<title level="a" type="main">A Note on the Stability of Solving a Rank-p Modification of a Linear System by the Sherman-Morrison-Woodbury Formula</title>
		<author>
			<persName coords=""><forename type="first">E</forename><forename type="middle">L</forename><surname>Yip</surname></persName>
		</author>
		<idno type="DOI">10.1137/0907034</idno>
		<ptr target="https://doi.org/10.1137/0907034" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="507" to="513" />
			<date type="published" when="1986-04">1986. April 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,554.39,224.81,9.64;11,333.39,562.36,224.82,9.64" xml:id="b30">
	<analytic>
		<title level="a" type="main">A Practical Solution for KKT Systems</title>
		<author>
			<persName coords=""><forename type="first">Hong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianlin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11075-007-9129-8</idno>
		<ptr target="https://doi.org/10.1007/s11075-007-9129-8" />
	</analytic>
	<monogr>
		<title level="j">Numerical Algorithms</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="105" to="119" />
			<date type="published" when="2007-10">2007. Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,570.33,224.97,9.64;11,333.18,578.30,163.66,9.64" xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Zielke</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02268390</idno>
		<ptr target="https://doi.org/10.1007/BF02268390" />
	</analytic>
	<monogr>
		<title level="j">Testmatrizen mit maximaler Konditionszahl. Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="1974-03">1974. March 1974</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
