<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Autonomous Parallelization of Transformer Model Inference on Heterogeneous Edge Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2024-05-30">2024-05-30</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,110.16,131.56,60.70,10.59;1,170.86,129.30,1.00,7.77"><forename type="first">Juhyeon</forename><surname>Lee</surname></persName>
							<idno type="ORCID">0009-0008-2042-1159</idno>
						</author>
						<author>
							<persName coords="1,275.89,131.56,61.53,10.59"><forename type="first">Insung</forename><surname>Bahk</surname></persName>
							<email>insung3511@hallym.ac.kr</email>
							<idno type="ORCID">0009-0001-0177-4636</idno>
						</author>
						<author>
							<persName coords="1,437.22,131.56,66.85,10.59"><forename type="first">Hoseung</forename><surname>Kim</surname></persName>
							<idno type="ORCID">0009-0005-1454-3617</idno>
						</author>
						<author>
							<persName coords="1,112.23,215.98,59.24,10.59"><forename type="first">Sinjin</forename><surname>Jeong</surname></persName>
							<email>sjjeong@pusan.ac.kr</email>
							<idno type="ORCID">0000-0002-6016-1696</idno>
						</author>
						<author>
							<persName coords="1,277.89,215.98,56.21,10.59"><forename type="first">Suyeon</forename><surname>Lee</surname></persName>
							<idno type="ORCID">0000-0002-5526-6127</idno>
						</author>
						<author>
							<persName coords="1,428.07,215.98,75.02,10.59;1,503.09,213.73,1.73,7.77"><forename type="first">Donghyun</forename><surname>Min</surname></persName>
							<idno type="ORCID">0000-0002-6043-9264</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electric Engineering</orgName>
								<orgName type="institution">Sogang University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Artificial Intelligence Convergence</orgName>
								<orgName type="institution">Hallym University Chuncheon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University Suwon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Electronic Engineering</orgName>
								<orgName type="institution">Pusan University Pusan</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">Sogang University Seoul</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Autonomous Parallelization of Transformer Model Inference on Heterogeneous Edge Devices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 38th ACM International Conference on Supercomputing</title>
						<meeting>the 38th ACM International Conference on Supercomputing						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="page" from="50" to="61"/>
							<date type="published" when="2024-05-30" />
						</imprint>
					</monogr>
					<idno type="MD5">C7DA958D9456DB7EA2BD83AE9528A61F</idno>
					<idno type="DOI">10.1145/3650200.3656628</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-07-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Parallel algorithms</term>
					<term>Distributed algorithms</term>
					<term>Cooperation and coordination Transformer architecture, model parallelism and partitioning, heterogeneous edge environments</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The utilization of advancing transformer-based deep neural network (DNN) models in edge environments holds the promise of improving productivity for intelligent tasks. However, deploying these models on edge devices with limited resources encounters significant performance challenges. Previous solutions have attempted to distribute computation tasks across devices and perform parallel inferences but often fall short of meeting service-level objectives (SLO). This limitation arises from their inability to effectively harness parallelization in transformer-based models and consider the resource diversity of edge devices. In this paper, we propose Hepti, a practical framework designed to facilitate parallel inference of transformer-based DNN models on heterogeneous edge environments. Hepti is armed with: 1) an understanding of transformer model architecture to enable effective parallel inference and 2) dynamic workload optimization to adapt to changing network and device resource capabilities. Our evaluations confirmed that the Hepti autonomously assesses the resource diversity of edge devices and network status. Furthermore, Hepti achieves a maximum performance improvement of 49.1% and 37.1% compared to the local inference approach and state-of-the-art model parallelisms on the BERT-Large model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Edge AI is a paradigm of operating AI workflows near the users at the network's edge, close to data sources. Unlike the traditional AI workflow, which sends data to the cloud, Edge AI benefits from low latency and low network traffic <ref type="bibr" coords="1,448.49,460.40,9.23,7.94" target="#b3">[4,</ref><ref type="bibr" coords="1,459.96,460.40,10.27,7.94" target="#b12">13,</ref><ref type="bibr" coords="1,472.47,460.40,10.27,7.94" target="#b20">21,</ref><ref type="bibr" coords="1,484.98,460.40,10.05,7.94" target="#b41">42]</ref>. Edge AI enables GPU-trained models to be performed in various edge device environments <ref type="bibr" coords="1,354.99,482.31,9.32,7.94" target="#b2">[3,</ref><ref type="bibr" coords="1,366.56,482.31,10.30,7.94" target="#b20">21,</ref><ref type="bibr" coords="1,379.12,482.31,10.11,7.94" target="#b34">35]</ref>, including non-GPU devices, expanding DNN models to real-world applications. This expansion is expected to contribute to the growth of the Edge AI market, which is projected to reach $3.1 billion by 2027 <ref type="bibr" coords="1,422.38,515.19,13.50,7.94" target="#b9">[10,</ref><ref type="bibr" coords="1,438.12,515.19,10.31,7.94" target="#b18">19,</ref><ref type="bibr" coords="1,450.68,515.19,10.13,7.94" target="#b23">24]</ref>.</p><p>Recently, state-of-the-art DNNs, especially transformer-based models, have revolutionized intelligent tasks in various fields, such as computer vision and natural language processing <ref type="bibr" coords="1,515.54,548.07,9.43,7.94" target="#b6">[7,</ref><ref type="bibr" coords="1,527.42,548.07,6.18,7.94" target="#b7">8,</ref><ref type="bibr" coords="1,536.04,548.07,10.35,7.94" target="#b10">11,</ref><ref type="bibr" coords="1,548.84,548.07,10.35,7.94" target="#b19">20,</ref><ref type="bibr" coords="1,317.96,559.03,10.28,7.94" target="#b21">22,</ref><ref type="bibr" coords="1,330.48,559.03,10.28,7.94" target="#b35">36,</ref><ref type="bibr" coords="1,343.01,559.03,10.07,7.94" target="#b36">37]</ref>. The overwhelming effectiveness of transformer models has made their deployment in edge environments crucial, shaping the landscape of upcoming Edge AI. For instance, such integration can increase real-time productivity and personalized interactions like detecting heartbeat frequency <ref type="bibr" coords="1,445.47,602.86,14.72,7.94" target="#b32">[33]</ref> and voice recognition <ref type="bibr" coords="1,543.94,602.86,9.39,7.94" target="#b5">[6]</ref>.</p><p>However, deploying transformer-based DNN models in practical edge environments poses a critical challenge due to performance delays, which fail to meet service-level objectives (SLOs). This is primarily attributed to the limited computing resources of most edge hardware, while transformer model inference processes demand significant computational resources <ref type="bibr" coords="1,446.62,668.62,9.23,7.94" target="#b3">[4,</ref><ref type="bibr" coords="1,457.61,668.62,10.27,7.94" target="#b15">16,</ref><ref type="bibr" coords="1,469.64,668.62,10.05,7.94" target="#b24">25]</ref>. A case in point is the BERT-Large model <ref type="bibr" coords="1,388.77,679.57,9.37,7.94" target="#b6">[7]</ref>, which is a representative NLP transformer model occupying 336 million parameters <ref type="bibr" coords="1,467.42,690.53,13.27,7.94" target="#b31">[32]</ref>. Similarly, the vision transformer (e.g., ViT-Large) <ref type="bibr" coords="1,422.60,701.49,10.42,7.94" target="#b8">[9]</ref> requires approximately 478 billion FLOPs for a single inference <ref type="bibr" coords="2,163.91,264.64,13.49,7.94" target="#b26">[27]</ref>. On the other hand, non-GPU edge devices commonly used for Internet-of-Thing (IoT) devices typically integrate low-performance processors, such as the ARM Cortex-A72 in the Raspberry Pi 4 board <ref type="bibr" coords="2,200.36,297.52,9.39,7.94" target="#b1">[2]</ref>.</p><p>One promising approach to ensuring SLOs is to offload partial DNN model onto multiple nearby edge devices and parallelize the inference process <ref type="bibr" coords="2,121.33,330.40,9.44,7.94" target="#b4">[5,</ref><ref type="bibr" coords="2,133.23,330.40,10.35,7.94" target="#b11">12,</ref><ref type="bibr" coords="2,146.04,330.40,10.35,7.94" target="#b14">15,</ref><ref type="bibr" coords="2,158.84,330.40,10.35,7.94" target="#b16">17,</ref><ref type="bibr" coords="2,171.65,330.40,10.35,7.94" target="#b22">23,</ref><ref type="bibr" coords="2,184.46,330.40,10.35,7.94" target="#b31">32,</ref><ref type="bibr" coords="2,197.27,330.40,10.35,7.94" target="#b37">38,</ref><ref type="bibr" coords="2,210.08,330.40,10.35,7.94" target="#b39">40,</ref><ref type="bibr" coords="2,222.88,330.40,10.21,7.94" target="#b40">41]</ref>. The essence of DNN inference parallelization lies in accurately distributing computational tasks across edge devices <ref type="bibr" coords="2,186.64,352.32,13.47,7.94" target="#b14">[15,</ref><ref type="bibr" coords="2,202.35,352.32,10.10,7.94" target="#b38">39]</ref>. Attaining an optimal workload distribution requires a comprehensive understanding of the factors: <ref type="bibr" coords="2,97.21,374.23,9.57,7.94" target="#b0">(1)</ref> The architecture of the target DNN model; <ref type="bibr" coords="2,267.96,374.23,9.57,7.94" target="#b1">(2)</ref> The available network status for communication among devices; <ref type="bibr" coords="2,284.88,385.19,9.70,7.94" target="#b2">(3)</ref> The computing resources available on each device. If an excessive number of tasks are assigned to resource-limited local edge devices or a substantial workload is offloaded through limited network bandwidth to remote edge devices, there is a high likelihood of failing to meet the SLOs.</p><p>There have been prior works that involved parallel and distributed inference of DNN models in heterogeneous environments with varying computing resources <ref type="bibr" coords="2,186.91,472.86,13.60,7.94" target="#b14">[15,</ref><ref type="bibr" coords="2,203.45,472.86,10.35,7.94" target="#b16">17,</ref><ref type="bibr" coords="2,216.74,472.86,10.35,7.94" target="#b37">38,</ref><ref type="bibr" coords="2,230.02,472.86,10.21,7.94" target="#b39">40]</ref>. For example, both CoEdge <ref type="bibr" coords="2,102.55,483.82,14.62,7.94" target="#b37">[38]</ref> and EdgeFlow <ref type="bibr" coords="2,172.56,483.82,14.62,7.94" target="#b25">[26]</ref> proposed workload partitioning algorithms to parallelize CNN-based model inference in diverse edge environments. In CNN-based models, the output feature map of the convolution (Conv) operation is typically used as input for the next Conv, resulting in a significant volume of Conv operations. These algorithms were mainly designed to divide computation tasks specifically for these Conv operations. However, the architecture of the transformer model is more complex, featuring a distinct self-attention mechanism <ref type="bibr" coords="2,148.65,571.49,14.70,7.94" target="#b36">[37]</ref> that significantly differs from Conv operations. For a BERT <ref type="bibr" coords="2,143.83,582.45,10.68,7.94" target="#b6">[7]</ref> example, it consists of self-attention, layer normalization, and Multi-Layer Perceptron (MLP). As shown in Figure <ref type="figure" coords="2,89.04,604.37,3.09,7.94" target="#fig_0">1</ref>, the self-attention structure performs MatMul, GeMM, and SoftMax, rather than a sequence of Conv operations. CoEdge and EdgeFlow algorithms are not tailored for handling the structure of transformer models (Lack of (1)). Consequently, they cannot be directly applied to parallelize transformer-based model inference.</p><p>On the other hand, Megatron-LM <ref type="bibr" coords="2,183.18,659.16,14.60,7.94" target="#b31">[32]</ref> proposed a strategy for the transformer model parallelism across multiple same GPUs. Specifically, Megatron-LM reported the model parallelism mechanism for large language models by considering the operational logic of selfattention and MLP layers together while also taking into account the memory constraints of GPUs. However, it should be noted that Megatron-LM operates under the assumption of homogeneous environments. Thus, the predefined model parallelism in Megatron-LM cannot guarantee optimization when faced with varying memory constraints of heterogeneous edge environments. Furthermore, it does not provide workload partitioning algorithms that consider network dynamics and computing capabilities of different computing devices (Lack of ( <ref type="formula" coords="2,397.99,363.27,3.23,7.94" target="#formula_1">2</ref>) and ( <ref type="formula" coords="2,426.00,363.27,2.89,7.94">3</ref>)). Hence, the proposed solution is still less practical for achieving best distributed DNN inference in diverse edge environments.</p><p>In this paper, we propose Hepti (Heterogeneous Edges' Parallel Transformer Inference), an inference framework, which supports heterogeneity-conscious parallelization of the transformer model on edge environments. Our main contributions are the following:</p><p>(1) Hepti supports three different parallel inference strategies by understanding the transformer architecture to cope with varying edge devices' memory status. (2) Hepti generates the acceptable workload partitioning during inference by considering the network status and computing capabilities of heterogeneous devices. (3) We have confirmed that Hepti reduces the latency of transformer model inference by up to 30.7% compared to existing transformer model parallelism methods. (4) In the face of changing network conditions and computing capabilities, Hepti exhibits greater robustness than existing workload partitioning methodologies in heterogeneous edge environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Architecture of Transformer-based DNNs</head><p>The transformer model, a prominent deep learning architecture in natural language processing, uses a self-attention mechanism <ref type="bibr" coords="2,542.96,635.74,13.30,7.94" target="#b33">[34]</ref>. This mechanism computes the quantified relevance for each input element based on its relationships with all other elements in the input sequence. The self-attention is employed in parallel, creating multi-heads per input tensor. It allows the model to consider contextual information, ensuring an improved understanding of long-range connections of the input sequence. Figure <ref type="figure" coords="2,512.64,701.49,4.12,7.94" target="#fig_0">1</ref> depicts the basic structure of the transformer. It is composed of a multi-head self-attention block and a feed-forward block such as MLP. Both layer normalization and residual connection exist at the beginning and end of the self-attention and MLP blocks, respectively. The details of the components are as follows.</p><formula xml:id="formula_0">R 2 R 1 L LR 2 LR 1 m k n 1 n 2 k (a) R column-wise MatMul k n R L 1 R L 2 R m 1 m 2 L 1 L 2 k (b) L row-wise MatMul L 1 R 1 k 2 k 1 k 1 k 2 n R 1 R 2 L 2 L 1 m m n L 2 R 2 m n LR<label>(</label></formula><p>Multi-head Self-Attention. In the multi-head self-attention block, Query (Q), Key (K), and Value (V) are obtained by multiplying the input with each corresponding weight matrix and splitting them by the number of heads. For each head, attention scores are calculated by performing MatMul for Q and K. Next, the softmax function is used for these scores, generating attention weights (U). These weights are used to compute a weighted sum of the V, generating an output representation of each head (UV). After the outputs of each head are merged (Y), the final multi-head selfattention output is produced through GeMM.</p><p>Multi-Layer Perceptron. The input of the MLP block is the output that comes from the self-attention mechanism. This input undergoes a linear transformation using GeMM operation. Subsequently, the result is passed through an activation function, such as the GeLU <ref type="bibr" coords="3,100.14,490.08,13.28,7.94" target="#b13">[14]</ref>. The activation function introduces non-linearity to the model, allowing it to capture features in the data. Lastly, the final output is generated through another GeMM operation, aiming to capture the higher-level features within the input.</p><p>Layer Normalization and Residual Connection. The final output of the self-attention and MLP block is added to the original input (residual connection) and then normalized across all channels (layer normalization) within the input. It helps stabilize training and prevents vanishing gradient problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior Works for DNN Model Parallelism</head><p>Within the domain of DNN model parallelism, the common approach involves distributing tensor operations across multiple computational devices. An example of such an approach is the fused-tile partitioning (FTP) technique <ref type="bibr" coords="3,159.36,646.70,13.29,7.94" target="#b39">[40]</ref>. The FTP disassembles Conv layers into sub-tasks for distribution among edge devices. The FTP process entails fusing layers and backpropagating the input's receptive field to compute the output tensor for each task. It can reduce the memory traffic when accessing tensor data for fused Conv layers. However, an issue arises due to the potential input tensor overlap, leading to redundant computations, such as halo cells within tiled convolutions. Moreover, the FTP is limited to CNN models, diverging notably from transformer architectures.</p><p>In contrast, the state-of-the-art approaches <ref type="bibr" coords="3,490.55,120.67,13.61,7.94" target="#b28">[29,</ref><ref type="bibr" coords="3,506.49,120.67,11.59,7.94" target="#b31">32]</ref> have been proposed for parallelizing both multi-head self-attention and MLP blocks of the transformer model within either multiple GPUs or TPUs, respectively. For multi-head self-attention, they distribute the 1-dimensional tiled (1D-tiled) weight parameters along with a column for MatMul operations associated with Q, K, and V across computing devices (depicted as (a) in Figure <ref type="figure" coords="3,479.75,186.42,2.92,7.94" target="#fig_1">2</ref>). This design allows each MatMul calculation corresponding to each attention head to be performed independently on a GPU, eliminating the need for inter-GPU communication for self-attention. For the last GeMM operation within self-attention, it is confirmed that further splitting the weight into 2-dimension (2D-tiled) makes more efficient in terms of communication cost according to the study <ref type="bibr" coords="3,510.91,252.18,13.36,7.94" target="#b28">[29]</ref>.</p><p>In the case of MLP of the Megatron-LM, the weight of the first GeMM is split by columns in an R column-wise manner, while the weight of the second GeMM is split by rows in a block-wise manner (illustrated as (c) in Figure <ref type="figure" coords="3,414.47,296.01,2.89,7.94" target="#fig_1">2</ref>). The design allows the second GeMM independently on each GPU using the output of the first GeMM without communication. In the case of MLP of the study <ref type="bibr" coords="3,527.48,317.93,13.39,7.94" target="#b28">[29]</ref>, the weight of the first GeMM is 2D-tiled. The weight of the second GeMM is also 2D-tiled and the activation tensor is 1D-tiled by columns (illustrated as (c) in Figure <ref type="figure" coords="3,450.90,350.81,4.22,7.94" target="#fig_1">2</ref> with the 2D-tiled weight in the case). Compared to the Megatron-LM, in which partial-sum must be transferred between devices, this method is more efficient in terms of communication costs because it requires only exchanges for partial-sum involved in tiled areas of the final output. However, they unavoidably need all-reduce (i.e., collective communication consisting of reduce-scatter and all-gather operations) operation for the intermediate results among devices to perform the subsequent operation in self-attention and MLP block. Their model parallelism also assumes homogeneous GPUs or TPUs rather than heterogeneous environments. Thus, they cannot determine the acceptable partitioning for distributing workloads in heterogeneous edge environments. This highlights the need for a more practical parallelism strategy to achieve acceptable distributed DNN inference across diverse edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Heterogeneity-aware Workload Partitioning</head><p>The workload of the neural network encompasses the tensor operations needed for layer execution. In previous research <ref type="bibr" coords="3,517.59,559.03,13.42,7.94" target="#b14">[15,</ref><ref type="bibr" coords="3,533.25,559.03,10.28,7.94" target="#b16">17,</ref><ref type="bibr" coords="3,545.77,559.03,10.06,7.94" target="#b37">38]</ref>, various partitioning methods have been explored to offload specific tensor operations within CNN workloads onto different heterogeneous devices, enabling parallel processing. The primary aim of these methods is to satisfy the service-level objectives (SLOs), such as latency metrics. To achieve this goal, they collect device-specific profiled data and use this information to determine how workloads should be partitioned before inference. The profiled data includes several parameters such as computing intensity, CPU frequency, maximum available memory capacity for each device, and network status. Based on the profiled data, they make optimal partitioning decisions for parallel inference on heterogeneous devices. Specifically, they determine how to divide the input tensor along the height dimension in CNN models, allowing each device to execute  tiled Conv operations. This process is only activated during the setup phase whenever a DNN application is launched. Once the decision is laid out, their DNN inference runtime engines statically adhere to the predefined partitioning decision until the entire task is completed. However, when dealing with transformers that integrate both self-attention and MLP components, applying conventional partitioning methodologies dedicated to Conv operation is not immediately feasible. Furthermore, if the actual values of profiled data change during the inference phase, it can fail to meet the SLOs. Since existing partitioning algorithms operate under the assumption of static network or device capabilities, they do not change predetermined decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>Lack of Understanding of Transformer Parallelism. Existing workload partitioning algorithms designed for parallel inference on heterogeneous devices lack an understanding of the transformer architecture. These algorithms were initially tailored for partitioning CNN-based models, primarily relying on dividing tensor by height dimension for Conv operation. In contrast, the transformer architecture heavily relies on MatMul and GeMM operations. This primitive operation is frequently used in self-attention and MLP. Therefore, as depicted in Figure <ref type="figure" coords="4,130.21,507.22,3.01,7.94" target="#fig_1">2</ref>, MatMul-dedicated parallelization strategies are required based on matrix split in a row or column direction. In order to quantitatively analyze how the partitioning approach in Figure <ref type="figure" coords="4,89.13,540.10,4.21,7.94" target="#fig_1">2</ref> contributes to the actual performance gain of MatMul operation in an edge environment, we conduct experiments for the following scenarios. The edge device that triggers DNN inference is a primary device and another is a secondary device. The detailed experimental setup is described in Section 5.</p><p>• Scenario 1: When MatMul operation is serialized on a single primary edge device • Scenario 2: When MatMul operation is parallelized on two heterogeneous edge devices (Case (b) in Figure <ref type="figure" coords="4,224.69,631.75,3.42,7.94" target="#fig_1">2</ref>)</p><p>Figure <ref type="figure" coords="4,89.20,646.70,4.15,7.94" target="#fig_3">3</ref> (a) shows the execution time of each scenario. We conducted MatMul of two matrices with sizes [m, 1024] and [1024, 1024] by varying the value of m to change the matrix size. The network bandwidth and the offloading ratio from primary to secondary devices are fixed at 940 Mbps and at 90%, respectively. Because selfattention involves five MatMul operations in total, we performed MatMul five times in this experiment. In Scenario 1, the entire Mat-Mul takes place only on a primary device, where the computation delay itself accounts for the total latency of MatMul. In contrast, in Scenario 2, the execution time is a summation of the maximum computation delay for divided MatMul operations and the communication delay for tensor offloading. According to Figure <ref type="figure" coords="4,526.54,142.59,4.17,7.94" target="#fig_3">3</ref> (a), the execution time of Scenario 2 decreased by at least 25.1% and by as much as 64.2% compared to Scenario 1. This is mainly due to the significant reduction in computation delay in Scenario 2 compared to the communication delay incurred. For example, when the size of the MatMul is [256, 1024], Scenario 1 had a computation delay of 109.3 ms. On the other hand, in Scenario 2 where Mat-Mul is parallelized, each device's computation delay decreased by a maximum of 34.2 ms. The communication delay showed only 20.8 ms. We observed the total execution time eventually shortened to 55.1 ms. The performance gains obtained in the parallel processing of matrices between heterogeneous devices remain consistent even when the size of the MatMul operation varies. We recognized the need to devise a parallelization approach for the transformer model by applying the parallel operation method of MatMul to the conventional inference workflow.</p><p>Insufficient Workload Partitioning for Heterogeneous Devices. The DNN workload partitioning involves deciding how tensor operations within a layer should be divided and offloaded to different devices for processing. However, earlier approaches to transformer model parallelism focused solely on parallelizing layer execution within homogeneous environments. The absence of practical workload partitioning logic is crucial because the overall performance relies on the computing capabilities of individual devices involved in parallel execution, as well as the network bandwidth. Figure <ref type="figure" coords="4,342.49,416.56,4.09,7.94" target="#fig_3">3</ref> (b) provides a quantitative analysis of overall latency fluctuations while varying offloading ratios for five iterations of MatMul operation. In this experiment, we gradually adjusted the MatMul workload on the secondary device in 10% increments, upon the two network bandwidths (240 and 940 Mbps) and two computing capabilities (100% and 25% CPU utilization) of the secondary device. The value of m of the entire MatMul was kept fixed at 256. The detailed experimental setup is described in Section 5.</p><p>At a network bandwidth of 940 Mbps and CPU utilization of 100%, we observed that increasing the offloaded tensor operation from the primary device to the secondary device reduced computation delay while increasing communication delay. To illustrate, at a 10% offloading ratio, computation delay accounted for 89.3% of the total latency, while communication delay constituted 10.69%. On the other hand, at a 100% offloading ratio point, computation delay and communication delay made up 57.5% and 42.5%, respectively. We identified the best offloading point, resulting in a total latency of only 55.1 ms at a 90% offloading ratio. When the CPU utilization is limited to 25%, while keeping the same network bandwidth, the optimal offloading ratio shifted from 90% to 50%. This is because the performance gain from the secondary device compared to the communication delay has decreased. A similar phenomenon was also observed with a 240 Mbps network bandwidth. Specifically, if CPU utilization decreases from 100% to 25%, the optimal offloading point shifts from 70% to 30%. This experimental result underscores the fact that determining the optimal offloading ratio depends on the unique computing capabilities of each device. Next, when the network bandwidth is changed from 940 Mbps to 240 Mbps while keeping CPU utilization of 100%, we observed a change in the optimal offloading ratio from 90% to 70%. This is because each device's computation delay relative to the communication delay shifted as the network bandwidth decreased. We found that network bandwidth and computing capability variations also affect the optimal offloading ratio. These observations and implications motivated our research into the optimal parallel inference methods for MatMulcentric transformer models, taking into account the computing capabilities and network status of heterogeneous devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HEPTI DESIGN AND WORKFLOW</head><p>Hepti is a framework that supports both parallel inference of transformer model ( § 4.2) and a partitioning engine for dynamic workload partitioning ( § 4.3) in a heterogeneous environment. Hepti considers transformer architecture but provides three inference modes depending on how the weight is stationary for each edge device. It also takes into account variations in network status and computing capabilities of each edge device for acceptable workload partitioning decisions. With these technical functionalities, Hepti autonomously determines and employs the parallel execution strategies for transformer DNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Behavior of Hepti</head><p>When the inference is requested, then Hepti transitions to the profiling stage to identify each edge device available for inference and collect accurate profiles for the computing resources of each device. The profiling targets statistics data about the CPU frequency, available memory size, and network bandwidth of other devices. These factors are summarized in Table <ref type="table" coords="5,197.64,591.90,4.20,7.94" target="#tab_0">1</ref> (a) and will be leveraged for partitioning transformer workload.</p><p>Once the profiling is completed, the Hepti initiates the parallel inference. During the runtime process, Hepti can repeatedly transition between the partitioning and actual parallel inference stages. At the partitioning stage, Hepti determines how much workload each participating device will handle for computation. At the parallel inference stage, Hepti performs the parallel inference executions. The repeated transitions between these two stages are intended to maintain the acceptable workload partitioning decision, which can be changed during the inference process. Specifically, Hepti finds the acceptable workload partitioning that can minimize the delay of computation and communication between devices by using the proposed dynamic workload partitioning. If partitioning is performed in the middle of the inference process, Hepti does not transition to the profiling stage again. Instead, the primary device identifies the available memory size and computing intensity of the secondary devices. The size of memory information is conveyed at every moment when the secondary device returns intermediate inference results or exchanges tensors with the primary device. The primary device also estimates the computing intensity of the secondary device based on the time taken until the previously requested parallel inference results are returned and finally approximately estimates the network bandwidth. Whenever the workload partitioning decision for each device is made, the primary device offloads the allocated tensor workload to other devices and initiates their inferences. Simultaneously, the inference of the primary device begins.</p><p>The profiling and partitioning are triggered by the primary device while the runtime process of model inference involves parallel execution by both the primary and secondary devices. Any edge device can perform dual roles as the primary and the secondary device, owing to its capability for in-device transformer inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hepti Inference Workflow</head><p>Once the workload partitioning decision is made, Hepti can perform parallel inference using the primary and the secondary devices. Figure <ref type="figure" coords="5,333.15,559.03,4.25,7.94" target="#fig_4">4</ref> illustrates the three types of parallel inference workflows of Hepti. If the available memory on the secondary devices for offloading operations is sufficient to load the weight parameters, Hepti operates in a Weight-stationary (WS) manner. Motivated by Megatron-LM <ref type="bibr" coords="5,371.98,602.86,14.83,7.94" target="#b31">[32]</ref> and to cope with the case where the available memory of the secondary devices is severely limited, Hepti also supports 2D-tiled Weight-stationary (2D-tiled WS), and 1D-tiled Weight-stationary (1D-tiled WS). These manners are based on a method of splitting weight parameters to fit within the available memory size of secondary devices. According to the study <ref type="bibr" coords="5,542.33,657.66,13.49,7.94" target="#b28">[29]</ref>, it has been observed and validated that the 2D-tiled WS manner exhibits superior performance when the number of computing devices (or cores) exceeds 16. Otherwise, the 1D-tiled WS manner outperforms. Consequently, in the design strategy adopted by  Hepti, the 2D-tiled WS manner is prioritized in the case where a scalable inference design is needed due to a substantial number of devices and limited memory capacity. Conversely, the 1D-tiled WS manner is employed in alternative scenarios. The mode of Hepti is determined during the dynamic workload partitioning algorithm, described in Section 4.3.</p><p>Weight-stationary (WS) manner. First, in Hepti, the initial input data generated near the edge is transformed into matrix form. Then, it is copied to each secondary device to perform transformer operations in parallel. Afterward, Hepti performs R column-wise MatMul for K, Q, and V in self-attention with input matrix. It allows for facilitating subsequent splitting and merging operations on the resulting matrix, which are carried out with respect to attention heads. Then, for the last GeMM operation in self-attention, the WS uses the L row-wise MatMul. Accordingly, certain portions of matrices from each device are interchanged to construct input matrices for GeMM, organized in a row-wise partitioned manner. When using L row-wise MatMul, each device can independently perform computations up to the final layer normalization of the transformer. Figure <ref type="figure" coords="6,125.90,519.84,4.12,7.94" target="#fig_6">5</ref> (a) illustrates how the matrix tensors of each device are exchanged and processed in Hepti's WS mode. In Figure <ref type="figure" coords="6,289.96,530.80,4.09,7.94" target="#fig_6">5</ref> (a), each device exchanges data using all-gather communication for the final GeMM operation in self-attention. Consequently, each device acquires all tensors along with the row dimension, enabling L row-wise MatMul operations. Thus, each operation can be performed independently during the next layer normalization and MLP without further exchanges. After the iteration of the transformer, all tensors held by each device are propagated to prepare for the next iteration.</p><p>1D-tiled weight-stationary (1D-tiled WS) manner. Once all devices occupy the initial input matrix, the operation process of self-attention is identical to the Weight-stationary (WS) manner. To leverage the inherent parallelism in multi-head attention operations within self-attention, Hepti performs an R column-wise MatMul, enabling the matrix results corresponding to each attention head to be used as inputs directly on each device. However, the 1D-tiled WS manner uses block-wise MatMul to perform the final GeMM of selfattention. This approach divides the weight parameters, which are often larger than the input size. After the MatMul computation, collective communication such as all-reduce, involving reduce-scatter and all-gather operations between devices and additional matrix addition operations are needed, as illustrated in Figure <ref type="figure" coords="6,518.85,366.42,4.15,7.94" target="#fig_6">5</ref> (b). After self-attention, and during the layer normalization operation, both devices must perform the same operation for the entire matrix. The reason for redundant layer normalization operations on each device is to prepare for the subsequent R column-wise parallel MatMul operation in the MLP. The R column-wise parallel MatMul also divides weight parameters across devices to save memory. The final GeMM in MLP is again performed using block-wise MatMul, necessitating an all-gather operation for communicating tensors at the end.</p><p>2D-tiled weight-stationary (2D-tiled WS) manner. The 2Dtiled WS manner still follows the above process of the self-attention except the last GeMM operation. For the last GeMM, this approach takes block-wise MatMul as a basic but divides weight parameters along with both row and column. Specifically, to perform block-wise MatMul when weights are split on a row-wise basis, it is necessary to gather the columns of the tensor corresponding to the portion of the tensor operated on the rows of the weight. For instance, assuming four devices utilize 2D-tiled WS, as depicted in Figure <ref type="figure" coords="6,553.06,563.68,3.06,7.94" target="#fig_4">4</ref>, the first two devices share tensors, while the remaining two devices share tensors through an all-gather operation, as illustrated in Figure <ref type="figure" coords="6,343.32,596.56,11.84,7.94" target="#fig_6">5 (c</ref>). As a result, each device generates partial sums for the block-wise MatMul. To obtain the final result of the block-wise MatMul, an all-reduce operation is subsequently performed on the partial sums, concluding the GeMM operation of self-attention. This all-reduce process is also essential for the two GeMM operations in the MLP. Although the implementation of the 2D-tiled WS manner is more difficult compared to the 1D-tiled WS manner, it not only allows for a further reduction in the size of the weight split across devices but also proves to be communication-efficient due to the minimized amount of tensor movement between devices.</p><p>Hepti's WS manner involves relatively fewer tensor exchanges and computations compared to the 1D-and 2D-tiled WS manners. This is because the L row-wise parallel MatMul approach is used instead of block-wise MatMul. However, if the available memory on the secondary device significantly decreases during inference in Hepti's WS mode, it leads to an out-of-memory issue, forcing the inference to terminate prematurely. To prevent this, Hepti continuously monitors the available memory status of devices and dynamically switches between three modes. The memory status information is conveyed during intermediate tensor exchanges between the primary and the secondary device during inference. With the received memory information, Hepti can repeatedly execute the dynamic workload partitioning algorithm when the tensors are fully merged on the primary device at the specific step of the inference workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dynamic Workload Partitioning</head><p>The goal of Hepti is to meet the SLO by controlling the amount of workload distribution from a latency perspective for parallel inference. To achieve this, Hepti needs to solve the problem of finding an acceptable workload partitioning decision, considering the computational capacity of each device and the network status. To address this optimization problem, Hepti's partitioning engine employs a dynamic workload partitioning algorithm. This algorithm autonomously makes an acceptable decision during the runtime inference phase. For example, Hepti can make a new partitioning decision at the beginning of self-attention, as the primary device possesses the entire tensor at that point. In our dynamic workload partitioning algorithm, we set the mathematical model which is composed of constraints based on profiled data and solves an objective function to meet the latency deadline of SLO. In the following, we define a problem formulation using notations in Table <ref type="table" coords="7,265.71,431.64,3.06,7.94" target="#tab_0">1</ref>. Then, we present the dynamic workload partitioning algorithm.</p><p>Problem Formulation. Consider e edge devices and the MatMul operation between matrices of dimensions [m, k] and [k, n] where the m, k, and n are integers. There are several numerical constraints when deciding the partitioning size of each device. Equations ( <ref type="formula" coords="7,272.92,486.43,3.17,7.94">1</ref>) and (2) represent size constraints when splitting tensors. Specifically, these require that the 𝑎 𝑖 values in 𝜋 must be non-negative integers. The concatenation size of all partitioned tensors, along with the split direction, should match the length of the specific direction. For example, in the case of L row-wise parallel MLP of Hepti, the sum of 𝑎 𝑖 values must equal the m value.</p><formula xml:id="formula_1">∑︁ 𝑖 ∈𝑁 𝑎 𝑖 =          m if L row-wise MatMul n if R column-wise MatMul k if Block-wise MatMul (1) 𝑎 𝑖 ≥ 0, 𝑖 ∈ 𝑁<label>(2)</label></formula><p>Then, using Equation (3), we determine the workload size as the magnitude of elements involved in the dot-product operation for MatMul. The size of workload 𝑟 𝑏,𝑖 determined by 𝑎 𝑖 is constrained by the Equation ( <ref type="formula" coords="7,115.46,679.57,2.86,7.94">4</ref>). This equation restricts the workload size of the transformer block to ensure that it does not exceed the available memory of the device. </p><formula xml:id="formula_2">𝑟 𝑏,𝑖 =          𝑘 • 𝑛 • 𝑎 𝑖 if L row-wise MatMul 𝑘 • 𝑚 • 𝑎 𝑖 if R column-wise MatMul 𝑎 𝑖 • 𝑚 • 𝑛 if Block-wise MatMul (3) 𝑟 𝑏,𝑖 ≤ 𝑠 𝑖 , 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁 (4)</formula><p>While executing a single transformer block, Hepti experiences delays in both computation and communication aspects. We estimate each delay by approximating the computing cycles of specific partitioned tensor operations. The total computation latency of the 𝑖-th partition is evaluated using Equation ( <ref type="formula" coords="7,468.04,679.57,3.17,7.94" target="#formula_3">5</ref>) and <ref type="bibr" coords="7,490.98,679.57,8.59,7.94" target="#b5">(6)</ref>. To estimate the required CPU clock cycles for processing 𝑟 𝑏,𝑖 , we calculate 𝜌 𝑖 • 𝑟 𝑏,𝑖 . Then, we divide it by 𝑓 𝑖 to calculate the overall computation time.</p><p>We evaluate the total communication latency corresponding to the 𝑖-th device based on Equation <ref type="bibr" coords="8,170.41,98.75,8.64,7.94" target="#b5">(6)</ref>. The 𝑐 𝑏,𝑖 can be derived from 𝑎 𝑖 . For example, 𝑐 𝑏,𝑖 is (m -𝑎 𝑖 ) • n • 4 (byte size) for the last layer normalization block in a WS manner. By dividing 𝑐 𝑏,𝑖 by network bandwidth, we measured the time required for the necessary communication. The latency, 𝑇 ′ 𝑏𝑖 , required for collective communication (e.g., all-gather and reduce-scatter) is aggregated across devices, and the resulting sum is denoted as 𝑇 𝑥 𝑏 . Equation ( <ref type="formula" coords="8,244.20,164.79,3.17,7.94" target="#formula_5">7</ref>) defines the total latency 𝑇 , which is a sum of computation and communication latency. The 𝑇 represents the longest time taken to perform a single inference. Finally, our target objective is to decide on an optimal 𝜋 solution, minimizing the 𝑇 so that it does not exceed the execution deadline 𝐷.</p><formula xml:id="formula_3">𝑇 𝑐 𝑏𝑖 = 𝜌 𝑖 • 𝑟 𝑏,𝑖 𝑓 𝑖 (<label>5</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">𝑇 𝑥 𝑏 = ∑︁ 𝑖 ∈𝑁 𝑇 ′ 𝑏𝑖 , 𝑇 ′ 𝑏𝑖 = 𝑐 𝑏,𝑖 𝐵𝑊 𝑖,𝑗 𝑏 ∈ 𝐵 𝑖, 𝑗 ∈ 𝑁 (6) 𝑇 = ∑︁ 𝑏 ∈𝐵 max 𝑖 ∈𝑁 (𝑇 𝑐 𝑏𝑖 + 𝑇 𝑥 𝑏 )<label>(7)</label></formula><p>Hence, our defined Hepti's inference optimization can be formulated as the following Equation ( <ref type="formula" coords="8,173.01,347.66,2.87,7.94" target="#formula_6">8</ref>).</p><p>P1 : min𝑇 𝑠.𝑡 . 𝑇 ≤ 𝐷, (1), ( <ref type="formula" coords="8,213.92,373.24,3.16,7.70" target="#formula_1">2</ref>), ( <ref type="formula" coords="8,228.95,373.24,3.50,7.70">4</ref>)</p><p>Linear Programming-based Approximation. Solving P1 is not practical in terms of time complexity since P1 has non-convex optimization constraints and objective functions. Particularly, it is challenging due to the discrete nature of the integer variable 𝑎 𝑖 in Equation Thus, by introducing continuous real numeric variable 𝜆 to the P1, we transform it into a linearly approximated problem.</p><p>First, we set the variable 𝜆 to the 𝑎 𝑖 as shown in Equation ( <ref type="formula" coords="8,269.78,468.61,2.86,7.94">9</ref>). The range of 𝜆 is constrained by Equation <ref type="bibr" coords="8,186.39,479.57,12.49,7.94" target="#b9">(10)</ref>. Then, optimization of our dynamic partitioning algorithms can be rewritten as the following Equation <ref type="bibr" coords="8,90.54,501.49,12.56,7.94" target="#b10">(11)</ref>. The objective function and other constraints are still linear with 𝑎 𝑖 , which is now continuous. For other numerical constraints of Equation <ref type="bibr" coords="8,145.20,523.41,12.56,7.94" target="#b9">(10)</ref>, they have still linear relationships. Hence, P2 is a mixed integer linear programming problem.</p><formula xml:id="formula_7">𝑎 𝑖 =          𝜆 𝑖 • m if L row-wise MatMul 𝜆 𝑖 • n if R column-wise MatMul 𝜆 𝑖 • k if Block-wise MatMul (9) ∑︁ 𝑖 ∈𝑁 𝜆 𝑖 = 1, 𝑠.𝑡 . 0 ≤ 𝜆 𝑖 ≤ 1, 𝑖 ∈ 𝑁 (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>P2 : min𝑇 𝑠.𝑡 . 𝑇 ≤ 𝐷, (4), ( <ref type="formula" coords="8,211.84,638.99,3.16,7.70">9</ref>), (10)</p><p>Algorithms. Our dynamic workload partitioning engine takes as inputs the (𝑁 , 𝐵, 𝐷) tuple, as well as (𝜌, 𝑓 𝑖 , 𝑠 𝑖 ) for each device and 𝐵𝑊 𝑖,𝑖+1 between devices. If all secondary devices are unavailable, then Hepti executes serial transformer inference. Otherwise, the engine checks 𝑠 𝑖 and determines the Hepti's operation mode. The Hepti uses 1D-tiled WS when there are fewer than 16 devices, and 2D-tiled WS otherwise. Then, the engine uses a mixed-ILP solver to find 𝜆 𝑖 of 𝜋 that can solve P2. If there are elements with 𝜆 𝑖 equal to zero in 𝜋, the corresponding devices do not participate in parallel inference. In this case, we remove the device from 𝑁 and re-evaluate the partitioning decision for the remaining devices. For the obtained 𝑎 𝑖 values, rounding to the nearest integer is applied to determine the final element of 𝜋. The resulting 𝜋 from our partitioning finally contains a decision on how much transformer model inference workload is allocated to each device. In the case of the 2D-tiled WS manner, the aforementioned specific-axis partitioning algorithm is simply executed twice, once for the row and once for the column direction to split the matrix in 2-dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Experimental Setup</head><p>Our evaluation was conducted on a Raspberry Pi 4 board and non-GPU server as a heterogeneous edge computing platform. To increase heterogeneity, we leveraged cpulimit software tool to limit the CPU utilization and tc tool to change network bandwidth between devices. The detailed configurations of the testbed are described in Table <ref type="table" coords="8,386.01,526.15,3.04,7.94" target="#tab_1">2</ref>. We used PyTorch <ref type="bibr" coords="8,459.59,526.15,14.67,7.94" target="#b27">[28]</ref> to implement the deep learning inference engine of Hepti and PyTorch Distributed communication package as a distributed backend framework. All experiments are initiated from the Raspberry Pi 4 board, which acts as a role of the primary edge device. The primary edge device also performs a dynamic workload partitioning algorithm based on IBM DOcplex <ref type="bibr" coords="8,381.19,591.90,9.45,7.94" target="#b0">[1]</ref>, which provides a library for linear programming. Our transformer model workloads include BERT-large and MobileBERT, representing BERT <ref type="bibr" coords="8,442.13,613.82,10.68,7.94" target="#b6">[7]</ref> variants to examine models that have a fundamental transformer architecture. Additionally, our approach extends to other models sharing basic transformer structures such as Whisper <ref type="bibr" coords="8,402.27,646.70,13.46,7.94" target="#b29">[30]</ref>, and Vision Transformers <ref type="bibr" coords="8,515.98,646.70,10.65,7.94" target="#b7">[8]</ref> without significant modifications. However, models necessitating a fundamentally different attention mechanism lie beyond the scope of our approach. We compare and evaluate Hepti with the following baselines by processing text input with 256 tokens in a single batch. First, we compare our parallel inference approach against BERT-large (940Mbps)  In BERT-large with WS manner, at 940 Mbps, 87.5% of the workload was offloaded, while at 470 Mbps, 93.7% of the workload was offloaded.</p><p>the parallelism approach of previous studies <ref type="bibr" coords="9,212.91,259.54,13.40,7.94" target="#b28">[29,</ref><ref type="bibr" coords="9,228.27,259.54,10.05,7.94" target="#b31">32]</ref>, while keeping the partitioning decision the same. We also compare our dynamic workload partitioning algorithm approach against the partitioning approach of CoEdge <ref type="bibr" coords="9,129.35,292.41,13.28,7.94" target="#b37">[38]</ref>, while our parallel inference methods for the transformer are equally provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parallel Inference Performance Comparison</head><p>To see the effectiveness of our parallel inference method, we measured and compared the end-to-end latency of the Hepti with the approaches of local inference and 1D-and 2D-tiled WS mannerbased inferences which are taken by previous studies <ref type="bibr" coords="9,253.42,372.72,13.59,7.94" target="#b28">[29,</ref><ref type="bibr" coords="9,269.25,372.72,10.35,7.94" target="#b30">31,</ref><ref type="bibr" coords="9,281.83,372.72,10.19,7.94" target="#b31">32]</ref>.</p><p>In this experiment, we control our partitioning algorithm to be performed only once before the inference to determine the workload partitioning ratio. A memory size of 1.63 GB was allocated to the secondary device to ensure that model parameters could be loaded. Figure <ref type="figure" coords="9,79.47,427.52,4.19,7.94" target="#fig_8">6</ref> shows the average end-to-end latency for each inference methodology when the network bandwidth is 940 Mbps and 470 Mbps.</p><p>According to the results of the 940 Mbps environment of Figure <ref type="figure" coords="9,69.10,471.35,3.13,7.94" target="#fig_8">6</ref>, serial execution takes 8 and 0.95 seconds for BERT-large and MobileBERT, respectively. On the other hand, the 2D-tiled WS methodology takes only 6.4 and 0.77 seconds, respectively. This method performs parallel inference, minimizing the number of data communications between the two devices and saving 21.6% of the memory of the secondary device. Similarly, the 1D-tiled WS methodology takes only 5.4 and 0.71 seconds respectively. The 1D-tiled WS approach reduces 16.6% of the end-to-end latency compared to the 2D-tiled WS approach because of the number of devices. 2D-tiled WS manner demands more weight-slicing operations and more communication points than the 1D-tiled WS manner. However, as revealed in the previous study <ref type="bibr" coords="9,165.79,591.90,13.32,7.94" target="#b28">[29]</ref>, the 2D-tiled WS approach can reduce communication overhead as the number of devices increases, leading to performance improvements. Consequently, this enables faster parallel inference compared to the 1D-tiled WS method in such environments.</p><p>We found that the parallel inference method in a WS manner significantly improves performance, taking only 4.07 seconds for BERT-large and 0.61 seconds for MobileBERT. This reduction in time corresponds to a decrease of 37.1% and 20.4% in total overhead compared to 2D-tiled WS-based inference. Also, compared to the parallel execution with 1D-tiled WS manner, Hepti's WS manner Even when the network bandwidth decreases to 470 Mbps, Hepti's WS manners still outperform serial execution for BERTlarge and MobileBERT as shown in Figure <ref type="figure" coords="9,475.05,350.81,3.09,7.94" target="#fig_8">6</ref>. Particularly, we confirm that Hepti, operating in a WS manner, reduces the latency for BERT-large by around 46.6% and 29.8% compared to 2D-and 1D-tiled WS manner each. However, the 2D-tiled WS and the 1Dtiled WS of the Hepti show longer end-to-end latency compared to the serial execution for MobileBERT. This is due to communication overhead outweighing performance gains from parallelization. Unlike large models, MobileBERT's lightweight nature contributes to low computational latency. Applying 1D-or 2D-tiled WS-based inference techniques to lightweight models actually incurs significant communication overhead, leading to overall performance degradation. More specifically, MobileBERT includes additional feed-forward network (FFN) layers that contain two MatMuls each like the MLP block. The 1D-and 2D-tiled WS manner requires communications in every FFN layer because they use the block-wise parallel MatMul. On the contrary, Hepti can execute all FFN layers without communication as WS manner uses L row-wise MatMul.</p><p>To identify the underlying factors influencing the overall inference performance of Hepti's parallelization strategies, we conducted measurements of both computational and communication overheads in each transformer component. Table <ref type="table" coords="9,502.09,569.99,4.24,7.94" target="#tab_2">3</ref> compares the amount of data transfer during the parallel inference in 1D-tiled WS and Hepti. In the case of 1D-tiled WS, we observed that 1.05 GB of data is transferred per device at the beginning of every layer normalization, which requires exchanges for the entire tensor. However, Hepti's WS manner uses L row-wise parallel MatMul operations. Thus, it reduces primary device data transfer to just 245.8 KB, an 88.3% reduction compared to the 1D-tiled WS manner. We have confirmed that the WS manner is more efficient in terms of indevice communication compared to other methods. Because our experimental setup uses only two devices, the case of the 2D-tiled WS manner is the same as the 1D-tiled WS manner in terms of communication points and transferred tensor size. To analyze computational overhead, we calculated the number of element-wise multiplications and additions required for MatMul and GeMM operations in self-attention and MLP for both 1D-tiled WS manner and WS manner of the Hepti with BERT-large. Our experiments revealed that the total number of element-wise multiplications was the same for 1D-tiled WS, and Hepti. However, 1D-tiled WS manner required 1,048,576 more element-wise additions compared to Hepti for each transformer layer. This is because 1D-tiled WS manner uses block-wise parallel MatMul method during the last GeMM of self-attention and the second GeMM of MLP, necessitating additional addition operations for sub-matrices. In contrast, Hepti in a WS manner, only uses L row-wise MatMul and does not require additional addition operations. Next, both 1D-tiled WS manner and Hepti perform layer normalization for the tensor data each device possesses. In the case of 1D-tiled WS, since both primary and secondary devices share the entire tensor data, they perform an equal number of normalization operations, which increases with the number of devices. This approach aims to minimize inter-device communication and the model size by partitioning weights of MatMul and GeMM operations. On the other hand, Hepti conducts layer normalization solely for the submatrices split in the row direction on both devices, regardless of the number of devices, resulting in a constant total workload. In the case of 2D-tiled WS, the total number of element-wise multiplications is the same as in the 1D-tiled WS approach. Also, the number of total addition operations is the same as in the 1D-tiled WS approach due to the experimental setting. Generally, the entire number of element-wise multiplications is identical in 1D-and 2D-tiled WS manners. In contrast, in the 2D-tiled WS method, the number of element-wise addition operations is influenced by the number of devices dividing each dimension of weights. In our experimental setup, the dimensions of weights are partitioned by the same devices as in the 1D-tiled WS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adaptability to Heterogeneous Settings</head><p>In this experiment, we used BERT-large to investigate the impact of Hepti's dynamic partitioning algorithm on parallel inference within a heterogeneous environment, comparing it to CoEdge. We considered variations in network bandwidth and available memory size on the secondary device. Since CoEdge lacks support for parallel inference with transformer models, we configured it to follow Hepti's parallel inference approach in WS manner. Figure <ref type="figure" coords="10,522.41,246.13,4.09,7.94">7</ref> presents recorded parallel processing latency during the transformer layer computation point across different heterogeneous scenarios. In Segment 1, the network bandwidth is 800-940 Mbps, and there is sufficient memory space on the secondary device to accommodate model parameters. Specifically, a minimum of 1.63GB of memory is guaranteed to enable parallel inference in the WS manner on the secondary device. Therefore, both CoEdge and Hepti can perform parallel inference in WS mode during Segment 1.</p><p>Moving to Segment 2, the network bandwidth drops to 200-300 Mbps, causing CoEdge to experience a 12.42% latency increase compared to Hepti. The reason is that CoEdge sticks to the partitioning decision fixed during the setup phase before the inference initiation, with no modifications during the inference phase. Conversely, Hepti continues to deliver optimal performance because it operates a dynamic partitioning algorithm. This algorithm allows Hepti to adapt to changes in network bandwidth and generate optimal partitioning decisions during the inference phase.</p><p>Transitioning from Segment 2 to Segment 3, the available memory on the secondary device decreases from 1.63 GB to 1 GB. In this case, WS transformer parallel processing becomes infeasible due to limited memory space. In the case of CoEdge, the inference program halts due to out-of-memory issues. This is because CoEdge is unable to adjust its partitioning decisions during inference. In contrast, Hepti can dynamically switch to the 1D-tiled WS manner, which requires only 757 MB of memory space on the secondary device in the experiment. Thus, Hepti can sustain inference even in environments with reduced memory usage. Our experiment demonstrates that Hepti exhibits superior inference robustness in heterogeneous edge environments compared to CoEdge. Note that in a scenario where network bandwidth is also heavily constrained, the performance may degrade compared to serial inference. For example, when limited to a network bandwidth of less than 100 Mbps, Hepti's 1D-tiled WS experiences a 1.9% latency increase compared to the serial execution. This was attributed to communication overhead outweighing the computational gains from parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overhead of Dynamic Workload Partitioning Algorithm</head><p>Hepti's dynamic workload partitioning algorithm operates iteratively, even during inference. To assess its impact on overall inference latency, we measured the performance overhead of this dynamic partitioning algorithm. Figure <ref type="figure" coords="11,198.70,87.79,4.15,7.94" target="#fig_9">8</ref> illustrates how the partitioning algorithm's overhead varies as the number of participating devices in parallel inference increases. The algorithm's behavior is influenced by the values of computing intensity and network bandwidth in the input, so we conducted experiments with a hundred randomly sampled instances. Figure <ref type="figure" coords="11,184.37,142.59,4.09,7.94" target="#fig_9">8</ref> reveals that as the number of devices increases up to ten, Hepti's average latency for the partitioning algorithm becomes negligible. For example, with ten devices, the average latency is only 11.89 milliseconds Compared to the previous end-to-end latency of Hepti for BERT-large at 940 Mbps network bandwidth (as shown in Figure <ref type="figure" coords="11,208.30,197.38,3.00,7.94" target="#fig_8">6</ref>), which was 4.07 seconds, the partitioning algorithm latency contributes to only 0.29%. However, the maximum execution time can reach 44.9 milliseconds, particularly when using ten devices. It occurs when a specific 𝜆 𝑖 , a constituent of 𝜋, becomes zero, resulting in some devices being excluded from inference. This necessitates a rerun of the partitioning algorithm. Even so, the partitioning algorithm is iterated every 24 iterations of the BERT-large transformer, adding about 1.076 seconds of parallel inference delay. The total latency is still significantly less than the 8 seconds, a latency of local inference time for BERT-large at 940 Mbps bandwidth (as shown in Figure <ref type="figure" coords="11,259.50,306.97,2.94,7.94" target="#fig_8">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORKS</head><p>DeepThings <ref type="bibr" coords="11,101.09,350.81,14.85,7.94" target="#b39">[40]</ref> proposed a method of distributing input tensor data to enable independent model inference on Convolution (Conv) layers. This method aims to measure and distribute the receptive field of the input in order to compute the output data with a specific range across homogeneous edge devices. MoDNN <ref type="bibr" coords="11,236.91,394.64,14.67,7.94" target="#b22">[23]</ref> partitioned the input tensor data of Conv layers and fully-connected layers on a mobile computing environment. In MoDNN, the workload is divided into segments using a greedy approach, and these segments are allocated more heavily to devices that possess higher computational capacity. CoEdge <ref type="bibr" coords="11,95.50,460.40,14.85,7.94" target="#b37">[38]</ref> proposed a framework for parallel inference for Conv-centric CNN architecture across heterogeneous edge devices. CoEdge introduced an adaptive workload partitioning technique that considers not only available computing resources but also network bandwidth among devices. This workload partitioning aims to reduce both inference latency and energy consumption. EdgeFlow <ref type="bibr" coords="11,91.17,526.15,14.60,7.94" target="#b16">[17]</ref> redesigned and extended existing partitioning methods to suit the directed acyclic graph model. This adaptation was rooted in the assumption that the architecture of the DNN model is represented as a graph rather than a linear chain. Specifically, the partitioning algorithm of EdgeFlow comprehends the model graph's input-output relationship to allocate specific layer operations to each edge device. These studies successfully achieved the parallelism of CNN model inference. Hepti performs parallel inference following the CoEdge approach for CNN models.</p><p>In contrast, Megatron-LM <ref type="bibr" coords="11,161.41,624.78,14.77,7.94" target="#b31">[32]</ref> and several studies <ref type="bibr" coords="11,250.37,624.78,13.55,7.94" target="#b28">[29,</ref><ref type="bibr" coords="11,266.17,624.78,11.56,7.94" target="#b30">31]</ref> proposed model parallelism for transformer models. By leveraging Mat-Mul parallelism, Megatron-LM examined the operational behavior of the transformer model. In Megatron-LM, a partitioning algorithm that takes into account the network conditions and computational capabilities of each heterogeneous device has not been proposed. Unlike an intra-layer partitioning-based transformer parallelism of these studies, an inter-layer model parallelism was suggested in PipeEdge <ref type="bibr" coords="11,353.61,87.79,13.24,7.94" target="#b17">[18]</ref>. Specifically, PipeEdge divides the batch of the input into multiple micro-batches, and pipelines the execution of these micro-batches across multiple edge devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper proposes Hepti, a practical inference framework designed to facilitate the transformer model parallelism in heterogeneous edge device environments. Hepti incorporates three parallelization methods for transformer inference and autonomously determines acceptable workload partitioning strategies for parallelization. The experimental results show the partitioning overhead is negligible while simultaneously improving the performance advantages of inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,221.96,504.40,7.70;2,53.80,232.21,505.09,8.41;2,53.80,243.87,504.29,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Block diagram of the general architecture of transformer. The transformer block itself is repeatedly used during inference. Note that if the ⊗ is accompanied by the GeMM notation, it indicates General Matrix Multiplication (GeMM); otherwise, it indicates Matrix-Matrix Multiplication (MatMul). In GeMM operation, MatMul and a bias addition are involved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,53.80,194.14,241.78,7.70;3,53.50,205.10,241.64,7.70;3,53.80,216.06,240.25,7.70;3,53.80,227.02,240.25,7.70;3,53.80,237.98,240.25,7.70;3,53.80,248.94,241.85,7.70;3,53.80,259.90,207.43,7.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The divide-and-conquer approach-based MatMul. The L and R matrices are an activation tensor and a weight, respectively. In cases (a) and (b), the R and L matrices are split by column and row, respectively. These split sub-matrices are then computed in parallel and results are concatenated to form the final output. Case (c) additionally requires intermediate matrix transfer and summation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,79.26,170.54,84.48,8.86;4,76.55,175.76,86.88,11.28;4,67.96,168.17,2.98,8.86;4,62.00,153.60,8.94,8.86;4,62.00,139.04,8.94,8.86;4,62.00,124.48,8.94,8.86;4,62.00,109.92,8.94,8.86;4,62.00,95.35,8.94,8.86;4,62.00,80.79,8.94,8.86;4,49.95,146.27,11.28,14.95;4,49.95,120.79,11.28,23.58;4,49.95,98.37,11.28,20.52;4,88.88,86.79,47.22,8.86;4,88.88,93.67,51.04,8.86;4,88.88,98.92,34.00,8.86;4,88.88,105.80,51.04,8.86;4,88.88,111.04,40.60,8.86;4,53.55,194.62,115.56,6.85;4,53.80,204.59,112.82,6.85;4,196.74,171.00,95.05,8.95;4,212.30,176.34,60.93,11.40;4,187.16,166.06,6.02,8.95;4,187.16,157.49,6.02,8.95;4,187.16,148.92,6.02,8.95;4,187.16,140.34,6.02,8.95;4,184.15,131.77,9.03,8.95;4,184.15,123.20,9.03,8.95;4,184.15,114.62,9.03,8.95;4,184.15,106.05,9.03,8.95;4,184.15,97.48,9.03,8.95;4,184.15,88.90,9.03,8.95;4,184.15,80.33,9.03,8.95;4,171.98,146.47,11.40,15.10;4,171.98,120.74,11.40,23.82;4,171.98,98.09,11.40,20.73;4,209.72,86.33,78.82,8.14;4,209.72,92.64,76.08,8.14;4,209.72,98.96,78.82,8.14;4,209.72,105.28,76.08,8.14;4,176.24,194.62,115.79,6.85;4,176.49,204.59,109.89,6.85"><head></head><label></label><figDesc>, CPU Utilization: 100% BW: 940Mbps, CPU Utilization: 25% BW: 240Mbps, CPU Utilization: 100% BW: 240Mbps, CPU Utilization: 25% (b) A comparison of total latency depending on offloading ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,53.80,222.98,240.25,7.70;4,53.80,233.94,192.16,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of parallelization for MatMul operation and change of optimal workload partitioning point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,169.97,247.78,272.06,7.70;5,60.24,83.69,489.26,153.70"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Block diagram of Transformer architecture used in Hepti.</figDesc><graphic coords="5,60.24,83.69,489.26,153.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,150.48,141.92,5.17,12.16;6,150.48,126.99,5.17,4.61;6,150.48,93.17,5.17,32.29;6,162.95,136.51,5.17,13.78;6,162.95,100.75,5.17,34.26;6,176.01,117.44,5.17,12.14;6,188.75,136.51,5.17,13.78;6,188.75,100.75,5.17,34.26;6,57.26,94.24,7.47,4.53;6,69.61,86.76,10.15,3.54;6,69.96,91.60,9.47,3.54;6,57.26,113.51,7.47,4.53;6,69.61,105.95,10.15,3.54;6,69.96,110.79,9.47,3.54;6,87.89,93.84,3.73,4.53;6,87.63,113.18,3.73,4.53;6,237.85,94.40,3.73,4.53;6,237.70,113.51,3.73,4.53;6,57.42,132.61,7.47,4.53;6,69.78,125.06,10.15,3.54;6,70.13,129.89,9.47,3.54;6,57.42,151.88,7.47,4.53;6,69.78,144.32,10.15,3.54;6,70.13,149.16,9.47,3.54;6,88.06,132.21,3.73,4.53;6,87.80,151.56,3.73,4.53;6,238.02,132.77,3.73,4.53;6,237.87,151.88,3.73,4.53;6,115.33,111.31,5.17,25.69;6,224.35,111.55,5.17,25.60;6,122.99,170.52,78.26,6.85;6,347.64,142.15,5.20,12.25;6,347.64,127.07,5.20,4.64;6,347.64,93.06,5.20,32.51;6,440.26,136.70,5.20,13.88;6,440.26,100.69,5.20,34.50;6,453.42,117.50,5.20,12.22;6,544.61,136.78,5.20,13.88;6,544.61,100.77,5.20,34.50;6,289.25,94.13,7.52,4.56;6,301.69,86.52,10.22,3.56;6,302.04,91.39,9.53,3.56;6,289.25,113.53,7.52,4.56;6,301.69,105.92,10.22,3.56;6,302.04,110.79,9.53,3.56;6,320.10,93.72,3.75,4.56;6,319.84,113.21,3.75,4.56;6,517.32,93.97,3.75,4.56;6,517.17,113.21,3.75,4.56;6,289.42,132.77,7.52,4.56;6,301.86,125.16,10.22,3.56;6,302.21,130.03,9.53,3.56;6,289.42,152.10,7.52,4.56;6,301.86,144.57,10.22,3.56;6,302.21,149.44,9.53,3.56;6,320.27,132.37,3.75,4.56;6,320.01,151.85,3.75,4.56;6,517.49,132.61,3.75,4.56;6,517.34,151.85,3.75,4.56;6,383.25,105.60,5.20,36.46;6,396.25,110.75,5.20,25.87;6,410.81,94.13,3.75,4.56;6,410.66,113.45,3.75,4.56;6,410.98,132.77,3.75,4.56;6,410.83,152.01,3.75,4.56;6,489.01,106.08,5.20,36.46;6,502.00,111.32,5.20,25.79;6,363.36,93.06,7.76,6.21;6,363.60,112.06,7.76,6.21;6,363.36,131.79,7.76,6.21;6,363.60,151.03,7.76,6.21;6,470.14,92.17,7.76,6.21;6,470.38,111.25,7.76,6.21;6,470.14,130.81,7.76,6.21;6,470.38,150.06,7.76,6.21;6,366.17,170.52,111.45,6.85;6,297.66,234.40,5.38,14.35;6,297.66,197.17,5.38,35.66;6,311.26,230.89,5.38,11.31;6,311.26,213.76,5.38,4.79;6,311.26,199.56,5.38,12.64;6,492.67,235.66,5.38,14.35;6,492.67,198.43,5.38,35.66;6,106.65,190.39,7.77,4.72;6,119.52,182.52,10.57,3.68;6,119.88,187.56,9.86,3.68;6,106.65,210.45,7.77,4.72;6,119.52,202.58,10.57,3.68;6,119.88,207.62,9.86,3.68;6,138.55,189.97,3.88,4.72;6,138.28,210.11,3.88,4.72;6,106.83,230.34,7.77,4.72;6,119.69,222.47,10.57,3.68;6,120.05,227.51,9.86,3.68;6,106.83,250.31,7.77,4.72;6,119.69,242.53,10.57,3.68;6,120.05,247.57,9.86,3.68;6,138.72,229.92,3.88,4.72;6,138.45,249.98,3.88,4.72;6,251.11,208.75,5.38,26.74;6,267.36,190.39,3.88,4.72;6,267.21,210.36,3.88,4.72;6,267.54,230.34,3.88,4.72;6,267.38,250.23,3.88,4.72;6,167.20,188.86,5.38,26.66;6,167.30,228.98,5.38,26.66;6,200.83,240.70,5.38,12.66;6,200.83,225.11,5.38,4.79;6,200.83,189.95,5.38,33.61;6,215.73,190.32,5.01,4.04;6,222.00,209.96,5.01,4.04;6,215.64,230.27,5.01,4.04;6,222.07,250.24,5.01,4.04;6,237.97,185.60,4.72,32.94;6,237.73,225.80,4.72,33.02;6,326.86,190.23,5.01,4.04;6,326.78,209.87,5.01,4.04;6,333.09,229.85,5.01,4.04;6,333.20,250.07,5.01,4.04;6,348.35,185.52,4.72,32.94;6,348.11,225.72,4.72,32.94;6,362.40,188.61,5.38,26.74;6,362.16,228.72,5.38,26.74;6,397.94,228.21,5.38,18.11;6,397.94,211.09,5.38,4.79;6,397.94,196.89,5.38,12.64;6,448.09,208.83,5.38,26.74;6,464.34,190.47,3.88,4.72;6,464.19,210.45,3.88,4.72;6,464.52,230.42,3.88,4.72;6,464.36,250.40,3.88,4.72;6,412.71,190.40,5.01,4.04;6,418.98,210.12,5.01,4.04;6,412.62,230.35,5.01,4.04;6,419.06,250.33,5.01,4.04;6,434.20,185.68,4.72,32.94;6,433.96,225.97,4.72,32.94;6,249.49,272.47,110.53,6.85"><head>Final</head><label></label><figDesc>2D Tiled Weight-stationary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,53.80,290.86,504.41,7.70"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of communication and computation of tensors during transformer inference in three manners of Hepti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="9,53.80,177.54,240.25,7.70;9,53.80,188.50,240.25,7.70;9,53.80,199.46,241.78,7.70;9,53.80,210.42,240.25,7.70;9,53.45,221.38,240.60,7.70;9,53.45,232.34,99.21,7.70"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: End-to-end latency of serial execution and parallel execution with 2D-tiled WS, 1D-tiled WS and Hepti parallel inference approaches for two network bandwidth variants.In BERT-large with WS manner, at 940 Mbps, 87.5% of the workload was offloaded, while at 470 Mbps, 93.7% of the workload was offloaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="10,317.96,180.50,240.24,7.70;10,317.96,191.46,203.48,7.70"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The performance overhead of dynamic workload partitioning under varying the number of devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="7,317.66,85.73,242.15,456.81"><head>Table 1 :</head><label>1</label><figDesc>Notation and description of arguments of the profiling and partitioning engine of Hepti. 𝜋 = [𝑎 1 , 𝑎 2 , . . . , 𝑎 𝑖 ], 𝑖 ∈ 𝑁 , 𝑎 𝑖 ∈ N The set of the sizes in the split direction of the partitioned tensors (matrices) each device covers. 𝑟 𝑏,𝑖 , 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁 The workload size of partitioned MatMul that need to be computed on each device for specific transformer block. 𝑐 𝑏,𝑖 , 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁</figDesc><table coords="7,322.22,122.85,233.30,419.69"><row><cell>Argument</cell><cell>Description</cell></row><row><cell>𝑁 = [1, 2, . . . , 𝑒], 𝑒 ∈ N</cell><cell>The set of available edge devices in-</cell></row><row><cell></cell><cell>cluding the primary edge device.</cell></row><row><cell>𝜌 𝑖 , 𝑖 ∈ N</cell><cell>Computing intensity (# of CPU cy-</cell></row><row><cell></cell><cell>cles required to FLOP) of 𝑖-th device.</cell></row><row><cell>𝑓 𝑖 , 𝑖 ∈ N</cell><cell>Average CPU frequency of 𝑖-th de-</cell></row><row><cell></cell><cell>vice during feed-forward process.</cell></row><row><cell>𝑠 𝑖 , 𝑖 ∈ N</cell><cell>The available memory size of 𝑖-th</cell></row><row><cell></cell><cell>device, excluding system memory.</cell></row><row><cell cols="2">(a) Each argument used in profiling of the Hepti.</cell></row><row><cell>Argument</cell><cell>Description</cell></row><row><cell>𝐵 = [1, 2, . . . , 𝑏], 𝑏 ∈ N</cell><cell>The sequence of transformer blocks</cell></row><row><cell></cell><cell>(e.g., self-attention, layer normaliza-</cell></row><row><cell></cell><cell>tion, and MLP).</cell></row><row><cell></cell><cell>The size of data to be received from</cell></row><row><cell></cell><cell>other devices to proceed the specific</cell></row><row><cell></cell><cell>transformer block.</cell></row><row><cell>𝐷 (msec)</cell><cell>The service-level deadline as an ob-</cell></row><row><cell></cell><cell>jective variable of the partitioning</cell></row><row><cell></cell><cell>engine.</cell></row><row><cell>𝐵𝑊 𝑖,𝑗 (Mbps), 𝑖 ∈ 𝑁</cell><cell>Bandwidth between 𝑖-th and 𝑗-th</cell></row><row><cell></cell><cell>devices.</cell></row><row><cell>𝑇 𝑐 𝑏𝑖 (msec), 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁</cell><cell>Computation inference latency in</cell></row><row><cell></cell><cell>𝑏-th block on 𝑖-th device</cell></row><row><cell>𝑇 𝑥 𝑏𝑖 (msec), 𝑏 ∈ 𝐵, 𝑖 ∈ 𝑁</cell><cell>Communication delay in 𝑏-th block</cell></row><row><cell></cell><cell>of 𝑖-th device</cell></row><row><cell>𝑇 (msec)</cell><cell>Maximum sum of 𝑇 𝑥 𝑏𝑖 and 𝑇 𝑐 𝑏𝑖 for</cell></row><row><cell></cell><cell>each block and device</cell></row><row><cell cols="2">(b) Each argument used in partitioning of the Hepti.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="8,317.96,85.73,241.76,199.31"><head>Table 2 :</head><label>2</label><figDesc>Testbed configurations for Hepti evaluation.</figDesc><table coords="8,317.96,111.89,241.76,173.15"><row><cell>Primary device</cell><cell>Specifications</cell></row><row><cell>CPU</cell><cell>ARM Quad core Cortex-A72 @ 1.8GHz</cell></row><row><cell>Memory</cell><cell>8GB LPDDR4-3200 SDRAM</cell></row><row><cell>Network</cell><cell>2.4 GHz LAN &amp; Gigabit Ethernet</cell></row><row><cell>Kernel version</cell><cell>Linux-Kernel-5.15.0-1035-raspi</cell></row><row><cell>Secondary device</cell><cell>Specifications</cell></row><row><cell>CPU</cell><cell>Intel(R) Dual Core i3-8130U @ 2.20GHz</cell></row><row><cell>Memory</cell><cell>4GB DDR4 SDRAM</cell></row><row><cell>Network</cell><cell>2.4 GHz LAN &amp; Gigabit Ethernet</cell></row><row><cell>Kernel version</cell><cell>Linux-Kernel-6.2.0-26-Generic</cell></row><row><cell cols="2">engine conservatively decides on 1D-tiled WS or 2D-tiled WS man-</cell></row><row><cell cols="2">ner if at least one of the devices participating in parallel inference</cell></row><row><cell cols="2">does not have sufficient memory to load the model parameters.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="9,317.66,85.73,242.15,240.15"><head>Table 3 :</head><label>3</label><figDesc>The total byte size of tensors required for communication between devices during BERT-large inference.</figDesc><table coords="9,317.96,122.32,241.76,203.55"><row><cell>Communication</cell><cell cols="2">1D-tiled WS manner</cell><cell cols="2">WS manner</cell></row><row><cell>point</cell><cell>Pri. dev</cell><cell>Sec. dev</cell><cell cols="2">Pri. dev Sec. dev</cell></row><row><cell>Before final GeMM in self-attention</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">114.7 KB 114.7 KB</cell></row><row><cell>Before first layer normalization</cell><cell>1.05 GB</cell><cell>1.05 GB</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Before second layer normalization</cell><cell>1.05 GB</cell><cell>1.05 GB</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>After second layer normalization</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">131.1 KB 917.5 KB</cell></row><row><cell>Total</cell><cell>2.1 MB</cell><cell>2.1 MB</cell><cell cols="2">245.8 KB 1.03 GB</cell></row><row><cell cols="5">takes 24.6% and 13.1% less total overhead. In the parallel approach</cell></row><row><cell cols="5">of 1D-and 2D-tiled WS, block-wise parallel MatMul is continuously</cell></row><row><cell cols="5">used in both self-attention and MLP. In contrast, Hepti also sup-</cell></row><row><cell cols="5">ports L row-wise MatMul. Unlike block-wise MatMul, L row-wise</cell></row><row><cell cols="5">MatMul does not require inter-device data transfers for partitioned</cell></row><row><cell cols="5">matrices or additional matrix addition operations. Thus, it reduces</cell></row><row><cell cols="4">performance overhead in MatMul (or GeMM).</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their insightful comments and suggestions, which greatly contributed to refining this work. This work was carried out by members of VERITROSS (VRTSS-1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="11,326.67,312.26,231.83,6.18;11,333.39,320.23,76.29,6.18" xml:id="b0">
	<monogr>
		<title level="m" type="main">User&apos;s Manual for CPLEX</title>
		<ptr target="https://www.ibm.com/docs/en/icos/12.9.0?topic=cplex-users-manual" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,326.67,328.20,231.98,6.18;11,333.39,336.17,113.46,6.18" xml:id="b1">
	<monogr>
		<ptr target="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/" />
		<title level="m">Raspberry Pi 4 Tech Specs</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,326.67,344.14,183.58,6.18" xml:id="b2">
	<monogr>
		<title level="m" type="main">Edge AI and Vision Alliance. www.edge-ai-vision</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,352.11,225.89,6.18;11,333.39,360.02,99.65,6.25" xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning with edge computing: A review</title>
		<author>
			<persName coords=""><forename type="first">Jiasi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xukan</forename><surname>Ran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1655" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,368.05,225.58,6.18;11,333.39,376.02,224.94,6.18;11,333.39,383.93,224.81,6.25;11,333.39,391.90,126.26,6.25" xml:id="b4">
	<analytic>
		<title level="a" type="main">Dianne: Distributed artificial neural networks for the internet of things</title>
		<author>
			<persName coords=""><forename type="first">Elias</forename><surname>De Coninck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tim</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bert</forename><surname>Vankeirsbilck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pieter</forename><surname>Simoens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Middleware for Context-Aware Applications in the IoT</title>
				<meeting>the 2nd Workshop on Middleware for Context-Aware Applications in the IoT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,399.93,224.81,6.18;11,333.39,407.90,225.89,6.18;11,333.39,415.81,225.88,6.25;11,333.39,423.84,48.30,6.18" xml:id="b5">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8599" to="8603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,431.81,225.63,6.18;11,333.39,439.72,224.81,6.25;11,333.39,447.69,91.11,6.25" xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,455.72,225.88,6.18;11,333.39,463.63,224.81,6.25;11,333.18,471.66,18.66,6.18" xml:id="b7">
	<monogr>
		<author>
			<persName coords=""><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Unterthiner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Transformers for image recognition at scale</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,479.63,225.99,6.18;11,333.39,487.60,224.81,6.18;11,333.39,495.57,224.81,6.18;11,333.39,503.49,196.74,6.25" xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName coords=""><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,511.46,224.81,6.25;11,333.39,519.43,225.57,6.25;11,333.39,527.40,188.26,6.25" xml:id="b9">
	<monogr>
		<title level="m">Edge AI Market Size, Share and Trends Analysis Report By Component (Hardware, Software, Edge Cloud Infrastructure, Services), By End-use Industry, By Region, And Segment Forecasts</title>
				<imprint>
			<publisher>Grand View Research</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2023" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,535.42,225.58,6.18;11,333.39,543.34,224.81,6.25;11,333.23,551.36,50.20,6.18" xml:id="b10">
	<analytic>
		<title level="a" type="main">Pct: Point cloud transformer</title>
		<author>
			<persName coords=""><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jun-Xiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,559.33,224.81,6.18;11,333.39,567.30,225.89,6.18;11,333.39,575.22,151.77,6.25" xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward collaborative inferencing of deep neural networks on Internet-of-Things devices</title>
		<author>
			<persName coords=""><forename type="first">Ramyad</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiashen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyesoon</forename><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4950" to="4960" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,583.24,224.81,6.18;11,333.19,591.21,226.09,6.18;11,333.39,599.13,167.76,6.25" xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid: Enabling hierarchical neural networks with edge computing</title>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeqian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Feifei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuiguang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weifa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
				<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1860" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,607.15,225.88,6.18;11,333.39,615.07,108.33,6.25" xml:id="b13">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,623.09,224.81,6.18;11,333.39,631.01,224.81,6.25;11,333.39,638.98,225.57,6.25;11,333.23,647.00,31.30,6.18" xml:id="b14">
	<analytic>
		<title level="a" type="main">DistrEdge: Speeding up convolutional neural network inference on distributed edge devices</title>
		<author>
			<persName coords=""><forename type="first">Xueyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongjie</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1097" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,654.97,224.81,6.18;11,333.06,662.94,225.97,6.18;11,333.39,670.86,224.81,6.25;11,333.39,678.83,90.76,6.25" xml:id="b15">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,686.85,225.99,6.18;11,333.39,694.77,224.81,6.25;11,333.39,702.74,161.62,6.25" xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed inference with deep learning models across heterogeneous edge devices</title>
		<author>
			<persName coords=""><forename type="first">Chenghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2022-IEEE Conference on Computer Communications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,89.10,224.81,6.18;12,69.23,97.07,225.99,6.18;12,69.23,104.98,224.81,6.25;12,69.23,112.95,165.14,6.25" xml:id="b17">
	<analytic>
		<title level="a" type="main">Pipeedge: Pipeline parallelism for largescale model inference on heterogeneous edge devices</title>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Connor</forename><surname>Imes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xuanang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Souvik</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Beerel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">P</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><forename type="middle">Paul</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 25th Euromicro Conference on Digital System Design (DSD)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,120.98,225.11,6.18;12,69.23,128.95,78.46,6.18" xml:id="b18">
	<monogr>
		<title level="m" type="main">International Data Corporation (IDC)</title>
		<ptr target="https://www.idc.com/getdoc.jsp?containerId=prAP50688623" />
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct coords="12,69.23,136.92,225.99,6.18;12,69.23,144.83,224.81,6.25;12,69.23,152.80,105.43,6.25" xml:id="b19">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName coords=""><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,160.83,225.99,6.18;12,69.23,168.74,224.81,6.25;12,69.23,176.71,133.55,6.25" xml:id="b20">
	<analytic>
		<title level="a" type="main">Edge AI: On-demand accelerating deep neural network inference via edge computing</title>
		<author>
			<persName coords=""><forename type="first">En</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liekang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Wireless Communications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="447" to="457" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,184.74,225.57,6.18;12,69.23,192.71,225.99,6.18;12,69.23,200.62,224.81,6.25;12,69.23,208.59,84.90,6.25" xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName coords=""><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
				<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,216.62,225.88,6.18;12,69.23,224.59,225.88,6.18;12,69.23,232.50,225.88,6.25;12,69.23,240.53,48.30,6.18" xml:id="b22">
	<analytic>
		<title level="a" type="main">Modnn: Local distributed mobile computing system for deep neural network</title>
		<author>
			<persName coords=""><forename type="first">Jiachen</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kent</forename><forename type="middle">W</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1396" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,248.50,224.81,6.18;12,69.23,256.47,225.49,6.18;12,69.23,264.44,65.35,6.18" xml:id="b23">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Markets</forename><surname>Markets</surname></persName>
		</author>
		<ptr target="https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-70030817.html" />
		<title level="m">Edge AI Software Market: Global Forecast to 2028</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,272.41,225.99,6.18;12,69.23,280.32,224.81,6.25;12,69.03,288.35,18.66,6.18" xml:id="b24">
	<monogr>
		<title level="m" type="main">Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer</title>
		<author>
			<persName coords=""><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,69.23,296.32,225.99,6.18;12,69.23,304.23,186.80,6.25" xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge machine learning for ai-enabled iot devices: A review</title>
		<author>
			<persName coords=""><forename type="first">Massimo</forename><surname>Merenda</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carlo</forename><surname>Porcaro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Demetrio</forename><surname>Iero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">2533</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,312.26,225.58,6.18;12,69.23,320.23,225.58,6.18;12,69.12,328.20,224.92,6.18;12,69.23,336.11,224.81,6.25;12,69.23,344.08,225.88,6.25;12,69.07,352.11,15.08,6.18" xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,360.08,224.99,6.18;12,69.23,368.05,225.88,6.18;12,69.23,375.96,224.81,6.25;12,69.23,383.93,141.66,6.25" xml:id="b27">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,391.96,225.99,6.18;12,69.23,399.93,224.99,6.18;12,69.23,407.84,224.81,6.25;12,69.03,415.87,18.66,6.18" xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficiently scaling transformer inference</title>
		<author>
			<persName coords=""><forename type="first">Reiner</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sholto</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kefan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shivani</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,423.84,224.81,6.18;12,69.23,431.81,225.88,6.18;12,69.23,439.72,197.87,6.25" xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust speech recognition via large-scale weak supervision</title>
		<author>
			<persName coords=""><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christine</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="28492" to="28518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,447.75,225.99,6.18;12,69.23,455.72,224.94,6.18;12,69.07,463.63,224.97,6.25;12,69.23,471.60,177.73,6.25" xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,479.63,225.58,6.18;12,69.23,487.60,224.94,6.18;12,69.23,495.52,223.31,6.25" xml:id="b31">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,69.23,503.54,224.81,6.18;12,69.23,511.46,203.00,6.25" xml:id="b32">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,69.23,519.48,224.81,6.18;12,68.98,527.40,225.23,6.25;12,69.03,535.42,18.66,6.18" xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName coords=""><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,543.39,224.81,6.18;12,69.23,551.31,224.81,6.25;12,69.03,559.33,51.70,6.18" xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName coords=""><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tien-Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,567.25,224.81,6.25;12,69.23,575.22,146.34,6.25" xml:id="b35">
	<monogr>
		<title level="m" type="main">Natural language processing with transformers</title>
		<author>
			<persName coords=""><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,583.24,225.58,6.18;12,68.99,591.21,225.06,6.18;12,69.23,599.13,199.83,6.25" xml:id="b36">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,607.15,225.63,6.18;12,69.23,615.12,225.99,6.18;12,69.23,623.04,223.20,6.25" xml:id="b37">
	<analytic>
		<title level="a" type="main">Coedge: Cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices</title>
		<author>
			<persName coords=""><forename type="first">Liekang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junshan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,631.06,224.81,6.18;12,69.23,639.03,225.99,6.18;12,69.23,646.95,224.81,6.25;12,68.81,654.92,66.14,6.25" xml:id="b38">
	<analytic>
		<title level="a" type="main">DUET: A compiler-runtime subgraph scheduling approach for tensor programs on a coupled CPU-GPU architecture</title>
		<author>
			<persName coords=""><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zehua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingqin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,662.94,225.88,6.18;12,69.23,670.91,225.99,6.18;12,69.23,678.83,224.81,6.25;12,69.23,686.80,158.87,6.25" xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepthings: Distributed adaptive deep learning inference on resourceconstrained iot edge clusters</title>
		<author>
			<persName coords=""><forename type="first">Zhuoran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kamyar</forename><forename type="middle">Mirzazad</forename><surname>Barijough</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Gerstlauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2348" to="2359" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,694.82,224.81,6.18;12,69.23,702.79,224.81,6.18;12,333.39,89.04,224.81,6.25;12,333.39,97.01,73.96,6.25" xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive parallel execution of deep neural networks on heterogeneous edge devices</title>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Hossein Samavatian</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anys</forename><surname>Bacha</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saikat</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Teodorescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE Symposium on Edge Computing</title>
				<meeting>the 4th ACM/IEEE Symposium on Edge Computing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,105.04,224.81,6.18;12,333.39,113.01,225.89,6.18;12,333.39,120.92,99.65,6.25" xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge intelligence: Paving the last mile of artificial intelligence with edge computing</title>
		<author>
			<persName coords=""><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">En</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liekang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Junshan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
				<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="1738" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
