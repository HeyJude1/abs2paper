<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher>IEEE</publisher>
				<availability status="unknown"><p>Copyright IEEE</p>
				</availability>
				<date type="published" when="2023-12-18">2023-12-18</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,239.28,154.56,57.85,9.81"><forename type="first">Daegun</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ajou University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName coords="1,319.45,154.56,58.01,9.81"><forename type="first">Sangyoon</forename><surname>Oh</surname></persName>
							<email>syoh@ajou.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Ajou University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and Accelerating Distributed DNN Training</title>
					</analytic>
					<monogr>
						<title level="m">2023 IEEE 30th International Conference on High Performance Computing, Data, and Analytics (HiPC)</title>
						<imprint>
							<publisher>IEEE</publisher>
							<biblScope unit="page" from="87" to="96"/>
							<date type="published" when="2023-12-18" />
						</imprint>
					</monogr>
					<idno type="MD5">67EEB91A37544568D139221E5F568084</idno>
					<idno type="DOI">10.1109/hipc58850.2023.00024</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-07-26T10:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>distributed deep learning</term>
					<term>gradient sparsification</term>
					<term>scalability 87</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gradient sparsification is a communication optimisation technique for scaling and accelerating distributed deep neural network (DNN) training. It reduces the increasing communication traffic for gradient aggregation. However, existing sparsifiers have poor scalability because of the high computational cost of gradient selection and/or increase in communication traffic. In particular, an increase in communication traffic is caused by gradient build-up and inappropriate threshold for gradient selection.</p><p>To address these challenges, we propose a novel gradient sparsification method called MiCRO. In MiCRO, the gradient vector is partitioned, and each partition is assigned to the corresponding worker. Each worker then selects gradients from its partition, and the aggregated gradients are free from gradient build-up. Moreover, MiCRO estimates the accurate threshold to maintain the communication traffic as per user requirement by minimising the compression ratio error. MiCRO enables nearzero cost gradient sparsification by solving existing problems that hinder the scalability and acceleration of distributed DNN training. In our extensive experiments, MiCRO outperformed state-of-the-art sparsifiers with an outstanding convergence rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Over the past decade, overcoming the excessive communication traffic for gradient aggregation has been a major challenge to enhancing the distributed training performance of deep neural network (DNN) models. Gradient sparsification <ref type="bibr" coords="1,269.80,501.04,11.49,8.92" target="#b0">[1]</ref>- <ref type="bibr" coords="1,285.12,501.04,11.49,8.92" target="#b8">[9]</ref> is a widely-adopted solution for reducing the size of payloads in communication between workers. Gradient sparsification aims to select only large gradients from the entire gradient vector, and the number of sparsified gradients is quantified by the density <ref type="foot" coords="1,113.82,556.41,3.31,6.24" target="#foot_0">1</ref> . Therefore, gradient sparsification can alleviate the communication bottleneck when the communication bandwidth is insufficient to transmit all the gradients at every training iteration.</p><p>Gradient sparsification can be categorised into sorting-and threshold-based approaches. In sorting-based sparsifiers <ref type="bibr" coords="1,278.47,614.76,14.51,8.92" target="#b9">[10]</ref>, <ref type="bibr" coords="1,58.10,626.12,14.51,8.92" target="#b10">[11]</ref>, all gradients are sorted, and the k largest gradients are selected (top-k) for aggregation through communication. However, gradient vector sorting is an expensive operation because of its high computational complexity (e.g., O(n log k) <ref type="bibr" coords="1,58.10,671.54,14.19,8.92" target="#b11">[12]</ref>). Moreover, sorting operations cannot properly utilise the parallelism of streaming processors on GPUs <ref type="bibr" coords="1,237.97,682.90,14.51,8.92" target="#b12">[13]</ref>. Figure <ref type="figure" coords="1,287.67,682.90,8.93,8.92" target="#fig_0">1a</ref>  Both types of sparsifiers cause gradient build-up. All experiments were conducted using d = 0.01 and n = {2, 4, 8, 16} with ResNet-18 on CIFAR-10.</p><p>shows the high computational cost for the gradient vector sorting of the Top-k sparsifier <ref type="bibr" coords="1,416.42,530.15,15.77,8.92" target="#b9">[10]</ref> based on the breakdown of the training time of one iteration. The computational cost remains constant and consumes a significant portion of the training time, regardless of the scale-out degree. Therefore, sortingbased sparsifiers are inadequate for accelerating distributed DNN training. Threshold-based sparsifiers <ref type="bibr" coords="1,430.60,598.29,14.51,8.92" target="#b13">[14]</ref>, <ref type="bibr" coords="1,451.56,598.29,15.77,8.92" target="#b14">[15]</ref> select gradients using a conditional statement that indicates whether each gradient is larger than a threshold. Threshold-based sparsifiers are easier to parallelise and faster than sorting-based sparsifiers. Thus, threshold-based sparsifiers can significantly reduce the computational cost of gradient selection. However, threshold-based sparsifiers have difficulty effectively reducing communication traffic owing to inappropriate thresholds. Predicting a threshold that satisfies the density set by a user is challenging. Figure <ref type="figure" coords="1,543.08,689.15,9.46,8.92" target="#fig_0">1b</ref> shows the excessively high actual density of the hard-threshold sparsifier <ref type="bibr" coords="1,352.05,711.86,15.77,8.92" target="#b13">[14]</ref> where the user-set density was 0.01.</p><p>Additionally, most sparsifiers have difficulties scaling owing to gradient build-up <ref type="bibr" coords="2,140.59,85.06,14.51,8.92" target="#b10">[11]</ref>, which causes that the number of aggregated gradients in the communication becomes larger than the number of gradients selected by each worker. This is because a lot of gradients selected by each worker do not overlap with those of the other workers, although all workers have the same search range for gradient selection. Consequently, the density increases at most n times the userset density, where n is the number of workers. As shown in Figure <ref type="figure" coords="2,85.64,175.92,3.55,8.92" target="#fig_0">1</ref>, the communication traffic increases as the number of workers increases because of the gradient build-up.</p><p>Therefore, the existing sparsifiers cannot effectively scale and accelerate distributed DNN training. Based on our observations, we address the following challenges:</p><p>â€¢ Gradient build-up. This hinders the scalability of the distributed training because the communication traffic increases as the cluster scales out. . With this partitioning approach, MiCRO can not only reduce the computational complexity of gradient selection but also prevent gradient build-up because each worker selects gradients from exclusively assigned partition.</p><p>To reduce the computational cost of the gradient selection, MiCRO adopts threshold-based sparsification instead of sorting-based sparsification <ref type="bibr" coords="2,164.94,497.63,14.51,8.92" target="#b9">[10]</ref>, <ref type="bibr" coords="2,186.28,497.63,14.51,8.92" target="#b10">[11]</ref>. Moreover, the gradient selection of MiCRO is faster than that of existing thresholdbased sparsifiers <ref type="bibr" coords="2,124.06,520.34,14.51,8.92" target="#b13">[14]</ref>, <ref type="bibr" coords="2,145.71,520.34,15.77,8.92" target="#b14">[15]</ref> because the gradient vector partitioning of MiCRO reduces the computational complexity of the gradient selection from O(n g ) to O( ng n ). In addition, the model accuracy of MiCRO can be maintained at the same level as that of other sparsifiers <ref type="bibr" coords="2,157.95,565.77,14.51,8.92" target="#b9">[10]</ref>, <ref type="bibr" coords="2,178.70,565.77,14.51,8.92" target="#b13">[14]</ref>. This is because the result of filtering elements (gradients) larger than the threshold in an array (gradient vector) is invariant, regardless of whether the array is partitioned or not.</p><p>Furthermore, MiCRO satisfies the communication traffic at the user-set level by estimating the threshold more accurately. MiCRO estimates the threshold by minimising the compression ratio error, which is defined as the difference between the actual and user-set densities. Therefore, MiCRO can maintain a low communication cost throughout the training period by estimating the accurate threshold and eliminating 2 MiCRO is an acronym for minimising compression ratio error on-the-fly.</p><p>gradient build-up. By addressing these challenges, MiCRO enables near-zero cost gradient sparsification for scalable and accelerated distributed DNN training.</p><p>This study makes the following contributions:</p><p>â€¢ Exclusive gradient selection. This eliminates the gradient build-up. In other words, communication efficiency can be improved because exclusive gradients are aggregated between workers. Moreover, computational cost can be reduced as the cluster scales out. This primarily contributes to the scalability of distributed DNN training. â€¢ Accurate threshold estimation. This prevents an excessively high density due to an inappropriate threshold. In other words, the communication traffic can be maintained as low as the user-set value. This mainly contributes to the acceleration of distributed DNN training. â€¢ Multidimensional evaluation. This study provides an extensive set of experiments and analyses for performance and efficiency comparisons between MiCRO and stateof-the-art sparsifiers. The remainder of this paper is organised as follows. Section II presents the preliminaries of the study. Section III clarifies the limitations of the state-of-the-art gradient sparsification methods. Section IV proposes MiCRO, which is designed to address the challenges stated in this study. Section V verifies our contributions through thorough empirical comparisons between MiCRO and state-of-the-art gradient sparsifiers. Finally, Section VI concludes the paper. II. PRELIMINARIES Gradient sparsification is a type of lossy algorithm because most of the computed gradients are discarded at every iteration. In terms of computational efficiency, discarding the majority of gradients is unproductive because backward propagation comprises a massive number of computational operations in DNNs. Moreover, discarded gradients cause a difference between sparsified and non-sparsified DNN models in distributed training. Thus, the fidelity loss of the sparsified model must be reduced to apply gradient sparsification to distributed training.</p><p>Error feedback <ref type="bibr" coords="2,383.65,511.87,15.77,8.92" target="#b15">[16]</ref> is an auxiliary method for sparsifiers to reduce the fidelity loss caused by discarded gradients. Instead of discarding unselected gradients, the error feedback locally accumulates them into the n g -dimensional vector e i,t , where i and t are the iteration and worker numbers, respectively. In other words, each element of e i,t represents the accumulated gradient of one parameter. When each gradient is selected, the accumulated value is initialised to zero because the gradient contributes to the model update. Hereafter, we refer to the L2norm of e i,t as the local error denoted by e i,t . Accordingly, the error<ref type="foot" coords="2,347.75,624.02,3.31,6.24" target="#foot_1">3</ref> at iteration t is defined as follows:  In other words, minimising e t results in a reduction of difference between sparsified and non-sparsified models. However, it is challenging to maintain a high model training performance while minimising e t . To minimise e t , the sparsifier should initialise a lot of accumulated gradients to zero in e i,t . Because this implies a high density of sparsified gradients, the training performance slows down because of the huge communication traffic.</p><formula xml:id="formula_0">e t = 1 n nâˆ’1 i=0 e i,t .<label>(1)</label></formula><p>Figure <ref type="figure" coords="3,95.48,404.18,4.73,8.92" target="#fig_1">2</ref> shows the error variations in the two sparsifiers over the training iterations. The hard-threshold sparsifier selects all the gradients larger than the threshold, thus maintaining a consistent error level. However, the error of the hardthreshold sparsifier is much lower than that of Top-k in most iterations; thus, it is clear that the hard-threshold sparsifier can show a significantly higher density than Top-k. In other words, communication for the gradient aggregation of hard-threshold sparsifier is expensive. We verify how much runtime of hardthreshold sparsifier is occupied by the communication through experiments detailed in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LIMITATIONS OF STATE-OF-THE-ART METHODS</head><p>In this section, we discuss the limitations of state-of-the-art gradient sparsifiers. Table I lists the strengths and weaknesses of the state-of-the-art sparsifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sorting-based sparsifiers</head><p>In gradient sparsification, most computationally inefficient results are obtained from the gradient vector sorting phase of sorting-based sparsifiers, such as Top-k <ref type="bibr" coords="3,214.50,629.73,15.77,8.92" target="#b9">[10]</ref> and cyclic local top-k (CLT-k) <ref type="bibr" coords="3,111.91,641.08,14.51,8.92" target="#b10">[11]</ref>. The extremely high sorting cost is the main hindrance to the scalability and acceleration of distributed training.</p><p>In terms of communication efficiency, CLT-k maintains the user-set density by eliminating the gradient build-up of the Top-k. In CLT-k, a worker becomes the leader worker at each iteration and is delegated to determine all the gradient indices to be aggregated. Consequently, the number of aggregated gradients is the same as that of the selected gradients. However, the delegation policy for gradient selection has two side effects. First, most of the computing resources used by all other workers cannot be utilised during the gradient selection of the leader worker. Second, the model fidelity may be reduced because only one worker determines the gradient indices that contribute to the model update at each iteration. Therefore, Top-k and CLT-k exhibit limitations in terms of scalability and training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Threshold-based sparsifiers</head><p>The computational cost of gradient selection can be reduced considerably using threshold-based sparsifiers such as hardthreshold sparsifier <ref type="bibr" coords="3,396.58,492.60,15.77,8.92" target="#b13">[14]</ref> and SIDCo <ref type="bibr" coords="3,470.52,492.60,14.51,8.92" target="#b14">[15]</ref>. However, they have limitations in terms of communication inefficiency. In addition to the gradient build-up problem, the actual density is excessively high because of inaccurate threshold estimation. This is mainly because their threshold estimation methods are insufficiently generalised to fit various types of models and datasets well. In particular, hard-threshold sparsifier may not estimate the appropriate threshold with untested training settings because its threshold should be defined before training begins and remain constant for every iteration. Therefore, hard-threshold sparsifier requires hyperparameter tuning for each model and dataset.</p><p>In contrast, SIDCo derives a threshold using a statistical model at each iteration. Because the threshold changes over the training iterations, the density can be adjusted more flexibly than with a hard-threshold sparsifier. However, SIDCo is based on several predefined statistical models, and threshold estimation by statistical models requires a high computational overhead. In summary, both hard-threshold sparsifier and The limitations of state-of-the-art sparsifiers show that it is challenging to satisfy all the criteria listed in Table <ref type="table" coords="4,275.10,370.97,2.76,8.92" target="#tab_1">I</ref>. We not only overcome the limitations of state-of-the-art methods using a novel sparsifier but also achieve it with near-zero cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MICRO DESIGN</head><p>We designed MiCRO as a threshold-based gradient sparsifier, in which each worker selects gradients from the exclusive partition of the entire gradient vector. Figure <ref type="figure" coords="4,254.95,449.33,4.73,8.92">3</ref> presents an overview of MiCRO. MiCRO comprises the following sequences: 1) coarse-grained gradient vector partitioning; 2) exclusive gradient selection by threshold; and 3) minimisation of compression ratio error by threshold scaling. These processes begin after backward propagation at each iteration. The following subsections present a detailed discussion of each process of MiCRO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Coarse-grained gradient vector partitioning</head><p>MiCRO equally divides the entire gradient vector into n partitions and assigns each partition to the corresponding worker. By gradient vector partitioning, each worker can obtain a search range that is exclusive to that particular worker. The partition assignment is not fixed for every iteration. At the beginning of each iteration, the partitioned vectors are assigned to the workers in a cyclic order. That is, each worker has the opportunity to search for gradients in the entire gradient vector in every n iterations, and it also has the chance to select its local gradients at every iteration.</p><p>As shown in Figure <ref type="figure" coords="4,153.15,675.24,3.55,8.92">3</ref>, the entire gradient vector is partitioned in a coarse-grained manner. Each partitioned vector has a subsequent range of gradient indices. We designed this coarse-grained gradient vector partitioning method by considering the GPU memory access pattern. Because each thread group of the GPU (i.e., warp in CUDA <ref type="bibr" coords="4,481.53,359.55,15.14,8.92" target="#b16">[17]</ref>) accesses the global memory simultaneously, the elements that the threads want to access should be in a cache line to utilise parallelism. Therefore, coarse-grained partitioning enables efficient GPU global memory access, unlike fine-grained partitioning, such as interleaved partitioning, which severely degrades performance owing to memory divergence <ref type="bibr" coords="4,431.28,427.69,14.51,8.92" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exclusive gradient selection</head><p>To prevent gradient build-up, a sparsifier should provide workers with a non-overlapping search space. Because the gradient build-up results from overlapping search spaces between workers, MiCRO restricts the search space of each worker to one partitioned vector, which is divided by coarse-grained gradient vector partitioning. Accordingly, each worker in the MiCRO can select gradients in its exclusively partitioned gradient vector, and gradient build-up never occurs. As shown in Figure <ref type="figure" coords="4,342.66,560.82,3.55,8.92">3</ref>, exclusive gradient selection enables nonoverlapping selected gradients.</p><p>Notably, MiCRO prevents loss of model fidelity, unlike CLT-k <ref type="bibr" coords="4,341.45,595.74,14.51,8.92" target="#b10">[11]</ref>. In CLT-k, each worker has the chance to select its local gradients once every n iterations. However, workers have no selection authority for the remaining n âˆ’ 1 iterations. In other words, the locally computed and accumulated gradients of each worker will become stale. By contrast, MiCRO does not suffer from model fidelity loss because all workers can participate in the model update at every iteration. Moreover, the threshold-based gradient selection of MiCRO prevents model fidelity loss because all gradients are inspected to determine whether each of them is larger than the threshold. In other words, the significance of selecting the largest nâˆ’1 i=0 k i,t gradients is maintained.</p><p>In addition, exclusive gradient selection reduces the computational complexity of threshold-based gradient selection from O(n g ) to O( ng n ). Therefore, scalability is enhanced because of the reduction in computational cost as the number of workers increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compression ratio error minimisation by threshold scaling</head><p>To prevent a high density caused by an inappropriate threshold, a sparsifier should estimate the accurate threshold to achieve the user-set density. MiCRO focuses on minimising the compression ratio error at each iteration. Let k and k i,t be the number of gradients that should be selected and the number of gradients selected by worker i at iteration t, respectively. If |k âˆ’ k i,t | is close to zero, it implies that the threshold is appropriate to satisfy the user-set value. Thus, minimising |k âˆ’ k i,t | is crucial for adjusting the communication traffic of gradient aggregation.</p><p>As shown in Figure <ref type="figure" coords="5,148.14,297.41,3.55,8.92">3</ref>, k i,0 may be far from the user set k because it is difficult to predict the initial threshold Î´ 0 accurately at iteration 0. To minimise |k âˆ’k i,t |, MiCRO adjusts the threshold at each iteration by inspecting whether the number of selected gradients are larger than k. This threshold scaling has two advantages over statistical threshold estimation <ref type="bibr" coords="5,256.30,354.20,14.51,8.92" target="#b14">[15]</ref>. First, it is robust to varying training settings such as models, datasets, and learning parameters because only the compression ratio error is considered when adjusting the threshold. Moreover, additional overhead does not occur because inspecting |kâˆ’k i,t | and adjusting the threshold are performed by inspecting the condition statement and merely assigning an adjusted value to the threshold, respectively. Therefore, the threshold scaling of MiCRO is generally applicable to various training settings and is faster than statistical model-based threshold estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall workflow of MiCRO</head><p>Algorithm 1 presents the pseudocode of the distributed SGD with gradient sparsification of MiCRO. In line 6, the gradients computed by backward propagation accumulate in local error. In line 7, the dedicated partition number is assigned to each worker in cyclic order. In line 8, the entire gradient vector is divided into n partitions, and each partition is assigned to a dedicated worker. In line 9, each worker selects gradients based on the threshold in its exclusive partition and returns the indices of the selected gradients. According to the partition and selection policies, gradient build-up never occurs because the selected indices of each worker do not overlap with those of the other workers. From lines 10 to 12, the globally selected indices are collected and the averages of the selected gradients are computed. In line 13, the threshold of the next iteration is derived based on the compression ratio error minimisation. As the iterations proceed, the threshold approaches a value that satisfies the user-set density. In line 14, the model is updated using averaged gradients. In lines 15 and 16, the accumulated value of each selected gradient is initialised to zero, and those of the unselected gradients become the local error of the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION A. Methodology</head><p>System configuration. All the experiments were conducted on a cluster equipped with 16 GPUs. A cluster comprises two nodes, each with eight NVIDIA Tesla V100 GPUs with NVLink, two 16-core Intel Xeon Gold 6226R @ 2.90 GHz CPUs, and 384 GB DDR4 memory. For all the experiments, mpirun of OpenMPI 4.0.5 <ref type="bibr" coords="5,425.01,551.94,15.77,8.92">[19]</ref> was used for multiprocess execution to automatically assign an exclusive rank to each worker. Each worker was run on one GPU with CUDA 10.1 <ref type="bibr" coords="5,313.59,586.01,14.51,8.92" target="#b16">[17]</ref>.</p><p>Models and datasets. We evaluated the performance of Mi-CRO and other sparsifiers (Top-k, CLT-k, and hard-threshold sparsifiers) on computer vision applications. For the DNN models, we used ResNet-18 <ref type="bibr" coords="5,420.89,631.36,14.51,8.92" target="#b18">[20]</ref>, GoogLeNet <ref type="bibr" coords="5,488.07,631.36,14.51,8.92" target="#b19">[21]</ref>, and SENet-18 <ref type="bibr" coords="5,327.24,642.72,14.51,8.92" target="#b20">[22]</ref>. For datasets, CIFAR-10 and CIFAR-100 <ref type="bibr" coords="5,513.77,642.72,15.77,8.92" target="#b21">[23]</ref> were used. To conduct an extensive set of experiments and analyses, our evaluation comprised multidimensional training settings, as listed in Table <ref type="table" coords="5,391.97,676.79,5.78,8.92" target="#tab_1">II</ref>. By changing one factor among the model, dataset, and density, the impact of each factor on the performance of each sparsifier can be identified. Implementation. We implemented MiCRO and other approaches on top of the deep learning framework PyTorch 1.5 <ref type="bibr" coords="6,58.02,496.59,14.51,8.92" target="#b22">[24]</ref>. The distributed communication package PyTorch was used to implement the communication routine for the distributed training. Moreover, NCCL 2.4 [25] was adopted as a backend to support multi-GPU and multi-node communication primitives such as broadcast, all-gather, and all-reduce, which are optimised for NVIDIA GPUs and networking. To fairly compare the appropriateness of the thresholds between the MiCRO and the hard-threshold sparsifier, the initial threshold Î´ 0 of the MiCRO was set to that of the hard-threshold sparsifier. The source code includes everything required to reproduce the results of this study, and is available publicly at https://github.com/kljp/micro.</p><p>Metrics. The metrics used for each type of performance evaluation are as follows:</p><p>â€¢ Convergence performance: The test accuracy by runtime was measured to evaluate how fast each sparsifier attained the final accuracy in 200 epochs. â€¢ Sparsification performance: The actual density was mea-sured to evaluate whether each sparsifier could satisfy the user-set density. sorting and the model fidelity is reduced by the delegated gradient selection policy. Top-k requires a long training time, similar to that of CLT-k. However, the convergence rate of Top-k was faster than that of CLT-k. This is because Top-k entails a gradient build-up, which makes Top-k select a lot more gradients than CLT-k (i.e., at most n times the userset density). Although the hard-threshold sparsifier has no computational cost for gradient sorting, its convergence rate is slower than that of MiCRO because it suffers from a large increase in communication traffic owing to the inappropriate threshold and gradient build-up. Sparsification performance. Fig <ref type="figure" coords="7,198.26,608.38,4.73,8.92" target="#fig_4">5</ref> shows the sparsification performance of each sparsifier in multidimensional training settings. In every experiment, MiCRO exhibited the actual density close to the user-set density owing to the threshold based on compression ratio error minimisation. Moreover, gradient build-up does not occur because of the exclusive gradient selection with gradient vector partitioning.</p><p>However, the actual densities of the Top-k and hardthreshold sparsifiers were not close to the user-set densities because of gradient build-up. In particular, hard-threshold sparsifier exhibited the excessively high actual density owing to inappropriate threshold in every experiment. Despite the increasing gradient accumulation during training iterations, the hard-threshold sparsifier used only a fixed threshold. In other words, the actual density increased as the iterations proceeded. In every experiment, the density of the hardthreshold sparsifier dropped suddenly after iteration 14,600. This is because we set the learning rate decay at epoch 150.</p><p>That is, the model almost converges after that epoch.</p><p>End-to-end training performance. Fig <ref type="figure" coords="7,480.66,597.02,4.73,8.92" target="#fig_5">6</ref> shows the breakdown of the training time for one iteration. For each sparsifier, the wall-clock time of one iteration was measured by the slowest worker, and the average time was calculated across all iterations. The experiment was repeated using four different seeds for each sparsifier. Finally, the average wall-clock time shown in Figure <ref type="figure" coords="7,379.57,665.16,4.73,8.92" target="#fig_5">6</ref> was obtained from the average value of the four executions.</p><p>In Figure <ref type="figure" coords="7,362.56,687.54,3.55,8.92" target="#fig_5">6</ref>, the training time comprises the forward propagation, backward propagation, gradient selection, communica- In contrast, MiCRO showed considerably faster training performance than the other sparsifiers in every experiment. This is because MiCRO reduces the computational cost to near-zero by using exclusive gradient selection with gradient vector partitioning and reduces the communication cost by eliminating gradient build-up and estimating the accurate threshold without overhead. As discussed in Section IV-C, the threshold estimation yields zero overhead because it only includes the condition statement for inspecting |k âˆ’ k i,t | and the assignment of the adjusted value to a variable (i.e., the threshold). Moreover, the gradient vector partitioning of MiCRO does not yield any overhead because this process only determines the starting index of each partition. Therefore, when MiCRO is applied to distributed DNN training, the cost of gradient sparsification is near-zero, and this advantage contributes significantly to the scalability and acceleration of distributed training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effectiveness evaluation of MiCRO</head><p>Scalability. MiCRO shows that every case consistently attains a similar convergence point, regardless of the number of workers. Moreover, the convergence rate was significantly accelerated by scale-out. These scalability and acceleration mainly result from communication cost reduction by eliminating gradient build-up and estimating the accurate threshold.</p><p>Threshold estimation performance. In this experiment, we evaluate the threshold estimation performance of the MiCRO. To maintain the accurate density, the threshold should be changed according to the error variation. That is, the threshold should increase when the error increases to prevent an increase in density. To identify whether MiCRO can satisfy this principle, we plotted the threshold and error trends. As the magnitude of the error is much larger than the threshold, we scaled the magnitude of the error to fit within a range of threshold changes. Thus, we multiplied the error of each iteration by the scaling factor, defined as the ratio of T âˆ’1 j=0 Î´ j to T âˆ’1 j=0 e j , where T is the number of iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we propose MiCRO, which partitions the gradient vector and assigns each partition to a corresponding worker. The design of MiCRO comprises coarse-grained gradient vector partitioning, exclusive gradient selection, and compression ratio error minimisation through threshold scaling. Using these components, MiCRO can achieve high performance in terms of convergence, sparsification, and threshold estimation. Consequently, it enables near-zero cost gradient sparsification by providing remarkable training efficiency owing to its reduced computational and communication costs. In our thorough empirical experiments, MiCRO outperformed state-of-the-art sparsifiers in terms of the scalability and acceleration of distributed DNN training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,314.05,418.35,238.50,8.92;1,314.05,429.71,238.51,8.92;1,314.05,441.06,238.51,8.92;1,314.05,452.42,238.50,8.92;1,314.05,463.78,238.51,8.92;1,314.05,474.76,238.50,9.46;1,314.04,486.49,118.86,8.92"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Challenges in scalable and accelerated gradient sparsification: (a) High computational cost due to gradient vector sorting in sorting-based sparsifiers; (b) high communication cost due to inappropriate threshold in threshold-based sparsifiers. Both types of sparsifiers cause gradient build-up. All experiments were conducted using d = 0.01 and n = {2, 4, 8, 16} with ResNet-18 on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,64.65,151.77,488.38,8.92;3,64.65,163.13,69.24,8.92"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Error minimisation performance of sparsifiers on 16 GPUs. The Y-axis indicates the error, which is the average of local errors of workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,246.00,304.47,112.92,8.92"><head>1 Fig. 3 :</head><label>13</label><figDesc>Fig. 3: Overview of MiCRO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,85.13,441.51,434.16,8.92"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Convergence performance of sparsifiers on 16 GPUs. All experiments were conducted over 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,64.64,440.07,488.37,8.92;7,64.64,451.43,37.59,8.92"><head>â€¢Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Sparsification performance of sparsifiers on 16 GPUs. The Y-axis indicates the actual density measured over training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,64.40,404.79,488.38,8.92"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Training time breakdown of sparsifiers on 16 GPUs. The training time is the average wall-clock time for one iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="8,372.83,480.04,179.95,8.92;8,314.27,491.39,238.51,8.92"><head></head><label></label><figDesc>Fig 7 shows the convergence performance of MiCRO by scale-out in our multidimensional training settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="9,96.24,441.60,424.69,8.92"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Convergence performance of MiCRO by scale-out. All experiments were conducted over 200 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="10,178.88,358.74,259.72,8.92"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Threshold estimation performance of MiCRO on 16 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="2,57.09,268.47,238.51,172.64"><head></head><label></label><figDesc>In this study, we propose MiCRO 2 to address these challenges. MiCRO divides the tensor of the entire model equally into multiple partitions and assigns them exclusively to workers to reduce the search range for the gradient selection from n g to</figDesc><table /><note>â€¢ Inappropriate threshold. Because the inappropriate threshold results in an extremely high actual density, it is difficult to reduce the communication traffic of the gradient aggregation and accelerate the distributed training.â€¢ Gradient selection cost. Sorting the gradient vector for sparsification incurs a high computational cost. Because its high cost remains constant regardless of the scale-out of a cluster, it limits the acceleration of distributed DNN training.ng n</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="3,70.33,184.14,435.84,38.24"><head>TABLE I :</head><label>I</label><figDesc>Strengths and weaknesses of state-of-the-art gradient sparsifiers and the proposed MiCRO.</figDesc><table coords="3,70.33,205.90,344.95,16.48"><row><cell>Sparsifier</cell><cell>Gradient build-up</cell><cell>Unpredictable high density</cell><cell>Hyperparameter tuning</cell><cell>Model fidelity loss</cell><cell>Worker idling</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We refer to the ratio of selected gradients (k) to all gradients (ng) as density, which is defined as d = k ng .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">In this study, the terms 'error' and 'compression ratio error' are distinguished. The error indicates the difference between sparsified and nonsparsified models. The compression ratio error indicates the difference between actual and user-set densities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Authorized licensed use limited to: HUNAN UNIVERSITY. Downloaded on June 03,2025 at 15:22:21 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their insightful feedback. This work was jointly supported by the ITRC program (IITP-2023-2018-0-01431) of IITP, BK21 FOUR program (NRF5199991014091), and Basic Science Research Program (2021R1F1A1062779) of National Research Foundation of Korea.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient selection cost</head><p>Additional overhead Top-k [10] Yes Yes No No No Very high No CLT-k [11] No No No Yes Yes Very high No Hard-threshold [14] Yes Yes Yes No No Very low No SIDCo [15] Yes Yes No No No Very low Very high MiCRO No No No No No Near-zero No</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="9,327.85,580.79,221.16,7.13;9,327.85,589.31,221.16,7.13;9,327.85,597.69,221.16,7.26;9,327.85,606.21,211.14,7.26" xml:id="b0">
	<analytic>
		<title level="a" type="main">Adacomp: Adaptive residual gradient compression for data-parallel distributed training</title>
		<author>
			<persName coords=""><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,327.85,614.93,221.16,7.13;9,327.85,623.45,221.16,7.13;9,327.85,631.84,221.16,7.26;9,327.85,640.48,77.94,7.13" xml:id="b1">
	<analytic>
		<title level="a" type="main">Giannis Bekoulis, and Nikos Deligiannis. Learned gradient compression for distributed deep learning</title>
		<author>
			<persName coords=""><forename type="first">Lusine</forename><surname>Abrahamyan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yiming</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7330" to="7344" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,327.85,649.07,221.16,7.13;9,327.85,657.46,221.16,7.26;9,327.85,665.98,176.20,7.26" xml:id="b2">
	<analytic>
		<title level="a" type="main">Errorcompensatedx: error compensation for variance reduced algorithms</title>
		<author>
			<persName coords=""><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18102" to="18113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,327.85,674.69,221.16,7.13;9,327.85,683.21,221.16,7.13;9,327.84,691.60,221.16,7.26;9,327.84,700.24,73.24,7.13" xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural compression for distributed deep learning</title>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>HorvÃ³th</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen-Yu</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ludovit</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Atal</forename><surname>Narayan Sahu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>RichtÃ¡rik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical and Scientific Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="129" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.81,392.45,221.15,7.13;10,74.81,400.97,221.16,7.13;10,74.81,409.36,221.16,7.26;10,74.81,417.88,88.92,7.26" xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive gradient communication via critical learning regime identification</title>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="55" to="80" />
		</imprint>
	</monogr>
	<note>Shivaram Venkataraman, and Dimitris Papailiopoulos</note>
</biblStruct>

<biblStruct coords="10,74.81,426.58,221.16,7.13;10,74.81,435.10,221.16,7.13;10,74.81,443.61,221.16,7.13;10,74.81,452.00,221.16,7.26;10,74.81,460.52,154.57,7.26" xml:id="b5">
	<analytic>
		<title level="a" type="main">A distributed synchronous sgd algorithm with global top-k sparsification for low bandwidth networks</title>
		<author>
			<persName coords=""><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2238" to="2247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.81,469.22,221.16,7.13;10,74.81,477.74,221.16,7.13;10,74.81,486.13,221.16,7.26;10,74.81,494.65,76.89,7.26" xml:id="b6">
	<monogr>
		<title level="m" type="main">Layer-wise adaptive gradient sparsification for distributed deep learning with convergence guarantees</title>
		<author>
			<persName coords=""><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaiyong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08727</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="10,74.81,503.35,221.16,7.13;10,74.81,511.86,221.16,7.13;10,74.81,520.25,221.16,7.26;10,74.81,528.77,221.16,7.26;10,74.81,537.42,17.04,7.13" xml:id="b7">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed deep learning with merged gradient sparsification on gpus</title>
		<author>
			<persName coords=""><forename type="first">Shaohuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xinxiao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2020-IEEE Conference on Computer Communications</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="406" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.81,545.99,221.16,7.13;10,74.81,554.38,221.16,7.26;10,74.81,562.90,221.16,7.26;10,74.81,571.54,48.08,7.13" xml:id="b8">
	<analytic>
		<title level="a" type="main">Near-optimal sparse allreduce for distributed deep learning</title>
		<author>
			<persName coords=""><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
				<meeting>the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="135" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.81,580.12,221.16,7.13;10,74.81,588.63,221.16,7.13;10,74.80,597.02,221.16,7.26;10,74.80,605.67,29.15,7.13" xml:id="b9">
	<analytic>
		<title level="a" type="main">The convergence of sparsified gradient methods</title>
		<author>
			<persName coords=""><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mikael</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nikola</forename><surname>Konstantinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sarit</forename><surname>Khirirat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">CÃ©dric</forename><surname>Renggli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.80,614.24,221.16,7.13;10,74.80,622.76,221.15,7.13;10,74.80,631.28,221.16,7.13;10,74.80,639.79,221.16,7.13;10,74.80,648.18,221.16,7.26;10,74.80,656.83,72.89,7.13" xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalecom: Scalable sparsified gradient compression for communication-efficient distributed training</title>
		<author>
			<persName coords=""><forename type="first">Chia-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiamin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Naigang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Viji</forename><surname>Vijayalakshmi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kailash</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Gopalakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13551" to="13563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.80,665.40,221.16,7.13;10,74.80,673.79,221.16,7.26;10,74.80,682.31,221.16,7.26;10,74.80,690.96,17.04,7.13" xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient top-k query processing on massively parallel hardware</title>
		<author>
			<persName coords=""><forename type="first">Anil</forename><surname>Shanbhag</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Management of Data</title>
				<meeting>the 2018 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1557" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,74.80,699.52,221.16,7.13;10,331.76,392.47,221.15,7.13;10,331.76,400.86,221.16,7.26;10,331.76,409.38,221.16,7.26;10,331.76,418.02,17.04,7.13" xml:id="b12">
	<analytic>
		<title level="a" type="main">Dr. top-k: delegate-centric top-k on gpus</title>
		<author>
			<persName coords=""><forename type="first">Anil</forename><surname>Gaihre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Weitze</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lingda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoye</forename><forename type="middle">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,426.57,221.16,7.13;10,331.76,435.09,221.16,7.13;10,331.76,443.48,221.16,7.26;10,331.76,452.00,93.84,7.26" xml:id="b13">
	<analytic>
		<title level="a" type="main">Rethinking gradient sparsification as total error minimization</title>
		<author>
			<persName coords=""><forename type="first">Atal</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aritra</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Abdelmoniem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trambak</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Panos</forename><surname>Kalnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8133" to="8146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,460.67,221.16,7.13;10,331.76,469.19,221.16,7.13;10,331.76,477.58,221.16,7.26;10,331.76,486.10,126.91,7.26" xml:id="b14">
	<analytic>
		<title level="a" type="main">An efficient statistical-based gradient compression technique for distributed training systems</title>
		<author>
			<persName coords=""><forename type="first">Ahmed</forename><surname>Ahmed M Abdelmoniem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohamed-Slim</forename><surname>Elzanaty</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marco</forename><surname>Alouini</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Canini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="297" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,494.78,221.16,7.13;10,331.76,503.29,221.17,7.13;10,331.76,511.68,221.16,7.26;10,331.76,520.20,148.87,7.26" xml:id="b15">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</title>
		<author>
			<persName coords=""><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth annual conference of the international speech communication association</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,528.88,190.94,7.13" xml:id="b16">
	<monogr>
		<ptr target="https://developer.nvidia.com/cuda-toolkit/" />
		<title level="m">Nvidia cuda</title>
				<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,537.43,221.16,7.13;10,331.76,545.94,221.16,7.13;10,331.76,554.33,176.33,7.26" xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling emerging memory-divergent gpu applications</title>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Magnus</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Almutaz</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhiying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="98" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,571.56,221.16,7.13;10,331.76,579.95,221.16,7.26;10,331.76,588.47,221.16,7.26;10,331.76,597.11,17.04,7.13" xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,605.66,221.16,7.13;10,331.76,614.18,221.16,7.13;10,331.76,622.57,221.15,7.26;10,331.76,631.09,221.16,7.26;10,331.76,639.73,25.37,7.13" xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName coords=""><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,648.28,221.16,7.13;10,331.76,656.67,221.16,7.26;10,331.76,665.19,114.64,7.26" xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.76,673.87,221.16,7.13;10,331.76,682.26,142.50,7.26" xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct coords="10,331.76,690.93,108.56,7.13" xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
