<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIREncoder: Multi-modal IR-based Pretrained Embeddings for Performance Optimizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>ACM</publisher>
				<availability status="unknown"><p>Copyright ACM</p>
				</availability>
				<date type="published" when="2024-10-13">2024-10-13</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Akash</forename><surname>Dutta</surname></persName>
							<idno type="ORCID">0009-0007-0947-1182</idno>
						</author>
						<author role="corresp">
							<persName coords="1,397.84,131.50,63.12,10.59"><forename type="first">Ali</forename><surname>Jannesari</surname></persName>
							<email>jannesar@iastate.edu</email>
							<idno type="ORCID">0000-0001-8672-5317</idno>
							<affiliation key="aff0">
								<orgName type="institution">Iowa State University United States</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MIREncoder: Multi-modal IR-based Pretrained Embeddings for Performance Optimizations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques</title>
						<meeting>the 2024 International Conference on Parallel Architectures and Compilation Techniques						</meeting>
						<imprint>
							<publisher>ACM</publisher>
							<biblScope unit="page" from="156" to="167"/>
							<date type="published" when="2024-10-13" />
						</imprint>
					</monogr>
					<idno type="MD5">181C7573726AD2FD030C9FAFE9D8710C</idno>
					<idno type="DOI">10.1145/3656019.3676895</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2025-08-05T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Computing methodologies → Parallel computing methodologies</term>
					<term>Knowledge representation and reasoning</term>
					<term>Supervised learning by classification</term>
					<term>Neural networks</term>
					<term>Modeling methodologies Pre-training, GNN, Multi-modal Modeling, Performance Optimization, Auto-tuning, LLVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the primary areas of interest in High Performance Computing is the improvement of performance of parallel workloads. Nowadays, compilable source code-based optimization tasks that employ deep learning often exploit LLVM Intermediate Representations (IRs) for extracting features from source code. Most such works target specific tasks, or are designed with a pre-defined set of heuristics. So far, pre-trained models are rare in this domain, but the possibilities have been widely discussed. Especially approaches mimicking large-language models (LLMs) have been proposed. But these have prohibitively large training costs. In this paper, we propose MIREncoder, a Multi-modal IR-based Auto-Encoder that can be pre-trained to generate a learned embedding space to be used for downstream tasks by machine learning-based approaches. A multi-modal approach enables us to better extract features from compilable programs. It allows us to better model code syntax, semantics and structure. For code-based performance optimizations, these features are very important while making optimization decisions. A pre-trained model/embedding implicitly enables the usage of transfer learning, and helps move away from task-specific trained models. Additionally, a pre-trained model used for downstream performance optimization should itself have reduced overhead, and be easily usable. These considerations have led us to propose a modeling approach that i) understands code semantics and structure, ii) enables use of transfer learning, and iii) is small and simple enough to be easily re-purposed or reused even with low resource availability. Our evaluations will show that our proposed approach can outperform the state of the art while reducing overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The complexity, scale, and heterogeneity of HPC hardware has increased significantly over the past several years improving performance over traditional multi-core systems. However, this has also opened up new opportunities of performance optimizations. Performance engineers and application developers devote considerable time in trying to tune and optimize hardware and software knobs. However, it is extremely difficult to adapt to a constantly changing landscape. Automated techniques are thus necessary to help optimize performance of HPC applications. Prior Works. A large chunk of performance gains for parallel applications come from compiler optimizations, such as those seen in LLVM and GCC. Although such optimizations are painstakingly designed, it might not work in all cases due to the variety of applications seen in HPC. In addition to compiler-driven optimizations, runtime performance tuning by online auto-tuners <ref type="bibr" coords="1,509.70,427.52,9.44,7.94" target="#b6">[7,</ref><ref type="bibr" coords="1,521.40,427.52,10.35,7.94" target="#b41">42,</ref><ref type="bibr" coords="1,534.00,427.52,10.35,7.94" target="#b49">50,</ref><ref type="bibr" coords="1,546.61,427.52,11.59,7.94" target="#b57">58]</ref> also help identify configurations/parameters that might often be non-intuitive. Although this improves performance, it comes with significant tuning overhead.</p><p>Machine learning (ML) based techniques have also been widely used for such performance optimizations. Several works have used ML to model handcrafted features for specific tasks <ref type="bibr" coords="1,500.55,493.27,10.27,7.94" target="#b0">[1,</ref><ref type="bibr" coords="1,513.00,493.27,6.10,7.94" target="#b6">7,</ref><ref type="bibr" coords="1,521.28,493.27,10.27,7.94" target="#b12">13,</ref><ref type="bibr" coords="1,533.73,493.27,10.27,7.94" target="#b29">30,</ref><ref type="bibr" coords="1,546.18,493.27,10.05,7.94" target="#b42">43]</ref>. These handcrafted features are not universal and might not be suitable for other optimization tasks. To overcome these shortcomings, studies based on code representational learning were proposed. Most of these works proposed a means of representing source code in a way understandable by machine learning models. Various works <ref type="bibr" coords="1,338.39,559.03,8.31,7.94" target="#b2">[3]</ref><ref type="bibr" coords="1,346.69,559.03,4.15,7.94" target="#b3">[4]</ref><ref type="bibr" coords="1,350.84,559.03,8.31,7.94" target="#b4">[5]</ref> designed representations on top of source code for tasks such as variable misuse and method name prediction. However, such representations put a lot of emphasis on stylistic choices in source code, are language dependent, thus are not ideal candidates for performance optimization tasks of compilable source code. Our proposed approach can, on the other hand, work with multiple languages as shown later in Section 4.</p><p>These aforementioned representations are also not adept at capturing program dependencies. Thus LLVM IR based approaches have been proposed. Several works <ref type="bibr" coords="1,449.58,657.66,12.17,7.94" target="#b8">[9,</ref><ref type="bibr" coords="1,464.86,657.66,10.35,7.94" target="#b17">18,</ref><ref type="bibr" coords="1,478.31,657.66,10.35,7.94" target="#b50">51,</ref><ref type="bibr" coords="1,491.76,657.66,11.59,7.94" target="#b52">53]</ref> have outlined IR-based code representations for downstream optimizations. However, these are dependent on manual design choices and heuristics. Additionally, these representations usually need complex, resource intensive modeling techniques for each downstream task and might increase the barrier to entry for new researchers. Working with self-supervised pre-trained models and using transfer learning for downstream tasks might help alleviate such shortcomings. This is our aim in this work.</p><p>To better represent source code/IRs, we believe modeling both syntax and semantics are equally important. And modeling each as separate modalities seems logical. However, representing source code as each such modality, and re-training from scratch for each target task adds complexity and increases resource requirements. Therefore, we propose an IR-based pre-trained encoder for performance optimizations. This allows us to remove dependency on individual programming languages and target optimizations on both CPUs and GPUs with the same pre-trained encoder. Our Contributions. In this paper, we have proposed an IR-based self-supervised multi-modal pre-training approach (MIREncoder) with the aim of generating encodings/features for downstream tasks. Unlike prior code representations, our pipeline is completely self-supervised and only needs an LLVM IR as input for both pretraining and target optimization tasks. The IR statements in the input files are modeled to extract syntactic features during the pre-training process. This represents the first modality in our pretraining pipeline. The input IRs are also converted to multi-graphs that includes data-flow, control-flow, and call-flow information. This forms the second modality of our approach.</p><p>MIREncoder employs three pre-training tasks. The first modality, or IR statements are pre-trained on the task of Masked Language Modeling (MLM) with a Transformer based model. MLM is widely used in pre-training deep learning approaches with code or text generation capabilities. The second modality, or code graphs, are pre-trained with an auto-encoding task (Graph Auto-Encoder), where the aim is for a Graph Neural Network (GNN) based model to reconstruct the input graph. To the best of our knowledge, this study is the first to pre-train a multi-modal encoder using Transformers and GNNs to model individual modalities for parallel code. We also propose a new pre-training task to link the two modalities. We design a pre-training task to match the code graphs to the tokenized IRs (IR-Graph Matching). This allows our pre-trained model to better understand how the IR text translates to its corresponding graph, thus implicitly allowing the model to understand and link the syntactic, semantic, and structural aspects of the input IR.</p><p>We will show in later sections that the features/embeddings generated by our pre-trained model helps us match or outperform the state-of-the-art task specific approaches. Our MIREncoder-based embeddings lead to accuracy of upto ≈ 94% for CPU/GPU device mapping, speedups of upto 1.3×, 1.32×, ≈ 3× on thread coarsening, loop vectorization, and OpenMP paramter tuning tasks. Our predictions also reduce error rates by upto ≈ 40% and ≈ 70% over the state of the art for NUMA/Prefetcher optimizations, and tuning thread block sizes for CUDA code respectively.</p><p>To summarize, the contributions of this works are as follows:</p><p>• A multi-modal IR-based pre-training approach for source code representation. • A novel pipeline that aims to i) model IRs as streams of lexical tokens with transformers, and ii) as multi-graphs with GNNs, to extract and understand syntactic, semantic, and structural features.</p><p>• A novel pre-training task, IR-Graph Matching, to link the two modalities and help the model relate syntactic, semantic, and structural features. • Extensive experimental evaluations on six downstream tasks, including CPU/GPU device mapping, thread coarsening, loop vectorization, OpenMP parameter tuning, NUMA/ Prefetcher optimization, and tuning CUDA code with thread blocks, with superior results over state of the art. • Analysis of the importance of each modality and the overheads of our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we briefly describe the topics relevant to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Code Representations and Deep Learning</head><p>Recently, representation learning has been widely used for code modeling tasks. Several prior works have represented programs as a sequence of lexical tokens. However, this fails to capture program structure. To overcome this, syntax as well as semantics based representations have been proposed <ref type="bibr" coords="2,454.78,303.32,9.43,7.94" target="#b1">[2,</ref><ref type="bibr" coords="2,466.49,303.32,6.18,7.94" target="#b2">3,</ref><ref type="bibr" coords="2,474.96,303.32,6.18,7.94" target="#b8">9,</ref><ref type="bibr" coords="2,483.43,303.32,10.35,7.94" target="#b10">11,</ref><ref type="bibr" coords="2,496.06,303.32,10.35,7.94" target="#b20">21,</ref><ref type="bibr" coords="2,508.70,303.32,10.35,7.94" target="#b32">33,</ref><ref type="bibr" coords="2,521.34,303.32,10.35,7.94" target="#b40">41,</ref><ref type="bibr" coords="2,533.97,303.32,10.35,7.94" target="#b45">46,</ref><ref type="bibr" coords="2,546.61,303.32,11.59,7.94" target="#b50">51]</ref> that aim to extract and understand code structure as well.</p><p>PROGRAML <ref type="bibr" coords="2,366.03,325.24,14.61,7.94" target="#b17">[18]</ref> is such an IR-based code representation tool that can model code flow information along with the code structure as multi-graphs. Each multi-graph has a vertex for instruction and control-flow edges between them. Data flow is represented by including separate vertices for variables and constants and associated data-flow edges to instructions. Call flow is represented by edges between callee functions and caller instruction vertices. We use PROGRAML to extract data, control, and call flow graphs from IRs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Deep Learning</head><p>Multi-modal learning relates information from multiple sources towards a common goal <ref type="bibr" coords="2,403.97,449.44,13.22,7.94" target="#b36">[37]</ref>. If a task can be represented in multiple ways, it can be assigned as multi-modal, with each representation defined as a unique modality. Multi-modal learning has been mostly applied to audio and video analysis, speech synthesis, and gesture recognition tasks <ref type="bibr" coords="2,381.55,493.27,13.22,7.94" target="#b47">[48]</ref>. For example, in image and video description tasks, the visual content and associated textual description can be considered different modalities of the same problem.</p><p>We take inspiration from these ideas and apply it to the task of code representation. A sequential and graphical code representation has been used to represent different modalities of the same piece of code. High-level embeddings obtained from each pre-trained modality are combined and associated to generate the feature space for downstream tasks.</p><p>Multi-modal Pre-trained Models. The remarkable success of pretrained models in NLP has driven the development of multi-modal pre-trained model that learns implicit alignment between inputs of different modalities. These models are typically learned from bimodal data, such as pairs of language-image or pairs of language video, for example, ViLBERT <ref type="bibr" coords="2,418.60,646.91,16.07,7.20" target="#b33">[34]</ref>. Similarly, VideoBERT <ref type="bibr" coords="2,516.46,646.91,16.98,7.20" target="#b48">[49]</ref> learns from language-video data and is trained by video and text masked token prediction. With respect to pre-trained models targeting programming languages, CodeBERT <ref type="bibr" coords="2,433.80,679.79,16.89,7.20" target="#b26">[27]</ref> was trained on bimodal data with natural language and programming language pairs. Code comment and source code pairs were used for pre-training. However, our work is different from these prior works, as we aim to only work with source code, and we consider two ways of representing code as separate modalities. Also, unlike prior pre-trained works, we only work with compilable code with a focus on generating features for performance optimization, rather than code generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIREncoder</head><p>Most source-code based performance optimization tasks in HPC usually involve compilable languages such as C, C++, CUDA, and so on. A large number of these languages can be compiled and optimized using the LLVM infrastructure. LLVM IRs are a portable, high-level assembly language that can be optimized with a variety of transformations over multiple passes. It is fairly simple to extract IRs from source code such as C, C++. IRs generated from source code are usually devoid of most stylistic choices and redundant code. This is why we choose to work with IRs for performance optimizations. Figure <ref type="figure" coords="3,123.71,477.51,4.13,7.94" target="#fig_0">1</ref> shows a high-level overview of our approach.</p><p>For the first modality, we first tokenize the input IRs into meaningful "tokens" before they are mapped to an embedded dimension. Our approach then learns the embedding of the IR instructions after splitting them into sub-words. For the second modality, the IRs are first converted to dependence graphs that include in them data flow, control flow, and call flow information that represents the semantic information in the source code. These two modalities are then passed into the modeling pipeline either for pre-training or inference. The following paragraphs outline our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tokenization</head><p>Simply put, tokenization is the process of breaking down a piece of text into smaller units called tokens, and assigning a numerical value to each token. A deep learning (DL) model does not understand text or images in its raw form. It needs to be represented as numbers for the model to make sense from it. This is why tokenization is extremely important for such works. In this paper, our tokenization process follows the same approach taken while designing and training the BERT <ref type="bibr" coords="3,141.78,690.75,16.26,7.20" target="#b22">[23]</ref> model. However, the pre-trained BERT tokenizer readily available online is trained on natural language (NL). However, source code (IRs in our paper) is more structured than NL, and quite possibly has fewer "words". Thus, we had to train our tokenizer from scratch. We initially collect a large set of IRs by compiling programs in existing datasets into their LLVM IRs. For training the tokenizer, we have used C, C++, and CUDA code from CodeNet <ref type="bibr" coords="3,386.25,352.38,16.08,7.20" target="#b38">[39]</ref>, HPCorpus <ref type="bibr" coords="3,447.07,352.16,13.49,7.94" target="#b30">[31]</ref>, and LS-CAT <ref type="bibr" coords="3,506.41,352.38,15.91,7.20" target="#b9">[10]</ref>. We first define a set of special tokens to handle unknown inputs, and a token that will be used during Masked Language Modeling. 10, 000 unique programs are randomly selected and compiled into LLVM IRs. These are then passed through a WordPiece <ref type="bibr" coords="3,493.18,396.21,16.95,7.20" target="#b58">[59]</ref> tokenizer, as done in BERT, and trained to generate a learned vocabulary. BERT uses a sequence length of 512. However, for the sake of simplicity and faster training, we limit the sequence length for each encoded IR statement to 64. Increasing the sequence length might improve results, but the aim of our work is to extract features from IRs, rather than have code generation capabilities. Thus, such an approach might be sufficient for performance optimization tasks, as we will show later.  <ref type="figure" coords="3,340.96,564.04,208.74,4.60;3,326.50,568.83,218.44,4.60;3,326.50,573.62,222.69,4.60;3,326.50,578.41,108.31,4.60" target="#fig_6">', '%', '"', 'class', '.', 'std', ':', ':', 'ios', '_', 'base', '"', '=', 'type', '{', 'i32', '(', '.', '.', '.', ')', '*', '*', ',',  'i64', ',', 'i32', ',', 'i32', ',', 'i32', ',', 'i64', ',', 'i64', ',', '%', '"', 'struct', '.', 'std', ':', ':', 'ios', '_', 'base', ':',  ':', '_', 'iosarray', '"', '*', ',', '%', '"', 'struct', '.', 'std', ':', ':', 'ios', '_', 'base', ':', ':', '_', 'fnarray', '"', '*', ',',  '%', '"', 'class', '.', 'std', ':', ':', 'locale', '"', '*', '}', '</ref>[SEP]'] In Figure <ref type="figure" coords="3,361.60,635.74,3.01,7.94" target="#fig_1">2</ref>, we show an example of the tokenization process with our trained tokenizer. For this example, we select an IR statement from a file that was not used to train the tokenizer. We feed the statement to the tokenizer, which outputs a sequence of numbers (input ids). This is what a DL model will work with. To show that the encoding is correct, we decode the tokenized input ids to show that it is exactly the same as the given IR input, with a few minor but important differences. As shown in Figure <ref type="figure" coords="4,221.82,87.79,3.02,7.94" target="#fig_1">2</ref>, the outputs are in array format, as the tokenizer decodes each input id individually. The array includes a '[CLS]' and a '[SEP]' token at each end. The '[CLS]' token is used to denote the class of the input, if applicable, and the '[SEP]' token is used to separate two statements in the same input. The upper case alphabets in the inputs have also been converted to lower case to make the sequences case-insensitive. If we remove the first and last tokens in the array, and join the elements, we end up with the same output as the input, which shows the success of our tokenizer training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Generation and Pre-Processing</head><p>Several works ( <ref type="bibr" coords="4,109.14,221.64,8.88,7.94" target="#b1">[2,</ref><ref type="bibr" coords="4,120.18,221.64,6.10,7.94" target="#b2">3,</ref><ref type="bibr" coords="4,128.43,221.64,6.10,7.94" target="#b8">9,</ref><ref type="bibr" coords="4,136.69,221.64,10.27,7.94" target="#b10">11,</ref><ref type="bibr" coords="4,149.11,221.64,10.27,7.94" target="#b20">21,</ref><ref type="bibr" coords="4,161.54,221.64,10.27,7.94" target="#b32">33,</ref><ref type="bibr" coords="4,173.96,221.64,10.27,7.94" target="#b40">41,</ref><ref type="bibr" coords="4,186.39,221.64,10.56,7.94" target="#b45">46]</ref>) have outlined that simply looking at source code as a stream of lexical tokens is not sufficient to represent code. Modeling IRs only as stream of tokens does not provide enough details about the structural properties of the program. Code structure can highlight dependencies in source code. It can show the flow of execution in source code, or can also show dependencies between variables. Given that such dependencies are sparse in nature, a graph seems to be an appropriate data structure to represent such structure and dependencies. The dependencies also highlight the meaning of a source code. The sequence of execution or the control flow, how the variables are dependent on each other or the data flow and the function call stack in a program are indicators of the underlying semantics of source code. Prior literature <ref type="bibr" coords="4,118.79,569.99,13.40,7.94" target="#b23">[24,</ref><ref type="bibr" coords="4,134.02,569.99,10.27,7.94" target="#b25">26,</ref><ref type="bibr" coords="4,146.13,569.99,11.47,7.94" target="#b51">52]</ref> has used such structural and semantic information to good effect. We build on these ideas and work with graphs generated from IRs as the second modality. These graphs are generated with a tool called PROGRAML <ref type="bibr" coords="4,193.36,603.08,16.22,7.20" target="#b17">[18]</ref>. The generated multigraphs contain data-flow, control-flow, and call-flow dependencies in them. During pre-training, these graphs allow our model to extract semantic and structural features from source code (IRs). This is necessary as code structure and semantics should dictate the performance of an application/kernel. An example of such a graph is shown in Figure <ref type="figure" coords="4,147.25,668.62,3.07,7.94" target="#fig_2">3</ref>.</p><p>The nodes in our generated graphs (example shown in Figure <ref type="figure" coords="4,53.80,690.53,3.49,7.94" target="#fig_2">3</ref>) contain IR statements. These form the node features in our graphs. Node features are used by Graph Neural Networks (GNNs) in forward and backward propagation during training. However, DL models cannot use such statements directly. Therefore, we use the trained tokenizer described in Section 3.1 to convert the IR statements into sequence of numbers. These become the node features and are used in the pre-training process by the GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-Training MIREncoder</head><p>In this section we outline the pre-training process of MIREncoder. The quality of a pre-trained model usually depends on the pretraining tasks considered. For this work, we have used three pretraining tasks; one task that targets each modality, and another one that is used to explicitly link together the two modalities. Namely, the pre-training tasks are Masked Language Modeling, Graph Auto-Encoding, and IR Code-Graph Matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Masked Language Modeling. Masked Language Modeling</head><p>(MLM) is a widely used pre-training task in natural language based pre-trained models. It is also commonly used as a pre-training task in studies working with programming languages such as CodeBERT <ref type="bibr" coords="4,317.96,294.61,13.36,7.94" target="#b26">[27]</ref>.</p><p>MLM for this paper can be defined as follows. Given an IR statement 𝑐 as input, we select a random set of positions 𝑚 𝑐 that will be masked out; i.e. replaced with the '[MASK'] token. Following ideas presented in <ref type="bibr" coords="4,364.06,338.44,13.40,7.94" target="#b22">[23,</ref><ref type="bibr" coords="4,379.43,338.44,10.05,7.94" target="#b26">27]</ref>, we mask out 15% of the input. The task in this pre-training step is for the model to successfully predict the masked out words from the adjoining context. This is a self-supervised approach as the model is expected to produce the correct outputs without any explicit labels. Throughout the training process, the model is updated based on the difference between the predicted words and the actual words in the statements.</p><p>However, it is worth noting that the '[MASK]' token does not appear during the downstream tasks. To overcome this, as done in <ref type="bibr" coords="4,317.96,437.07,13.36,7.94" target="#b22">[23]</ref>, we perform the following steps:</p><p>• Select 15% of the token positions at random.</p><p>• Randomly replace 80% of the selected positions with the '[MASK]' token. • Replace 10% of the selected positions with a random token.</p><p>• Keep the token unchanged for the remaining cases.</p><p>These steps help the model learn the meaning of a word in the context of a statement, and not assign a single meaning to a word. Also, not including the '[MASK]' token in each statement during pre-training ensures that the model does not always expect that token. For this pre-training task, we use transformer layers with attention mechanism for improved training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Graph</head><p>Auto-Encoding. Graph Auto-Encoders (GAEs) like traditional auto-encoders also aim to reconstruct the given inputs. The aim of this pre-training task is for the model to produce a learned low-dimensional embedded representation from the IR graphs during the downstream tasks. During pre-training, our model setup follows the widely used encode-code-decode setup. An input graph is first fed through GNN layers (Graph Convolution Layers or GCN ) to produce node embeddings in a two-dimensional latent space. This forms the encoder part of the network. In the decoder part of the network, the aim is to reconstruct the graph from the low-dimensional encoded form. The aim is not to reconstruct the original nodes, but to reconstruct the adjacency matrix identical to the input graph through an inner product between latent variables in order to understand the structure of the graphs. Now the multi-graphs used in this study have three sub-graphs in them denoting control-flow graphs, data-flow graphs, and callflow graphs. However, it is quite difficult to auto-encode graphs with multiple edge types. Therefore, we tweak the training process slightly by extracting each sub-graph from the IR multi-graph, and train the auto-encoder for each of the three sub-graphs. But, we do not train the model thrice. The modeling and the loss calculation phases are updated to work with the node features and adjacency matrices of each sub-graph. The loss is back-propagated as an aggregation of the difference in graph reconstruction of each subgraph. There are two main benefits to this: i) calculating the loss and back-propagating over the whole graph instead of each sub-graph allows the model to improve its learning over the whole graph and enables it to implicitly learn the relations between the three types of semantics in the graphs (control-flow, data-flow, call-flow), ii) it improves overall training time when compared to training three separate GAEs, one for each sub-graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">IR-Graph Matching.</head><p>Here, we propose a novel pre-training task IR-Graph Matching to link the two modalities together. The modalities considered in this paper have different data structures, one being a sequence of tokens, the other being a graph. Intuitively, it might be difficult for the model to understand how these two modalities are linked together, and by extension, difficult to link the syntax and structure.</p><p>Therefore, we propose this pre-training task where the aim is for the model to correctly identify if the code sequence and the code graphs are from the same IR source. We setup this as a binary classification task, where the inputs are the code sequences (𝑆) and the code graphs (𝐺). Positive and negative samples are automatically generated as data pairs to train the model. Positive samples are those where 𝑆 and 𝐺 are from the same IR, while the negative samples are those where the graphs and sequences are from different IRs. Negative samples are selected in 50% cases by randomly selecting a different IR from the dataset. The code graph of the negative sample is paired with the code sequence to create the negative data pair.</p><p>As outlined in Section 3.3.1, the Masked Language Modeling task is performed on IR statements. However, in this task, we need to work with whole files to match text in IR files to the corresponding graphs. Although embedding an IR statement/instruction to a sequence of length 64 might work, embedding a complete file with a large number of statements to a sequence of length 64 will not provide enough information to the model. Therefore, we embed each statement in the file, and then aggregate all the vectors. The aggregated input and the generated code graph with the embedded node features (Section 3.2) are then trained together as a binary classification problem. The transformer layers used in Section 3.3.1 and the GCN layers used in Section 3.3.2 are reused to model the code sequences and the code graphs. Their outputs are concatenated and passed through linear layers with binary cross-entropy used for the loss calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we outline the experiments undertaken to show the strength of our approach. We test our pre-trained model on six downstream tasks different from each other. For each task, we work with metrics used in prior works for evaluation. Experimental setup and evaluation metrics are outlined in more detail in the corresponding sections.</p><p>A few things, however, are common for all experiments. For each downstream task, the pre-trained model is not fine-tuned. The pretrained model is set to inference mode to generate embeddings for input IRs. A few trainable linear/MLP layers are added to the pretrained model to perform task specific training. For downstream tasks, only these final layers are trained which substantially reduces the optimization/tuning overhead. In the following sections, we outline each downstream optimization task performed in this paper, and compare and contrast our results with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Heterogeneous Device Mapping</head><p>Grewe et al. <ref type="bibr" coords="5,363.02,263.14,14.59,7.94" target="#b27">[28]</ref> proposed the device mapping task to map OpenCL kernels to the CPU or GPU. This task has been widely used <ref type="bibr" coords="5,535.26,274.09,11.35,7.94" target="#b8">[9,</ref><ref type="bibr" coords="5,548.85,274.09,10.34,7.94" target="#b17">18,</ref><ref type="bibr" coords="5,317.75,285.05,10.27,7.94" target="#b18">19,</ref><ref type="bibr" coords="5,330.16,285.05,10.27,7.94" target="#b50">51,</ref><ref type="bibr" coords="5,342.58,285.05,11.47,7.94" target="#b52">53]</ref> to evaluate the performance of code representations. We also use this task to evaluate the effectiveness of our approach and compare against the state-of-the-art results.</p><p>Dataset. We use the dataset published by Ben-Nun et al. <ref type="bibr" coords="5,535.13,317.93,10.49,7.94" target="#b8">[9]</ref> for this experiment. It has 256 unique OpenCL kernels from seven benchmark suites comprising of AMD SDK[6], NPB <ref type="bibr" coords="5,476.86,339.85,12.18,7.94" target="#b7">[8]</ref>, NVIDIA SDK[17], Parboil <ref type="bibr" coords="5,343.33,350.81,14.50,7.94" target="#b46">[47]</ref>, Polybench <ref type="bibr" coords="5,400.50,350.81,16.01,7.94" target="#b37">[38]</ref>, Rodinia <ref type="bibr" coords="5,450.51,350.81,15.41,7.94" target="#b13">[14,</ref><ref type="bibr" coords="5,468.95,350.81,11.59,7.94" target="#b14">15]</ref> and SHOC <ref type="bibr" coords="5,518.30,350.81,17.91,7.94" target="#b21">[22]</ref>. The data size and workgroup size were varied for each kernel to obtain a labeled dataset with 670 CPU or GPU-labeled data points for each of the two devices, AMD Tahiti 7970 and NVIDIA 970.</p><p>Baseline. We have compared the results of our approach with prior works with the same dataset. Prior evaluations were presented in terms of accuracy and performance improvements (speedups). We have also adhered to these metrics. For analysing speedups, we use the same static mapping baseline proposed in <ref type="bibr" coords="5,501.05,438.48,13.36,7.94" target="#b52">[53]</ref>.</p><p>Results. We use our pre-trained model to encode the available IRs, and perform the classification experiments. The MIREncoder pipeline is first used to embed each IR statement in a file. The generated embeddings are then aggregated to encode the first modality. For the second modality, the IRs are first converted to graphs as outlined in Section 3.2, and are fed through the Graph Auto-Encoder (GAE) layers to encode the graphs. These two sets of embeddings (sequences of vectors) are passed through three linear/MLP layers to train and validate the model. As done in prior works, we also add transfer and workgroup sizes from the dataset to the feature set before passing the feature set onto the linear layers. Following techniques used before <ref type="bibr" coords="5,402.22,569.99,13.40,7.94" target="#b17">[18,</ref><ref type="bibr" coords="5,417.71,569.99,10.27,7.94" target="#b50">51,</ref><ref type="bibr" coords="5,430.08,569.99,10.05,7.94" target="#b52">53]</ref>, we have used ten-fold stratified cross-validation to evaluate our results.</p><p>Our experimental setup leads to state-of-the-art results in identifying the correct device. We achieve accuracy of 93.7% and F1-score of 0.94 in identifying the best device on the NVIDIA GPU. On the AMD GPU, we achieve accuracy and F1-score of 93.6% and 0.92. We see that our approach is better or equivalent in all cases compared to prior works in literature. The accuracies are shown in Table <ref type="table" coords="5,552.93,646.70,3.13,7.94" target="#tab_1">1</ref>, and the numbers in parenthesis shows the improvement in accuracy by using MIREncoder over prior works.</p><p>The model predictions also lead to significant performance improvement over static mappings. On the NVIDIA 970 system, our approach leads to speedups of 1.28× compared to oracle speedups  of 1.34×. The oracle speedups are calculated by analyzing the execution time on the best device and comparing it to the static mapping baseline. On the AMD Tahiti system, our predictions lead to speedups of 2.24× versus oracle speedups of 2.39×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Thread Coarsening</head><p>Thread coarsening <ref type="bibr" coords="6,119.33,427.52,15.35,7.94" target="#b53">[54]</ref> is used to increase the work done by a single thread by fusing two or more concurrent threads. Thread coarsening factor (TCF) corresponds to the number of threads that can be fused together. Selection of an optimal TCF can lead to substantial improvement <ref type="bibr" coords="6,131.50,471.35,17.31,7.94" target="#b35">[36]</ref> in performance on GPU devices and a naive coarsening could lead to slowdown. Due to differences in architectural characteristics across devices, a TCF that gives the best speedup on one GPU might show degraded performance on another GPU <ref type="bibr" coords="6,98.93,515.19,17.89,7.94" target="#b34">[35,</ref><ref type="bibr" coords="6,119.49,515.19,10.21,7.94" target="#b44">45]</ref>. As an example, nbody kernel has a higher degree of Instruction Level Parallelism and can be better exploited by VLIW-based AMD Radeon than SIMD-based AMD Tahiti <ref type="bibr" coords="6,273.09,537.11,13.90,7.94" target="#b34">[35]</ref>.</p><p>Dataset. In this experiment, we follow the experimental setup proposed in <ref type="bibr" coords="6,98.22,559.03,14.60,7.94" target="#b34">[35]</ref> and reused in <ref type="bibr" coords="6,165.18,559.03,14.59,7.94" target="#b52">[53]</ref> to predict the best thread coarsening factor from {1, 2, 4, 8, 16, 32}. We use the dataset provided in <ref type="bibr" coords="6,63.75,580.94,9.52,7.94" target="#b8">[9]</ref>, which consists of 68 data points from 17 OpenCL kernels on 4 different GPUs, namely AMD Radeon 5900, AMD Tahiti 7970, NVIDIA GTX 480, and NVIDIA Tesla K20c. These include kernels from AMD SDK[6], NVIDIA SDK[17] and Parboil <ref type="bibr" coords="6,232.43,613.82,14.68,7.94" target="#b46">[47]</ref> benchmarks.</p><p>Baseline. As done in prior works <ref type="bibr" coords="6,194.15,624.78,16.28,7.94" target="#b50">[51,</ref><ref type="bibr" coords="6,214.45,624.78,10.21,7.94" target="#b52">53]</ref>, we evaluate the predictions from our model in terms of performance improvement/speedups over default coarsening behavior. The results from our approach has been compared against prior works on this dataset.</p><p>Results. For this experiment, we pass the input IRs through our pre-trained encoder as before to generate the embeddings. The embeddings are then passed through linear layers to train and validate the best thread coarsening factors. Similar to prior works on this task, we also perform leave-one-out cross validation and report the geometric mean speedups across all folds in Table <ref type="table" coords="6,538.30,182.10,3.02,7.94" target="#tab_2">2</ref>. We observe that our approach performs better in all cases. In Table <ref type="table" coords="6,553.00,193.06,3.10,7.94" target="#tab_2">2</ref>, speedups are presented for each device included in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loop Vectorization</head><p>Modern compilers can automatically detect when loops should be vectorized so that multiple iterations of the loop can be performed together. When compilers vectorize loops, it must determine the number of instructions to pack together and the interleaving level (stride). This task was proposed in <ref type="bibr" coords="6,443.70,282.84,14.63,7.94" target="#b28">[29]</ref> as a potential candidate for DL-based optimization. Modern compilers allow users to select and define the vectorization factor (VF) and interleave factor (IF) to control the loop vectorization process. However, manually evaluating and testing all possible combinations might not be feasible, especially for a large number of applications. To this end, we propose a MIREncoder-based static loop vectorizer.</p><p>Dataset. We define a search space based on ideas in <ref type="bibr" coords="6,531.37,359.55,14.85,7.94" target="#b28">[29]</ref> by considering pairs of VF and IF and execute them to create a dataset. Their definitions are given below in Equation <ref type="formula" coords="6,486.19,381.47,3.07,7.94" target="#formula_0">1</ref>,</p><formula xml:id="formula_0">𝑉 𝐹 ∈ [2 0 , 2 1 , ... 𝑀𝐴𝑋 _𝑉 𝐹 ], 𝐼 𝐹 ∈ [2 0 , 2 1 , ... 𝑀𝐴𝑋 _𝐼 𝐹 ],<label>(1)</label></formula><p>where we set MAX_VF and MAX_IF to 64 and 16 for the architecture under test (Intel Skylake). We reuse the set of kernels collected in <ref type="bibr" coords="6,317.96,449.44,14.85,7.94" target="#b28">[29]</ref> and execute them with each (VF, IF) pair to create a dataset of kernels, (VF, IF) pairs, and their runtimes. The training and testing set were defined separately in <ref type="bibr" coords="6,455.34,471.35,14.66,7.94" target="#b28">[29]</ref> and we follow the same setup here as well. Overall, we collect more than 273𝐾 samples for training. We label the kernels with the best (VF, IR) pair by selecting the vectorization/interleave factor with the fastest runtime. For the test set, we perform the same steps to create the test set.</p><p>Baseline. For this experiment, we select the default LLVM Loop Vectorizer as a baseline and evaluate the predicted performance with respect to this. We compare our work with Neurovectorizer <ref type="bibr" coords="6,380.99,559.24,16.81,7.20" target="#b28">[29]</ref>. This paper first proposed this task as suitable for DL-based tuning. They used inst2vec <ref type="bibr" coords="6,482.29,570.20,12.62,7.20" target="#b8">[9]</ref> embeddings with reinforcement learning for their experiments.</p><p>Results. We follow the same steps as before in this experiment as well. We pass the IRs through the pre-trained model to generate the embeddings. The embeddings are then passed through the trainable MLP layers to train and test the model. Both vectorization factor and interleave factor can be varied during compilation. Therefore, we depend only on the compiled IR for training and testing. We train our model on the training data collected on our Intel Skylake server, and test it on the collected test set. From our experiments we see that MIREncoder-based vectorization leads to mean speedups of ≈ 1.32× over LLVM vectorization heuristics. We repeat the same experiments as done in <ref type="bibr" coords="6,401.26,701.49,14.60,7.94" target="#b28">[29]</ref> on the same Skylake server and observe  <ref type="table" coords="7,169.93,195.07,2.94,7.94" target="#tab_3">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tuning OpenMP Runtime Configurations</head><p>OpenMP is one of the most widely used shared memory programming models. It is mostly used to parallelize sequential code by inserting pragmas. Dutta et al. in <ref type="bibr" coords="7,172.00,252.18,13.22,7.94" target="#b25">[26]</ref>, proposed a GNN-based tuner (PnP Tuner) for identifying the best OpenMP parameters for improving performance. They also used power limits to increase the size of the search space and evaluate the impact that limiting power has on OpenMP applications. We build on ideas presented in that paper to set up our own tuner based on MIREncoder. Dataset. As in <ref type="bibr" coords="7,123.62,317.93,13.49,7.94" target="#b25">[26]</ref>, we work with 25 applications from the Polybench benchmark suite. We define the search space as done in <ref type="bibr" coords="7,63.34,339.85,14.74,7.94" target="#b25">[26]</ref> (Table <ref type="table" coords="7,105.07,339.85,3.43,7.94" target="#tab_4">4</ref>) with 504 configurations. We also modify the data sizes used to run these applications by changing compile time options provided by the benchmark suite. We use two input sizes for the purposes of evaluation. For each application, input size, and parameter set (power limit, # of threads, schedule, chunk) we compile and execute the application to collect the runtimes and generate a dataset of 25200 samples. We also collect the runtimes for each of these applications when run with default OpenMP settings (all threads, static scheduling, compiler defined chunk sizes) at Thermal Design Power (TDP). We collect this data on a 64-core Intel Skylake system with a TDP of 150𝑊 .</p><p>Baseline. The metric of choice for evaluating the performance of our MIREncoder-based tuner is speedups as done in <ref type="bibr" coords="7,262.32,471.35,13.49,7.94" target="#b25">[26]</ref>. We calculate the speedups with the default OpenMP configurations at TDP as the baseline and compare the performance of our predicted configurations with those predicted by the PnP tuner, the current state-of-the-art for this experiment. In fact, PnP tuner also works with graphs generated from IRs. Their approach, like ours, also aims to model control and data flow in a program with the help of GNNs. PnP tuner uses RGCN (Relational Graph Convolutional Networks) as the GNNs of choice.</p><p>Results. Following the approach in <ref type="bibr" coords="7,198.27,569.99,13.49,7.94" target="#b25">[26]</ref>, for this set of experiments we consider application speedups instead of performance improvement of individual OpenMP loops. Also, the OpenMP parameters were modified at runtime. Thereby, all OpenMP loops in an application were run with the same set of parameters. The modeling process used in this experiment is also similar to the ones used in previous sections. For validation, we perform leave-one-out validation as done in <ref type="bibr" coords="7,106.35,646.70,13.22,7.94" target="#b25">[26]</ref>. Each application is assigned to the test set, while all other applications are assigned to the training set. We repeat this for all applications in the dataset. Based on our experiments, we find that the tuner designed with MIREncoder embeddings has better or equivalent performance to PnP tuner. It is able to identify configurations that lead to faster code execution in most cases,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Optimizing NUMA/Prefetcher Parameters</head><p>TehraniJamsaz et al. in <ref type="bibr" coords="7,401.58,438.48,14.60,7.94" target="#b51">[52]</ref> proposed a novel GNN-based LLVM IR modeling technique for optimizing NUMA (Non-Uniform Memory Access) and Prefetcher configurations. In particular, this work built on top of prior works <ref type="bibr" coords="7,396.29,471.35,16.83,7.94" target="#b42">[43]</ref> to explore the impact of various cache prefetching options along with NUMA-related hardware parameters such as number of threads, degree of NUMA node, thread mapping and page mapping. The authors used graph embeddings generated from LLVM IRs to statically map each kernel to the best NUMA/prefetcher configuration. Dataset. In <ref type="bibr" coords="7,372.09,537.11,13.34,7.94" target="#b51">[52]</ref>, the authors used data from 57 parallel kernels from Rodinia <ref type="bibr" coords="7,363.33,548.07,14.92,7.94" target="#b13">[14,</ref><ref type="bibr" coords="7,380.22,548.07,10.05,7.94" target="#b14">15]</ref>, NAS Parallel Benchmarks <ref type="bibr" coords="7,484.31,548.07,12.40,7.94" target="#b7">[8,</ref><ref type="bibr" coords="7,498.68,548.07,10.05,7.94" target="#b43">44]</ref>, CLOMP <ref type="bibr" coords="7,536.62,548.07,18.06,7.94" target="#b11">[12]</ref>, and LULESH <ref type="bibr" coords="7,361.52,559.03,17.74,7.94" target="#b31">[32]</ref>. The data was collected on Intel SandyBridge and Intel Skylake processors on a search space with 288 and 320 configurations respectively. This dataset was pared down to 13 configurations as the authors found that 99% of the performance gains were obtained using these 13 configurations. To increase the quality of their code modeling and improve results, TehraniJamsaz et al. augmented the dataset by re-compiling the kernels in the dataset with 1000 different compiler sequences. We use this collected dataset to further test the strength of our approach.</p><p>Baseline. In this study, we are using a pre-trained model to generate the embeddings for each IR. In addition to testing the quality of optimizations made by our MIREncoder embeddings, we also use this experiment to highlight reduced data requirements when a pre-trained model is used to generate features. Transfer  learning allows us to achieve this as MIREncoder generates learned embeddings, thus implicitly transferring its knowledge to the tuner. Therefore, during training we only use ≈ 5% of the complete dataset for training. During validation and testing, the authors in <ref type="bibr" coords="8,266.73,290.29,13.53,7.94" target="#b50">[51,</ref><ref type="bibr" coords="8,282.50,290.29,11.54,7.94" target="#b51">52]</ref> used 10-fold validation. We also use the same folds for our tests and compare the results from MIREncoder with the state of the art <ref type="bibr" coords="8,64.12,323.17,13.76,7.94" target="#b50">[51,</ref><ref type="bibr" coords="8,80.76,323.17,11.59,7.94" target="#b51">52]</ref> in this dataset. As with prior works on this task, we also use error rate (relative difference between best and predicted performance) as the evaluation metric.</p><p>Results. To perform 10-fold validation, we first separate the validation set from the training set by assigning the kernels specified in each fold to the validation set. During training, we only select ≈ 5% of the IRs in the dataset at random for the kernels in the training set. However, we validate on all IRs corresponding to the kernels in the validation set. Training with such reduced data also produces good results as we can leverage transfer learning from our pre-trained model to generate embeddings for the IRs in the training set. For both SandyBridge and Skylake, we outperform <ref type="bibr" coords="8,228.46,443.72,14.78,7.94" target="#b51">[52]</ref> in 8 out of 10 folds (Figure <ref type="figure" coords="8,100.93,454.68,2.88,7.94" target="#fig_5">5</ref>). The modeling for this experiment only uses simple MLP layers in contrast to <ref type="bibr" coords="8,148.13,465.63,13.48,7.94" target="#b50">[51,</ref><ref type="bibr" coords="8,163.86,465.63,10.11,7.94" target="#b51">52]</ref>, which trains resource intensive GNNs for each experiment. Overall, across 10 folds, MIREncoder embeddings help reduce performance error rates by ≈ 15% (Sandy-Bridge) and ≈ 29% (Skylake) over <ref type="bibr" coords="8,182.27,498.51,13.49,7.94" target="#b51">[52]</ref>. MIREncoder embeddings outperform Perfograph <ref type="bibr" coords="8,141.54,509.69,17.05,7.20" target="#b50">[51]</ref> in 8 out of 10 folds for SandyBridge improving error rates by ≈ 14%. It outperforms Perfograph in all cases for Skylake, improving error rates by ≈ 40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Tuning Thread Blocks for CUDA Programs</head><p>So far the auto-tuning experiments have targeted programs written in C and C++. However, state-of-the-art GPUs have contributed immensely to performance improvement of HPC workloads and CUDA is often the language of choice for programming such GPUs, specifically NVIDIA GPUs. With this in mind, we have tried to optimize the performance of CUDA kernels in this section. As in the prior sections, we work with a previously published dataset and use a MIREncoder based tuner to identify the best parameters to run CUDA kernels.</p><p>Dataset. To address the lack of large scale datasets suitable for machine learning based optimizations of CUDA kernels, Bjertnes et al. published the LS-CAT <ref type="bibr" coords="8,149.48,690.75,16.62,7.20" target="#b9">[10]</ref> dataset with 19, 683 CUDA kernels. They also open source scripts to modify the input matrix sizes, and  <ref type="bibr" coords="8,388.74,301.15,8.99,7.94" target="#b7">(8,</ref><ref type="bibr" coords="8,397.73,301.15,5.99,7.94" target="#b7">8)</ref>, <ref type="bibr" coords="8,410.41,301.15,13.24,7.94" target="#b15">(16,</ref><ref type="bibr" coords="8,423.65,301.15,9.93,7.94" target="#b15">16)</ref>, <ref type="bibr" coords="8,440.58,301.15,13.24,7.94" target="#b23">(24,</ref><ref type="bibr" coords="8,453.83,301.15,9.93,7.94" target="#b23">24)</ref>, <ref type="bibr" coords="8,470.76,301.15,13.24,7.94" target="#b31">(32,</ref><ref type="bibr" coords="8,484.01,301.15,9.93,7.94" target="#b31">32)</ref>, <ref type="bibr" coords="8,500.94,301.15,9.53,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,510.47,301.15,9.53,7.94">64)</ref>, <ref type="bibr" coords="8,526.87,301.15,9.93,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,536.81,301.15,13.24,7.94">128)</ref>, <ref type="bibr" coords="8,388.74,312.11,9.59,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,398.33,312.11,12.79,7.94">192)</ref>, <ref type="bibr" coords="8,416.55,312.11,9.59,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,426.14,312.11,12.79,7.94">256)</ref>, <ref type="bibr" coords="8,444.35,312.11,9.59,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,453.94,312.11,12.79,7.94">320)</ref>, <ref type="bibr" coords="8,472.16,312.11,9.59,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,481.75,312.11,12.79,7.94">384)</ref>, <ref type="bibr" coords="8,499.97,312.11,9.59,7.94" target="#b0">(1,</ref><ref type="bibr" coords="8,509.56,312.11,12.79,7.94">448)</ref>, (</p><p>thread blocks used to execute these kernels. We compile and run these CUDA kernels with the matrix sizes and thread blocks shown in Table <ref type="table" coords="8,349.90,405.60,4.22,7.94" target="#tab_6">5</ref> to collect a dataset with more than 2.7 million samples on an NVIDIA A100 GPU. From the collected dataset, we identify the minimum runtime of each kernel and input matrix. The block size corresponding to the fastest runtime is then selected as the best configuration. This processed and labelled data is then used to train a simple MLP model on the MIREncoder embeddings to predict the best configuration for a CUDA kernel and input matrix unknown to the model. Baseline. To the best of our knowledge, this study is one of the first works to perform optimizations using this dataset for CUDA code. To evaluate our MIREncoder representation, we use the embeddings from three prior works (IR2Vec <ref type="bibr" coords="8,485.85,526.15,15.35,7.94" target="#b52">[53]</ref>, PROGRAML <ref type="bibr" coords="8,539.04,526.37,16.12,7.20" target="#b17">[18]</ref>, Perfograph <ref type="bibr" coords="8,358.14,537.32,16.07,7.20" target="#b50">[51]</ref>), and adapt the modeling techniques specified in those papers to the best of our ability for this task. We follow the same strategy used in <ref type="bibr" coords="8,414.08,559.03,14.78,7.94" target="#b51">[52]</ref> and Section 4.5 and use error rates (relative difference between best and predicted performance) as a metric to present and compare the results for this section.</p><p>Results. The LS-CAT dataset does not have a designated test set. Therefore, we perform 10-fold validation as done in prior works <ref type="bibr" coords="8,337.97,613.82,16.28,7.94" target="#b17">[18,</ref><ref type="bibr" coords="8,356.72,613.82,11.59,7.94" target="#b51">52]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Observation and Analysis</head><p>In this section, we outline and analyse the merits of our approach. We primarily hope to show the importance of each modality to our pre-training pipeline. We will also show how using our approach reduces the overheads associated with deep learning based performance optimization. Ablation Studies. Ablation studies are commonly used in deep learning to highlight the importance of individual components of the modeling process. Here, we hope to highlight the impact of each modality on the modeling process. We first remove the modules associated with each modality from the pipeline and pre-train the uni-modal models from scratch. However, for each uni-modal model, we only train it on one pre-training task as each pre-training task was designed with a modality in mind. For example, a Masked Language Modeling pre-training task would not be appropriate for the code graph modality. And the IR-Graph Matching task is dependent on both modalities being a part of the pre-training process. With this setup, we pre-train our uni-modal models and follow the same experimental setups as before. The uni-modal pre-trained models are tested on three tasks from the previous sections, namely heterogeneous device mapping (Section 4.1), thread coarsening (Section 4.2), and loop vectorization (Section 4.3). Case Study 1 (CS1): Each experiment shows that our pre-training is highly dependent on each modality. For device mapping, the quality of the predictions fall significantly when modality 2 (code graphs) is not included. When modality 1 (IR text) is removed, the performance drops, but less drastically. When we only pre-train with modality 1, performance drops by ≈ 37% and ≈ 14% for the NVIDIA and the AMD GPUs, whereas performance drops by ≈ 8% and ≈ 4% when only code graphs are used for pre-training (Table <ref type="table" coords="9,317.96,438.48,2.93,7.94" target="#tab_7">6</ref>). The higher dependence on the code graphs is expected as code semantics dictate which device is chosen as the best one for some of the kernels in this dataset. For example, the makea kernel from the CG application in NPB <ref type="bibr" coords="9,394.62,471.35,12.49,7.94" target="#b7">[8]</ref>, has a faster runtime on the GPU with a smaller input size, whereas it is mapped to the CPU when run with larger inputs. This behavior can be due to the presence of a number of function calls inside the parallel kernel. Such semantic details might be difficult for an NLP-style model to understand. However, a graph that embeds such dependencies as edges between nodes can help highlight such semantic information to the model.</p><p>Case Study 2 (CS2): When predicting the thread coarsening factors, we see that not including the code graphs has a smaller impact on thread coarsening factors than device mapping. Moreover, using only the code graphs leads to a bigger drop in application performance than when using both modalities. We see that performance drops by 5% when the code graphs are not used, whereas performance drops by 11.7% when only code graphs are used (Table <ref type="table" coords="9,545.82,613.82,2.94,7.94" target="#tab_8">7</ref>).</p><p>Case Study 3 (CS3): We also test how unimodality impacts the performance of loop vectorization. Loop vectorization is an important compiler optimization for modern processors. For this set of experiments as well, we see that removing a modality impacts the performance of the predictions (Table <ref type="table" coords="9,457.26,668.62,2.92,7.94">8</ref>). Using IRs only in textual format the performance drops by ≈ 30%, and when we use only the code graphs as a modality the performance of our vectorizer drops by ≈ 12%. Analyzing Overheads. Most advanced DL-based works usually have significant training and inference overheads. We use the experiment in Section 4.4 as a template to evaluate the overhead of our approach. We first train and test the MIREncoder-based tuner and PnP tuner <ref type="bibr" coords="10,88.27,216.52,15.11,7.94" target="#b25">[26]</ref> from Section 4.4 and capture the wall times. The PnP tuner is a GNN based code modeling approach that first proposed this downstream task. Across all experiments, to reduce overhead, we simply generate embeddings from the pre-trained model instead of fine-tuning it. PnP on the other hand needs to train GNNs for each experiment. Compared to our MIREncoder-based tuner, which only trains a few MLP layers, training and tesing a GNN based model is much more expensive as shown in Table <ref type="table" coords="10,237.13,293.23,3.07,7.94" target="#tab_9">9</ref>.</p><p>Most studies working with pre-trained models usually suggest fine-tuning the pre-trained models for downstream tasks. However, in this work, we do not fine-tune our pre-trained model for downstream tasks. The embeddings generated by MIREncoder are good enough to be used with simple shallow networks. To show this, we perform a set of tests with two setups: i) we use our regular set up where we do set the pre-training model to inference mode and use only the final MLP layers for training and testing, and ii) we do not set the pre-training model to inference mode, and use the complete network to fine-tune for downstream tasks. We observe that for the experiment in Section 4.4, performance (speedups) improves &lt; 5% when we fine-tune. However, the training time balloons by 238×. This is a significant increase in overhead for fairly marginal gains. We avoid this overhead by not fine-tuning, but simply generating the embeddings to achieve good results as shown in Section 4. Such overheads are seen even for our relatively small model with 22 million parameters. Recently, large language models, with billions of parameters, have been proposed <ref type="bibr" coords="10,185.18,490.49,16.43,7.94" target="#b19">[20]</ref> for addressing compiler optimizations. This would increase the training time exponentially, especially when computational resources are limited. Thus, new innovative techniques, like the one proposed in this paper, is necessary to reduce overheads and large-scale resource dependence. Multi-modality allows us to work with a small model and helps offset the loss in learning when a small uni-modal model is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>This paper proposes a new pre-trained multi-modal code representation technique for LLVM IRs. For most source code based optimization tasks, analyzing code can provide pointers to the pertinent optimizations. In fact, most compiler optimizations are code dependent. Therefore, a suitable code representation technique is also essential for using deep learning (DL) to make optimization decisions in HPC. To this end, several code representations have been proposed <ref type="bibr" coords="10,109.40,668.62,9.32,7.94" target="#b2">[3,</ref><ref type="bibr" coords="10,120.97,668.62,6.14,7.94" target="#b3">4,</ref><ref type="bibr" coords="10,129.35,668.62,6.14,7.94" target="#b8">9,</ref><ref type="bibr" coords="10,137.72,668.62,10.31,7.94" target="#b10">11,</ref><ref type="bibr" coords="10,150.27,668.62,10.31,7.94" target="#b17">18,</ref><ref type="bibr" coords="10,162.82,668.62,10.31,7.94" target="#b20">21,</ref><ref type="bibr" coords="10,175.37,668.62,10.31,7.94" target="#b40">41,</ref><ref type="bibr" coords="10,187.92,668.62,10.12,7.94" target="#b52">53]</ref>, which have been used to good effect for optimization tasks such as CPU/GPU device mapping, thread coarsening factor, loop vectorization, etc. to name a few. Preliminary works in this field such as <ref type="bibr" coords="10,210.88,701.49,7.11,7.94" target="#b2">[3]</ref><ref type="bibr" coords="10,217.99,701.49,3.56,7.94" target="#b3">[4]</ref><ref type="bibr" coords="10,221.55,701.49,7.11,7.94" target="#b4">[5]</ref>, focused more on lexical tokens which often fails to capture code semantics. The next generation of representational learning works <ref type="bibr" coords="10,482.41,98.75,11.88,7.94" target="#b8">[9,</ref><ref type="bibr" coords="10,496.54,98.75,10.30,7.94" target="#b17">18,</ref><ref type="bibr" coords="10,509.08,98.75,10.30,7.94" target="#b23">24,</ref><ref type="bibr" coords="10,521.62,98.75,10.30,7.94" target="#b24">25,</ref><ref type="bibr" coords="10,534.16,98.75,10.30,7.94" target="#b50">51,</ref><ref type="bibr" coords="10,546.70,98.75,11.50,7.94" target="#b52">53]</ref> leverage LLVM IRs to make semantic features available to DL models. However, the embeddings generated by these often require advanced modeling techniques such as GNNs for each individual task. In contrast, for downstream tasks, our approach can leverage transfer learning to generate learned embeddings that can be easily modeled with simple MLP layers to get better results than these.</p><p>An alternative to DL-based auto-tuning is to use non-neural network based machine learning approaches. Several works have used ML for a variety of tasks. <ref type="bibr" coords="10,411.41,197.38,13.47,7.94" target="#b39">[40,</ref><ref type="bibr" coords="10,427.12,197.38,11.51,7.94" target="#b54">55]</ref> propose machine learning based approaches for auto-tuning OpenMP applications. Artemis <ref type="bibr" coords="10,532.88,208.34,16.31,7.94" target="#b55">[56]</ref> is another work that performs automatic parameter tuning using machine learning. ytopt <ref type="bibr" coords="10,394.31,230.48,15.95,7.20" target="#b56">[57,</ref><ref type="bibr" coords="10,412.49,230.26,10.10,7.94" target="#b57">58]</ref>, BLISS <ref type="bibr" coords="10,448.80,230.48,16.48,7.20" target="#b41">[42]</ref> are examples of learningbased tuners that employ Bayesian optimization for online tuning tasks. These approaches are often domain or application specific. Although often faster than search-based alternatives, these do need multiple code executions to identify good performing parameters.</p><p>Studies highlighted so far in this section were all proposed as means to improve upon traditional search based auto-tuning. Works such as ActiveHarmony <ref type="bibr" coords="10,400.64,307.19,16.63,7.20" target="#b49">[50]</ref>, OpenTuner <ref type="bibr" coords="10,461.78,307.19,12.70,7.20" target="#b6">[7]</ref> have leveraged several search space optimization techniques to reduce the auto-tuning overhead compared to brute-force tuning. These optimization techniques include Hillclimbers, random search, Nelder-Mead, and many more. However, due to their sampling overhead, works such as ytopt and BLISS were proposed to reduce tuning overhead.</p><p>DL-based approaches, including ours, further help alleviate such overhead by making predictions without having to execute applications. This helps with configuring commonly used parameters across applications, without having to devote significant resources to the tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we have proposed a pre-trained multi-modal encoder for IRs with source code based performance optimizations in mind. Such an approach enables a model to understand syntactic, semantic and structural characteristics of source code. Prior works in this domain often depend only on NLP-style stylistic choices or compiler based code semantics and might require advanced modeling techniques with significant overheads.</p><p>Not only do our embeddings help reduce overhead on downstream tasks (Section 4.7), our pre-trained model is itself much smaller in scale than the latest pre-trained models in literature. Very large models, such as LLMs, often have billions/trillions of parameters. This makes training and fine-tuning them quite expensive, often requiring multiple state-of-the-art GPUs. Our pretrained multi-modal model on the other hand, only consists of 22 million parameters, and can be easily trained using a single GPU. However, most very large models have text or image generation capabilities; our model does not. This is by design as the aim of this work is to simplify and speed up the process of deep learning based performance optimization in HPC.</p><p>Moreover, for downstream tasks, we do not need to fine-tune the pre-trained model as is often necessary for larger models. We simply put the pre-trained model in inference mode, and output the embeddings of an input LLVM IR. Transfer learning allows us to do this and still achieve good results across multiple languages (C, C++, CUDA) and programming models (OpenCL, OpenMP). Because the pre-trained model has already been trained to understand and relate code syntax, semantics and structure, during downstream optimization tasks, the pre-trained model can leverage its prior knowledge to generate good quality embeddings. This also allows us to reduce data requirement while training DL models, as shown in Section 4.5, where we train our model with only 5% of the data that the state of the art <ref type="bibr" coords="11,137.68,175.46,13.58,7.94" target="#b50">[51,</ref><ref type="bibr" coords="11,153.50,175.46,11.53,7.94" target="#b51">52]</ref> had been trained on.</p><p>Additionally, our pipeline is modular by design. This can inspire future research on how each modality can be represented. For example, we could replace the graphs used in this work by other graphical representations such as ASTs, Perfograph, Graph2Par <ref type="bibr" coords="11,261.75,219.52,16.91,7.20" target="#b15">[16]</ref> and evaluate their impact. We hope to do this in future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Works</head><p>This paper proposes MIREncoder, a multi-modal pre-training approach to encode/embed LLVM IRs for easy use by deep learning based models targeting performance optimizations in HPC. Our pretrained encoder will allow researchers to focus more on adapting deep learning for HPC optimization problems instead of focusing on how it can be done. Moreover, as seen widely in literature, it is often possible to re-train existing pre-trained models for multiple domains. With this in mind, our model has been designed to be smaller in scale compared to existing pre-trained models. This would allow further research on such topics, and would not make researchers completely dependent on high-end and large-scale resources as is the case with very large models. Our aim with this paper was to propose a pre-training pipeline for HPC that would be small-scale. We helped alleviate the loss in learning from using a smaller model by introducing multi-modality to help our model better understand code "meaning". Our experimental results and further analysis support our claims of better performance with reduced overheads. Furthermore, our pre-trained model could easily be used in conjunction with online auto-tuners to help aid the search process. We hope to investigate this in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,53.80,258.44,504.40,7.70;3,53.80,269.40,369.60,7.70"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MIREncoder: Overview of our Multi-modal pre-training approach with two modalities using Masked Language Modeling (MLM), Graph Auto-Encoder(GAE), and IR-Graph Matching as pre-training tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,317.96,599.45,240.25,7.70;3,317.96,610.41,93.72,7.70"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example showing encoding and decoding with the MIREncoder tokenizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,53.80,531.99,240.25,7.70;4,53.80,542.95,199.65,7.70"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of code graphs used in this study. This is a graph for a simple program adding two numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="8,74.79,229.45,462.42,7.70"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Error rates for predicting NUMA and prefetcher configurations for parallel code regions (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="9,53.80,223.10,240.58,7.70;9,53.80,234.06,98.49,7.70"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Error rates for predicting thread blocks for CUDA kernels (lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="3,326.14,482.15,226.49,86.49"><head>Pros Sample IR: %"class.std::ios_base" = type { i32 (...)**, i64, i32, i32, i32, i64, i64, %"struct.std::ios_base::_Iosarray"*, %"struct.std::ios_base::_Fnarray"*, %"class.std::locale"* } Encoded IR: [1, 9, 6, 114, 18, 108, 30, 30, 251, 41, 174, 6, 33, 273, 68, 124, 12, 18, 18, 18, 13, 14, 14, 16, 127, 16, 124, 16, 124, 16, 124, 16, 127, 16, 127, 16, 9, 6, 253, 18, 108, 30, 30, 251, 41, 174, 30, 30, 41, 2257, 6, 14, 16, 9, 6, 253, 18, 108, 30, 30, 251, 41, 174, 30, 30, 41, 2258, 6, 14, 16, 9, 6, 114, 18, 108, 30, 30, 237, 6, 14, 70, 2] Decoded IR: ['[CLS]</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,53.80,85.73,241.32,121.80"><head>Table 1 :</head><label>1</label><figDesc>Accuracy: (CPU/GPU) device mapping. Numbers in parenthesis are percentage improvements in accuracy over prior works.</figDesc><table coords="6,53.80,106.27,228.34,97.82"><row><cell>State-of-the-art</cell><cell cols="2">NVIDIA GPU (%)  *  AMD GPU (%)  *</cell></row><row><cell>Grewe et al. [28]</cell><cell>74.56 (25.67)</cell><cell>70.29 (33.16)</cell></row><row><cell>DeepTune [19]</cell><cell>80.88 (15.85)</cell><cell>83.24 (12.44)</cell></row><row><cell>inst2Vec [9]</cell><cell>82.65 (13.37)</cell><cell>82.35 (13.66)</cell></row><row><cell>PROGRAML [18]</cell><cell>80 (17.13)</cell><cell>86.6 (8.08)</cell></row><row><cell>IR2Vec [53]</cell><cell>89.68 (4.48)</cell><cell>92.82 (0.84)</cell></row><row><cell>Perfograph [51]</cell><cell>90 (4.47)</cell><cell>94 (-0.42)</cell></row><row><cell cols="2">MIREncoder (ours) 93.7</cell><cell>93.6</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>*  </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="6,53.50,225.04,240.96,109.83"><head>Table 2 :</head><label>2</label><figDesc>Thread Coarsening Factors: Speedups obtained by prior works. Best speedups are highlighted in bold.</figDesc><table coords="6,53.80,259.31,240.66,75.56"><row><cell>Device</cell><cell>DT</cell><cell cols="2">NCC IV</cell><cell>PG</cell><cell>ME</cell></row><row><cell cols="2">AMD Radeon HD 5900 1.1</cell><cell>1.29</cell><cell>1.24</cell><cell cols="2">1.19 1.29</cell></row><row><cell>AMD Tahiti 7970</cell><cell cols="2">1.05 1.07</cell><cell>1.30</cell><cell cols="2">1.14 1.30</cell></row><row><cell>NVIDIA GTX 480</cell><cell>1.1</cell><cell>0.97</cell><cell>1.25</cell><cell cols="2">1.03 1.26</cell></row><row><cell>NVIDIA Tesla K20c</cell><cell cols="2">0.99 1.01</cell><cell cols="3">1.16 1.01 1.16</cell></row><row><cell>Average</cell><cell cols="2">1.06 1.09</cell><cell>1.23</cell><cell cols="2">1.09 1.24</cell></row><row><cell cols="6">DT=DeepTune, NCC=inst2vec, IV=IR2Vec, PG=Perfograph, ME=MIREncoder (ours)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="6,317.66,85.73,240.54,53.35"><head>Table 3 :</head><label>3</label><figDesc>Speedups: Improvements in runtime over LLVM vectorization. LLVM vectorization speedups are always 1.0×.</figDesc><table coords="6,329.49,120.00,214.66,19.08"><row><cell>LLVM</cell><cell>Neurovectorizer</cell><cell>MIREncoder (ours)</cell></row><row><cell>1.0×</cell><cell>1.22×</cell><cell>1.32×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="7,53.80,85.73,240.25,117.29"><head>Table 4 :</head><label>4</label><figDesc>Search space for tuning OpenMP parameters.</figDesc><table coords="7,53.80,109.04,240.25,93.98"><row><cell>Parameter Name</cell><cell>Parameter Values</cell></row><row><cell>Power Limits</cell><cell>75W, 100W, 120W, 150W</cell></row><row><cell cols="2">Number of threads 1, 4, 8, 16, 32, 64</cell></row><row><cell>Scheduling Policy</cell><cell>STATIC, DYNAMIC, GUIDED</cell></row><row><cell>Chunk Sizes</cell><cell>1, 8, 32, 64, 128, 256, 512</cell></row><row><cell cols="2">that using Neurovectorizer leads to speedups of ≈ 1.22× across</cell></row><row><cell cols="2">all kernels in the test set (Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="8,323.79,255.35,211.91,53.75"><head>Table 5 :</head><label>5</label><figDesc>Parameters Modified for CUDA kernels.</figDesc><table coords="8,323.79,278.66,194.74,30.43"><row><cell cols="2">Param. Name Param. Values</cell></row><row><cell>Matrix Size</cell><cell>240, 496, 784, 1016, 1232, 1680, 2024</cell></row><row><cell>Block Sizes</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="9,53.50,262.10,242.15,191.59"><head>Table 6 :</head><label>6</label><figDesc>Ablation Studies (CS1): Heterogeneous device mapping. Change in accuracy when one modality is removed.</figDesc><table coords="9,53.80,296.37,240.24,157.31"><row><cell>State-of-the-art</cell><cell>NVIDIA(%)</cell><cell>AMD(%)</cell></row><row><cell>Grewe et al. [28]</cell><cell>74.56</cell><cell>70.29</cell></row><row><cell>DeepTune [19]</cell><cell>80.88</cell><cell>83.24</cell></row><row><cell>inst2Vec [9]</cell><cell>82.65</cell><cell>82.35</cell></row><row><cell>PROGRAML [18]</cell><cell>80</cell><cell>86.6</cell></row><row><cell>IR2Vec [53]</cell><cell>89.68</cell><cell>92.82</cell></row><row><cell>Perfograph [51]</cell><cell>90</cell><cell>94</cell></row><row><cell>MIREncoder (only IR text)</cell><cell>59.1</cell><cell>79.7</cell></row><row><cell cols="2">MIREncoder (only IR graphs) 86.2</cell><cell>88.5</cell></row><row><cell>MIREncoder</cell><cell>93.7</cell><cell>93.6</cell></row><row><cell cols="3">our predictions reduce error rate over IR2Vec, Perfograph, and</cell></row><row><cell cols="3">PROGRAML by ≈ 39%, ≈ 63% and ≈ 70% respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="9,317.66,85.73,242.15,242.46"><head>Table 7 :</head><label>7</label><figDesc>Ablation Studies (CS2): Thread Coarsening Factors: Changes in speedups when one modality is removed.</figDesc><table coords="9,317.66,120.00,242.15,208.19"><row><cell>Device</cell><cell></cell><cell cols="4">DT NCC IV PG ME(T) ME(G) ME</cell></row><row><cell cols="2">AMD Radeon</cell><cell cols="2">1.1 1.29 1.24 1.19 1.24</cell><cell>1.13</cell><cell>1.29</cell></row><row><cell>HD 5900</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AMD Tahiti</cell><cell cols="2">1.05 1.07 1.30 1.14 1.25</cell><cell>1.15</cell><cell>1.30</cell></row><row><cell>7970</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NVIDIA GTX</cell><cell cols="2">1.1 0.97 1.25 1.03 1.19</cell><cell>1.09</cell><cell>1.26</cell></row><row><cell>480</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NVIDIA</cell><cell></cell><cell cols="2">0.99 1.01 1.16 1.01 1.07</cell><cell>1.08</cell><cell>1.16</cell></row><row><cell cols="2">Tesla K20c</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell></cell><cell cols="2">1.06 1.09 1.23 1.09 1.18</cell><cell>1.11</cell><cell>1.24</cell></row><row><cell cols="6">DT=DeepTune, NCC=inst2vec, IV=IR2Vec, PG=Perfograph, ME=MIREncoder, T=Text</cell></row><row><cell cols="2">Only, G=Graph Only</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Table 8: Ablation Studies (CS3): Speedups over LLVM vector-</cell></row><row><cell cols="4">ization when individual modalities are used.</cell><cell></cell></row><row><cell cols="3">LLVM Neurovec. ME (T)</cell><cell>ME (G)</cell><cell>ME</cell></row><row><cell>1.0×</cell><cell>1.22×</cell><cell>1.01×</cell><cell>1.18×</cell><cell>1.32×</cell></row><row><cell cols="5">ME=MIREncoder, Neurovec=Neurovectorizer, T=Text Only, G=Graphs Only</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" coords="10,53.80,85.72,231.86,64.74"><head>Table 9 :</head><label>9</label><figDesc>Slowdowns over MIREncoder (no FT) wall times.</figDesc><table coords="10,53.80,109.04,222.43,41.41"><row><cell>Process ME (w/o FT )</cell><cell cols="2">PnP (GNNs) ME (w. FT )</cell></row><row><cell>Training 1×</cell><cell>37×</cell><cell>238×</cell></row><row><cell>Inference 1×</cell><cell>1.8×</cell><cell>1×</cell></row><row><cell cols="2">FT=Fine-tuning, ME=MIREncoder, w/o=without, w.=with</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the National Science Foundation under Grant number 2211982. We would also like to thank the Re-searchIT team 1 at Iowa State University for their constant support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="11,69.23,565.18,224.81,6.18;11,69.23,573.15,224.81,6.18;11,69.23,581.07,184.06,6.25" xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting number of threads using balanced datasets for openmp regions</title>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Alcaraz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Tehranijamsaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akash</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Jannesari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joan</forename><surname>Sorribes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduardo</forename><surname>Cesar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="999" to="1017" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,589.09,225.88,6.18;11,68.99,597.01,225.05,6.25;11,69.23,604.98,95.22,6.25" xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName coords=""><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,613.00,224.81,6.18;11,69.23,620.92,211.33,6.25" xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName coords=""><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00740</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,69.23,628.94,225.99,6.18;11,69.23,636.86,224.81,6.25;11,69.23,644.88,59.97,6.18" xml:id="b3">
	<analytic>
		<title level="a" type="main">A general pathbased representation for predicting program properties</title>
		<author>
			<persName coords=""><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,69.23,652.85,225.99,6.18;11,69.23,660.77,224.81,6.25;11,69.23,668.74,91.34,6.25" xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning distributed representations of code</title>
		<author>
			<persName coords=""><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Meital</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>POPL</note>
</biblStruct>

<biblStruct coords="11,127.18,676.76,167.94,6.18;11,69.23,684.73,209.25,6.18" xml:id="b5">
	<monogr>
		<ptr target="https://developer.amd.com/amd-accelerated-parallel-processing-app-sdk/" />
		<title level="m">AMD OpenCL accelerated parallel processing SDK</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,89.10,225.58,6.18;11,333.28,97.07,225.75,6.18;11,333.15,104.98,225.05,6.25;11,333.39,112.95,209.50,6.25" xml:id="b6">
	<analytic>
		<title level="a" type="main">Opentuner: An extensible framework for program autotuning</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kalyan</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><surname>Bosboom</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Una-May O'</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Parallel architectures and compilation</title>
				<meeting>the 23rd international conference on Parallel architectures and compilation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,120.98,225.04,6.18;11,333.16,128.95,225.04,6.18;11,333.39,136.86,225.88,6.25;11,333.39,144.89,24.44,6.18" xml:id="b7">
	<analytic>
		<title level="a" type="main">The nas parallel benchmarks</title>
		<author>
			<persName coords=""><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Barton</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Weeratunga</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">The International Journal of Supercomputer Applications</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,152.86,224.81,6.18;11,333.39,160.77,224.81,6.25;11,333.39,168.74,114.86,6.25" xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural code comprehension: A learnable representation of code semantics</title>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alice</forename><forename type="middle">Shoshana</forename><surname>Jakobovits</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,176.77,224.81,6.18;11,333.39,184.68,224.81,6.25;11,333.39,192.65,90.14,6.25" xml:id="b9">
	<analytic>
		<title level="a" type="main">LS-CAT: a large-scale CUDA AutoTuning dataset</title>
		<author>
			<persName coords=""><forename type="first">Lars</forename><surname>Bjertnes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><forename type="middle">O</forename><surname>Tørring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anne</forename><forename type="middle">C</forename><surname>Elster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Applied Artificial Intelligence (ICAPAI)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,200.68,225.89,6.18;11,333.39,208.65,225.88,6.18;11,333.39,216.56,225.88,6.25;11,333.39,224.59,24.81,6.18" xml:id="b10">
	<analytic>
		<title level="a" type="main">Compiler-based graph representations for deep learning models of code</title>
		<author>
			<persName coords=""><forename type="first">Alexander</forename><surname>Brauckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrés</forename><surname>Goens</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sebastian</forename><surname>Ertel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeronimo</forename><surname>Castrillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Compiler Construction</title>
				<meeting>the 29th International Conference on Compiler Construction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="201" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,232.56,225.99,6.18;11,333.39,240.47,224.81,6.25;11,333.39,248.44,81.02,6.25" xml:id="b11">
	<analytic>
		<title level="a" type="main">CLOMP: accurately characterizing OpenMP application overheads</title>
		<author>
			<persName coords=""><forename type="first">Greg</forename><surname>Bronevetsky</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Gyllenhaal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bronis R De</forename><surname>Supinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on OpenMP</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,256.47,224.99,6.18;11,333.39,264.44,224.81,6.18;11,333.39,272.35,224.81,6.25;11,333.39,280.32,196.23,6.25" xml:id="b12">
	<analytic>
		<title level="a" type="main">A machine learning-based approach for thread mapping on transactional memory applications</title>
		<author>
			<persName coords=""><forename type="first">Márcio</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luis</forename><surname>Fabricio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanderley</forename><surname>Goes</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christiane</forename><forename type="middle">Pousa</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Murray</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marcelo</forename><surname>Cintra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jean-Francois</forename><surname>Mehaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 18th International Conference on High Performance Computing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,288.35,225.99,6.18;11,333.39,296.32,224.81,6.18;11,333.39,304.23,224.81,6.25;11,332.97,312.20,58.12,6.25" xml:id="b13">
	<analytic>
		<title level="a" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiayuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sang-Ha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE international symposium on workload characterization (IISWC)</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,320.23,225.58,6.18;11,333.39,328.20,224.81,6.18;11,333.39,336.11,224.81,6.25;11,333.39,344.08,150.26,6.25" xml:id="b14">
	<analytic>
		<title level="a" type="main">A characterization of the Rodinia benchmark suite with comparison to contemporary CMP workloads</title>
		<author>
			<persName coords=""><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liang</forename><surname>Lukasz G Szafaryn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization (IISWC&apos;10)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,352.11,225.99,6.18;11,333.39,360.08,224.81,6.18;11,333.15,367.99,212.15,6.25" xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to parallelize with openMP by augmented heterogeneous AST representation</title>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Quazi</forename><surname>Ishtiaque Mahmud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hung</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nesreen</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2023">Jannesari. 2023. 2023</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,413.63,376.02,148.02,6.18" xml:id="b16">
	<monogr>
		<ptr target="http://developer.nvidia.com/object/cuda.html" />
		<title level="m">CUDA</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,391.96,224.81,6.18;11,333.39,399.93,225.99,6.18;11,333.39,407.84,224.81,6.25;11,333.39,415.81,113.90,6.25" xml:id="b17">
	<analytic>
		<title level="a" type="main">Programl: A graph-based program representation for data flow analysis and compiler optimizations</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zacharias</forename><forename type="middle">V</forename><surname>Fisches</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fp O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugh</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2244" to="2253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,423.84,225.88,6.18;11,333.39,431.75,224.81,6.25;11,333.39,439.72,225.58,6.25;11,333.39,447.75,24.81,6.18" xml:id="b18">
	<analytic>
		<title level="a" type="main">End-to-end deep learning of optimization heuristics</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pavlos</forename><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="219" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,455.72,225.58,6.18;11,333.39,463.69,225.99,6.18;11,333.39,471.60,224.81,6.25;11,333.39,479.57,91.05,6.25" xml:id="b19">
	<monogr>
		<title level="m" type="main">Large language models for compiler optimization</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Volker</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dejan</forename><surname>Grubisic</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mostafa</forename><surname>Elhoushi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Youwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Roziere</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabian</forename><surname>Gloeckle</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07062</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,487.60,224.81,6.18;11,333.39,495.57,224.94,6.18;11,333.39,503.49,186.45,6.25" xml:id="b20">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Hoa</forename><surname>Khanh Dam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shien</forename><forename type="middle">Wee</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Grundy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Taeksu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chul-Joo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00921</idno>
		<title level="m">A deep tree-based model for software defect prediction</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,511.51,224.81,6.18;11,333.39,519.48,224.81,6.18;11,333.39,527.40,224.81,6.25;11,333.39,535.37,217.65,6.25" xml:id="b21">
	<analytic>
		<title level="a" type="main">The scalable heterogeneous computing (SHOC) benchmark suite</title>
		<author>
			<persName coords=""><forename type="first">Anthony</forename><surname>Danalis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gabriel</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Collin</forename><surname>Mccurdy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremy</forename><forename type="middle">S</forename><surname>Meredith</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philip</forename><forename type="middle">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kyle</forename><surname>Spafford</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vinod</forename><surname>Tipparaju</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on general-purpose computation on graphics processing units</title>
				<meeting>the 3rd workshop on general-purpose computation on graphics processing units</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,543.39,225.63,6.18;11,333.39,551.31,224.81,6.25;11,333.39,559.28,91.11,6.25" xml:id="b22">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,567.30,225.58,6.18;11,333.39,575.27,224.81,6.18;11,333.39,583.19,224.81,6.25;11,333.39,591.16,172.40,6.25" xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance Optimization using Multimodal Modeling and Heterogeneous GNN</title>
		<author>
			<persName coords=""><forename type="first">Akash</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Alcaraz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Tehranijamsaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduardo</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Jannesari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing</title>
				<meeting>the 32nd International Symposium on High-Performance Parallel and Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,599.18,224.81,6.18;11,333.15,607.15,225.05,6.18;11,333.39,615.07,224.81,6.25;11,333.39,623.04,179.76,6.25" xml:id="b24">
	<analytic>
		<title level="a" type="main">Pattern-based autotuning of openmp loops using graph neural networks</title>
		<author>
			<persName coords=""><forename type="first">Akash</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jordi</forename><surname>Alcaraz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Tehranijamsaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Anna</forename><surname>Sikora</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eduardo</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Jannesari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Workshop on Artificial Intelligence and Machine Learning for Scientific Applications</title>
		<imprint>
			<biblScope unit="issue">AI4S</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2022">2022. 2022</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,631.06,225.99,6.18;11,333.39,638.98,224.81,6.25;11,333.39,646.95,225.58,6.25;11,333.39,654.97,161.63,6.18" xml:id="b25">
	<analytic>
		<title level="a" type="main">Power Constrained Autotuning using Graph Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Jannesari</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS54959.2023.00060</idno>
		<ptr target="https://doi.org/10.1109/IPDPS54959.2023.00060" />
	</analytic>
	<monogr>
		<title level="m">2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="535" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="11,333.39,662.94,225.57,6.18;11,333.39,670.91,224.81,6.18;11,333.39,678.83,224.81,6.25;11,333.18,686.85,18.66,6.18" xml:id="b26">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">Codebert: A pre-trained model for programming and natural languages</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="11,333.39,694.82,224.81,6.18;11,333.39,702.74,224.81,6.25;12,69.23,89.04,224.81,6.25;12,68.81,97.01,53.13,6.25" xml:id="b27">
	<analytic>
		<title level="a" type="main">Portable mapping of data parallel programs to opencl for heterogeneous systems</title>
		<author>
			<persName coords=""><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fp O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
				<meeting>the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,105.04,224.81,6.18;12,68.99,113.01,225.06,6.18;12,69.23,120.92,224.81,6.25;12,69.23,128.89,167.06,6.25" xml:id="b28">
	<analytic>
		<title level="a" type="main">Neurovectorizer: End-to-end vectorization with deep reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ted</forename><surname>Willke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization</title>
				<meeting>the 18th ACM/IEEE International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
	<note>Yakun Sophia Shao, Krste Asanovic, and Ion Stoica</note>
</biblStruct>

<biblStruct coords="12,69.23,136.92,225.88,6.18;12,69.23,144.89,225.88,6.18;12,69.23,152.80,225.88,6.25;12,69.23,160.83,18.33,6.18" xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine-learning-based performance heuristics for runtime cpu/gpu selection</title>
		<author>
			<persName coords=""><forename type="first">Akihiro</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kazuaki</forename><surname>Ishizaki</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gita</forename><surname>Koblents</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivek</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the principles and practices of programming on the Java platform</title>
				<meeting>the principles and practices of programming on the Java platform</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,168.80,225.88,6.18;12,69.23,176.71,224.81,6.25;12,69.23,184.68,207.99,6.25" xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantifying openmp: Statistical insights into usage and adoption</title>
		<author>
			<persName coords=""><forename type="first">Tal</forename><surname>Kadosh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niranjan</forename><surname>Hasabnis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Timothy</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gal</forename><surname>Oren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 IEEE High Performance Extreme Computing Conference (HPEC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,192.65,225.88,6.25;12,69.03,200.68,225.26,6.18;12,69.03,208.65,43.51,6.18" xml:id="b31">
	<monogr>
		<title level="m" type="main">Lulesh 2.0 updates and changes</title>
		<author>
			<persName coords=""><forename type="first">Ian</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Keasler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Neely</forename><surname>Robert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Livermore, CA (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Lawrence Livermore National Lab.(LLNL)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct coords="12,69.23,216.62,225.89,6.18;12,69.23,224.59,225.88,6.18;12,69.23,232.50,187.89,6.25" xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName coords=""><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenjie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,240.53,225.99,6.18;12,69.23,248.50,225.89,6.18;12,69.23,256.41,169.65,6.25" xml:id="b33">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName coords=""><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,264.44,224.81,6.18;12,69.23,272.35,224.81,6.25;12,69.23,280.32,223.07,6.25" xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic optimization of thread-coarsening for graphics processors</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Magni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Parallel architectures and compilation</title>
				<meeting>the 23rd international conference on Parallel architectures and compilation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="455" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,288.35,225.99,6.18;12,69.23,296.26,224.81,6.25;12,69.23,304.23,224.81,6.25;12,69.23,312.20,54.03,6.25" xml:id="b35">
	<analytic>
		<title level="a" type="main">A largescale cross-architecture evaluation of thread-coarsening</title>
		<author>
			<persName coords=""><forename type="first">Alberto</forename><surname>Magni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fp O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,320.23,225.99,6.18;12,69.23,328.14,152.09,6.25" xml:id="b36">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Multimodal deep learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct coords="12,69.23,336.17,225.89,6.18;12,69.23,344.08,207.24,6.25" xml:id="b37">
	<monogr>
		<title level="m" type="main">Polybench: The polyhedral benchmark suite</title>
		<author>
			<persName coords=""><forename type="first">Louis-Noël</forename><surname>Pouchet</surname></persName>
		</author>
		<ptr target="http://www.cs.ucla.edu/pouchet/software/polybench437" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,352.11,225.58,6.18;12,69.00,360.08,225.81,6.18;12,69.00,368.05,225.04,6.18;12,69.23,376.02,224.81,6.18;12,69.23,383.99,117.72,6.18" xml:id="b38">
	<monogr>
		<title level="m" type="main">CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks</title>
		<author>
			<persName coords=""><forename type="first">Ruchir</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geert</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladmir</forename><surname>Zolotov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Julian</forename><surname>Dolby</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihir</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lindsey</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Buratti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saurabh</forename><surname>Pujar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shyam</forename><surname>Ramji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ulrich</forename><surname>Finkler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Malaika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Frederick</forename><surname>Reiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,391.96,225.99,6.18;12,69.23,399.93,225.99,6.18;12,68.98,407.84,225.07,6.25;12,69.23,415.81,224.81,6.25;12,69.23,423.78,224.81,6.25;12,69.23,431.75,139.96,6.25" xml:id="b39">
	<analytic>
		<title level="a" type="main">Rigel: A Framework for OpenMP PerformanceTuning</title>
		<author>
			<persName coords=""><forename type="first">Piyumi</forename><surname>Rameshka</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pasindu</forename><surname>Senanayake</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thulana</forename><surname>Kannangara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Praveen</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sanath</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tharindu</forename><surname>Rusira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems</title>
		<title level="s">HPCC/SmartCity/DSS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2093" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,439.78,224.81,6.18;12,69.23,447.69,203.79,6.25" xml:id="b40">
	<analytic>
		<title level="a" type="main">Predicting program properties from&quot; big code</title>
		<author>
			<persName coords=""><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,455.72,225.63,6.18;12,69.23,463.69,224.81,6.18;12,69.23,471.60,224.81,6.25;12,69.23,479.57,179.52,6.25" xml:id="b41">
	<analytic>
		<title level="a" type="main">Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models</title>
		<author>
			<persName coords=""><forename type="first">Rohan</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roy</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tirthak</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vijay</forename><surname>Gadepally</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devesh</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</title>
				<meeting>the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1280" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,487.60,225.99,6.18;12,69.23,495.57,224.81,6.18;12,69.23,503.49,224.81,6.25;12,69.23,511.46,141.55,6.25" xml:id="b42">
	<analytic>
		<title level="a" type="main">Anastasiia Stupnikova, and Mihail Popov. 2020. Modeling and optimizing numa effects and prefetching with machine learning</title>
		<author>
			<persName coords=""><forename type="first">Isaac Sánchez</forename><surname>Barrera</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marc</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Miquel</forename><surname>Moretó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM International Conference on Supercomputing</title>
				<meeting>the 34th ACM International Conference on Supercomputing</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,519.48,224.81,6.18;12,69.23,527.40,224.81,6.25;12,69.23,535.37,149.24,6.25" xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance characterization of the NAS Parallel Benchmarks in OpenCL</title>
		<author>
			<persName coords=""><forename type="first">Sangmin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gangwon</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international symposium on workload characterization (IISWC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="137" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,543.34,224.81,6.25;12,69.23,551.31,219.88,6.25" xml:id="b44">
	<analytic>
		<title level="a" type="main">Predictable thread coarsening</title>
		<author>
			<persName coords=""><forename type="first">Nicolai</forename><surname>Stawinoga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tony</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>TACO)</note>
</biblStruct>

<biblStruct coords="12,69.23,559.33,224.81,6.18;12,69.23,567.25,224.81,6.25;12,69.23,575.22,111.63,6.25" xml:id="b45">
	<analytic>
		<title level="a" type="main">Value learning for throughput optimization of deep learning workloads</title>
		<author>
			<persName coords=""><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,583.24,225.58,6.18;12,69.23,591.21,224.81,6.18;12,69.23,599.13,224.81,6.25;12,69.23,607.10,160.72,6.25" xml:id="b46">
	<analytic>
		<title level="a" type="main">Parboil: A revised benchmark suite for scientific and commercial throughput computing</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>John A Stratton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I-Jui</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nady</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Li-Wen</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nasser</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Geng</forename><surname>Anssari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wen-Mei W</forename><surname>Daniel Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Computing</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,615.12,225.88,6.18;12,69.23,623.09,225.88,6.18;12,69.23,631.01,108.66,6.25" xml:id="b47">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Jabeen</forename><surname>Summaira</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amin</forename><surname>Muhammad Shoib</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jabbar</forename><surname>Abdul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11087</idno>
		<title level="m">Recent Advances and Trends in Multimodal Deep Learning: A Review</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,69.23,639.03,225.88,6.18;12,69.23,647.00,225.88,6.18;12,69.23,654.92,225.58,6.25;12,69.07,662.94,14.51,6.18" xml:id="b48">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
				<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,670.91,225.99,6.18;12,69.23,678.83,224.81,6.25;12,69.23,686.80,155.06,6.25" xml:id="b49">
	<analytic>
		<title level="a" type="main">Active harmony: Towards automated performance tuning</title>
		<author>
			<persName coords=""><forename type="first">Cristian</forename><surname>Tapus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I-Hsin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">K</forename><surname>Hollingsworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;02: Proceedings of the 2002 ACM/IEEE Conference on Supercomputing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="44" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,69.23,694.82,224.81,6.18;12,69.12,702.79,225.05,6.18;12,333.39,89.04,224.81,6.25;12,333.39,97.01,80.91,6.25" xml:id="b50">
	<analytic>
		<title level="a" type="main">Perfograph: A numerical aware program graph representation for performance optimization and program analysis</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Tehranijamsaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ishtiaque</forename><surname>Quazi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Le</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Jannesari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,105.04,224.81,6.18;12,333.28,113.01,226.10,6.18;12,333.13,120.92,225.07,6.25;12,333.39,128.89,181.54,6.25" xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning intermediate representations using graph neural networks for numa and prefetchers optimization</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Tehranijamsaz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mihail</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Akash</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emmanuelle</forename><surname>Saillard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Jannesari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1206" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,136.92,225.58,6.18;12,333.39,144.89,224.81,6.18;12,333.39,152.80,224.81,6.25;12,332.97,160.77,73.20,6.25" xml:id="b52">
	<analytic>
		<title level="a" type="main">Ir2vec: Llvm ir based scalable program embeddings</title>
		<author>
			<persName coords=""><forename type="first">Rohit</forename><surname>Venkatakeerthy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shalini</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maunendra</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ramakrishna</forename><surname>Sankar Desarkar</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Upadrasta</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,168.80,224.81,6.18;12,333.39,176.71,225.51,6.25;12,333.39,184.68,64.70,6.25" xml:id="b53">
	<analytic>
		<title level="a" type="main">Benchmarking GPUs to tune dense linear algebra</title>
		<author>
			<persName coords=""><forename type="first">Vasily</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">James</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;08: Proceedings of the 2008 ACM/IEEE conference on Supercomputing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,192.71,225.88,6.18;12,333.39,200.68,224.81,6.18;12,333.39,208.59,224.97,6.25;12,333.18,216.62,35.49,6.18" xml:id="b54">
	<analytic>
		<title level="a" type="main">Integrating profile-driven parallelism detection and machine-learning-based mapping</title>
		<author>
			<persName coords=""><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Georgios</forename><surname>Tournavitis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Björn</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fp O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,224.59,224.81,6.18;12,333.39,232.56,225.99,6.18;12,333.39,240.53,225.88,6.18;12,333.39,248.44,223.04,6.25" xml:id="b55">
	<analytic>
		<title level="a" type="main">Artemis: Automatic Runtime Tuning of Parallel Execution Parameters Using Machine Learning</title>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Giorgis</forename><surname>Georgakoudis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Beckingsale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Poliakoff</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alfredo</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kevin</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Allen</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Gamblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="453" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,256.47,225.57,6.18;12,333.39,264.44,225.88,6.18;12,333.39,272.41,225.88,6.18;12,333.39,280.32,108.33,6.25" xml:id="b56">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Xingfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasanna</forename><surname>Balaprakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaehoon</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brice</forename><surname>Videau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Hovland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valerie</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brad</forename><surname>Geltz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Siddhartha</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16245</idno>
		<title level="m">ytopt: Autotuning scientific applications for energy efficiency at large scales</title>
				<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct coords="12,333.39,288.35,225.58,6.18;12,333.16,296.32,225.04,6.18;12,333.39,304.23,225.51,6.25;12,333.39,312.20,200.35,6.25" xml:id="b57">
	<monogr>
		<title level="m" type="main">Autotuning PolyBench benchmarks with LLVM Clang/Polly loop optimization pragmas using Bayesian optimization. Concurrency and Computation: Practice and Experience</title>
		<author>
			<persName coords=""><forename type="first">Xingfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasanna</forename><surname>Balaprakash</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hal</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Hovland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Valerie</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mary</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">e6683</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="12,333.39,320.23,225.58,6.18;12,333.06,328.20,226.22,6.18;12,333.39,336.17,224.81,6.18;12,333.39,344.08,203.00,6.25" xml:id="b58">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
