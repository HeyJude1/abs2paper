import os
import json
import logging
from typing import Dict, List, Optional, Any

from abs2paper.utils.llm_client import LLMClient

class PaperGenerator:
    """é¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆå™¨ - è§£å†³é€»è¾‘è¿è´¯æ€§é—®é¢˜"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–è®ºæ–‡ç”Ÿæˆå™¨"""
        # è®¾ç½®é¡¹ç›®æ ¹ç›®å½•å’Œé…ç½®æ–‡ä»¶è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.config_path = config_path or os.path.join(self.project_root, "config", "config.json")
        self.config = self._load_config()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient()
        
        # è·å–paper_promptè·¯å¾„
        data_paths = self.config["data_paths"]
        paper_prompt_path = data_paths["paper_prompt"]["path"].lstrip('/')
        self.paper_prompt_dir = os.path.join(self.project_root, paper_prompt_path)
        
        # åŠ è½½è®ºæ–‡ç”Ÿæˆæç¤ºè¯
        self.paper_prompts = self._load_paper_prompts()
        
        # è®ºæ–‡éƒ¨åˆ†ç”Ÿæˆé¡ºåºå’Œä¾èµ–å…³ç³»
        self.generation_order = [
            {
                "section": "å¼•è¨€",
                "dependencies": [],
                "context_sources": ["Background", "Challenges", "Innovations"],
                "include_source_text": False,
                "previous_context_needed": False
            },
            {
                "section": "ç›¸å…³å·¥ä½œ", 
                "dependencies": ["å¼•è¨€"],
                "context_sources": ["RelatedWork", "Challenges"],
                "include_source_text": False,
                "previous_context_needed": True,
                "previous_context_type": "æ¦‚è¿°"  # åªéœ€è¦å¼•è¨€çš„æ¦‚è¿°
            },
            {
                "section": "æ–¹æ³•",
                "dependencies": ["å¼•è¨€", "ç›¸å…³å·¥ä½œ"],
                "context_sources": ["Methodology"],
                "include_source_text": True,
                "previous_context_needed": True,
                "previous_context_type": "æ¦‚è¿°"  # éœ€è¦å¼•è¨€+ç›¸å…³å·¥ä½œçš„æ¦‚è¿°
            },
            {
                "section": "å®éªŒè¯„ä»·",
                "dependencies": ["æ–¹æ³•"],
                "context_sources": ["ExpeDesign", "Baseline", "Metric", "ResultAnalysis"],
                "include_source_text": True,
                "previous_context_needed": True,
                "previous_context_type": "è¯¦ç»†"  # éœ€è¦æ–¹æ³•çš„è¯¦ç»†å†…å®¹
            },
            {
                "section": "æ€»ç»“",
                "dependencies": ["å¼•è¨€", "ç›¸å…³å·¥ä½œ", "æ–¹æ³•", "å®éªŒè¯„ä»·"],
                "context_sources": ["Conclusion", "ResultAnalysis", "Innovations"],
                "include_source_text": False,
                "previous_context_needed": True,
                "previous_context_type": "æ¦‚è¿°"  # éœ€è¦å…¨æ–‡æ¦‚è¿°
            }
        ]
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_paper_prompts(self) -> Dict[str, str]:
        """åŠ è½½è®ºæ–‡ç”Ÿæˆæç¤ºè¯"""
        paper_prompts = {}
        
        # 5ä¸ªè®ºæ–‡éƒ¨åˆ†å¯¹åº”çš„promptæ–‡ä»¶
        section_prompts = {
            "å¼•è¨€": "Introduction_prompt",
            "ç›¸å…³å·¥ä½œ": "RelatedWork_prompt", 
            "æ–¹æ³•": "Methodology_prompt",
            "å®éªŒè¯„ä»·": "Experiments_prompt",
            "æ€»ç»“": "Conclusion_prompt"
        }
        
        for section, prompt_file in section_prompts.items():
            prompt_path = os.path.join(self.paper_prompt_dir, prompt_file)
            
            try:
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    prompt_content = f.read().strip()
                    if prompt_content:
                        paper_prompts[section] = prompt_content
                        logging.info(f"âœ… å·²åŠ è½½ {section} ç”Ÿæˆæç¤ºè¯")
                    else:
                        logging.warning(f"âš ï¸ {section} æç¤ºè¯æ–‡ä»¶ä¸ºç©º: {prompt_path}")
            
            except FileNotFoundError:
                logging.error(f"âŒ æœªæ‰¾åˆ° {section} æç¤ºè¯æ–‡ä»¶: {prompt_path}")
            except Exception as e:
                logging.error(f"âŒ åŠ è½½ {section} æç¤ºè¯æ—¶å‡ºé”™: {e}")
        
        logging.info(f"ğŸ“ æ€»å…±åŠ è½½äº† {len(paper_prompts)} ä¸ªè®ºæ–‡ç”Ÿæˆæç¤ºè¯")
        return paper_prompts
    
    def _generate_section_content(self, section_name: str, 
                                context: str, 
                                user_requirement: str) -> str:
        """ç”Ÿæˆç‰¹å®šéƒ¨åˆ†çš„å†…å®¹"""
        # ä½¿ç”¨åŠ è½½çš„æç¤ºè¯æ¨¡æ¿
        if section_name not in self.paper_prompts:
            logging.error(f"æœªæ‰¾åˆ° {section_name} çš„æç¤ºè¯æ¨¡æ¿")
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ç”Ÿæˆè®ºæ–‡çš„{section_name}éƒ¨åˆ†ï¼š

{context}

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}

è¦æ±‚ï¼š
1. å†…å®¹è¦ä¸ç”¨æˆ·éœ€æ±‚"{user_requirement}"é«˜åº¦ç›¸å…³
2. ä¿æŒå­¦æœ¯è®ºæ–‡çš„è§„èŒƒæ ¼å¼
3. ç¡®ä¿é€»è¾‘æ¸…æ™°ï¼Œè¡¨è¾¾å‡†ç¡®
4. å­—æ•°æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´
5. ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯å†™ä½œæ ¼å¼
"""
        else:
            # ä½¿ç”¨åŠ è½½çš„æç¤ºè¯æ¨¡æ¿
            base_prompt = self.paper_prompts[section_name]
            prompt = f"{base_prompt}\n\n{context}"
        
        return self.llm_client.get_completion(prompt)
    
    def _generate_section_summary(self, section_name: str, content: str) -> str:
        """ç”Ÿæˆéƒ¨åˆ†å†…å®¹çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨"""
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹è®ºæ–‡{section_name}éƒ¨åˆ†ç”Ÿæˆä¸€ä¸ª150å­—å·¦å³çš„æ¦‚è¿°ï¼Œçªå‡ºå…³é”®ç‚¹ï¼š

{content}

è¦æ±‚ï¼š
1. æ¦‚è¿°è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºæ ¸å¿ƒå†…å®¹
2. ä¸ºåç»­éƒ¨åˆ†æä¾›å¿…è¦çš„é€»è¾‘è¡”æ¥ä¿¡æ¯
3. é¿å…è¿‡äºè¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚
"""
        
        return self.llm_client.get_completion(prompt)
    
    def _build_full_context_for_section(self, section_name: str,
                                       base_context: str,
                                       previous_sections: Dict[str, str],
                                       section_summaries: Dict[str, str],
                                       step_config: Dict,
                                       user_requirement: str) -> str:
        """ä¸ºå½“å‰éƒ¨åˆ†æ„å»ºåŒ…å«å‰ç½®ä¾èµ–çš„å®Œæ•´ä¸Šä¸‹æ–‡"""
        
        context_parts = [
            f"# ç”Ÿæˆè®ºæ–‡{section_name}éƒ¨åˆ†",
            f"**ç”¨æˆ·éœ€æ±‚**: {user_requirement}",
            ""
        ]
        
        # æ·»åŠ å‰ç½®éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if step_config.get("previous_context_needed", False):
            context_parts.append("## å·²ç”Ÿæˆçš„å‰ç½®éƒ¨åˆ†")
            
            for dep_section in step_config["dependencies"]:
                if dep_section in section_summaries:
                    context_parts.append(f"### {dep_section}éƒ¨åˆ†æ¦‚è¿°")
                    context_parts.append(section_summaries[dep_section])
                    context_parts.append("")
        
        # æ·»åŠ åŸºç¡€RAGä¸Šä¸‹æ–‡
        context_parts.append("## å‚è€ƒèµ„æ–™")
        context_parts.append(base_context)
        
        # æ·»åŠ å†™ä½œæŒ‡å¯¼
        context_parts.extend([
            "",
            "## å†™ä½œè¦æ±‚",
            f"1. ç¡®ä¿ä¸å‰é¢å·²ç”Ÿæˆçš„éƒ¨åˆ†åœ¨é€»è¾‘ä¸Šè¿è´¯",
            f"2. é¿å…ä¸å‰é¢éƒ¨åˆ†å†…å®¹é‡å¤",
            f"3. ä¿æŒå­¦æœ¯è®ºæ–‡çš„ä¸¥è°¨æ€§å’Œä¸“ä¸šæ€§",
            f"4. å­—æ•°æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´",
            f"5. ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯å†™ä½œæ ¼å¼"
        ])
        
        return "\n".join(context_parts)
    
    def _polish_entire_paper(self, sections: Dict[str, str], 
                           user_requirement: str) -> Dict[str, str]:
        """å¯¹æ•´ç¯‡è®ºæ–‡è¿›è¡Œç»Ÿä¸€æ¶¦è‰²ï¼Œç¡®ä¿é€»è¾‘è¿è´¯æ€§"""
        
        polish_prompt = f"""
è¯·å¯¹ä»¥ä¸‹è®ºæ–‡å„éƒ¨åˆ†è¿›è¡Œæ•´ä½“æ¶¦è‰²ï¼Œç¡®ä¿é€»è¾‘è¿è´¯æ€§ï¼š

ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}

è¯·é‡ç‚¹å…³æ³¨ï¼š
1. å„éƒ¨åˆ†ä¹‹é—´çš„é€»è¾‘è¿æ¥å’Œè¿‡æ¸¡
2. æœ¯è¯­ä½¿ç”¨çš„ä¸€è‡´æ€§  
3. è¡¨è¾¾çš„æµç•…æ€§å’Œå­¦æœ¯è§„èŒƒæ€§
4. é¿å…å†…å®¹é‡å¤å’ŒçŸ›ç›¾
5. ç¡®ä¿è®ºæ–‡æ•´ä½“ç»“æ„çš„å®Œæ•´æ€§

è¯·åˆ†åˆ«è¿”å›æ¶¦è‰²åçš„å„éƒ¨åˆ†å†…å®¹ï¼Œä¿æŒä»¥ä¸‹æ ¼å¼ï¼š

## å¼•è¨€
[æ¶¦è‰²åçš„å¼•è¨€å†…å®¹]

## ç›¸å…³å·¥ä½œ
[æ¶¦è‰²åçš„ç›¸å…³å·¥ä½œå†…å®¹]

## æ–¹æ³•
[æ¶¦è‰²åçš„æ–¹æ³•å†…å®¹]

## å®éªŒè¯„ä»·
[æ¶¦è‰²åçš„å®éªŒè¯„ä»·å†…å®¹]

## æ€»ç»“
[æ¶¦è‰²åçš„æ€»ç»“å†…å®¹]

åŸå§‹å†…å®¹ï¼š
{self._format_sections_for_polish(sections)}
"""
        
        polished_content = self.llm_client.get_completion(polish_prompt)
        
        # è§£ææ¶¦è‰²åçš„å†…å®¹
        polished_sections = self._parse_polished_content(polished_content)
        
        return polished_sections if polished_sections else sections
    
    def _format_sections_for_polish(self, sections: Dict[str, str]) -> str:
        """æ ¼å¼åŒ–å„éƒ¨åˆ†å†…å®¹ç”¨äºæ¶¦è‰²"""
        formatted_content = []
        for section_name in ["å¼•è¨€", "ç›¸å…³å·¥ä½œ", "æ–¹æ³•", "å®éªŒè¯„ä»·", "æ€»ç»“"]:
            if section_name in sections:
                formatted_content.append(f"## {section_name}")
                formatted_content.append(sections[section_name])
                formatted_content.append("")
        
        return "\n".join(formatted_content)
    
    def _parse_polished_content(self, content: str) -> Dict[str, str]:
        """è§£ææ¶¦è‰²åçš„å†…å®¹ï¼Œåˆ†å‰²æˆå„ä¸ªéƒ¨åˆ†"""
        sections = {}
        current_section = None
        current_content = []
        
        lines = content.split('\n')
        
        for line in lines:
            if line.startswith('## '):
                # ä¿å­˜å‰ä¸€ä¸ªéƒ¨åˆ†
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                # å¼€å§‹æ–°éƒ¨åˆ†
                current_section = line[3:].strip()
                current_content = []
            elif current_section:
                current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªéƒ¨åˆ†
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections
    
    def generate_paper_sequentially(self, paper_section_contexts: Dict[str, str],
                                   user_requirement: str) -> Dict[str, str]:
        """
        æŒ‰é¡ºåºç”Ÿæˆè®ºæ–‡å„éƒ¨åˆ†ï¼Œä¿æŒé€»è¾‘è¿è´¯æ€§
        
        æµç¨‹ï¼š
        1. æŒ‰ä¾èµ–é¡ºåºç”Ÿæˆå„éƒ¨åˆ†
        2. æ¯ç”Ÿæˆä¸€éƒ¨åˆ†å°±æå–æ¦‚è¿°ä¾›åç»­ä½¿ç”¨
        3. æœ€åç»Ÿä¸€æ¶¦è‰²ä¿è¯æ•´ä½“è¿è´¯æ€§
        
        Args:
            paper_section_contexts: å„éƒ¨åˆ†çš„RAGä¸Šä¸‹æ–‡
            user_requirement: ç”¨æˆ·éœ€æ±‚
        
        Returns:
            ç”Ÿæˆçš„è®ºæ–‡å„éƒ¨åˆ†å†…å®¹
        """
        logging.info("å¼€å§‹é¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹")
        
        generated_sections = {}
        section_summaries = {}  # å­˜å‚¨å„éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨
        
        for step in self.generation_order:
            section_name = step["section"]
            logging.info(f"ğŸ”„ ç”Ÿæˆè®ºæ–‡{section_name}éƒ¨åˆ†...")
            
            # æ„å»ºå½“å‰éƒ¨åˆ†çš„å®Œæ•´ä¸Šä¸‹æ–‡
            full_context = self._build_full_context_for_section(
                section_name=section_name,
                base_context=paper_section_contexts[section_name],
                previous_sections=generated_sections,
                section_summaries=section_summaries,
                step_config=step,
                user_requirement=user_requirement
            )
            
            # ç”Ÿæˆå½“å‰éƒ¨åˆ†
            generated_content = self._generate_section_content(
                section_name=section_name,
                context=full_context,
                user_requirement=user_requirement
            )
            
            generated_sections[section_name] = generated_content
            logging.info(f"âœ… {section_name}éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
            
            # ç”Ÿæˆå½“å‰éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨
            section_summary = self._generate_section_summary(
                section_name, generated_content
            )
            section_summaries[section_name] = section_summary
        
        # æœ€åè¿›è¡Œå…¨æ–‡ç»Ÿä¸€æ¶¦è‰²
        logging.info("ğŸ¨ å¼€å§‹å…¨æ–‡ç»Ÿä¸€æ¶¦è‰²...")
        polished_sections = self._polish_entire_paper(
            generated_sections, user_requirement
        )
        logging.info("âœ… è®ºæ–‡ç”Ÿæˆå®Œæˆ")
        
        return polished_sections
    
    def get_generation_statistics(self, generated_sections: Dict[str, str]) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_sections": len(generated_sections),
            "total_length": sum(len(content) for content in generated_sections.values()),
            "section_lengths": {section: len(content) for section, content in generated_sections.items()},
            "average_length": 0,
            "sections_generated": list(generated_sections.keys())
        }
        
        if stats["total_sections"] > 0:
            stats["average_length"] = stats["total_length"] // stats["total_sections"]
        
        return stats 