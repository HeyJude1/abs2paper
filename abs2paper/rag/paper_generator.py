import os
import json
import logging
from typing import Dict, List, Optional, Any

from abs2paper.utils.llm_client import LLMClient

class PaperGenerator:
    """é¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆå™¨ - è§£å†³é€»è¾‘è¿è´¯æ€§é—®é¢˜"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–è®ºæ–‡ç”Ÿæˆå™¨"""
        # è®¾ç½®é¡¹ç›®æ ¹ç›®å½•å’Œé…ç½®æ–‡ä»¶è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.config_path = config_path or os.path.join(self.project_root, "config", "config.json")
        self.config = self._load_config()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient()
        
        # è·å–paper_promptè·¯å¾„
        data_paths = self.config["data_paths"]
        paper_prompt_path = data_paths["paper_prompt"]["path"].lstrip('/')
        self.paper_prompt_dir = os.path.join(self.project_root, paper_prompt_path)
        
        # ä»é…ç½®åŠ è½½ç”Ÿæˆé¡ºåºå’Œæç¤ºè¯æ˜ å°„
        paper_config = self.config["paper"]
        self.generation_order = paper_config["generation_order"]["steps"]
        self.prompt_files = paper_config["prompt_files"]
        
        # åŠ è½½æ‰€æœ‰è®ºæ–‡ç”Ÿæˆæç¤ºè¯
        self.paper_prompts = self._load_all_prompts()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_all_prompts(self) -> Dict[str, str]:
        """åŠ è½½æ‰€æœ‰è®ºæ–‡ç”Ÿæˆç›¸å…³çš„æç¤ºè¯"""
        all_prompts = {}
        
        # åŠ è½½ç« èŠ‚ç”Ÿæˆæç¤ºè¯
        for section, prompt_file in self.prompt_files["section_prompts"].items():
            prompt_path = os.path.join(self.paper_prompt_dir, prompt_file)
            prompt_content = self._load_single_prompt(prompt_path, f"{section}ç”Ÿæˆ")
            if prompt_content:
                all_prompts[f"section_{section}"] = prompt_content
        
        # åŠ è½½å·¥å…·æç¤ºè¯
        for tool_name, prompt_file in self.prompt_files["utility_prompts"].items():
            prompt_path = os.path.join(self.paper_prompt_dir, prompt_file)
            prompt_content = self._load_single_prompt(prompt_path, tool_name)
            if prompt_content:
                all_prompts[tool_name] = prompt_content
        
        logging.info(f"ğŸ“ æ€»å…±åŠ è½½äº† {len(all_prompts)} ä¸ªè®ºæ–‡ç”Ÿæˆæç¤ºè¯")
        return all_prompts
    
    def _load_single_prompt(self, prompt_path: str, prompt_name: str) -> Optional[str]:
        """åŠ è½½å•ä¸ªæç¤ºè¯æ–‡ä»¶"""
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                prompt_content = f.read().strip()
                if prompt_content:
                    logging.info(f"âœ… å·²åŠ è½½ {prompt_name} æç¤ºè¯")
                    return prompt_content
                else:
                    logging.warning(f"âš ï¸ {prompt_name} æç¤ºè¯æ–‡ä»¶ä¸ºç©º: {prompt_path}")
                    return None
        
        except FileNotFoundError:
            logging.error(f"âŒ æœªæ‰¾åˆ° {prompt_name} æç¤ºè¯æ–‡ä»¶: {prompt_path}")
            return None
        except Exception as e:
            logging.error(f"âŒ åŠ è½½ {prompt_name} æç¤ºè¯æ—¶å‡ºé”™: {e}")
            return None
    
    def _generate_section_content(self, section_name: str, 
                                context: str, 
                                user_requirement: str) -> str:
        """ç”Ÿæˆç‰¹å®šéƒ¨åˆ†çš„å†…å®¹"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æç¤ºè¯
        prompt_key = f"section_{section_name}"
        if prompt_key not in self.paper_prompts:
            error_msg = f"æœªæ‰¾åˆ° {section_name} çš„æç¤ºè¯æ¨¡æ¿ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œæç¤ºè¯æ–‡ä»¶"
            logging.error(error_msg)
            raise ValueError(error_msg)
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        base_prompt = self.paper_prompts[prompt_key]
        full_prompt = f"{base_prompt}\n\n{context}"
        
        return self.llm_client.get_completion(full_prompt)
    
    def _generate_section_summary(self, section_name: str, content: str) -> str:
        """ç”Ÿæˆéƒ¨åˆ†å†…å®¹çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨"""
        if "section_summary" not in self.paper_prompts:
            error_msg = "æœªæ‰¾åˆ°ç« èŠ‚æ¦‚è¿°ç”Ÿæˆæç¤ºè¯ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’ŒSectionSummary_promptæ–‡ä»¶"
            logging.error(error_msg)
            raise ValueError(error_msg)
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        base_prompt = self.paper_prompts["section_summary"]
        full_prompt = f"{base_prompt}\n\n{content}"
        
        return self.llm_client.get_completion(full_prompt)
    
    def _build_full_context_for_section(self, section_name: str,
                                       base_context: str,
                                       previous_sections: Dict[str, str],
                                       section_summaries: Dict[str, str],
                                       step_config: Dict,
                                       user_requirement: str) -> str:
        """ä¸ºå½“å‰éƒ¨åˆ†æ„å»ºåŒ…å«å‰ç½®ä¾èµ–çš„å®Œæ•´ä¸Šä¸‹æ–‡"""
        
        context_parts = [
            f"# ç”Ÿæˆè®ºæ–‡{section_name}éƒ¨åˆ†",
            f"**ç”¨æˆ·éœ€æ±‚**: {user_requirement}",
            ""
        ]
        
        # æ·»åŠ å‰ç½®éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if step_config.get("previous_context_needed", False):
            context_parts.append("## å·²ç”Ÿæˆçš„å‰ç½®éƒ¨åˆ†")
            
            for dep_section in step_config["dependencies"]:
                if dep_section in section_summaries:
                    context_parts.append(f"### {dep_section}éƒ¨åˆ†æ¦‚è¿°")
                    context_parts.append(section_summaries[dep_section])
                    context_parts.append("")
        
        # æ·»åŠ åŸºç¡€RAGä¸Šä¸‹æ–‡
        context_parts.append("## å‚è€ƒèµ„æ–™")
        context_parts.append(base_context)
        
        # æ·»åŠ å†™ä½œæŒ‡å¯¼
        context_parts.extend([
            "",
            "## å†™ä½œè¦æ±‚",
            f"1. ç¡®ä¿ä¸å‰é¢å·²ç”Ÿæˆçš„éƒ¨åˆ†åœ¨é€»è¾‘ä¸Šè¿è´¯",
            f"2. é¿å…ä¸å‰é¢éƒ¨åˆ†å†…å®¹é‡å¤",
            f"3. ä¿æŒå­¦æœ¯è®ºæ–‡çš„ä¸¥è°¨æ€§å’Œä¸“ä¸šæ€§",
            f"4. å­—æ•°æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´",
            f"5. ä½¿ç”¨è§„èŒƒçš„å­¦æœ¯å†™ä½œæ ¼å¼"
        ])
        
        return "\n".join(context_parts)
    
    def _polish_entire_paper(self, sections: Dict[str, str], 
                           user_requirement: str) -> Dict[str, str]:
        """å¯¹æ•´ç¯‡è®ºæ–‡è¿›è¡Œç»Ÿä¸€æ¶¦è‰²ï¼Œç¡®ä¿é€»è¾‘è¿è´¯æ€§"""
        
        if "paper_polish" not in self.paper_prompts:
            error_msg = "æœªæ‰¾åˆ°è®ºæ–‡æ¶¦è‰²æç¤ºè¯ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’ŒPaperPolish_promptæ–‡ä»¶"
            logging.error(error_msg)
            raise ValueError(error_msg)
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        base_prompt = self.paper_prompts["paper_polish"]
        formatted_sections = self._format_sections_for_polish(sections)
        full_prompt = f"{base_prompt}\n\nåŸå§‹å†…å®¹ï¼š\n{formatted_sections}"
        
        polished_content = self.llm_client.get_completion(full_prompt)
        
        # è§£ææ¶¦è‰²åçš„å†…å®¹
        polished_sections = self._parse_polished_content(polished_content)
        
        return polished_sections if polished_sections else sections
    
    def _format_sections_for_polish(self, sections: Dict[str, str]) -> str:
        """æ ¼å¼åŒ–å„éƒ¨åˆ†å†…å®¹ç”¨äºæ¶¦è‰²"""
        formatted_content = []
        for section_name in ["å¼•è¨€", "ç›¸å…³å·¥ä½œ", "æ–¹æ³•", "å®éªŒè¯„ä»·", "æ€»ç»“"]:
            if section_name in sections:
                formatted_content.append(f"## {section_name}")
                formatted_content.append(sections[section_name])
                formatted_content.append("")
        
        return "\n".join(formatted_content)
    
    def _parse_polished_content(self, content: str) -> Dict[str, str]:
        """è§£ææ¶¦è‰²åçš„å†…å®¹ï¼Œåˆ†å‰²æˆå„ä¸ªéƒ¨åˆ†"""
        sections = {}
        current_section = None
        current_content = []
        
        lines = content.split('\n')
        
        for line in lines:
            if line.startswith('## '):
                # ä¿å­˜å‰ä¸€ä¸ªéƒ¨åˆ†
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content).strip()
                
                # å¼€å§‹æ–°éƒ¨åˆ†
                current_section = line[3:].strip()
                current_content = []
            elif current_section:
                current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ä¸ªéƒ¨åˆ†
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections
    
    def generate_paper_sequentially(self, paper_section_contexts: Dict[str, str],
                                   user_requirement: str) -> Dict[str, str]:
        """
        æŒ‰é¡ºåºç”Ÿæˆè®ºæ–‡å„éƒ¨åˆ†ï¼Œä¿æŒé€»è¾‘è¿è´¯æ€§
        
        æµç¨‹ï¼š
        1. æŒ‰ä¾èµ–é¡ºåºç”Ÿæˆå„éƒ¨åˆ†
        2. æ¯ç”Ÿæˆä¸€éƒ¨åˆ†å°±æå–æ¦‚è¿°ä¾›åç»­ä½¿ç”¨
        3. æœ€åç»Ÿä¸€æ¶¦è‰²ä¿è¯æ•´ä½“è¿è´¯æ€§
        
        Args:
            paper_section_contexts: å„éƒ¨åˆ†çš„RAGä¸Šä¸‹æ–‡
            user_requirement: ç”¨æˆ·éœ€æ±‚
        
        Returns:
            ç”Ÿæˆçš„è®ºæ–‡å„éƒ¨åˆ†å†…å®¹
        """
        logging.info("å¼€å§‹é¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹")
        
        generated_sections = {}
        section_summaries = {}  # å­˜å‚¨å„éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨
        
        for step in self.generation_order:
            section_name = step["section"]
            logging.info(f"ğŸ”„ ç”Ÿæˆè®ºæ–‡{section_name}éƒ¨åˆ†...")
            
            # æ„å»ºå½“å‰éƒ¨åˆ†çš„å®Œæ•´ä¸Šä¸‹æ–‡
            full_context = self._build_full_context_for_section(
                section_name=section_name,
                base_context=paper_section_contexts[section_name],
                previous_sections=generated_sections,
                section_summaries=section_summaries,
                step_config=step,
                user_requirement=user_requirement
            )
            
            # ç”Ÿæˆå½“å‰éƒ¨åˆ†
            generated_content = self._generate_section_content(
                section_name=section_name,
                context=full_context,
                user_requirement=user_requirement
            )
            
            generated_sections[section_name] = generated_content
            logging.info(f"âœ… {section_name}éƒ¨åˆ†ç”Ÿæˆå®Œæˆ")
            
            # ç”Ÿæˆå½“å‰éƒ¨åˆ†çš„æ¦‚è¿°ï¼Œä¾›åç»­éƒ¨åˆ†ä½¿ç”¨
            section_summary = self._generate_section_summary(
                section_name, generated_content
            )
            section_summaries[section_name] = section_summary
        
        # æœ€åè¿›è¡Œå…¨æ–‡ç»Ÿä¸€æ¶¦è‰²
        logging.info("ğŸ¨ å¼€å§‹å…¨æ–‡ç»Ÿä¸€æ¶¦è‰²...")
        polished_sections = self._polish_entire_paper(
            generated_sections, user_requirement
        )
        logging.info("âœ… è®ºæ–‡ç”Ÿæˆå®Œæˆ")
        
        return polished_sections
    
    def get_generation_statistics(self, generated_sections: Dict[str, str]) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_sections": len(generated_sections),
            "total_length": sum(len(content) for content in generated_sections.values()),
            "section_lengths": {section: len(content) for section, content in generated_sections.items()},
            "average_length": 0,
            "sections_generated": list(generated_sections.keys())
        }
        
        if stats["total_sections"] > 0:
            stats["average_length"] = stats["total_length"] // stats["total_sections"]
        
        return stats 