import os
import json
import logging
from typing import Dict, List, Optional, Tuple
import re
import copy
from pymilvus import FieldSchema, DataType

from abs2paper.utils.topic_manager import TopicManager
from abs2paper.utils.llm_client import LLMClient
from abs2paper.utils.db_client import MilvusClient

class PaperIngestor:
    """
    è®ºæ–‡æ•°æ®å…¥åº“å·¥å…·ï¼šå°†è®ºæ–‡å„éƒ¨åˆ†æ–‡æœ¬åŠä¸»é¢˜æ ‡ç­¾ç”Ÿæˆembeddingåå†™å…¥Milvus
    """
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–PaperIngestorï¼ŒåŠ è½½é…ç½®ã€åˆå§‹åŒ–ä¸»é¢˜ç®¡ç†å™¨ã€LLMClientå’ŒMilvusClient
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºé¡¹ç›®config/config.json
        """
        # è®¾ç½®é¡¹ç›®æ ¹ç›®å½•å’Œé…ç½®æ–‡ä»¶è·¯å¾„
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.config_path = config_path or os.path.join(self.project_root, "config", "config.json")
        self.config = self._load_config()
        
        # åˆå§‹åŒ–å·¥å…·ç±»
        self.topic_manager = TopicManager()
        self.llm_client = LLMClient()
        
        # è¯»å–æ•°æ®åº“é…ç½®
        vector_db_config = self.config["vector_db"]          # æ•°æ®åº“è¿æ¥é…ç½®
        self.embedding_dim = vector_db_config["embedding_dim"]    # åµŒå…¥å‘é‡ç»´åº¦
        
        # è¯»å–è®ºæ–‡é…ç½®
        paper_config = self.config["paper"]
        self.section_names = paper_config["sections"]             # è®ºæ–‡éƒ¨åˆ†åç§°åˆ—è¡¨
        self.section_name_en = paper_config["section_mapping_en"] # éƒ¨åˆ†è‹±æ–‡åæ˜ å°„
        self.section_mapping = paper_config["chapter_mapping"]    # ç« èŠ‚æ˜ å°„è¡¨
        self.collection_fields_config = paper_config["collection_fields"] # é›†åˆå­—æ®µé…ç½®
        self.index_params = paper_config["index_params"]          # ç´¢å¼•å‚æ•°
        
        # åˆå§‹åŒ–Milvuså®¢æˆ·ç«¯
        db_config = {
            "host": vector_db_config["host"],         # æœåŠ¡å™¨åœ°å€
            "port": vector_db_config["port"],         # æœåŠ¡ç«¯å£
            "alias": vector_db_config["alias"],       # è¿æ¥åˆ«å
            "db_name": vector_db_config["db_name"]    # æ•°æ®åº“åç§°
        }
        self.db_client = MilvusClient(db_config)
        
        # è®¾ç½®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.roman_pattern = re.compile(r'^([IVX]+)\.\s+(.+)', re.IGNORECASE)     # ç½—é©¬æ•°å­—ç« èŠ‚å
        self.number_pattern = re.compile(r'^(\d+)(\.\d+)?\s+(.+)', re.IGNORECASE) # æ•°å­—ç« èŠ‚å
        
        # åˆ›å»ºé›†åˆ
        self._create_collections()

    def _load_config(self):
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        Returns:
            é…ç½®å­—å…¸
        """
        with open(self.config_path, "r", encoding="utf-8") as f:
            return json.load(f)
            
    def _get_collection_name(self, section: str) -> str:
        """
        è·å–æŒ‡å®šè®ºæ–‡éƒ¨åˆ†çš„Milvusé›†åˆåç§°
        Args:
            section: è®ºæ–‡éƒ¨åˆ†ä¸­æ–‡å
        Returns:
            é›†åˆåç§°å­—ç¬¦ä¸²
        """
        section_en = self.section_name_en[section]  # è·å–è‹±æ–‡åç§°
        return f"paper_{section_en}"
    
    def _create_field_schema(self) -> List[FieldSchema]:
        """
        æ ¹æ®é…ç½®åˆ›å»ºå­—æ®µæ¨¡å¼åˆ—è¡¨
        Returns:
            å­—æ®µæ¨¡å¼åˆ—è¡¨
        """
        fields = []
        
        # è§£æé…ç½®å¹¶åˆ›å»ºå­—æ®µæ¨¡å¼
        for field_name, field_config in self.collection_fields_config.items():
            # è·å–å­—æ®µç±»å‹
            field_type = field_config["type"]
            data_type = getattr(DataType, field_type)
            
            # å¤„ç†å‚æ•°
            kwargs = {}
            for key, value in field_config.items():
                if key == "type":
                    continue
                    
                # å¤„ç†å˜é‡æ›¿æ¢
                if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                    var_name = value[2:-1]
                    if var_name == "embedding_dim":
                        value = self.embedding_dim
                
                # ç‰¹æ®Šå¤„ç†æ•°ç»„å…ƒç´ ç±»å‹
                if key == "element_type" and isinstance(value, str):
                    value = getattr(DataType, value)
                
                # é‡å‘½åelement_max_lengthä¸ºmax_length
                if key == "element_max_length":
                    kwargs["max_length"] = value
                else:
                    kwargs[key] = value
            
            # åˆ›å»ºå­—æ®µæ¨¡å¼
            field = FieldSchema(name=field_name, dtype=data_type, **kwargs)
            fields.append(field)
        
        return fields
    
    def _create_collections(self):
        """å‡†å¤‡é›†åˆé…ç½®å¹¶è°ƒç”¨db_clientåˆ›å»ºé›†åˆ"""
        # å‡†å¤‡é›†åˆé…ç½®åˆ—è¡¨
        collection_configs = []
        fields = self._create_field_schema()
        
        # ä¸ºæ¯ä¸ªè®ºæ–‡éƒ¨åˆ†åˆ›å»ºé›†åˆé…ç½®
        for section in self.section_names:
            collection_name = self._get_collection_name(section)
            collection_configs.append({
                "name": collection_name,                    # é›†åˆåç§°
                "fields": fields,                           # å­—æ®µå®šä¹‰
                "description": f"Collection for paper {section}",  # é›†åˆæè¿°
                "index_field": "embedding",                 # ç´¢å¼•å­—æ®µ
                "index_params": self.index_params           # ç´¢å¼•å‚æ•°
            })
        
        # è°ƒç”¨db_clientåˆ›å»ºé›†åˆ
        self.db_client.create_collections(collection_configs)

    def split_text(self, text: str, chunk_size: int = 300, overlap_size: int = 50) -> List[str]:
        """
        æŒ‰æŒ‡å®šchunk_sizeå’Œoverlap_sizeå°†æ–‡æœ¬åˆ†å‰²æˆå—ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯æ€§
        
        å‚æ•°:
            text: å¾…åˆ‡åˆ†çš„æ–‡æœ¬
            chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap_size: å—ä¹‹é—´çš„é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        è¿”å›:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        # ç¡®ä¿nltkçš„punktåˆ†è¯å™¨å·²ä¸‹è½½
        try:
            from nltk.tokenize import sent_tokenize
            try:
                sentences = sent_tokenize(text)
            except:
                import nltk
                nltk.download('punkt')
                sentences = sent_tokenize(text)
        except ImportError:
            # å¦‚æœæ²¡æœ‰nltkï¼Œä½¿ç”¨ç®€å•çš„è§„åˆ™åˆ†å‰²
            sentences = re.split(r'(?<=[.!?])\s+', text)
        
        # å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]
        chunks = []
        i = 0
        
        while i < len(sentences):
            chunk = sentences[i]
            overlap = ''
            prev_len = 0
            prev = i - 1
            # å‘å‰è®¡ç®—é‡å éƒ¨åˆ†
            while prev >= 0 and len(sentences[prev]) + len(overlap) <= overlap_size:
                overlap = sentences[prev] + ' ' + overlap
                prev -= 1
            chunk = overlap + chunk if overlap else chunk
            
            next_idx = i + 1
            # å‘åè®¡ç®—å½“å‰chunk
            while next_idx < len(sentences) and len(chunk) + len(sentences[next_idx]) <= chunk_size:
                chunk = chunk + ' ' + sentences[next_idx]
                next_idx += 1
            
            chunks.append(chunk)
            i = next_idx
        
        return chunks

    def _extract_topics_from_file(self, paper_id: str, label_dir: str) -> List[str]:
        """
        ä»æ ‡ç­¾æ–‡ä»¶ä¸­æå–ä¸»é¢˜å…³é”®è¯ï¼Œæ”¯æŒåµŒå¥—ç›®å½•ç»“æ„æŸ¥æ‰¾
        Args:
            paper_id: è®ºæ–‡ID
            label_dir: æ ‡ç­¾ç›®å½•
        Returns:
            æå–çš„ä¸»é¢˜å…³é”®è¯åˆ—è¡¨
        """
        topics = []
        
        # å°è¯•å¤šç§æ–‡ä»¶åæ ¼å¼
        possible_names = [
            paper_id,  # å®Œæ•´çš„paper_id
            os.path.basename(paper_id),  # åªå–æ–‡ä»¶åéƒ¨åˆ†
            paper_id.replace('/', '_'),  # æ›¿æ¢è·¯å¾„åˆ†éš”ç¬¦
        ]
        
        logging.debug(f"  ğŸ” æ­£åœ¨æœç´¢è®ºæ–‡ {paper_id} çš„æ ‡ç­¾æ–‡ä»¶ï¼Œå°è¯•çš„åç§°: {possible_names}")
        
        # åœ¨æ•´ä¸ªç›®å½•æ ‘ä¸­é€’å½’æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        for root, dirs, files in os.walk(label_dir):
            for filename in files:
                if filename.endswith('.txt'):
                    # å»æ‰æ‰©å±•å
                    base_filename = filename[:-4]
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä¸€å¯èƒ½çš„åç§°
                    for possible_name in possible_names:
                        if base_filename == possible_name:
                            label_file = os.path.join(root, filename)
                            topics = self._read_topics_from_file(label_file)
                            if topics:  # æ‰¾åˆ°æœ‰æ•ˆä¸»é¢˜å°±è¿”å›
                                logging.info(f"  ğŸ“ æ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {os.path.relpath(label_file, label_dir)}")
                                return topics
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„æ–‡ä»¶ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
        logging.warning(f"  âš ï¸  æœªæ‰¾åˆ°è®ºæ–‡ {paper_id} çš„æ ‡ç­¾æ–‡ä»¶")
        return topics
    
    def _read_topics_from_file(self, label_file: str) -> List[str]:
        """
        ä»æ ‡ç­¾æ–‡ä»¶ä¸­è¯»å–ä¸»é¢˜å…³é”®è¯
        Args:
            label_file: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        Returns:
            ä¸»é¢˜å…³é”®è¯åˆ—è¡¨
        """
        topics = []
        try:
            with open(label_file, "r", encoding="utf-8") as lf:
                content = lf.read()
                # æŸ¥æ‰¾ä¸»é¢˜å…³é”®è¯è¡Œ
                for line in content.split('\n'):
                    if "æ•…è¯¥è®ºæ–‡çš„ä¸»é¢˜å…³é”®è¯æ€»ç»“ä¸º[" in line:
                        # æå–æ‹¬å·å†…çš„å†…å®¹
                        start_idx = line.find('[')
                        end_idx = line.find(']')
                        if start_idx != -1 and end_idx != -1:
                            topics_str = line[start_idx+1:end_idx]
                            topics = [x.strip() for x in topics_str.split(",") if x.strip()]
                        break
        except Exception as e:
            logging.error(f"è¯»å–æ ‡ç­¾æ–‡ä»¶ {label_file} æ—¶å‡ºé”™: {e}")
        
        return topics

    # ===== æ—§çš„äººå·¥è§„åˆ™ç« èŠ‚åŒ¹é…æ–¹å¼ï¼ˆå·²å¼ƒç”¨ï¼Œæ”¹ç”¨LLMæ™ºèƒ½åŒ¹é…ç»“æœï¼‰=====
    # def _map_section_name(self, section_name: str) -> str:
    #     """
    #     å°†ç« èŠ‚æ–‡ä»¶åæ˜ å°„åˆ°æ ‡å‡†è®ºæ–‡éƒ¨åˆ†ï¼ˆæ—§çš„äººå·¥è§„åˆ™åŒ¹é…æ–¹å¼ï¼‰
    #     Args:
    #         section_name: ç« èŠ‚æ–‡ä»¶å
    #     Returns:
    #         æ˜ å°„åçš„æ ‡å‡†éƒ¨åˆ†åç§°
    #     """
    #     # æ ‡å‡†åŒ–æ–‡ä»¶å
    #     section_title = section_name.lower()
    #     
    #     # å°è¯•åŒ¹é…ç½—é©¬æ•°å­—æ ¼å¼
    #     roman_match = self.roman_pattern.match(section_name)
    #     if roman_match:
    #         section_title = roman_match.group(2).lower()
    #     
    #     # å°è¯•åŒ¹é…æ•°å­—æ ¼å¼
    #     number_match = self.number_pattern.match(section_name)
    #     if number_match:
    #         section_title = number_match.group(3).lower()
    #     
    #     # å°è¯•åŒ¹é…æ ‡å‡†éƒ¨åˆ†
    #     for keyword, target_section in self.section_mapping.items():
    #         # æ¯”è¾ƒå®½æ³›çš„åŒ¹é…
    #         if keyword in section_title or section_title in keyword:
    #             return target_section
    #     
    #     # é»˜è®¤è¿”å›æ–¹æ³•éƒ¨åˆ†ï¼Œå› ä¸ºæœ‰äº›è®ºæ–‡å¯èƒ½ä½¿ç”¨é¡¹ç›®åç§°ä½œä¸ºæ–¹æ³•éƒ¨åˆ†
    #     return "æ–¹æ³•"

    def _load_section_mapping(self, paper_id: str) -> Dict[str, str]:
        """
        ä»section_matchç›®å½•åŠ è½½LLMæ™ºèƒ½åŒ¹é…çš„ç« èŠ‚æ˜ å°„ç»“æœ
        Args:
            paper_id: è®ºæ–‡IDï¼ˆå¦‚ "ICS/2023/3577193.3593731"ï¼‰
        Returns:
            ç« èŠ‚æ˜ å°„å­—å…¸ {ç« èŠ‚æ ‡é¢˜: æ ‡å‡†ç±»åˆ«}
        """
        # æ„å»ºæ˜ å°„æ–‡ä»¶è·¯å¾„
        section_match_dir = os.path.join(self.project_root, "abs2paper", "processing", "data", "section_match")
        mapping_file = os.path.join(section_match_dir, paper_id, "section_mapping.json")
        
        if not os.path.exists(mapping_file):
            logging.warning(f"âš ï¸  æœªæ‰¾åˆ°è®ºæ–‡ {paper_id} çš„ç« èŠ‚æ˜ å°„æ–‡ä»¶: {mapping_file}")
            logging.warning(f"âš ï¸  è¯·å…ˆè¿è¡Œç« èŠ‚åŒ¹é…: python -m scripts.conclude_papers --only-section-match")
            return {}
        
        try:
            with open(mapping_file, 'r', encoding='utf-8') as f:
                mapping_data = json.load(f)
                section_mapping = mapping_data.get("section_mapping", {})
                logging.debug(f"âœ… å·²åŠ è½½è®ºæ–‡ {paper_id} çš„ç« èŠ‚æ˜ å°„: {len(section_mapping)} ä¸ªç« èŠ‚")
                return section_mapping
        except Exception as e:
            logging.error(f"âŒ åŠ è½½è®ºæ–‡ {paper_id} ç« èŠ‚æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    def _map_section_name_with_llm_result(self, section_name: str, section_mapping: Dict[str, str]) -> str:
        """
        ä½¿ç”¨LLMæ™ºèƒ½åŒ¹é…ç»“æœå°†ç« èŠ‚æ–‡ä»¶åæ˜ å°„åˆ°æ ‡å‡†è®ºæ–‡éƒ¨åˆ†
        Args:
            section_name: ç« èŠ‚æ–‡ä»¶å
            section_mapping: ä»section_matchåŠ è½½çš„ç« èŠ‚æ˜ å°„å­—å…¸
        Returns:
            æ˜ å°„åçš„æ ‡å‡†éƒ¨åˆ†åç§°
        """
        # ç›´æ¥ä»æ˜ å°„å­—å…¸ä¸­æŸ¥æ‰¾
        if section_name in section_mapping:
            return section_mapping[section_name]
        
        # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        for original_title, standard_section in section_mapping.items():
            # å°è¯•åŒ…å«å…³ç³»åŒ¹é…
            if section_name in original_title or original_title in section_name:
                logging.debug(f"ğŸ“‹ æ¨¡ç³ŠåŒ¹é…: '{section_name}' -> '{original_title}' -> '{standard_section}'")
                return standard_section
        
        # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…åˆ°ï¼Œé»˜è®¤è¿”å›"æ–¹æ³•"éƒ¨åˆ†
        logging.warning(f"âš ï¸  ç« èŠ‚ '{section_name}' æœªåœ¨LLMæ˜ å°„ç»“æœä¸­æ‰¾åˆ°ï¼Œé»˜è®¤å½’ç±»ä¸º'æ–¹æ³•'")
        return "æ–¹æ³•"

    def _process_paper_sections(self, paper_path: str, paper_id: str) -> Dict[str, str]:
        """
        å¤„ç†è®ºæ–‡ç›®å½•ä¸­çš„æ‰€æœ‰ç« èŠ‚æ–‡ä»¶ï¼Œä½¿ç”¨LLMæ™ºèƒ½åŒ¹é…ç»“æœ
        Args:
            paper_path: è®ºæ–‡ç›®å½•è·¯å¾„
            paper_id: è®ºæ–‡IDï¼ˆç”¨äºåŠ è½½ç« èŠ‚æ˜ å°„ï¼‰
        Returns:
            æŒ‰æ ‡å‡†éƒ¨åˆ†ç»„ç»‡çš„æ–‡æœ¬å­—å…¸
        """
        # åŠ è½½è¯¥è®ºæ–‡çš„LLMç« èŠ‚æ˜ å°„ç»“æœ
        section_mapping = self._load_section_mapping(paper_id)
        
        if not section_mapping:
            logging.error(f"âŒ æ— æ³•åŠ è½½è®ºæ–‡ {paper_id} çš„ç« èŠ‚æ˜ å°„ï¼Œè·³è¿‡å¤„ç†")
            return {}
        
        # åˆå§‹åŒ–å„éƒ¨åˆ†æ–‡æœ¬
        section_texts = {section: "" for section in self.section_names}
        
        # éå†è®ºæ–‡ç›®å½•ä¸‹çš„æ‰€æœ‰txtæ–‡ä»¶ï¼ˆç« èŠ‚ï¼‰
        for section_file in os.listdir(paper_path):
            if not section_file.endswith(".txt"):
                continue
            
            section_file_path = os.path.join(paper_path, section_file)
            section_name = section_file[:-4]  # å»æ‰.txtåç¼€
            
            # è¯»å–ç« èŠ‚å†…å®¹
            with open(section_file_path, "r", encoding="utf-8") as f:
                section_content = f.read()
            
            # ä½¿ç”¨LLMæ™ºèƒ½åŒ¹é…ç»“æœæ˜ å°„ç« èŠ‚ååˆ°æ ‡å‡†éƒ¨åˆ†
            mapped_section = self._map_section_name_with_llm_result(section_name, section_mapping)
            
            # å°†å†…å®¹æ·»åŠ åˆ°å¯¹åº”éƒ¨åˆ†
            section_texts[mapped_section] += section_content + "\n\n"
            
            logging.debug(f"ğŸ“„ ç« èŠ‚æ˜ å°„: '{section_name}' -> '{mapped_section}'")
        
        return section_texts

    def _process_section_chunks(self, paper_id: str, section: str, text: str, topic_names: List[str]) -> bool:
        """
        å¤„ç†å•ä¸ªè®ºæ–‡éƒ¨åˆ†ï¼Œåˆ†å—å¹¶å…¥åº“
        Args:
            paper_id: è®ºæ–‡ID
            section: éƒ¨åˆ†åç§°
            text: éƒ¨åˆ†å†…å®¹
            topic_names: ä¸»é¢˜åç§°åˆ—è¡¨
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # å°†æ–‡æœ¬åˆ‡åˆ†æˆå°å—
            chunks = self.split_text(text, chunk_size=500, overlap_size=100)
            logging.info(f"      ğŸ“ å°† {section} éƒ¨åˆ†åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå— (æ€»å­—ç¬¦æ•°: {len(text)})")
            
            # ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
            logging.info(f"      ğŸ§  æ­£åœ¨ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—çš„embedding...")
            embeddings = self.llm_client.get_embedding(chunks)
            logging.info(f"      âœ¨ embeddingç”Ÿæˆå®Œæˆ")
            
            # æ„å»ºæ’å…¥æ•°æ®ï¼ˆæ¯ä¸ªchunkä¸€æ¡è®°å½•ï¼‰
            data = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                data.append({
                    "paper_id": f"{paper_id}_{i}",  # æ·»åŠ å—ç¼–å·
                    "section": section,
                    "text": chunk[:8000],  # é™åˆ¶æ–‡æœ¬é•¿åº¦
                    "topics": topic_names,
                    "embedding": embedding
                })
            
            # ä½¿ç”¨MilvusClientæ’å…¥æ•°æ®
            collection_name = self._get_collection_name(section)
            logging.info(f"      ğŸ’¾ æ­£åœ¨æ’å…¥æ•°æ®åˆ°é›†åˆ: {collection_name}")
            success = self.db_client.insert_data(collection_name, data)
            
            if success:
                logging.info(f"      âœ… æˆåŠŸå…¥åº“ {section} éƒ¨åˆ†: {len(chunks)} ä¸ªå—")
            else:
                logging.error(f"      âŒ å…¥åº“å¤±è´¥ {section} éƒ¨åˆ†")
                
            return success
            
        except Exception as e:
            logging.error(f"      âŒ å¤„ç† {section} éƒ¨åˆ†æ—¶å‡ºé”™: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
            return False

    def _process_single_paper(self, paper_id: str, paper_path: str, label_dir: str) -> bool:
        """
        å¤„ç†å•ä¸ªè®ºæ–‡çš„æ‰€æœ‰éƒ¨åˆ†
        Args:
            paper_id: è®ºæ–‡ID
            paper_path: è®ºæ–‡è·¯å¾„
            label_dir: æ ‡ç­¾ç›®å½•
        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†
        """
        logging.info(f"  ğŸ“„ å¼€å§‹å¤„ç†è®ºæ–‡: {paper_id}")
        
        # æå–ä¸»é¢˜æ ‡ç­¾
        topics = self._extract_topics_from_file(paper_id, label_dir)
        if not topics:
            logging.warning(f"  âš ï¸  è®ºæ–‡ {paper_id} æ— æ³•æå–ä¸»é¢˜å…³é”®è¯ï¼Œè·³è¿‡å¤„ç†")
            return False
        
        logging.info(f"  ğŸ·ï¸  æå–åˆ° {len(topics)} ä¸ªä¸»é¢˜æ ‡ç­¾: {topics}")
        
        # ä¸»é¢˜åæ˜ å°„
        topic_names = []
        for topic_id in topics:
            topic_info = self.topic_manager.get_topic_info(topic_id)
            if topic_info:
                topic_names.append(f"{topic_info['name_zh']} ({topic_info['name_en']})")
        
        if topic_names:
            logging.info(f"  ğŸ“‹ ä¸»é¢˜åç§°: {topic_names}")
        else:
            logging.warning(f"  âš ï¸  è®ºæ–‡ {paper_id} æ— æ³•è·å–æœ‰æ•ˆçš„ä¸»é¢˜åç§°")
        
        # æŒ‰ç« èŠ‚ç»„ç»‡çš„æ–‡æœ¬å†…å®¹
        logging.info(f"  ğŸ“– æ­£åœ¨å¤„ç†è®ºæ–‡ç« èŠ‚...")
        section_texts = self._process_paper_sections(paper_path, paper_id)
        
        # ç»Ÿè®¡æœ‰å†…å®¹çš„ç« èŠ‚
        sections_with_content = [(section, len(text.strip())) for section, text in section_texts.items() if text.strip()]
        if sections_with_content:
            logging.info(f"  ğŸ“‘ æ‰¾åˆ° {len(sections_with_content)} ä¸ªæœ‰å†…å®¹çš„ç« èŠ‚:")
            for section, length in sections_with_content:
                logging.info(f"    - {section}: {length} å­—ç¬¦")
        else:
            logging.warning(f"  âš ï¸  è®ºæ–‡ {paper_id} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆå†…å®¹")
            return False
        
        # å¯¹äºæ¯ä¸ªéƒ¨åˆ†ï¼Œå¦‚æœæœ‰å†…å®¹ï¼Œåˆ™åˆ›å»ºembeddingå¹¶å†™å…¥å‘é‡æ•°æ®åº“
        has_content = False
        success_sections = 0
        
        for section, text in section_texts.items():
            if not text.strip():
                continue
            
            has_content = True
            logging.info(f"    ğŸ”„ å¤„ç†ç« èŠ‚: {section}")
            
            try:
                if self._process_section_chunks(paper_id, section, text, topic_names):
                    success_sections += 1
                    logging.info(f"    âœ… ç« èŠ‚ {section} å¤„ç†æˆåŠŸ")
                else:
                    logging.warning(f"    âŒ ç« èŠ‚ {section} å¤„ç†å¤±è´¥")
            except Exception as e:
                logging.error(f"    âŒ ç« èŠ‚ {section} å¤„ç†å¼‚å¸¸: {e}")
        
        if has_content:
            logging.info(f"  ğŸ¯ è®ºæ–‡ {paper_id} å®Œæˆï¼ŒæˆåŠŸå¤„ç† {success_sections}/{len(sections_with_content)} ä¸ªç« èŠ‚")
        
        return has_content

    def _find_paper_directories(self, root_dir: str) -> List[Tuple[str, str]]:
        """
        é€’å½’æŸ¥æ‰¾åŒ…å«txtæ–‡ä»¶çš„è®ºæ–‡ç›®å½•
        Args:
            root_dir: èµ·å§‹ç›®å½•
        Returns:
            åŒ…å«(è®ºæ–‡ç›¸å¯¹è·¯å¾„, è®ºæ–‡ç›®å½•ç»å¯¹è·¯å¾„)çš„å…ƒç»„åˆ—è¡¨
        """
        paper_dirs = []
        
        def find_papers(dir_path):
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¶å­ç›®å½•(åŒ…å«txtæ–‡ä»¶)
            try:
                has_txt = any(item.endswith('.txt') and os.path.isfile(os.path.join(dir_path, item)) 
                          for item in os.listdir(dir_path))
            except:
                return
            
            # å¦‚æœåŒ…å«txtæ–‡ä»¶ï¼Œè®¤ä¸ºæ˜¯è®ºæ–‡ç›®å½•
            if has_txt:
                # è®¡ç®—ç›¸å¯¹äºcomponent_dirçš„ç›¸å¯¹è·¯å¾„ä½œä¸ºpaper_id
                paper_id = os.path.relpath(dir_path, root_dir)
                paper_dirs.append((paper_id, dir_path))
                logging.debug(f"æ‰¾åˆ°è®ºæ–‡ç›®å½•: {paper_id} -> {dir_path}")
                return
            
            # ç»§ç»­é€’å½’å­ç›®å½•
            try:
                for item in os.listdir(dir_path):
                    item_path = os.path.join(dir_path, item)
                    if os.path.isdir(item_path):
                        find_papers(item_path)
            except:
                pass
        
        find_papers(root_dir)
        return paper_dirs

    def ingest(self, component_dir: Optional[str] = None, label_dir: Optional[str] = None):
        """
        æ‰¹é‡è¯»å–è®ºæ–‡åˆ†æ®µå’Œä¸»é¢˜æ ‡ç­¾ï¼Œç”Ÿæˆembeddingå¹¶å†™å…¥Milvusã€‚
        Args:
            component_dir: è®ºæ–‡åˆ†æ®µç›®å½•ï¼Œå¿…é¡»é€šè¿‡å‚æ•°æŒ‡å®š
            label_dir: ä¸»é¢˜æ ‡ç­¾ç›®å½•ï¼Œå¿…é¡»é€šè¿‡å‚æ•°æŒ‡å®š
        """
        # ç¡®ä¿ç›®å½•è·¯å¾„å·²æä¾›
        if not component_dir or not label_dir:
            logging.error("å¿…é¡»æä¾›ç»„ä»¶ç›®å½•å’Œæ ‡ç­¾ç›®å½•è·¯å¾„")
            return
        
        logging.info(f"å¼€å§‹å¤„ç†è®ºæ–‡ç›®å½•: {component_dir}")
        logging.info(f"æ ‡ç­¾ç›®å½•: {label_dir}")
        
        # ä¿å­˜å¤„ç†çš„è®ºæ–‡è®¡æ•°
        processed_count = 0
        failed_count = 0
        
        try:
            # ä½¿ç”¨é€’å½’æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡ç›®å½•
            logging.info("æ­£åœ¨æ‰«æè®ºæ–‡ç›®å½•...")
            paper_directories = self._find_paper_directories(component_dir)
            logging.info(f"æ‰¾åˆ° {len(paper_directories)} ä¸ªè®ºæ–‡ç›®å½•")
            
            if not paper_directories:
                logging.warning("æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡ç›®å½•ï¼Œè¯·æ£€æŸ¥ç»„ä»¶ç›®å½•è·¯å¾„")
                return
            
            # å¤„ç†æ¯ä¸ªè®ºæ–‡ç›®å½•
            for i, (paper_id, paper_path) in enumerate(paper_directories, 1):
                logging.info(f"å¤„ç†è¿›åº¦: {i}/{len(paper_directories)} - è®ºæ–‡ID: {paper_id}")
                
                try:
                    if self._process_single_paper(paper_id, paper_path, label_dir):
                        processed_count += 1
                        logging.info(f"âœ… è®ºæ–‡ {paper_id} å¤„ç†æˆåŠŸ")
                    else:
                        failed_count += 1
                        logging.warning(f"âŒ è®ºæ–‡ {paper_id} å¤„ç†å¤±è´¥")
                except Exception as e:
                    failed_count += 1
                    logging.error(f"âŒ è®ºæ–‡ {paper_id} å¤„ç†å¼‚å¸¸: {e}")
                
                # æ¯å¤„ç†10ç¯‡è®ºæ–‡è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
                if i % 10 == 0:
                    logging.info(f"å·²å¤„ç† {i} ç¯‡è®ºæ–‡ï¼ŒæˆåŠŸ: {processed_count}ï¼Œå¤±è´¥: {failed_count}")
            
            # æœ€ç»ˆç»Ÿè®¡
            logging.info("=" * 60)
            logging.info(f"æ•°æ®å…¥åº“å®Œæˆ!")
            logging.info(f"æ€»è®¡å¤„ç†: {len(paper_directories)} ç¯‡è®ºæ–‡")
            logging.info(f"æˆåŠŸå…¥åº“: {processed_count} ç¯‡")
            logging.info(f"å¤„ç†å¤±è´¥: {failed_count} ç¯‡")
            logging.info(f"æˆåŠŸç‡: {processed_count/len(paper_directories)*100:.1f}%")
            logging.info("=" * 60)
            
        except Exception as e:
            logging.error(f"å¤„ç†è®ºæ–‡æ•°æ®æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            logging.error(traceback.format_exc())
    
    def search_papers(self, query_vector: List[float], section: Optional[str] = None,
                     top_n: int = 5, filter_expr: Optional[str] = None):
        """
        æœç´¢ç›¸ä¼¼è®ºæ–‡æ®µè½
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            section: æŒ‡å®šæœç´¢çš„è®ºæ–‡éƒ¨åˆ†ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ‰€æœ‰éƒ¨åˆ†
            top_n: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            filter_expr: è¿‡æ»¤è¡¨è¾¾å¼
        Returns:
            åŒ¹é…ç»“æœåˆ—è¡¨
        """
        # æœç´¢å‚æ•°
        params = {"metric_type": "L2", "params": {"nprobe": 10}}
        output_fields = ["text", "paper_id", "section", "topics"]
        
        if section:
            # æœç´¢æŒ‡å®šéƒ¨åˆ†
            collection_name = self._get_collection_name(section)
            return self.db_client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                expr=filter_expr,
                output_fields=output_fields,
                top_n=top_n,
                params=params
            )
        else:
            # æœç´¢æ‰€æœ‰éƒ¨åˆ†
            collection_names = [self._get_collection_name(sec) for sec in self.section_names]
            return self.db_client.search_multiple_collections(
                collection_names=collection_names,
                query_vector=query_vector,
                expr=filter_expr,
                output_fields=output_fields,
                top_n=top_n,
                params=params
            ) 