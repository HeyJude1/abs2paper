{
  "selected_source_texts": {
    "3656019.3676895": {
      "ÊñπÊ≥ï": [
        "3 MIREncoder\nMost source-code based performance optimization tasks in HPC usually involve compilable languages such as C, C++, CUDA, and so on. A large number of these languages can be compiled and optimized using the LLVM infrastructure. LLVM IRs are a portable, high-level assembly language that can be optimized with a variety of transformations over multiple passes. It is fairly simple to extract IRs from source code such as C, C++.",
        "It is fairly simple to extract IRs from source code such as C, C++. IRs generated from source code are usually devoid of most stylistic choices and redundant code. This is why we choose to work with IRs for performance optimizations. Figure shows a high-level overview of our approach. For the first modality, we first tokenize the input IRs into meaningful \"tokens\" before they are mapped to an embedded dimension.",
        "Our approach then learns the embedding of the IR instructions after splitting them into sub-words. For the second modality, the IRs are first converted to dependence graphs that include in them data flow, control flow, and call flow information that represents the semantic information in the source code. These two modalities are then passed into the modeling pipeline either for pre-training or inference. The following paragraphs outline our pipeline.",
        "The following paragraphs outline our pipeline. 3.1 Tokenization\nSimply put, tokenization is the process of breaking down a piece of text into smaller units called tokens, and assigning a numerical value to each token. A deep learning (DL) model does not understand text or images in its raw form. It needs to be represented as numbers for the model to make sense from it. This is why tokenization is extremely important for such works.",
        "This is why tokenization is extremely important for such works. In this paper, our tokenization process follows the same approach taken while designing and training the BERT model. However, the pre-trained BERT tokenizer readily available online is trained on natural language (NL). However, source code (IRs in our paper) is more structured than NL, and quite possibly has fewer \"words\". Thus, we had to train our tokenizer from scratch.",
        "Thus, we had to train our tokenizer from scratch. We initially collect a large set of IRs by compiling programs in existing datasets into their LLVM IRs. For training the tokenizer, we have used C, C++, and CUDA code from CodeNet , HPCorpus , and LS-CAT . We first define a set of special tokens to handle unknown inputs, and a token that will be used during Masked Language Modeling. 10, 000 unique programs are randomly selected and compiled into LLVM IRs.",
        "10, 000 unique programs are randomly selected and compiled into LLVM IRs. These are then passed through a WordPiece tokenizer, as done in BERT, and trained to generate a learned vocabulary. BERT uses a sequence length of 512. However, for the sake of simplicity and faster training, we limit the sequence length for each encoded IR statement to 64.",
        "Increasing the sequence length might improve results, but the aim of our work is to extract features from IRs, rather than have code generation capabilities. Thus, such an approach might be sufficient for performance optimization tasks, as we will show later. [SEP]'] In Figure , we show an example of the tokenization process with our trained tokenizer. For this example, we select an IR statement from a file that was not used to train the tokenizer.",
        "For this example, we select an IR statement from a file that was not used to train the tokenizer. We feed the statement to the tokenizer, which outputs a sequence of numbers (input ids). This is what a DL model will work with. To show that the encoding is correct, we decode the tokenized input ids to show that it is exactly the same as the given IR input, with a few minor but important differences.",
        "As shown in Figure , the outputs are in array format, as the tokenizer decodes each input id individually. The array includes a '[CLS]' and a '[SEP]' token at each end. The '[CLS]' token is used to denote the class of the input, if applicable, and the '[SEP]' token is used to separate two statements in the same input. The upper case alphabets in the inputs have also been converted to lower case to make the sequences case-insensitive.",
        "If we remove the first and last tokens in the array, and join the elements, we end up with the same output as the input, which shows the success of our tokenizer training process. 3.2 Graph Generation and Pre-Processing\nSeveral works ( ) have outlined that simply looking at source code as a stream of lexical tokens is not sufficient to represent code. Modeling IRs only as stream of tokens does not provide enough details about the structural properties of the program.",
        "Code structure can highlight dependencies in source code. It can show the flow of execution in source code, or can also show dependencies between variables. Given that such dependencies are sparse in nature, a graph seems to be an appropriate data structure to represent such structure and dependencies. The dependencies also highlight the meaning of a source code.",
        "The dependencies also highlight the meaning of a source code. The sequence of execution or the control flow, how the variables are dependent on each other or the data flow and the function call stack in a program are indicators of the underlying semantics of source code. Prior literature has used such structural and semantic information to good effect. We build on these ideas and work with graphs generated from IRs as the second modality. These graphs are generated with a tool called PROGRAML .",
        "These graphs are generated with a tool called PROGRAML . The generated multigraphs contain data-flow, control-flow, and call-flow dependencies in them. During pre-training, these graphs allow our model to extract semantic and structural features from source code (IRs). This is necessary as code structure and semantics should dictate the performance of an application/kernel. An example of such a graph is shown in Figure .",
        "An example of such a graph is shown in Figure . The nodes in our generated graphs (example shown in Figure ) contain IR statements. These form the node features in our graphs. Node features are used by Graph Neural Networks (GNNs) in forward and backward propagation during training. However, DL models cannot use such statements directly. Therefore, we use the trained tokenizer described in Section 3.1 to convert the IR statements into sequence of numbers.",
        "These become the node features and are used in the pre-training process by the GNN layers. 3.3 Pre-Training MIREncoder\nIn this section we outline the pre-training process of MIREncoder. The quality of a pre-trained model usually depends on the pretraining tasks considered. For this work, we have used three pretraining tasks; one task that targets each modality, and another one that is used to explicitly link together the two modalities.",
        "Namely, the pre-training tasks are Masked Language Modeling, Graph Auto-Encoding, and IR Code-Graph Matching. 3.3.1 Masked Language Modeling. Masked Language Modeling\n(MLM) is a widely used pre-training task in natural language based pre-trained models. It is also commonly used as a pre-training task in studies working with programming languages such as CodeBERT . MLM for this paper can be defined as follows.",
        "MLM for this paper can be defined as follows. Given an IR statement ùëê as input, we select a random set of positions ùëö ùëê that will be masked out; i.e. replaced with the '[MASK'] token. Following ideas presented in , we mask out 15% of the input. The task in this pre-training step is for the model to successfully predict the masked out words from the adjoining context. This is a self-supervised approach as the model is expected to produce the correct outputs without any explicit labels.",
        "Throughout the training process, the model is updated based on the difference between the predicted words and the actual words in the statements. However, it is worth noting that the '[MASK]' token does not appear during the downstream tasks. To overcome this, as done in , we perform the following steps:\n‚Ä¢ Select 15% of the token positions at random. ‚Ä¢ Randomly replace 80% of the selected positions with the '[MASK]' token. ‚Ä¢ Replace 10% of the selected positions with a random token.",
        "‚Ä¢ Replace 10% of the selected positions with a random token. ‚Ä¢ Keep the token unchanged for the remaining cases. These steps help the model learn the meaning of a word in the context of a statement, and not assign a single meaning to a word. Also, not including the '[MASK]' token in each statement during pre-training ensures that the model does not always expect that token. For this pre-training task, we use transformer layers with attention mechanism for improved training.",
        "3.3.2 Graph\nAuto-Encoding. Graph Auto-Encoders (GAEs) like traditional auto-encoders also aim to reconstruct the given inputs. The aim of this pre-training task is for the model to produce a learned low-dimensional embedded representation from the IR graphs during the downstream tasks. During pre-training, our model setup follows the widely used encode-code-decode setup.",
        "During pre-training, our model setup follows the widely used encode-code-decode setup. An input graph is first fed through GNN layers (Graph Convolution Layers or GCN ) to produce node embeddings in a two-dimensional latent space. This forms the encoder part of the network. In the decoder part of the network, the aim is to reconstruct the graph from the low-dimensional encoded form.",
        "The aim is not to reconstruct the original nodes, but to reconstruct the adjacency matrix identical to the input graph through an inner product between latent variables in order to understand the structure of the graphs. Now the multi-graphs used in this study have three sub-graphs in them denoting control-flow graphs, data-flow graphs, and callflow graphs. However, it is quite difficult to auto-encode graphs with multiple edge types.",
        "However, it is quite difficult to auto-encode graphs with multiple edge types. Therefore, we tweak the training process slightly by extracting each sub-graph from the IR multi-graph, and train the auto-encoder for each of the three sub-graphs. But, we do not train the model thrice. The modeling and the loss calculation phases are updated to work with the node features and adjacency matrices of each sub-graph.",
        "The loss is back-propagated as an aggregation of the difference in graph reconstruction of each subgraph.",
        "There are two main benefits to this: i) calculating the loss and back-propagating over the whole graph instead of each sub-graph allows the model to improve its learning over the whole graph and enables it to implicitly learn the relations between the three types of semantics in the graphs (control-flow, data-flow, call-flow), ii) it improves overall training time when compared to training three separate GAEs, one for each sub-graph. 3.3.3 IR-Graph Matching.",
        "3.3.3 IR-Graph Matching. Here, we propose a novel pre-training task IR-Graph Matching to link the two modalities together. The modalities considered in this paper have different data structures, one being a sequence of tokens, the other being a graph. Intuitively, it might be difficult for the model to understand how these two modalities are linked together, and by extension, difficult to link the syntax and structure.",
        "Therefore, we propose this pre-training task where the aim is for the model to correctly identify if the code sequence and the code graphs are from the same IR source. We setup this as a binary classification task, where the inputs are the code sequences (ùëÜ) and the code graphs (ùê∫). Positive and negative samples are automatically generated as data pairs to train the model.",
        "Positive and negative samples are automatically generated as data pairs to train the model. Positive samples are those where ùëÜ and ùê∫ are from the same IR, while the negative samples are those where the graphs and sequences are from different IRs. Negative samples are selected in 50% cases by randomly selecting a different IR from the dataset. The code graph of the negative sample is paired with the code sequence to create the negative data pair.",
        "As outlined in Section 3.3.1, the Masked Language Modeling task is performed on IR statements. However, in this task, we need to work with whole files to match text in IR files to the corresponding graphs. Although embedding an IR statement/instruction to a sequence of length 64 might work, embedding a complete file with a large number of statements to a sequence of length 64 will not provide enough information to the model.",
        "Therefore, we embed each statement in the file, and then aggregate all the vectors. The aggregated input and the generated code graph with the embedded node features (Section 3.2) are then trained together as a binary classification problem. The transformer layers used in Section 3.3.1 and the GCN layers used in Section 3.3.2 are reused to model the code sequences and the code graphs.",
        "Their outputs are concatenated and passed through linear layers with binary cross-entropy used for the loss calculations."
      ]
    },
    "2309.11930v2": {
      "ÂÆûÈ™åËØÑ‰ª∑": [
        "7 Additional Results\nIn addition, we conduct more experiments to validate the robustness of the proposed method. We first conduct a series experiments on CIFAR-100 dataset with different numbers of novel classes, and the results are reported in the Figure To further evaluate the performance when fine-tuning the pre-trained backbone, we conduct a series of experiments on the CIFAR dataset with 50% seen classes (10% labeled) and 50% novel classes.",
        "From Table , we can see that both ORCA and NACH show significant declines (over 10% overall accuracy), while our method LPS maintains high performance on CIFAR-100 and shows further improvements on CIFAR-10, which further verifies that LPS is not susceptible to the overfitting dilemma. 4 Experiments\n4.1 Experimental Setup\nDatasets. We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet .",
        "We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet . Following prior works , we assume that the number of novel classes is known. Specifically, we randomly select 50% of the classes as seen classes, and the remaining classes are regarded as novel classes, e.g., the number of novel classes is 50 for CIFAR-100. On each dataset, we consider two types of labeled ratios, i.e., only 10% or 50% of data in seen classes are labeled.",
        "For the ImageNet dataset, we subsample 100 classes to form the ImageNet-100 dataset for fair comparisons with existing works. Following prior works , we evaluate our method with respect to the accuracy of seen classes, novel classes, and all classes. For seen classes, the accuracy is calculated as the normal classification task.",
        "For seen classes, the accuracy is calculated as the normal classification task. For novel classes, we first utilize the Hungarian algorithm to solve the optimal prediction-target class assignment problem and then calculate the accuracy of novel classes. For overall accuracy, we also solve the optimal assignment in the entire unlabeled dataset to calculate the novel class accuracy, measuring the overall performance. Implementation Details.",
        "Implementation Details. Following , we utilize the self-supervised learning method SimCLR [Chen et al., 2020a] to pre-train the backbone and fix the first three blocks. In LPS, the weak augmentation contains random crop and horizontal flip, and the strong augmentation is RandAugment . For CIFAR-10 and CIFAR-100, we utilize ResNet-18 as our backbone which is trained by the standard SGD with a momentum of 0.9 and a weight decay of 0.0005. We train the model for 200 epochs with a batch size of 512.",
        "We train the model for 200 epochs with a batch size of 512. For the Im-ageNet dataset, we opt for ResNet-50 as our backbone. This choice also undergoes training via the standard SGD, featuring a momentum coefficient of 0.9 and a weight decay of 0.0001. The training process spans 90 epochs, with a batch size of 512. and The cosine annealing learning rate schedule is adopted on CIFAR and ImageNet datasets. These experiments are conducted on a single NVIDIA 3090 GPU.",
        "These experiments are conducted on a single NVIDIA 3090 GPU. 4.2 Comparing with Existing Methods\nBaselines. We compare LPS with SSL methods, open-set SSL methods, NCD methods, and OpenSSL methods. The NCD methods consider that the labeled data only has disjoint classes compared with the unlabeled data and aim at clustering novel classes without recognizing seen classes. For novel classes, clustering accuracy can be obtained directly.",
        "For novel classes, clustering accuracy can be obtained directly. For seen classes, we first regard them as novel classes and leverage the Hungarian algorithm to match some of the discovered classes with seen classes, and then calculate the classification accuracy. We select two competitive NCD methods DTC and RankStats in the experiments.",
        "We select two competitive NCD methods DTC and RankStats in the experiments. Moreover, we include GCD For the SSL and open-set SSL methods, we leverage their capability in estimating out-of-distribution samples to extend to the OpenSSL setting. For comparison, we select FixMatch , which assigns pseudo-labels to unlabeled samples based on confidence. The classification accuracy of seen classes can be reported directly according to pseudo-labels.",
        "The classification accuracy of seen classes can be reported directly according to pseudo-labels. For novel classes, we first estimate samples without pseudo-labels as novel classes and then utilize k-means to cluster them. The open-set SSL methods maintain the classification performance of seen classes by rejecting novel classes. We compare with DS 3 L and calculate its accuracy in the same way as FixMatch. For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD .",
        "For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD . We also compare the self-supervised pre-trained model SimCLR and conduct K-means on the primary features to calculate the accuracy. Results. The results on three datasets are reported in Table . The mean accuracy is computed over three runs for each method. Although the non-OpenSSL methods perform well on their original tasks, their overall performance is unsatisfactory in the OpenSSL setting.",
        "The results of SimCLR are obtained by the pre-trained model without extra fine-tuning, and the OpenSSL methods are based on the pre-trained model. It is obvious that the OpenSSL methods achieve significant performance improvements compared to non-OpenSSL methods. Compared with the state-of-the-art OpenSSL methods, our method LPS achieves the best overall performance across all datasets. On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy.",
        "On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy. Likewise, on the CIFAR-100 dataset, LPS demonstrates superiority, yielding a substantial 3.2% improvement. Particularly concerning the ImageNet-100 dataset, LPS has the capacity to surpass existing state-of-the-art methods, resulting in a 3.8% increase in overall accuracy. Experimental results demonstrate that LPS can effectively balance the learning of seen and novel classes. Distribution Analysis.",
        "Distribution Analysis. For further validation of our approach, we present a comprehensive analysis of the KL divergence trend between the estimated and prior class distributions, along with the estimated class distributions at the  foundation of SimCLR pre-trained backbone, the visualization, and NMI results highlight the efficacy of our approach in enhancing representation learning. Fine-tuning the Pre-trained Backbone.",
        "Fine-tuning the Pre-trained Backbone. Furthermore, it is noteworthy that all previous OpenSSL methods adopt a practice of freezing the parameters within the first three blocks of the backbone, solely fine-tuning the last block, with the intention of mitigating overfitting. However, such an approach constrains the extent of performance enhancement, as the backbone's parameters remain unmodifiable and unoptimized to better suit downstream tasks.",
        "To establish that our method is not susceptible to the overfitting dilemma, we conducted a series of experiments on the CIFAR dataset employing stateof-the-art OpenSSL methods while fine-tuning the backbone. The results are reported in Table . The experimental results reveal that existing OpenSSL methods manifest modest performance improvement, if any, in comparison to their initial frozen counterparts.",
        "In contrast, our proposed method, unaffected by overfitting concerns, consistently yields substantial performance gains across both seen and novel classes. Specifically, the overall accuracy for CIFAR-10 experiences a notable improvement of 2.9%, while an impressive 6.3% increase is observed for CIFAR-100. These results underscore 64.5 49.9 54.3\nTable : Accuracy when removing key components of our method.",
        "These results underscore 64.5 49.9 54.3\nTable : Accuracy when removing key components of our method. We report the average accuracy over three runs on CIFAR datasets with 50% seen classes (50% labeled) and 50% novel classes. the effectiveness of LPS in harnessing the additional learnable parameters for further enhancing model performance. Ablation Analysis. Moreover, we conduct a comprehensive analysis of the contributions of distinct components in our approach.",
        "The objective function of LPS comprises the adaptive margin loss (L AM ), the pseudo-label contrastive clustering loss (L PC ), the unsupervised contrastive learning loss (L UC ), and the entropy regularizer (R Entropy ). Concretely, the ablation study is mainly conducted by removing each term individually from the objective function except for the adaptive margin which is replaced by a standard cross-entropy. As observed in Table , the removal of any components leads to performance degradation.",
        "As observed in Table , the removal of any components leads to performance degradation. The substantial drop in novel performance after removing the entropy regularizer highlights its significant role in the process of novel class discovery. Moreover, the utilization of both pseudo-label contrastive loss and adaptive margin loss substantially improves the accuracy of novel classes."
      ]
    }
  },
  "statistics": {
    "total_papers": 2,
    "total_sections": 2,
    "total_chunks": 53,
    "paper_details": {
      "3656019.3676895": {
        "sections": [
          "ÊñπÊ≥ï"
        ],
        "section_count": 1
      },
      "2309.11930v2": {
        "sections": [
          "ÂÆûÈ™åËØÑ‰ª∑"
        ],
        "section_count": 1
      }
    }
  }
}