选定的最相关原文章节
================================================================================
【论文 3688609】
--------------------------------------------------
>>> 方法 章节 <<<
2 Deep Learning Acceleration Stack
2.1 Motivation
The recent growth of deep learning has been partially facilitated by the computational power of high-end GPUs as well as improvements in algorithmic representations . When combined with a tendency to focus narrowly on higher accuracies and the availability of large server-class GPUs, this has led to state-of-the-art DNN models to explode in size .This presents a large barrier to deploying many modern machine learning applications on constrained devices. Both machine learning researchers and systems engineers have proposed innovative solutions to overcome this barrier. However, these solutions are typically developed in isolation, meaning that machine learning practitioners may not explore the systems consequences of their approach and vice versa.For instance, sparsity is regarded by some in the machine learning community as a silver bullet for compressing models, whereas exploiting parallelism is generally seen as essential by system architects. Challenging these isolated preconceptions reveals that sparsity does not always excel at reducing the number of operations during inference, and parallelism does not necessarily bring the expected speedups. These observations are presented in greater detail in Section 6.These observations are presented in greater detail in Section 6. The goal of introducing DLAS as a conceptual model is to make it clearer to both machine learning and systems practitioners what the relevant contributors to performance for their DNN workloads are, allowing greater opportunities for co-design and co-optimization. This is not to 1:4 P. advocate for machine learning experts to re-train as systems experts and vice versa.Rather, we aim to provide a framework of reasoning so practitioners can understand the context in which their area of expertise exists in and give a "checklist" of other relevant performance contributing factors to be aware of. By exposing the wide range of choices and highlighting the impact of across-stack interaction, we also hope to encourage better tooling so practitioners can more easily experiment with perturbations.DLAS is an instantiation of the "systems stack, " whose layers have been chosen to highlight the most relevant components for deep learning acceleration. These goals are similar to other conceptual models such as the OSI model and the LAMP stack for web applications, which present an abstraction that organizes and illustrates the critical areas of each system while allowing choice of concrete implementation.Practically, by using DLAS as a conceptual model, multi-disciplinary teams may be able to align on a common language for accelerating their workloads, especially in constrained environments such as the edge.DLAS could help practitioners communicate how their techniques might interact with others and also act as a quick reference for new techniques that could be included in a given deployment to further accelerate their workloads, especially when a given acceleration strategy is giving diminishing returns. 2.2 Description of the Stack
We introduce the Deep Learning Acceleration Stack (DLAS), which spans from the machine learning domain all the way down to the hardware domain.Each layer can be tuned to optimize different goals (i.e., inference accuracy, execution time, memory footprint, power) or to yield further improvements in adjacent layers. However, for their potential to be fully realized, many optimizations are required to be implemented using techniques across several layers, i.e., co-design and co-optimization.DLAS contains the following six layers, with examples given in Figure : (3) Model Optimizations: approaches to reduce the size and costs of a DNN model (e.g., memory, inference time), while attempting to maintain the accuracy. (4) Algorithms & Data Formats: DNN layers (e.g., convolutions) can be implemented using various algorithms, with myriad tradeoffs in space and time. Interlinked with algorithms are data formats, i.e., how data is laid out in memory.Interlinked with algorithms are data formats, i.e., how data is laid out in memory. These choices can be consistent across a DNN model or vary per layer. (5) Systems Software: software used to run the DNNs, such as DNN frameworks, algorithmic implementations, supporting infrastructure, tensor compilers, and code-generators. (6) Hardware: devices the DNN is deployed on, from general purpose (e.g., CPUs, GPUs) to application-specific (e.g., FPGAs, NPUs, TPUs).It also includes hardware features, such as SIMD-units, cache behavior, and tensor cores. Although we have delineated the layers of the stack, it is critical to highlight that design decisions made at each layer of DLAS can directly impact adjacent layers and those across the stack. In addition, a domain expert may divide a given layer into more detailed sub-layers, and increased co-design may blur the separation between layers.However, we believe this six-layer structure strikes a balance between descriptiveness and simplicity. The six layers of DLAS should be understandable by experts at opposite ends of the stack, e.g., machine learning experts can understand that hardware choices can be important but may not want to reason about the tradeoffs of different cache policies or ISA extensions.In this article, we perform an across-stack perturbation of some parameters from each layer to determine their impact on inference and accuracy performance, and any interactions between parameters.In the future, practitioners will increasingly need to be aware of these across-stack interactions, as Moore's law scaling can no longer be relied upon by machine learning engineers , and increased competition between hardware designers will require progressively more innovative workload-aware approaches.2.3 Case Study
In the remainder of this article, we demonstrate the value of DLAS with a case study, choosing a small subset of popular parameters at each layer and showing how they can influence each other. Note that even examining a small number of parameters can result in a large number of experiment variants, due to the combinatorial growth for each new parameter added-there are nearly 1,000 combinations in our study alone.Considering a wider range of parameters poses significant research challenges regarding efficient design space exploration (DSE), which is a broader problem the community continues to tackle from a number of directions . We implement our case study using PyTorch and a modified version of Apache TVM. However, DLAS evaluations can be performed with any combination of software frameworks and techniques.Our case study highlights how an across-stack DLAS evaluation can be conducted with a narrow but deep investigation. We encourage readers to consider how the results could change if additional parameters were included, for example, Winograd convolution , Transformers , TPUs , or some other acceleration technique of interest.However, including these additional parameters will not change the core purpose of the case study, namely, to highlight that DLAS can be a useful delineation for DNN acceleration research, and across-stack thinking will be increasingly important to unlock the next generation of acceleration techniques.Section 3 gives the necessary background to understand the details of our case study, Section 4 describes the experimental setup, and Sections 5 and 6 provide the results and discussion, respectively.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
【论文 2309.11930v2】
--------------------------------------------------
>>> 实验评价 章节 <<<
7 Additional Results
In addition, we conduct more experiments to validate the robustness of the proposed method. We first conduct a series experiments on CIFAR-100 dataset with different numbers of novel classes, and the results are reported in the Figure To further evaluate the performance when fine-tuning the pre-trained backbone, we conduct a series of experiments on the CIFAR dataset with 50% seen classes (10% labeled) and 50% novel classes.From Table , we can see that both ORCA and NACH show significant declines (over 10% overall accuracy), while our method LPS maintains high performance on CIFAR-100 and shows further improvements on CIFAR-10, which further verifies that LPS is not susceptible to the overfitting dilemma. 4 Experiments
4.1 Experimental Setup
Datasets. We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet .We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet . Following prior works , we assume that the number of novel classes is known. Specifically, we randomly select 50% of the classes as seen classes, and the remaining classes are regarded as novel classes, e.g., the number of novel classes is 50 for CIFAR-100. On each dataset, we consider two types of labeled ratios, i.e., only 10% or 50% of data in seen classes are labeled.For the ImageNet dataset, we subsample 100 classes to form the ImageNet-100 dataset for fair comparisons with existing works. Following prior works , we evaluate our method with respect to the accuracy of seen classes, novel classes, and all classes. For seen classes, the accuracy is calculated as the normal classification task.For seen classes, the accuracy is calculated as the normal classification task. For novel classes, we first utilize the Hungarian algorithm to solve the optimal prediction-target class assignment problem and then calculate the accuracy of novel classes. For overall accuracy, we also solve the optimal assignment in the entire unlabeled dataset to calculate the novel class accuracy, measuring the overall performance. Implementation Details.Implementation Details. Following , we utilize the self-supervised learning method SimCLR [Chen et al., 2020a] to pre-train the backbone and fix the first three blocks. In LPS, the weak augmentation contains random crop and horizontal flip, and the strong augmentation is RandAugment . For CIFAR-10 and CIFAR-100, we utilize ResNet-18 as our backbone which is trained by the standard SGD with a momentum of 0.9 and a weight decay of 0.0005. We train the model for 200 epochs with a batch size of 512.We train the model for 200 epochs with a batch size of 512. For the Im-ageNet dataset, we opt for ResNet-50 as our backbone. This choice also undergoes training via the standard SGD, featuring a momentum coefficient of 0.9 and a weight decay of 0.0001. The training process spans 90 epochs, with a batch size of 512. and The cosine annealing learning rate schedule is adopted on CIFAR and ImageNet datasets. These experiments are conducted on a single NVIDIA 3090 GPU.These experiments are conducted on a single NVIDIA 3090 GPU. 4.2 Comparing with Existing Methods
Baselines. We compare LPS with SSL methods, open-set SSL methods, NCD methods, and OpenSSL methods. The NCD methods consider that the labeled data only has disjoint classes compared with the unlabeled data and aim at clustering novel classes without recognizing seen classes. For novel classes, clustering accuracy can be obtained directly.For novel classes, clustering accuracy can be obtained directly. For seen classes, we first regard them as novel classes and leverage the Hungarian algorithm to match some of the discovered classes with seen classes, and then calculate the classification accuracy. We select two competitive NCD methods DTC and RankStats in the experiments.We select two competitive NCD methods DTC and RankStats in the experiments. Moreover, we include GCD For the SSL and open-set SSL methods, we leverage their capability in estimating out-of-distribution samples to extend to the OpenSSL setting. For comparison, we select FixMatch , which assigns pseudo-labels to unlabeled samples based on confidence. The classification accuracy of seen classes can be reported directly according to pseudo-labels.The classification accuracy of seen classes can be reported directly according to pseudo-labels. For novel classes, we first estimate samples without pseudo-labels as novel classes and then utilize k-means to cluster them. The open-set SSL methods maintain the classification performance of seen classes by rejecting novel classes. We compare with DS 3 L and calculate its accuracy in the same way as FixMatch. For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD .For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD . We also compare the self-supervised pre-trained model SimCLR and conduct K-means on the primary features to calculate the accuracy. Results. The results on three datasets are reported in Table . The mean accuracy is computed over three runs for each method. Although the non-OpenSSL methods perform well on their original tasks, their overall performance is unsatisfactory in the OpenSSL setting.The results of SimCLR are obtained by the pre-trained model without extra fine-tuning, and the OpenSSL methods are based on the pre-trained model. It is obvious that the OpenSSL methods achieve significant performance improvements compared to non-OpenSSL methods. Compared with the state-of-the-art OpenSSL methods, our method LPS achieves the best overall performance across all datasets. On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy.On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy. Likewise, on the CIFAR-100 dataset, LPS demonstrates superiority, yielding a substantial 3.2% improvement. Particularly concerning the ImageNet-100 dataset, LPS has the capacity to surpass existing state-of-the-art methods, resulting in a 3.8% increase in overall accuracy. Experimental results demonstrate that LPS can effectively balance the learning of seen and novel classes. Distribution Analysis.Distribution Analysis. For further validation of our approach, we present a comprehensive analysis of the KL divergence trend between the estimated and prior class distributions, along with the estimated class distributions at the  foundation of SimCLR pre-trained backbone, the visualization, and NMI results highlight the efficacy of our approach in enhancing representation learning. Fine-tuning the Pre-trained Backbone.Fine-tuning the Pre-trained Backbone. Furthermore, it is noteworthy that all previous OpenSSL methods adopt a practice of freezing the parameters within the first three blocks of the backbone, solely fine-tuning the last block, with the intention of mitigating overfitting. However, such an approach constrains the extent of performance enhancement, as the backbone's parameters remain unmodifiable and unoptimized to better suit downstream tasks.To establish that our method is not susceptible to the overfitting dilemma, we conducted a series of experiments on the CIFAR dataset employing stateof-the-art OpenSSL methods while fine-tuning the backbone. The results are reported in Table . The experimental results reveal that existing OpenSSL methods manifest modest performance improvement, if any, in comparison to their initial frozen counterparts.In contrast, our proposed method, unaffected by overfitting concerns, consistently yields substantial performance gains across both seen and novel classes. Specifically, the overall accuracy for CIFAR-10 experiences a notable improvement of 2.9%, while an impressive 6.3% increase is observed for CIFAR-100. These results underscore 64.5 49.9 54.3
Table : Accuracy when removing key components of our method.These results underscore 64.5 49.9 54.3
Table : Accuracy when removing key components of our method. We report the average accuracy over three runs on CIFAR datasets with 50% seen classes (50% labeled) and 50% novel classes. the effectiveness of LPS in harnessing the additional learnable parameters for further enhancing model performance. Ablation Analysis. Moreover, we conduct a comprehensive analysis of the contributions of distinct components in our approach.The objective function of LPS comprises the adaptive margin loss (L AM ), the pseudo-label contrastive clustering loss (L PC ), the unsupervised contrastive learning loss (L UC ), and the entropy regularizer (R Entropy ). Concretely, the ablation study is mainly conducted by removing each term individually from the objective function except for the adaptive margin which is replaced by a standard cross-entropy. As observed in Table , the removal of any components leads to performance degradation.As observed in Table , the removal of any components leads to performance degradation. The substantial drop in novel performance after removing the entropy regularizer highlights its significant role in the process of novel class discovery. Moreover, the utilization of both pseudo-label contrastive loss and adaptive margin loss substantially improves the accuracy of novel classes.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
