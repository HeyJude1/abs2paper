{
  "selected_source_texts": {
    "2406.15763v2": {
      "方法": [
        "4 Imbalanced Semi-Supervised Learning\nSettings. We evaluate AllMatch in the context of imbalanced SSL, where both labeled and unlabeled data exhibit a long-tailed distribution. All experiments are conducted on the TorchSSL codebase. Following prior studies , we generate the labeled and unlabeled sets using the configurations of\nN c = N 1 • γ − c−1 C−1 and M c = M 1 • γ − c−1 C−1 . Specifically, for CIFAR-10-LT, we set N 1 to 1500, M 1 to 3000, and γ to range from 50 to 150.",
        "Specifically, for CIFAR-10-LT, we set N 1 to 1500, M 1 to 3000, and γ to range from 50 to 150. For CIFAR-100-LT, we set N 1 to 150, M 1 to 300, and γ to range from 20 to 100. In all experiments, we employ WRN-28-2 as the backbone and utilize the Adam optimizer with the weight decay of 4e-5. The batch sizes B L and B U are set to 64 and 128, respectively. The learning rate is initially set to 2e-3 and adjusted by a cosine decay scheduler during training.",
        "The learning rate is initially set to 2e-3 and adjusted by a cosine decay scheduler during training. We repeat each Table : Performance (%) on CIFAR-10-LT and CIFAR-100-LT.\nexperiment three times and report the overall performance. Detailed implementation is listed in Appendix C. Performance. In the context of imbalanced SSL, we compare AllMatch with several strong baselines, including Fix-Match, FlexMatch, SoftMatch, and FreeMatch.",
        "The results in Table demonstrate that AllMatch achieves state-of-theart performance on all benchmarks. It is particularly noteworthy that AllMatch outperforms the second-best approach by 1.69% and 1.65% at γ=100 and γ=150 on CIFAR-10-LT, respectively, highlighting its robustness in handling significant imbalances. Furthermore, as detailed in Appendix B, AllMatch is compatible with existing imbalanced SSL algorithms, and their combination can further enhance resilience against severe imbalances.",
        "The consistently impressive performance observed in imbalanced SSL suggests that All-Match can effectively address real-world challenges. 4.3 Ablation Study\nIn this part, we systematically evaluate each constituent component of AllMatch. Additionally, we provide the grid search of K (upper bound for the number of candidate classes) and λ b (weight for the BCC regulation) in Appendix A.1 and A.2. Component analysis. We conduct an ablation study on Table : Threshold comparison study (%).",
        "Component analysis. We conduct an ablation study on Table : Threshold comparison study (%). SoftMatch assigns trivial weights to samples with confidence significantly lower than the global confidence, approximating itself as a threshold-based model. four challenging datasets: CIFAR-10 with 10 labels, CIFAR-100 with 400 labels, STL-10 with 40 labels, and CIFAR-10-LT with an imbalance ratio of 150.",
        "For simplicity, we refer to the performance on the four benchmarks as (a, b, c, d) in subsequent analysis. As shown in Table , the global estimation step in CAT (line 2) promotes the performance by (7.35%, 8.00%, 16.45%, 1.20%) compared to the baseline model in line 1. The significant improvement highlights the crucial role of aligning the threshold with the model's global learning status.",
        "Furthermore, the local adjustment step in CAT (line 3) yields additional gains of (3.22%, 1.46%, 6.95%, 1.90%), suggesting that it effectively captures class-specific learning difficulties and facilitates the learning of classes facing challenges. Additionally, the BCC regulation enables a 100% utilization ratio of the unlabeled data and achieves the improvement of (2.25%, 0.52%, 0.71%, 0.77%).",
        "The substantial improvement observed on CIFAR-10 with 10 labels indicates the potential of the BCC regulation when dealing with extremely limited labeled data. Overall, the results in Table demonstrate the effectiveness of the proposed modules and the advantages of their combination in AllMatch. The threshold for STL-10-40 is restricted within [0.9, 1.0] to mitigate the adverse effects of noisy pseudo-labels. In the analysis of SoftMatch, we employ µt − σt as its threshold.",
        "In the analysis of SoftMatch, we employ µt − σt as its threshold. Samples with a confidence lower than µt − σt are assigned negligible weights, essentially treated as if they were discarded. Consequently, the class-average threshold of SoftMatch is µt − σt, and its selected pseudo-label acc and utilization ratio can be computed like other threshold-based models. Here, µt/σt denotes the mean/std of the overall confidence on unlabeled data.",
        "Here, µt/σt denotes the mean/std of the overall confidence on unlabeled data. Detailed analysis for SoftMatch is provided in Appendix A.3. Comparative study on threshold strategies. We conduct a comparative analysis of existing threshold mechanisms in two aspects. Firstly, we directly compare the proposed CAT with the threshold strategies adopted in previous models. The results are presented in line 1-4 of Table .",
        "The results are presented in line 1-4 of Table . Secondly, we assess the threshold strategies within the AllMatch framework, i.e., combining existing threshold schemes with the BCC regulation, and the results are provided in line 5-8 of Table . From both perspectives, AllMatch outperforms previous models in most cases, indicating the effectiveness of the proposed CAT.",
        "Moreover, the BCC regulation further boosts the performance of prior methods, suggesting its impressive compatibility and contribution to eliminating false options. 4.4 Quantitative Analysis\nTo gain further insights into AllMatch, we present various training indicators on CIFAR-10 with 40 labels and STL-10 with 40 labels, as illustrated in Figure . Besides, the indicators on CIFAR-10 with 10 labels and CIFAR-100 with 400 labels are presented in Appendix A.3.",
        "From Figure and Figure (e), it can be observed that the threshold exhibits the expected behavior, starting with a small value and gradually increasing thereafter. Moreover, AllMatch demonstrates a smoother threshold evolution in contrast to other classspecific threshold-based models, implying a preferred learning status estimation. Additionally, in comparison to previous algorithms, Figure\n\n3 Methodology\n3.1 Preliminary\nWe begin by reviewing the widely adopted SSL framework.",
        "Let D L = {(x i , y i )} N L i=1 and D U = {u i } N U i=1\nrepresent the labeled and unlabeled datasets, respectively. Here, x i and u i denote the labeled and unlabeled training samples, and y i represents the one-hot label for the labeled sample x i . We denote the prediction of sample x as p(y|x). Given a batch of labeled and unlabeled data, the model is optimized with the objective L = L s + λ u L u . Here, L s represents the crossentropy loss (H) for the labeled batch of size B L .",
        "Here, L s represents the crossentropy loss (H) for the labeled batch of size B L . L s = 1 B L B L i=1 H(y i , p(y|x i )) (1)\nL u indicates the consistency regulation between the prediction of the strongly augmented view Ω(u) and the pseudolabel derived from the corresponding weakly augmented view ω(u). To filter out incorrect pseudo-labels, FixMatch introduces a predefined threshold τ . Specifically, L u is defined as follows.",
        "Specifically, L u is defined as follows. L u = 1 B U B U i=1 λ(p i )H(p i , q i ) (2) λ(p) = 1 if max(p) ≥ τ 0 otherwise (3)\nHere, pi is the abbreviation for DA(p(y|ω(u i ))), where DA indicates the distribution alignment strategy . pi represents the one-hot pseudo-label obtained from argmax(p i ). Moreover, q i is the abbreviation for p(y|Ω(u i )). Lastly, B U corresponds to the batch size of unlabeled data.",
        "Lastly, B U corresponds to the batch size of unlabeled data. 3.2 Class-Specific Adapative Threshold\nPrevious studies have demonstrated that the threshold should be aligned with the evolving learning status of the model. To achieve this, these approaches leverage predictions on unlabeled data to establish the dynamic threshold. In this paper, we unveil the abil-ity of the classifier weights to differentiate the learning status of each class.",
        "By combining pseudo-labels and classifier weights, we introduce a class-specific adaptive threshold (CAT) mechanism. As depicted in Figure , CAT comprises the global estimation and local adjustment steps. The following parts provide a detailed description of these two steps. Global Estimation. The global estimation step learns from FreeMatch and evaluates the overall learning status of the model.",
        "Given that deep neural networks tend to prioritize fitting easier samples before memorizing harder and noisier ones, a lower threshold is necessary during early training stages to incorporate more correct pseudolabels. Conversely, as training progresses, a higher threshold is required to filter out incorrect pseudo-labels.",
        "Given that the cross-entropy loss encourages confident predictions, the average confidence of the unlabeled set captures information from all unlabeled data and steadily increases throughout training, thereby reflecting the overall learning status. However, making predictions for the entire unlabeled set at each time step incurs significant computational costs. Accordingly, we employ the mean confidence of the current batch as an estimation and update it using exponential moving average (EMA).",
        "Specifically, the global learning status estimation at t-th iteration, denoted as τ t , can be computed as follows:\nτ t =    1 C if t = 0 mτ t−1 + (1 − m) 1 B U B U i=1 max(p i ) otherwise (4)\nHere, p i represents p(y|ω(u i )), m denotes the momentum decay, and C corresponds to the number of classes. Local Adjustment.",
        "Local Adjustment. Due to the inherent variations in learning difficulty among different classes and the stochastic nature of parameter initialization, the model's learning status varies across categories. To address this issue, we introduce the local adjustment step, which makes the model pay more attention to the underfitting classes by decreasing their thresholds. Specifically, our study reveals that the L2 norm of the classifier weights provides insights into class-specific learning status.",
        "The reasons are explained as follows. Firstly, let M = G • F denote the model, where F and G According to the above analysis, the L2 norm of classifier weights characterizes the learning status of each class. Consequently, the local adjustment step leverages this indicator to establish the mapping from the global threshold to classspecific thresholds. Specifically, we linearly scale the threshold for each class based on the deviation of its learning status from the optimal learning status.",
        "As such, the threshold for class c at t-th iteration, denoted as τ t (c), can be computed as follows. τ t (c) = τ t • ||W c || max{||W c || : c ∈ [1, • • • , C]} (5)\nMoreover, to ensure a stable estimation of the learning status, we employ the classifier weights obtained from the EMA model.",
        "Notably, in contrast to FlexMatch, which maintains an additional list for recording the selected pseudo-label of each sample, the proposed CAT refrains from storing any samplespecific information during training. This eliminates the indexing budget concerns on large-scale datasets. With CAT incorporated, the mask for unlabeled samples in L u can be expressed as follows.",
        "With CAT incorporated, the mask for unlabeled samples in L u can be expressed as follows. λ(p) = 1 if max(p) ≥ τ t (argmax(p)) 0 otherwise (6)\n3.3 Binary Classification Consistency Regulation\nWhile the proposed class-specific adaptive threshold alleviates the underutilization of unlabeled data, a substantial number of pseudo-labels continue to be discarded.",
        "As illustrated in Figure , in the case of CIFAR-10 with 40 labeled samples, the top-5 accuracy of pseudo-labels effortlessly achieves 100% regardless of the adopted algorithm. In other words, pseudo-labels assigned lower confidence contribute to identifying candidate classes (e.g., top-k predictions) and excluding negative options (e.g., classes not included in top-k predictions).",
        "Motivated by these observations and the consistency regulation technique, we propose the binary classification consistency (BCC) regulation, whose overview is shown in Figure . In a nutshell, the strategy introduces semantic supervision for all unlabeled data by encouraging consistent candidate-negative division across diverse perturbed views of the same sample. The details are described as follows.",
        "The details are described as follows. Given the impressive top-k pseudo-label accuracy obtained by numerous algorithms, the BCC regulation adopts the top-k predictions of each unlabeled sample as its candidate classes and the rest as the negative options.",
        "Thus, the candidatenegative division is simplified to the selection of the parameter k. Moreover, considering the variations in learning difficulty among different samples and the evolving performance of the model, the candidate-negative division for each sample should be determined based on both individual and global learning status. To achieve this, the BCC regulation first computes sample-specific top-k confidence and the global top-k confidence of the entire unlabeled set.",
        "Specifically, let p k i denote the top-k probability of sample u i , and µ k t represent the global top-k probability at t-th iteration. The global top-k confidence can be estimated by the exponential moving aver-age (EMA) of the average top-k confidence at each time step. p k i = k j=1 p i,cj (p i,c1 ≥ p i,c2 ≥ • • • ) (7) µ k t = k C if t = 0 mµ k t−1 + (1 − m) 1 B U B U i=1 p k i otherwise (8)\nHere, c 1 , . . . , c k represent the k classes assigned the highest probability in p i .",
        ". . , c k represent the k classes assigned the highest probability in p i . With the global top-k confidence determined, the number of candidate classes for each unlabeled sample is defined as the minimum value that makes individual top-k confidence higher than global top-k confidence. Particularly, the candidate class for confident unlabeled samples is defined as the pseudo-label. Accordingly, the number of candidate classes k i for sample u i can be expressed as follows.",
        "Accordingly, the number of candidate classes k i for sample u i can be expressed as follows. k i = 1 if λ(p i ) = 1 min(min{k : pk i ≥ µ k t }, K) otherwise ( 9\n)\nwhere K is the upper bound for the number of candidate classes to prevent trivial candidate-negative division. With the division obtained, the candidate and negative probabilities for the weakly (b ω i ) and strongly (b Ω i ) perturbed views of the unlabeled sample u i can be calculated as follows.",
        "b ω i = [ ki j=1 pi,cj , C j=ki+1 pi,cj ] (p i,c1 ≥ pi,c2 ≥ • • • ) (10) b Ω i = [ ki j=1 q i,cj , C j=ki+1 q i,cj ] (11)\nHere, c 1 , . . . , c ki represents the k i classes assigned the highest probability in pi . Finally, the BCC regulation for a batch of unlabeled data can be calculated as follows:\nL b = 1 B U B U i=1 H(b ω i , b Ω i ) (12)\n3.4 Overall Objective\nThe overall objective of AllMatch is defined as the weighted sum of all semantic-level supervision.",
        "AllMatch is trained using the SGD optimizer with an initial learning rate of 0.03 and a momentum decay of 0.9. The learning rate is adjusted by a cosine decay scheduler over a total of 2 20 iterations. We set m to 0.999 and generate the EMA model with a momentum decay of 0.999 for inference. L = L s + λ u L u + λ b L b (13)\nThe upper bound K is set to 20 for ImageNet and 10 for the other datasets.",
        "For SVHN, CIFAR-10 with 10 labels, and STL-10 with 40 labels, we constrain the threshold within the range of [0.9, 1.0] to prevent overfitting noisy pseudo-labels in the early training stages. To account for randomness, we repeat each experiment three times and report the mean and standard deviation of the top-1 accuracy. Detailed implementation and data processing are listed in Appendix C.\nPerformance.",
        "Detailed implementation and data processing are listed in Appendix C.\nPerformance. Table presents the top-1 accuracy on CIFAR-10/100, SVHN, and STL-10 with various numbers of labeled samples. The performance on ImageNet is reported in Table . The experimental results demonstrate that AllMatch achieves state-of-the-art performance on almost all datasets.",
        "For CIFAR-10, AllMatch outperforms FullMatch with only 40 available labels, and performs comparably to FullMatch when there are 250 or 4000 labels. Moreover, regarding CIFAR-100, AllMatch outperforms ReMixMatch when only 400 or 2500 labels are available, while the latter achieves better performance when 10000 labels are available. The competitive results obtained by ReMixMatch mainly stem from the Mixup technique and the additional self-supervised learning part.",
        "Furthermore, AllMatch exhibits substantial advantages over previous algorithms when dealing with extremely limited labeled samples. Specifically, the approach surpasses the second-best counterpart by 1.87% on CIFAR-10 with 10 labels, 0.66% on CIFAR-100 with 400 labels, and 2.86% on STL-10 with 40 labels. Particularly, STL-10 poses significant challenges due to its large unlabeled set that comprises 100k images.",
        "Accordingly, the impressive improvement obtained on STL-10 highlights the potential of AllMatch to be deployed in real-world applications."
      ]
    }
  },
  "statistics": {
    "total_papers": 1,
    "total_sections": 1,
    "total_chunks": 41,
    "paper_details": {
      "2406.15763v2": {
        "sections": [
          "方法"
        ],
        "section_count": 1
      }
    }
  }
}