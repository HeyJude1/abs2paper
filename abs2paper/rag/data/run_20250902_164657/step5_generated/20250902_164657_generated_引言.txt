【引言】
================================================================================
近年来，自然语言处理（NLP）领域在大型语言模型（LLMs）的推动下取得了显著进展。基于Transformer架构的预训练模型通过自监督学习在多种下游任务中展现出卓越的泛化能力。然而，如何有效利用预训练模型中蕴含的知识并将其适配到特定任务场景，仍是当前研究面临的关键挑战。微调（fine-tuning）作为连接预训练与下游应用的核心技术环节，其优化策略直接影响模型在实际应用中的性能表现。

现有微调方法主要面临三个关键挑战：首先，预训练知识的利用率不足。研究表明，LLMs的内部表征可能包含正确知识（即使最终输出错误），但现有微调范式缺乏将这些潜在知识显式整合到下游任务的机制。其次，知识迁移方向不明确。微调过程中从通用知识到专业知识的转移过程呈现高度隐式特征，现有方法难以量化评估这种迁移效果。第三，数据稀缺场景下的性能瓶颈问题突出。高质量标注数据构建成本高昂，而现有数据增强方法难以充分挖掘预训练模型的潜在知识。

针对上述挑战，本研究提出基于知识向量显式建模的新型微调框架，主要贡献包括：（1）建立从概率分布偏移到知识迁移的量化理论框架；（2）设计可插拔的知识适应模块（KAM），增强模型的知识获取能力；（3）开发面向低资源场景的知识蒸馏策略。实验证明该方法在科学解释生成等任务中显著优于传统微调方案。
