选定的最相关原文章节
================================================================================
【论文 3656019.3676895】
--------------------------------------------------
>>> 方法 章节 <<<
3 MIREncoder
Most source-code based performance optimization tasks in HPC usually involve compilable languages such as C, C++, CUDA, and so on. A large number of these languages can be compiled and optimized using the LLVM infrastructure. LLVM IRs are a portable, high-level assembly language that can be optimized with a variety of transformations over multiple passes. It is fairly simple to extract IRs from source code such as C, C++.It is fairly simple to extract IRs from source code such as C, C++. IRs generated from source code are usually devoid of most stylistic choices and redundant code. This is why we choose to work with IRs for performance optimizations. Figure shows a high-level overview of our approach. For the first modality, we first tokenize the input IRs into meaningful "tokens" before they are mapped to an embedded dimension.Our approach then learns the embedding of the IR instructions after splitting them into sub-words. For the second modality, the IRs are first converted to dependence graphs that include in them data flow, control flow, and call flow information that represents the semantic information in the source code. These two modalities are then passed into the modeling pipeline either for pre-training or inference. The following paragraphs outline our pipeline.The following paragraphs outline our pipeline. 3.1 Tokenization
Simply put, tokenization is the process of breaking down a piece of text into smaller units called tokens, and assigning a numerical value to each token. A deep learning (DL) model does not understand text or images in its raw form. It needs to be represented as numbers for the model to make sense from it. This is why tokenization is extremely important for such works.This is why tokenization is extremely important for such works. In this paper, our tokenization process follows the same approach taken while designing and training the BERT model. However, the pre-trained BERT tokenizer readily available online is trained on natural language (NL). However, source code (IRs in our paper) is more structured than NL, and quite possibly has fewer "words". Thus, we had to train our tokenizer from scratch.Thus, we had to train our tokenizer from scratch. We initially collect a large set of IRs by compiling programs in existing datasets into their LLVM IRs. For training the tokenizer, we have used C, C++, and CUDA code from CodeNet , HPCorpus , and LS-CAT . We first define a set of special tokens to handle unknown inputs, and a token that will be used during Masked Language Modeling. 10, 000 unique programs are randomly selected and compiled into LLVM IRs.10, 000 unique programs are randomly selected and compiled into LLVM IRs. These are then passed through a WordPiece tokenizer, as done in BERT, and trained to generate a learned vocabulary. BERT uses a sequence length of 512. However, for the sake of simplicity and faster training, we limit the sequence length for each encoded IR statement to 64.Increasing the sequence length might improve results, but the aim of our work is to extract features from IRs, rather than have code generation capabilities. Thus, such an approach might be sufficient for performance optimization tasks, as we will show later. [SEP]'] In Figure , we show an example of the tokenization process with our trained tokenizer. For this example, we select an IR statement from a file that was not used to train the tokenizer.For this example, we select an IR statement from a file that was not used to train the tokenizer. We feed the statement to the tokenizer, which outputs a sequence of numbers (input ids). This is what a DL model will work with. To show that the encoding is correct, we decode the tokenized input ids to show that it is exactly the same as the given IR input, with a few minor but important differences.As shown in Figure , the outputs are in array format, as the tokenizer decodes each input id individually. The array includes a '[CLS]' and a '[SEP]' token at each end. The '[CLS]' token is used to denote the class of the input, if applicable, and the '[SEP]' token is used to separate two statements in the same input. The upper case alphabets in the inputs have also been converted to lower case to make the sequences case-insensitive.If we remove the first and last tokens in the array, and join the elements, we end up with the same output as the input, which shows the success of our tokenizer training process. 3.2 Graph Generation and Pre-Processing
Several works ( ) have outlined that simply looking at source code as a stream of lexical tokens is not sufficient to represent code. Modeling IRs only as stream of tokens does not provide enough details about the structural properties of the program.Code structure can highlight dependencies in source code. It can show the flow of execution in source code, or can also show dependencies between variables. Given that such dependencies are sparse in nature, a graph seems to be an appropriate data structure to represent such structure and dependencies. The dependencies also highlight the meaning of a source code.The dependencies also highlight the meaning of a source code. The sequence of execution or the control flow, how the variables are dependent on each other or the data flow and the function call stack in a program are indicators of the underlying semantics of source code. Prior literature has used such structural and semantic information to good effect. We build on these ideas and work with graphs generated from IRs as the second modality. These graphs are generated with a tool called PROGRAML .These graphs are generated with a tool called PROGRAML . The generated multigraphs contain data-flow, control-flow, and call-flow dependencies in them. During pre-training, these graphs allow our model to extract semantic and structural features from source code (IRs). This is necessary as code structure and semantics should dictate the performance of an application/kernel. An example of such a graph is shown in Figure .An example of such a graph is shown in Figure . The nodes in our generated graphs (example shown in Figure ) contain IR statements. These form the node features in our graphs. Node features are used by Graph Neural Networks (GNNs) in forward and backward propagation during training. However, DL models cannot use such statements directly. Therefore, we use the trained tokenizer described in Section 3.1 to convert the IR statements into sequence of numbers.These become the node features and are used in the pre-training process by the GNN layers. 3.3 Pre-Training MIREncoder
In this section we outline the pre-training process of MIREncoder. The quality of a pre-trained model usually depends on the pretraining tasks considered. For this work, we have used three pretraining tasks; one task that targets each modality, and another one that is used to explicitly link together the two modalities.Namely, the pre-training tasks are Masked Language Modeling, Graph Auto-Encoding, and IR Code-Graph Matching. 3.3.1 Masked Language Modeling. Masked Language Modeling
(MLM) is a widely used pre-training task in natural language based pre-trained models. It is also commonly used as a pre-training task in studies working with programming languages such as CodeBERT . MLM for this paper can be defined as follows.MLM for this paper can be defined as follows. Given an IR statement 𝑐 as input, we select a random set of positions 𝑚 𝑐 that will be masked out; i.e. replaced with the '[MASK'] token. Following ideas presented in , we mask out 15% of the input. The task in this pre-training step is for the model to successfully predict the masked out words from the adjoining context. This is a self-supervised approach as the model is expected to produce the correct outputs without any explicit labels.Throughout the training process, the model is updated based on the difference between the predicted words and the actual words in the statements. However, it is worth noting that the '[MASK]' token does not appear during the downstream tasks. To overcome this, as done in , we perform the following steps:
• Select 15% of the token positions at random. • Randomly replace 80% of the selected positions with the '[MASK]' token. • Replace 10% of the selected positions with a random token.• Replace 10% of the selected positions with a random token. • Keep the token unchanged for the remaining cases. These steps help the model learn the meaning of a word in the context of a statement, and not assign a single meaning to a word. Also, not including the '[MASK]' token in each statement during pre-training ensures that the model does not always expect that token. For this pre-training task, we use transformer layers with attention mechanism for improved training.3.3.2 Graph
Auto-Encoding. Graph Auto-Encoders (GAEs) like traditional auto-encoders also aim to reconstruct the given inputs. The aim of this pre-training task is for the model to produce a learned low-dimensional embedded representation from the IR graphs during the downstream tasks. During pre-training, our model setup follows the widely used encode-code-decode setup.During pre-training, our model setup follows the widely used encode-code-decode setup. An input graph is first fed through GNN layers (Graph Convolution Layers or GCN ) to produce node embeddings in a two-dimensional latent space. This forms the encoder part of the network. In the decoder part of the network, the aim is to reconstruct the graph from the low-dimensional encoded form.The aim is not to reconstruct the original nodes, but to reconstruct the adjacency matrix identical to the input graph through an inner product between latent variables in order to understand the structure of the graphs. Now the multi-graphs used in this study have three sub-graphs in them denoting control-flow graphs, data-flow graphs, and callflow graphs. However, it is quite difficult to auto-encode graphs with multiple edge types.However, it is quite difficult to auto-encode graphs with multiple edge types. Therefore, we tweak the training process slightly by extracting each sub-graph from the IR multi-graph, and train the auto-encoder for each of the three sub-graphs. But, we do not train the model thrice. The modeling and the loss calculation phases are updated to work with the node features and adjacency matrices of each sub-graph.The loss is back-propagated as an aggregation of the difference in graph reconstruction of each subgraph.There are two main benefits to this: i) calculating the loss and back-propagating over the whole graph instead of each sub-graph allows the model to improve its learning over the whole graph and enables it to implicitly learn the relations between the three types of semantics in the graphs (control-flow, data-flow, call-flow), ii) it improves overall training time when compared to training three separate GAEs, one for each sub-graph. 3.3.3 IR-Graph Matching.3.3.3 IR-Graph Matching. Here, we propose a novel pre-training task IR-Graph Matching to link the two modalities together. The modalities considered in this paper have different data structures, one being a sequence of tokens, the other being a graph. Intuitively, it might be difficult for the model to understand how these two modalities are linked together, and by extension, difficult to link the syntax and structure.Therefore, we propose this pre-training task where the aim is for the model to correctly identify if the code sequence and the code graphs are from the same IR source. We setup this as a binary classification task, where the inputs are the code sequences (𝑆) and the code graphs (𝐺). Positive and negative samples are automatically generated as data pairs to train the model.Positive and negative samples are automatically generated as data pairs to train the model. Positive samples are those where 𝑆 and 𝐺 are from the same IR, while the negative samples are those where the graphs and sequences are from different IRs. Negative samples are selected in 50% cases by randomly selecting a different IR from the dataset. The code graph of the negative sample is paired with the code sequence to create the negative data pair.As outlined in Section 3.3.1, the Masked Language Modeling task is performed on IR statements. However, in this task, we need to work with whole files to match text in IR files to the corresponding graphs. Although embedding an IR statement/instruction to a sequence of length 64 might work, embedding a complete file with a large number of statements to a sequence of length 64 will not provide enough information to the model.Therefore, we embed each statement in the file, and then aggregate all the vectors. The aggregated input and the generated code graph with the embedded node features (Section 3.2) are then trained together as a binary classification problem. The transformer layers used in Section 3.3.1 and the GCN layers used in Section 3.3.2 are reused to model the code sequences and the code graphs.Their outputs are concatenated and passed through linear layers with binary cross-entropy used for the loss calculations.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
【论文 3577193.3593731】
--------------------------------------------------
>>> 实验评价 章节 <<<
5 EXPERIMENTAL RESULTS
To investigate the feasibility of this approach in terms of numerical stability, scalability, and performance, we implemented it in the Software for Linear Algebra Targeting Exascale (SLATE) library and evaluated it on the Summit supercomputer. SLATE is a dense linear algebra library that targets distributed-memory, GPU-accelerated systems . Our code and results are available at https://doi.org/ 10.6084/m9.figshare.21982472.Our code and results are available at https://doi.org/ 10.6084/m9.figshare.21982472. Our implementation follows Algorithm 1 and uses a high-level structure based on SLATE's GENP routine . However, we separated BEAM's algorithmic block size from the distribution tile size (the former being smaller than the latter). For simplicity, our code does not support an algorithmic block to be split across multiple tiles of SLATE and will truncate the last block in a tile to fit.But all our experiments align the block and tile sizes so that truncation only happens in the last tile. After the factorization is complete, the capacitance matrix is built and factored. While our theory defines 𝜏 in terms of ∥𝐴∥ 2 , this is expensive to compute in practice. So, our experiments instead used the Frobenius norm, 𝜏 = τ ∥𝐴∥ 𝐹 , which is closely related.So, our experiments instead used the Frobenius norm, 𝜏 = τ ∥𝐴∥ 𝐹 , which is closely related. For performance purposes, we implemented a GPU routine for batched, block-triangular solves, using a recursive formulation similar to the MAGMA and KBLAS libraries. Because the diagonal blocks come from the SVD, these inverses can be realized by a matrix multiplication and sometimes a diagonal scaling.While cuBLAS's batched GEMM routine was effective for the trailingmatrix updates, its performance was lacking for small block sizes due to the subsequent copy or scale operation. For such cases, we implemented a custom routine that combined the multiplication and the copy to improve cache reuse and avoid extra kernel launch overheads. To reduce the effort in performance tuning, we used part of MAGMA's matrix-multiplication routine in our kernel.5.1 Experimental Setup
We ran our experiments on eight nodes of the Summit supercomputer at Oak Ridge National Laboratory. This machine is based on IBM Power System AC922 nodes, each containing two 22-core IBM POWER9 CPUs and six NVIDIA Volta V100 GPUs. One core of each socket is reserved for the OS (Red Hat Enterprise Linux version 8.2). Most of the computational power comes from the GPUs, each providing 7.8 TFLOP/s, 16 GiB HBM2, and 900 GB/s memory bandwidth.Each CPU provides 540 GFLOP/s, 256 GiB DDR4 memory, and 170 GB/s memory bandwidth. NVIDIA NVLink provides a bidirectional 50 GB/s between components in a socket. A dual-rail EDR InfiniBand network with a non-blocking fat-tree topology connects the nodes. We used a modified version of SLATE's test harness for our experiments. The tester was compiled with GCC 9.1.0, CUDA 11.0.3, IBM Spectrum MPI 10.4.0.3, IBM ESSL 6.1.0, Netlib LAPACK 3.8.0, and Netlib ScaLAPACK 2.1.0.We set the smt1 flag and started MPI with jsrun -n 16 -a 1 -c 21 -g 3 -b packed:21 -d packed which allocates 16 processes, each bound to a single socket and its GPUs. For all experiments, we configured the tester with --origin h --target d --ref n --grid 4x4 --panel-threads 20 --seed 1 --seedB 2 --matrixB randn --nrhs 1. We also set the --matrix, --dim, and --check flags as appropriate for the experiment. For GEPP, we also set --nb 768 --ib 64 --lookahead 1.For GEPP, we also set --nb 768 --ib 64 --lookahead 1. For GENP and BEAM, we set --nb 512 --lookahead 2 --ib 64, except for the experiment described in Section 5.3 which changed the last argument as appropriate for BEAM. This configuration gives a 2d block-cyclic distribution with a 4 × 4 process grid and blocks of size 512 or 768, as indicated by --nb.Note that SLATE's ib parameter corresponds to the 𝑛 𝑏 value discussed in this paper; SLATE's nb corresponds to the larger blocks used to distribute the matrix. All tests were preceded by extra tests of size 𝑛 = 5000 (with the otherwise identical configuration) to ensure that our results were not influenced by initialization costs. To measure the effects of system noise, we ran each performance test three times and computed the mean and 95% confidence interval.Except for svd_geo, the error and number of modifications were the same between the different runs. Due to minor non-determinism in SLATE's QR factorization, there is slight variability between runs in the entries of svd_geo. However, this variability is small and does not affect our conclusions or analysis, so we just present the error values and number of modifications from the first run.To understand how our BEAM algorithm behaves across various linear systems, we used seven random and eight structured matrices in our tests. Table describes these matrices. We choose a righthand side with each element randomly taken from the normal distribution. The matrix generator was always seeded with 1 for the matrices and with 2 for the right-hand sides so that the test problems can be reproduced.Accuracy was measured with the infinity-norm backward error:
𝜂 ∞ (𝑥) = ∥𝑏 − 𝐴𝑥 ∥ ∞ ∥𝐴∥ ∞ ∥𝑥 ∥ ∞ + ∥𝑏 ∥ ∞ . ( 8
)
Correspondingly, in experiments with iterative refinement, the refinement was terminated when this error was less than or equal to √ 𝑛 times the unit roundoff (∼3.5 × 10 −14 when 𝑛 = 10 5 ) or after 30 iterations. We selected this criterion based on the accuracy of GEPP (see Table ).We selected this criterion based on the accuracy of GEPP (see Table ). 5.2 Baseline Accuracy and Performance Experiments
First, Table compares the accuracy of BEAM against GEPP and GENP for varying values of tolerance τ. The matrices were of size 10 5 , with a blocking factor of 64 for BEAM. The reported error is the infinity-norm backward error of .The reported error is the infinity-norm backward error of . Most importantly, the error of BEAM with the Woodbury correction is smaller than or approximately equal to that of GENP for all but one case (orthog with τ = 10 −10 ). Furthermore, BEAM with Woodbury correction has a significantly smaller error than GENP for most matrices and only incurs NaN values for one matrix, zielkeNS. (Those NaN values resulting from growth-induced overflow.)(Those NaN values resulting from growth-induced overflow.) These results demonstrate the ability of our approach to provide better numerical stability than GENP. Moreover, the error was smaller than 10 −10 for many of the matrices. This implies that the iterative refinement should often converge quickly to double-precision accuracy .While τ = 10 −6 leads to modifications for most matrices, only five of the fifteen matrices required more than ten modifications when τ ≤ 10 −8 (one of which was accurately solved without any modifications when τ = 10 −10 ). This indicates that the proposed approach is likely effective for a large class of matrices. Additionally, many linear systems saw a significant improvement in accuracy compared to GENP, even without modification.Thus, even just applying an SVD factorization to invert the diagonal blocks increases the stability of a non-pivoted factorization. The results become more nuanced when considering BEAM without Woodbury correction. For τ = 10 −10 , the uncorrected solver behaved similarly to the corrected one. For larger tolerances, however, there is a significant gap between the two, particularly for τ = 10 −6 .This indicates that the perturbation of the uncorrected modification becomes the dominant source of error when τ ≳ 10 −8 . This aligns with both the τ term in the normwise backward error bound of ( ) and the recommended tolerance when applying scalar updates without correction . In contrast, when the Woodbury correction was applied, increasing the tolerances always saw similar or better accuracies.This suggests that the error in the corrected case comes from the presence of small diagonal singular values and the resulting growth and not from applying the modifications or the Woodbury correction process. Table augments Table by showing the time to solve the linear systems of equations (again with 𝑛 = 10 5 ). Up to 30 steps of iterative refinement were used for BEAM but not for GEPP or GENP.Up to 30 steps of iterative refinement were used for BEAM but not for GEPP or GENP. To clarify where iterative refinement was unsuccessful, we marked the cases which failed to achieve convergence criterion for iterative refinement (𝜂 ∞ (𝑥) ≲ 3.5 × 10 −14 ). Furthermore, we provide the number of refinement iterations, with 30 being the limit. First, note that by using iterative refinement, our approach achieved an error of less than 2 −53 √ 𝑛 for almost all cases.As above, zielkeNS's failure involved excessive growth generating NaN values. For the remaining failures, BEAM produced a non-NaN solution, but iterative refinement failed to converge to full accuracy. These cases included svd_geo, chebspec, fiedler, and riemann with larger tolerances (and many modifications) but without Woodbury correction. These matrices are all ill-conditioned, which limits the ability of iterative refinement to converge when the inner solution is only moderately accurate .For example, 𝜅 ∞ (fiedler) = 2𝑛(𝑛 − 1) ≈ 2 × 10 10 [27, pg. 159], so iterative refinement can only be expected to converge to full accuracy when 𝜂 ∞ (𝑥) ≲ 5 × 10 −11 . Furthermore, this further supports the implication of both (5) and Theorem 4.1 that τ should be chosen such that τ𝜅 2 (𝐴) ≪ 1.Interestingly, these systems were successfully solved when using the Woodbury formula, despite the dire implication of Theorems 4.1 and 4.3 that τ𝜅 2 (𝐴) ≲ 1 can lead to a large forward
5.3 Effect of tolerance
We next investigated the tradeoff in performance and accuracy for a larger variety of tolerance values and blocking sizes on select matrices without iterative refinement. Table shows the results. Matching Tables , the matrix sizes are all 𝑛 = 10 5 .Table shows the results. Matching Tables , the matrix sizes are all 𝑛 = 10 5 . Furthermore, the number of modifications and error in Table for τ = 10 −6 , 10 −8 , 10 −10 correspond to the values in Table ; however, the run times differ from Table due to the omission of iterative refinement. While GEPP and GENP do not have the algorithmic blocking parameter 𝑛 𝑏 of BEAM, they implement cache-blocking with a similar structure and a block size of 64.Both rand_dominant and rand matrix types saw BEAM significantly outperform GEPP for all configurations. However, as mentioned earlier, BEAM performed worse than GENP, even when no corrections were applied. Moreover, smaller block sizes performed slightly better, likely due to increased arithmetic for the SVDs and block-triangular solves. For rand_dominant, all configurations resulted in no modifications and the same accuracy.For rand_dominant, all configurations resulted in no modifications and the same accuracy. For rand, on the other hand, increasing the tolerance above 10 −10 increased the accuracy when the Woodbury correction was applied but decreased the accuracy when it was not, with the number of modifications increasing in both cases. Given the number of modifications introduced when τ ≤ 10 −6 , a tolerance of 10 −8 or 10 −10 is a better choice, particularly when not applying the Woodbury correction.Finally, increasing the block size reduced the number of modifications and increased the accuracy in all but one case. The structured matrices provided more interesting results. As in the previous tables, BEAM applied numerous modifications to the orthog matrix for all of the tested configurations. Increasing the blocking factor helped the accuracy, although it also increased the number of modifications.The best tradeoff between performance and accuracy seems to be for tolerances of 10 −6 or 10 −8 (depending on the block size) without the Woodbury correction. Unexpectedly, a smaller block size led to fewer modifications; we suspect this is due to element growth in the later diagonals. For zielkeNS, only τ = 10 −4 without the Woodbury formula produced a non-NaN solution. On the other hand, the block size had limited effect on the performance or the accuracy.On the other hand, the block size had limited effect on the performance or the accuracy. For τ = 10 −4 , all three block factor sizes resulted in the modification of about 95% of the diagonal singular values, whereas for smaller tolerance values, the number of modifications was just slightly larger than the number of blocks. 5.4 Scaling Results
Finally, we tested the performance of the different solvers as the size, 𝑛, varies for the rand_dominant, rand, and orthog matrices.BEAM achieved speedups ranging from 4× to almost 5× for the three matrices compared to GEPP applied to rand. BEAM was configured with a blocking factor size of 𝑛 𝑏 = 64 and a tolerance of τ = 10 −8 . BEAM with iterative refinement ran out of GPU memory for 𝑛 = 250 000 due to the extra copy of the system matrix.Note that a diagonally dominant matrix, such as the rand_dominant, is the best-case scenario for the performance of GEPP because the selected pivots already reside on the diagonal and the memory traffic of exchanging rows is avoided (though we still perform the pivot search for each column).For the large matrices, BEAM reached 80% of GENP's performance for rand_dominant and rand, as the 0 50,000 100,000 150,000 200,000 250,000 300,000 0 former required no modifications and the latter required just a few modifications. Without the Woodbury correction, BEAM also performed similarly on orthog. However, with the correction, the performance dropped to approximately that of the best case for GEPP.Adding iterative refinement slightly reduced the overall performance, but BEAM still outperformed the best-case scenario of GEPP by 84% to 162% on the rand_dominant and rand matrices. Without the Woodbury formula, BEAM performed almost as well on orthog as on rand with speedups of 70% to 144%. With the Woodbury formula, BEAM performed in the range of GEPP, between 40% and 112% faster than the GEPP's performance on rand (which is close to its performance on orthog, as per Table ).These speedups for orthog are particularly promising because most approaches struggle to accurately outperform GEPP on this matrix, especially for large sizes .

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
