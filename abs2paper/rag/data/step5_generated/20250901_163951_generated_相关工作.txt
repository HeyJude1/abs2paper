【相关工作】
================================================================================
相关工作

在深度学习模型分布式推理领域，现有研究主要围绕两大技术路线展开：基于输入张量数据划分的分布式推理方法和基于模型并行化的优化方法。这些方法在不同应用场景下展现出各自的优势与局限性，为本研究提供了重要的技术基础。

1. 输入张量数据分布式推理方法
该技术路线通过划分输入数据的感受野或特征图实现计算并行化。代表性工作包括DeepThings、MoDNN、CoEdge和EdgeFlow等系统。DeepThings创新性地提出感受野分配策略，使得卷积层可以独立进行推理计算，显著降低了设备间的通信开销。MoDNN则采用贪心算法对卷积层和全连接层的输入张量进行划分，根据边缘设备的计算能力动态分配负载。进一步发展的CoEdge系统提出异构设备自适应负载划分技术，综合考虑计算资源、网络带宽和设备能耗等多维因素。EdgeFlow则从计算图角度出发，基于DAG模型重构分区方法，通过分析模型图的输入输出关系实现更精细的层操作分配。

尽管这些方法在CNN架构上取得了显著成效，但仍存在明显局限：首先，其设计主要针对CNN模型特性，未能有效支持Transformer等新兴的非CNN架构；其次，除CoEdge部分考虑外，大多数方法对动态网络条件和设备异构性的适应性不足；最后，这些方法的优化目标相对单一，难以应对实际部署环境中多目标优化的需求。

2. Transformer模型并行化方法
随着Transformer架构的广泛应用，研究者开始探索其并行化方案。Megatron-LM是这一方向的代表性工作，其核心思想是通过矩阵乘法并行（Mat-Mul）分析Transformer的运算行为，实现算子级并行加速。该方法深入研究了自注意力机制和全连接层的计算特性，设计了高效的层内并行策略。

然而这类方法仍面临重要挑战：一方面缺乏针对异构设备和动态网络的适应性算法，难以在边缘计算场景中有效部署；另一方面仅支持层内并行而无法利用跨层流水线机会，限制了整体计算效率的提升。特别值得注意的是，现有工作鲜少考虑边缘设备间的协同推理问题，导致在实际部署时难以充分发挥分布式系统的整体计算能力。

3. 代码表示与自动调优方法
在模型优化方面，早期研究主要依赖词法标记进行代码表示。这类方法通过解析源代码的文本特征支持优化决策，但无法有效捕捉代码的深层语义信息。新一代方法转而利用LLVM中间表示（IR）提取语义特征，为深度学习模型提供结构化程序信息。然而这类方法需要为每个独立任务设计复杂的图神经网络建模框架，缺乏可迁移的通用表示能力。

传统机器学习方法如贝叶斯优化曾被应用于参数自动调优任务（如OpenMP调优），但其严重依赖领域特定知识且泛化能力有限。基于搜索的技术（如ActiveHarmony和OpenTuner）采用爬山算法、随机搜索等方法替代暴力搜索，虽然提高了效率但仍面临计算开销大的问题。

4. 开放世界半监督学习挑战
在开放世界场景下现有方法面临三个核心挑战：首先是对未标记数据中新类别样本的识别与聚类问题。传统半监督学习假设未标记数据仅包含已知类别样本这一前提在实际应用中往往不成立；其次是已知类别与新类别间的学习速度偏差问题；最后是预训练特征提取器的适应性不足问题。

综合现有研究可以发现以下关键研究缺口：（1）缺乏支持多种神经网络架构的统一分布式推理框架；（2）现有并行化方案对动态边缘环境的适应性不足；（3）模型优化过程缺乏端到端的自动化解决方案；（4）开放世界场景下的学习偏差问题尚未得到系统性解决。

本文提出的LPS框架针对上述缺口进行了创新性设计：首先通过自适应边缘损失函数缓解学习速度偏差问题；其次采用伪标签对比聚类增强新类别识别能力；最后引入混合特征网络提升特征表示的适应性。相较于现有工作本研究的创新点主要体现在三个方面：（1）首次在统一框架中解决了开放世界半监督学习的三大核心挑战；（2）提出了面向边缘计算的轻量化分布式推理方案；（3）实现了从特征学习到模型优化的端到端自动化流程。实验结果表明本文方法在保持已知类别分类精度的同时显著提升了新类别的聚类质量为开放世界场景下的智能应用提供了新的技术路径。
