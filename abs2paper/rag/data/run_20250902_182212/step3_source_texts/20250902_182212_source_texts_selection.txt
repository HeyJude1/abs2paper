选定的最相关原文章节
================================================================================
【论文 2309.11930v2】
--------------------------------------------------
>>> 方法 章节 <<<
3 Adaptive Synchronizing
To start with, we describe the proposed adaptive marginal loss which regularizes the learning pace of seen classes to synchronize the learning pace of the model. Conventionally, the margin is defined as the minimum distance of the data to the classification boundary.For a sample (x, y), we have:
∆(x, y) = f (x) y − max j̸ =y f (x) j (1)
Instead of employing a fixed margin, LDAM introduces a class-specific margin, where the margin between rare classes and other classes is larger than the margin for frequent classes, for tackling class-imbalanced data. Specifically, it sets the margin of class j as:
∆ j = C n 1/4 j (2)
The constant C controls the intensity and n j denotes the frequency of class j in the training data.Motivated by this, we
propose a new variant of the margin loss to synchronize the learning pace of seen and novel classes. We apply adaptive margin loss to demand a larger margin between the novel and other classes, so that scores for seen classes, towards which the model highly biased, do not overwhelm the novel classes.For each sample (x, y), the adaptive margin loss is defined as follows:
ℓ AM (x, y) = − log exp(z y − ∆ y ) exp(z y − ∆ y ) + j̸ =y exp(z j ) where ∆ j = −KL π π π j max( π) C, j ∈ [K] (3)
In this formulation, K represents the total number of classes, z j signifies the model output for the j-th class, z = f (x; θ), and π denotes the estimated class distribution by the model.Additionally, we introduce an approximation of the true class distribution π, which is naturally inaccessible during training. In line with prior studies, we assume a uniform distribution for π, leaving the exploration of arbitrary distributions for future investigations. The hyper-parameter C is introduced to control the maximum margin, and we empirically set C = 10 across all experiments. We conduct a series of studies on the value of C in the supplementary material.We conduct a series of studies on the value of C in the supplementary material. For the sake of simplicity, we assume that the mini-batch is comprised of labeled data B l and unlabeled data B u . Given that the computation of Eq. ( ) relies on the class distribution, we proceed to estimate the complete class distribution through labeled data and unlabeled data exhibiting high predictive confidence.Specifically, we endeavour to achieve this estimation through:
π = Normalize   xi∈B l y i + xj ∈Bu I max( y j ) ≥ λ y j   (4)
Here, y = softmax(z). In view of the tendency for novel class samples to exhibit underconfidence, we empirically introduce a progressively evolving confidence threshold λ novel = 0.4 + 0.4 × t T , where t and T signify the current training iteration and the total training iterations, respectively.For seen classes, a fixed confidence threshold λ seen = 0.95 is employed. Given that the estimated class distribution π mirrors the model's confidence in class predictions, we harness this insight to regulate the learning pace of both seen and novel classes. Notably, in the early training phases, the model is inclined towards seen classes, with the logit adjustment term ∆ j assuming a larger negative margin for seen classes, thereby attenuating their learning pace.As training progresses, the model attains a more balanced capability across both seen and novel classes, as reflected by a diminishing Kullback-Leibler (KL) divergence between π and π.In summary, the adaptive margin loss L AM for both labeled and pseudo-labeled data is defined as follows:
L AM = 1 |B l | xi∈B l ℓ AM (z w i , y)+ 1 |B u | xj ∈Bu I max( y j ) ≥ λ ℓ AM (z s j , p j ) (5)
In this context, z w and z s correspond to the output logits stemming from the weak and strong augmented versions of sample x, respectively. The symbol | • | denotes the set cardinality operation.The symbol | • | denotes the set cardinality operation. Additionally, we utilize p = arg max(softmax(z w )) to represent the pseudo-label associated with the sample. Distinctions and Connections with Alternatives. It is worth noting that the concept of adaptive margin has been used in prior literature .It is worth noting that the concept of adaptive margin has been used in prior literature . Different from LPS, leverages the semantic similarities between classes to generate adaptive margins with the motivation to separate similar classes in the embedding space, and [Ha and Blanz, 2021] utilizes the ground-truth distance between different samples to generate adaptive margins with the motivation to adapt to rating datasets.In OpenSSL, ORCA also integrates an adaptive margin mechanism based on the model's predictive uncertainty, which can only equally suppress the learning pace of seen classes. However, there are still differences in the learning paces of different classes among seen classes.However, there are still differences in the learning paces of different classes among seen classes. Our proposed adaptive margin is based on the current estimated distribution to reflect the learning pace of different classes, which offers increased flexibility for regulating the learning pace across classes by generating the class-specific negative margin.Furthermore, the inclusion of the KL divergence term effectively guards against the model converging to a trivial solution where all samples are arbitrarily assigned to a single class. 3.2 Pseudo-Label Contrastive Clustering
The basic idea of discovering novel classes is to explore the correlations between different samples and cluster them into several groups.Prior OpenSSL approaches often transform the clustering task into a pairwise similarity prediction task, wherein a modified form of binary cross-entropy loss is optimized. Different from existing works, we introduce a new clustering method to fully exploit reliable model predictions as supervisory signals. Our approach involves the construction of a multi-viewed mini-batch by using weak and strong augmentations.Within each mini-batch, we group the labeled and confident unlabeled samples, which is denoted as B l ′ . Concurrently, unlabeled samples exhibiting predicted confidence levels failing below the threshold λ are denoted as B u ′ . Pseudo-label contrastive clustering only takes B l ′ as inputs. For each sample in B l ′ , the set of the positive pairs contains samples with the same given label or pseudo-label. Conversely, the set of negative pairs contains samples of other classes.Conversely, the set of negative pairs contains samples of other classes. Formally, the objective of pseudo-label contrastive clustering is defined as follows:
L PC = − 1 |B l ′ | xi∈B l ′ log 1 |P (x i )| xp∈P (xi) exp (z i •z p /τ ) xa∈A(xi) exp (z i •z a /τ ) , (6)
where z i denotes the output logits, P (x i ) denotes the set of positives of x i and A(x i ) ≡ B l ′ \{x i }. τ is a tunable temperature parameter and we set τ = 0.4 in all experiments.τ is a tunable temperature parameter and we set τ = 0.4 in all experiments. In contrast to existing methods such as ORCA and NACH , which establish a single positive pair for each sample by identifying its nearest neighbour, the objective in Eq. ( ) adopts pseudo-labels to form multiple positive pairs.( ) adopts pseudo-labels to form multiple positive pairs. This approach offers dual advantages: firstly, the alignment of samples within the same class is more effectively harnessed through the utilization of multiple positive sample pairs; secondly, it leverages the consistency of distinct views of the same sample to mitigate the negative impact of erroneous positive pairs, while concurrently imparting a repulsion effect to samples from different classes through negative pairs. Since Eq.Since Eq. ( ) augments the labeled dataset by unlabeled samples of high predictive confidence, we ask whether unlabeled samples of low confidence can be used to enrich representation learning. In pursuit of this, we incorporate unsupervised contrastive learning [Wang and to encourage similar predictions rather than embeddings between a given sample and its augmented counterpart. This helps to signify the uniformity among unlabeled samples, ultimately leading to clearer separations.In detail, for each sample in the low-confidence set B u ′ , the unsupervised contrastive learning couples it with its augmented view to constitute a positive pair. Simultaneously, a set of negative pairs is formulated, containing all the samples within the mini-batch except the sample itself.The unsupervised contrastive learning loss L UC is formulated as follows:
L UC = − 1 |B u ′ | xj ∈B u ′ log exp (z j • z p /τ ) xa∈ A(xj ) exp (z j • z a /τ ) (7)
Here, x p is the positive sample of x j and A(x j ) ≡ B u ′ ∪ B l ′ \{x j } for the sample x j . In essence, unsupervised contrastive learning complements the pseudo-label contrastive clustering by fully exploiting the unlabeled samples.In the experiments, the ablation studies underscore the pivotal role played by both types of contrastive losses in our approach. Lastly, we incorporate a maximum entropy regularizer to address the challenge of converging during the initial training phases, when the predictions are mostly wrong (e.g., the model tends to assign all samples to the same class) . Specifically, we leverage the KL divergence between the class distribution predicted by the model and a uniform prior distribution.It is worth noting that the integration of an entropy regularizer is a widespread practice in dealing with the OpenSSL problem, including approaches such as ORCA, NACH, and OpenNCD. The final objective function of LPS is articulated as follows:
L total = L AM + η 1 L PC + η 2 L UC + R Entropy (8)
where R Entropy denotes the entropy regularizer, η 1 and η 2 are hyper-parameters set to 1 in all our experiments.We provide detailed analyses on the sensitivity of hyperparameters in the supplementary material.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>> 实验评价 章节 <<<
7 Additional Results
In addition, we conduct more experiments to validate the robustness of the proposed method. We first conduct a series experiments on CIFAR-100 dataset with different numbers of novel classes, and the results are reported in the Figure To further evaluate the performance when fine-tuning the pre-trained backbone, we conduct a series of experiments on the CIFAR dataset with 50% seen classes (10% labeled) and 50% novel classes.From Table , we can see that both ORCA and NACH show significant declines (over 10% overall accuracy), while our method LPS maintains high performance on CIFAR-100 and shows further improvements on CIFAR-10, which further verifies that LPS is not susceptible to the overfitting dilemma. 4 Experiments
4.1 Experimental Setup
Datasets. We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet .We evaluate our method on three commonly used datasets, i.e., CIFAR-10, CIFAR-100 , and ImageNet . Following prior works , we assume that the number of novel classes is known. Specifically, we randomly select 50% of the classes as seen classes, and the remaining classes are regarded as novel classes, e.g., the number of novel classes is 50 for CIFAR-100. On each dataset, we consider two types of labeled ratios, i.e., only 10% or 50% of data in seen classes are labeled.For the ImageNet dataset, we subsample 100 classes to form the ImageNet-100 dataset for fair comparisons with existing works. Following prior works , we evaluate our method with respect to the accuracy of seen classes, novel classes, and all classes. For seen classes, the accuracy is calculated as the normal classification task.For seen classes, the accuracy is calculated as the normal classification task. For novel classes, we first utilize the Hungarian algorithm to solve the optimal prediction-target class assignment problem and then calculate the accuracy of novel classes. For overall accuracy, we also solve the optimal assignment in the entire unlabeled dataset to calculate the novel class accuracy, measuring the overall performance. Implementation Details.Implementation Details. Following , we utilize the self-supervised learning method SimCLR [Chen et al., 2020a] to pre-train the backbone and fix the first three blocks. In LPS, the weak augmentation contains random crop and horizontal flip, and the strong augmentation is RandAugment . For CIFAR-10 and CIFAR-100, we utilize ResNet-18 as our backbone which is trained by the standard SGD with a momentum of 0.9 and a weight decay of 0.0005. We train the model for 200 epochs with a batch size of 512.We train the model for 200 epochs with a batch size of 512. For the Im-ageNet dataset, we opt for ResNet-50 as our backbone. This choice also undergoes training via the standard SGD, featuring a momentum coefficient of 0.9 and a weight decay of 0.0001. The training process spans 90 epochs, with a batch size of 512. and The cosine annealing learning rate schedule is adopted on CIFAR and ImageNet datasets. These experiments are conducted on a single NVIDIA 3090 GPU.These experiments are conducted on a single NVIDIA 3090 GPU. 4.2 Comparing with Existing Methods
Baselines. We compare LPS with SSL methods, open-set SSL methods, NCD methods, and OpenSSL methods. The NCD methods consider that the labeled data only has disjoint classes compared with the unlabeled data and aim at clustering novel classes without recognizing seen classes. For novel classes, clustering accuracy can be obtained directly.For novel classes, clustering accuracy can be obtained directly. For seen classes, we first regard them as novel classes and leverage the Hungarian algorithm to match some of the discovered classes with seen classes, and then calculate the classification accuracy. We select two competitive NCD methods DTC and RankStats in the experiments.We select two competitive NCD methods DTC and RankStats in the experiments. Moreover, we include GCD For the SSL and open-set SSL methods, we leverage their capability in estimating out-of-distribution samples to extend to the OpenSSL setting. For comparison, we select FixMatch , which assigns pseudo-labels to unlabeled samples based on confidence. The classification accuracy of seen classes can be reported directly according to pseudo-labels.The classification accuracy of seen classes can be reported directly according to pseudo-labels. For novel classes, we first estimate samples without pseudo-labels as novel classes and then utilize k-means to cluster them. The open-set SSL methods maintain the classification performance of seen classes by rejecting novel classes. We compare with DS 3 L and calculate its accuracy in the same way as FixMatch. For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD .For the OpenSSL methods, we compare with ORCA , NACH , and OpenNCD . We also compare the self-supervised pre-trained model SimCLR and conduct K-means on the primary features to calculate the accuracy. Results. The results on three datasets are reported in Table . The mean accuracy is computed over three runs for each method. Although the non-OpenSSL methods perform well on their original tasks, their overall performance is unsatisfactory in the OpenSSL setting.The results of SimCLR are obtained by the pre-trained model without extra fine-tuning, and the OpenSSL methods are based on the pre-trained model. It is obvious that the OpenSSL methods achieve significant performance improvements compared to non-OpenSSL methods. Compared with the state-of-the-art OpenSSL methods, our method LPS achieves the best overall performance across all datasets. On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy.On the CIFAR-10 dataset, LPS outperforms NACH by 1.2% in novel class accuracy. Likewise, on the CIFAR-100 dataset, LPS demonstrates superiority, yielding a substantial 3.2% improvement. Particularly concerning the ImageNet-100 dataset, LPS has the capacity to surpass existing state-of-the-art methods, resulting in a 3.8% increase in overall accuracy. Experimental results demonstrate that LPS can effectively balance the learning of seen and novel classes. Distribution Analysis.Distribution Analysis. For further validation of our approach, we present a comprehensive analysis of the KL divergence trend between the estimated and prior class distributions, along with the estimated class distributions at the  foundation of SimCLR pre-trained backbone, the visualization, and NMI results highlight the efficacy of our approach in enhancing representation learning. Fine-tuning the Pre-trained Backbone.Fine-tuning the Pre-trained Backbone. Furthermore, it is noteworthy that all previous OpenSSL methods adopt a practice of freezing the parameters within the first three blocks of the backbone, solely fine-tuning the last block, with the intention of mitigating overfitting. However, such an approach constrains the extent of performance enhancement, as the backbone's parameters remain unmodifiable and unoptimized to better suit downstream tasks.To establish that our method is not susceptible to the overfitting dilemma, we conducted a series of experiments on the CIFAR dataset employing stateof-the-art OpenSSL methods while fine-tuning the backbone. The results are reported in Table . The experimental results reveal that existing OpenSSL methods manifest modest performance improvement, if any, in comparison to their initial frozen counterparts.In contrast, our proposed method, unaffected by overfitting concerns, consistently yields substantial performance gains across both seen and novel classes. Specifically, the overall accuracy for CIFAR-10 experiences a notable improvement of 2.9%, while an impressive 6.3% increase is observed for CIFAR-100. These results underscore 64.5 49.9 54.3
Table : Accuracy when removing key components of our method.These results underscore 64.5 49.9 54.3
Table : Accuracy when removing key components of our method. We report the average accuracy over three runs on CIFAR datasets with 50% seen classes (50% labeled) and 50% novel classes. the effectiveness of LPS in harnessing the additional learnable parameters for further enhancing model performance. Ablation Analysis. Moreover, we conduct a comprehensive analysis of the contributions of distinct components in our approach.The objective function of LPS comprises the adaptive margin loss (L AM ), the pseudo-label contrastive clustering loss (L PC ), the unsupervised contrastive learning loss (L UC ), and the entropy regularizer (R Entropy ). Concretely, the ablation study is mainly conducted by removing each term individually from the objective function except for the adaptive margin which is replaced by a standard cross-entropy. As observed in Table , the removal of any components leads to performance degradation.As observed in Table , the removal of any components leads to performance degradation. The substantial drop in novel performance after removing the entropy regularizer highlights its significant role in the process of novel class discovery. Moreover, the utilization of both pseudo-label contrastive loss and adaptive margin loss substantially improves the accuracy of novel classes.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
