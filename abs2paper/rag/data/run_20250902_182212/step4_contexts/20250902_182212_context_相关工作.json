{
  "section_name": "相关工作",
  "context": "### RelatedWork 总结\n**总结1** (来源: 2406.15763v2):\n相关工作总结：\n\n1、现有方法一：一致性正则化与伪标签方法（Consistency Regulation & Pseudo Labeling）\n核心思想: \n- 通过弱增强样本生成伪标签，并强制其与强增强视图的预测保持一致（如FixMatch）。\n- 通过固定阈值（FixMatch）、课程学习类特定阈值（FlexMatch）、标记数据损失动态阈值（Dash）、无标记数据平均置信度阈值（FreeMatch）或高斯函数动态权重（SoftMatch）筛选高质量伪标签。\n\n主要局限性: \n- 固定阈值导致未标记数据利用率不足。\n- 现有方法对样本级约束（对比损失）或语义级监督的整合不足。\n\n2、现有方法二：对比学习增强方法（Contrastive-based Methods）\n核心思想: \n- 引入对比损失对全部未标记数据施加样本级约束（如CoMatch、SimMatch）。\n- 结合阈值策略与对比学习优势（如AllMatch的CAT阈值和BCC语义监督）。\n\n主要局限性: \n- FullMatch等并发工作仅通过强弱增强预测比较识别负类，未考虑样本与模型的全局学习状态。\n- 负类优化策略（如...\n\n**总结2** (来源: 2406.15763v2):\n相关工作总结：\n\n1、现有方法一：一致性正则化与伪标签方法（Consistency Regulation & Pseudo Labeling）\n核心思想: \n- 通过弱增强样本生成伪标签，并强制其与强增强视图的预测保持一致（如FixMatch）。\n- 通过固定阈值（FixMatch）、课程学习类特定阈值（FlexMatch）、标记数据损失动态阈值（Dash）、无标记数据平均置信度阈值（FreeMatch）或高斯函数动态权重（SoftMatch）筛选高质量伪标签。\n\n主要局限性: \n- 固定阈值导致未标记数据利用率不足。\n- 现有方法对样本级约束（对比损失）或语义级监督的整合不足。\n\n2、现有方法二：对比学习增强方法（Contrastive-based Methods）\n核心思想: \n- 引入对比损失对全部未标记数据施加样本级约束（如CoMatch、SimMatch）。\n- 结合阈值策略与对比学习优势（如AllMatch的CAT阈值和BCC语义监督）。\n\n主要局限性: \n- FullMatch等并发工作仅通过强弱增强预测比较识别负类，未考虑样本与模型的全局学习状态。\n- 负类优化策略（如...\n\n**总结3** (来源: 3656019.3689905):\n相关工作总结：\n\n1、现有方法一：结构化稀疏存储格式（Structured Sparse Formats）\n核心思想: 采用硬件友好的规则稀疏模式（如块稀疏），通过牺牲部分压缩率换取计算规整性，主要应用于机器学习领域。\n主要局限性: 存在精度与压缩率的固有权衡（accuracy-compression tradeoff），且受限于硬件强制的稀疏模式。\n\n2、现有方法二：坐标指针类格式（CSR及其变种）\n核心思想: 使用行偏移（row_ptr）和列索引（col_idx）的非规则存储方案，适用于超稀疏矩阵的科学计算场景。\n主要局限性: \n- 对中等稀疏度工作负载效率低下\n- 增加片上资源消耗（存储需求和通信开销）\n- 加剧内存带宽瓶颈（尤其在SpMM等带宽敏感任务中）\n\n3、现有方法三：位图存储格式（Bitmap-based Formats）\n核心思想: 通过位掩码标记非零元素位置，实现紧凑存储。\n主要局限性:\n- 缺乏成熟的索引策略\n- 处理长零值序列时效率低下\n- 需要昂贵比较操作消除无效计算\n\n4、现有方法四：分层存储技术（Hierarchical Storage Technique...\n\n### Challenges 总结\n**总结1** (来源: 0728):\n### 核心挑战总结：\n\n#### 挑战一：**固有知识利用不足**  \n**分析**:  \n现有微调方法（如PEFT）主要关注算法优化或数据构建，但忽视了对预训练大语言模型（LLMs）固有知识的系统性挖掘与利用。研究表明，预训练LLMs的内部表征可能包含正确知识（即使输出错误），但当前微调范式缺乏有效机制将这些潜在知识显式整合到下游任务中。这一挑战源于对模型行为与知识表征之间关联性的理解不足，以及缺乏量化知识迁移的数学工具。\n\n#### 挑战二：**知识适应方向不明确**  \n**分析**:  \n微调过程中，LLMs从通用预训练知识到任务特定知识的适应过程是隐式的（如概率分布偏移），现有方法无法显式捕捉和强化这种适应方向。例如，模型可能生成相同token（如\"engage\"），但其预测概率分布的变化隐含了专业化知识的转移（如\"catalyze\"概率提升）。这种挑战源于概率空间的高维复杂性，以及缺乏将分布差异转化为可操作信号的机制。\n\n#### 挑战三：**数据稀缺下的性能瓶颈**  \n**分析**:  \n高质量人工标注数据集构建成本高昂，而现有数据增强方法（如输入输出对反转）难以充分...\n\n**总结2** (来源: 0728):\n### 核心挑战总结：\n\n#### 挑战一：**固有知识利用不足**  \n**分析**:  \n现有微调方法（如PEFT）主要关注算法优化或数据构建，但忽视了对预训练大语言模型（LLMs）固有知识的系统性挖掘与利用。研究表明，预训练LLMs的内部表征可能包含正确知识（即使输出错误），但当前微调范式缺乏有效机制将这些潜在知识显式整合到下游任务中。这一挑战源于对模型行为与知识表征之间关联性的理解不足，以及缺乏量化知识迁移的数学工具。\n\n#### 挑战二：**知识适应方向不明确**  \n**分析**:  \n微调过程中，LLMs从通用预训练知识到任务特定知识的适应过程是隐式的（如概率分布偏移），现有方法无法显式捕捉和强化这种适应方向。例如，模型可能生成相同token（如\"engage\"），但其预测概率分布的变化隐含了专业化知识的转移（如\"catalyze\"概率提升）。这种挑战源于概率空间的高维复杂性，以及缺乏将分布差异转化为可操作信号的机制。\n\n#### 挑战三：**数据稀缺下的性能瓶颈**  \n**分析**:  \n高质量人工标注数据集构建成本高昂，而现有数据增强方法（如输入输出对反转）难以充分...\n\n**总结3** (来源: 2406.15763v2):\n### 核心挑战总结：\n\n#### 挑战一：**伪标签质量与数量的权衡问题**  \n**分析**:  \n- **具体内容**: 现有阈值策略（如FixMatch的高固定阈值）为确保伪标签质量，会丢弃大量低置信度样本，导致未标记数据利用率不足。  \n- **根源**:  \n  1. **问题复杂性**: 模型训练初期预测不稳定，高阈值会过滤潜在有用信息；  \n  2. **技术瓶颈**: 静态阈值无法动态适应不同类别/阶段的学习状态；  \n  3. **数据限制**: 标记数据稀缺时，过度依赖高置信度伪标签加剧样本浪费。  \n\n#### 挑战二：**学习状态估计的偏差问题**  \n**分析**:  \n- **具体内容**: 现有方法（如FlexMatch、FreeMatch）仅依赖伪标签置信度评估学习状态，易受数据采样偏差或类间相似性干扰。  \n- **根源**:  \n  1. **技术瓶颈**: 单指标（如平均置信度）难以全面反映模型学习进展；  \n  2. **问题复杂性**: 类别不平衡或特征重叠时，伪标签可靠性下降。  \n\n#### 挑战三：**低置信度伪标签的潜在价值浪费** ...\n\n### Baseline 总结\n**总结1** (来源: 2406.15763v2):\nBaseline选取总结：  \n1、对比方法:  \n[FixMatch]  \n[FlexMatch]  \n[Dash]  \n[FreeMatch]  \n[SoftMatch]  \n[CoMatch]  \n[SimMatch]  \n[FullMatch]  \n\n2、选取理由:  \n作者选择的Baseline覆盖了半监督学习（SSL）领域三大主流技术路线及最新进展：  \n- **技术路线覆盖性**：包括基于固定阈值（FixMatch）、课程学习启发的动态阈值（FlexMatch、Dash、FreeMatch）、软样本加权（SoftMatch）、对比损失约束（CoMatch、SimMatch）以及混合方法（FullMatch）。  \n- **代表性**：FixMatch是经典SOTA方法，结合了一致性正则化和伪标签技术；FlexMatch等动态阈值方法解决了FixMatch数据利用率低的问题；CoMatch/SimMatch代表对比学习路线；FullMatch是与本文同期工作的最新对比基线。  \n- **创新对比需求**：通过对比不同阈值策略（固定/动态/全局/类别自适应）和约束方式（阈值过...\n\n**总结2** (来源: 2406.15763v2):\nBaseline选取总结：  \n1、对比方法:  \n[FixMatch]  \n[FlexMatch]  \n[Dash]  \n[FreeMatch]  \n[SoftMatch]  \n[CoMatch]  \n[SimMatch]  \n[FullMatch]  \n\n2、选取理由:  \n作者选择的Baseline覆盖了半监督学习（SSL）领域三大主流技术路线及最新进展：  \n- **技术路线覆盖性**：包括基于固定阈值（FixMatch）、课程学习启发的动态阈值（FlexMatch、Dash、FreeMatch）、软样本加权（SoftMatch）、对比损失约束（CoMatch、SimMatch）以及混合方法（FullMatch）。  \n- **代表性**：FixMatch是经典SOTA方法，结合了一致性正则化和伪标签技术；FlexMatch等动态阈值方法解决了FixMatch数据利用率低的问题；CoMatch/SimMatch代表对比学习路线；FullMatch是与本文同期工作的最新对比基线。  \n- **创新对比需求**：通过对比不同阈值策略（固定/动态/全局/类别自适应）和约束方式（阈值过...\n\n**总结3** (来源: Allocation_Strategies_for_Disaggregated_Memory_in_HPC_Systems):\nBaseline选取总结：  \n1、对比方法:  \n- **Aggregated**（聚合内存分配策略）  \n- **Oldest-First**（基于任务运行时长优先的启发式算法）  \n- **Largest-First**（基于任务内存需求优先的启发式算法）  \n- **Priority**（动态优先级驱动的在线调度算法）  \n- **Stochastic**（基于统计内存行为的静态分配算法）  \n\n2、选取理由:  \n- **技术路线覆盖性**: 选择的Baseline涵盖了不同技术路径，包括传统聚合分配（Aggregated）、简单启发式规则（Oldest-First/Largest-First）、动态在线调度（Priority）和静态统计优化（Stochastic），以全面评估内存解耦场景下的性能差异。  \n- **SOTA代表性**: Priority和Stochastic是论文针对动态内存模式提出的新算法，需与经典启发式方法（Oldest-First/Largest-First）及默认聚合策略对比，验证其先进性。  \n- **场景适配验证**: Aggregated作为“...\n\n\n### 研究趋势分析\n**Challenges 趋势**:\n- 技术趋势: 数据稀缺技术广泛应用\n- 研究模式:  在40/5篇论文中被提及(800.0%), e在35/5篇论文中被提及(700.0%), n在33/5篇论文中被提及(660.0%)\n\n",
  "context_length": 5013
}