结构化RAG上下文
================================================================================
【方法 部分的上下文】
--------------------------------------------------
上下文长度：2436 字符

上下文内容：
### Methodology 总结
**总结1** (来源: 2309.11930v2):
方法概述：  
1、方法名称: **LPS (Learning Pace Synchronization)**  
2、核心思想: 通过自适应边际损失（Adaptive Margin Loss）和伪标签对比聚类（Pseudo-Label Contrastive Clustering），同步模型对已知类别（seen classes）和新类别（novel classes）的学习速度，解决开放世界半监督学习（OpenSSL）中类别不平衡和未知类别聚类的问题。  

3、主要流程/组件  
**组件/步骤一: 自适应边际损失（Adaptive Margin Loss, L_AM）**  
- **功能**: 动态调整不同类别的分类边界，抑制已知类别的过快学习，促进新类别的学习。  
  - 基于当前模型预测的类别分布估计（π），通过KL散度计算类别特异性负边际（∆_j）。  
  - 对高置信度的未标注数据生成伪标签，与标注数据共同优化损失。  

**组件/步骤二: 伪标签对比聚类（Pseudo-Label Contrastive Clustering, L_PC）**  
- **功能**...

**总结2** (来源: 2309.11930v2):
方法概述：  
1、方法名称: **LPS (Learning Pace Synchronization)**  
2、核心思想: 通过自适应边际损失（Adaptive Margin Loss）和伪标签对比聚类（Pseudo-Label Contrastive Clustering），同步模型对已知类别（seen classes）和新类别（novel classes）的学习速度，解决开放世界半监督学习（OpenSSL）中类别不平衡和未知类别聚类的问题。  

3、主要流程/组件  
**组件/步骤一: 自适应边际损失（Adaptive Margin Loss, L_AM）**  
- **功能**: 动态调整不同类别的分类边界，抑制已知类别的过快学习，促进新类别的学习。  
  - 基于当前模型预测的类别分布估计（π），通过KL散度计算类别特异性负边际（∆_j）。  
  - 对高置信度的未标注数据生成伪标签，与标注数据共同优化损失。  

**组件/步骤二: 伪标签对比聚类（Pseudo-Label Contrastive Clustering, L_PC）**  
- **功能**...

**总结3** (来源: Simulation_of_Large-Scale_HPC_Storage_Systems_Challenges_and_Methodologies):
方法概述：
1、方法名称: FIVES (Simulator for Scheduling on Storage Systems at Scale)

2、核心思想: 
FIVES是一个面向高性能存储系统的仿真框架，通过模块化设计和自动化校准，实现存储系统行为的高效准确模拟。其核心思想是通过抽象化硬件平台和作业模型，结合贝叶斯优化进行参数校准，在保证仿真可扩展性的同时最大化模拟精度。

3、主要流程/组件
组件/步骤一: 仿真架构设计
- 采用三层概念架构：作业管理器(创建/提交作业)、协调器(资源调度)、基础设施(硬件平台模拟)
- 基于WRENCH和SimGrid框架实现，新增复合存储服务(CSS)组件支持分布式存储模拟

组件/步骤二: 参数校准系统
- 使用贝叶斯优化(BO)自动校准17个关键参数（平台带宽、作业文件数、节点参与数等）
- 定义MAE损失函数评估仿真精度：真实与模拟I/O时间的百分比差异均值
- 采用带宽分类策略（快/常规/慢作业）处理数据异质性

组件/步骤三: 磁盘争用模型
- 开发经验性对数模型：bw = bw_max * (1/(C + log n))
...


### 研究趋势分析
**Methodology 趋势**:
- 研究模式:  在32/5篇论文中被提及(640.0%), '在26/5篇论文中被提及(520.0%), n在26/5篇论文中被提及(520.0%)


### 参考原文
**论文 2309.11930v2 - 方法 章节**:
片段1: 3 Adaptive Synchronizing
To start with, we describe the proposed adaptive marginal loss which regularizes the learning pace of seen classes to synchronize the learning pace of the model. Conventionally, the margin is defined as the minimum distance of the data to the classification boundary.
片段2: For a sample (x, y), we have:
∆(x, y) = f (x) y − max j̸ =y f (x) j (1)
Instead of employing a fixed margin, LDAM introduces a class-specific margin, where the margin between rare classes and other classes is larger than the margin for frequent classes, for tackling class-imbalanced data. Specifical...



