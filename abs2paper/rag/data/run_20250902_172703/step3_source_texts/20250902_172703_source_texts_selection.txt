选定的最相关原文章节
================================================================================
【论文 Retrospection_on_the_Performance_Analysis_Tools_for_Large-Scale_HPC_Programs】
--------------------------------------------------
>>> 方法 章节 <<<
III. METHODOLOGY
Although there are widely adopted performance tools in large-scale HPC systems, there is no existing work that provides a comprehensive study of the existing performance analysis tools for performance analysis of large-scale HPC programs according to our knowledge. In this section, we describe our testbed of the performance analysis tools and the corresponding comparable metrics on the common concerns for performance analysis.We evaluate a homebuilt cluster with hardware and software configuration as shown in Table . Specifically, the cluster consists of 32 computing nodes. Each node is equipped with one 36-core Intel Golden 6240 processor running at 2.60 GHz frequency. There is 384 GB of memory for each node. All nodes are connected with a 100 Gbps network. The storage has over 160 Gbps I/O bandwidth. All programs are compiled with GCC 9.4.0 with -O3 compiler optimizations. We use OpenMPI 4.0.7 for MPI communication.We use OpenMPI 4.0.7 for MPI communication. For a comprehensive comparison of the existing large-scale performance tools, we evaluate the following performance tools that are well-known for providing rich performance guidance for large-scale HPC programs:
• HPCToolkit is a sampling-based performance analysis tool that provides insights into the performance bottlenecks of the target programs. HPCToolkit can collect traces and generate profiles from the collected data.HPCToolkit can collect traces and generate profiles from the collected data. In our evaluation, we leverage the default sampling rate at 300Hz per event type for HPCToolkit. • TAU is an instrumentation-based performance analysis tool that provides the ability to collect wide ranges of performance data. TAU requires re-compilation with its own compilation toolchains (e.g., tau cc) to obtain the detailed function traces.For trading off the overhead and abundance of data collection, TAU provides both profile run (denoted as TAU-P) that do not collect detailed function traces and trace run (denoted as TAU-T) that collect detailed function traces.In our evaluation, we enable the auto event throttling with default settings (numcalls > 100, 000 && usecs/call < 10) for the TAU-P run and only collects MPI functions for TAU-T. • Scalasca is an instrumentation-based performance analysis tool that targets scalable performance tracing and analysis. Scalasca requires re-compilation with instrumentation compiler toolchains (e.g., scorep ) to obtain the detailed function traces.Scalasca requires one profile run (denoted as Scalasca-P) before collecting the full trace (denoted as Scalasca-T) for the target programs to obtain reasonable configurations as well as function filters for lower overhead. In our evaluation, we collect without any filters for Scalasca-P and tracing with the provided configuration as well as filters that only collect MPI communication traces.The aforementioned performance tools are representative of state-of-the-art performance analysis tools and are widely adopted in the HPC community. We evaluate these tools based on the following criteria: abundance and overhead of data collection, trace analysis, hotspots analysis, scalability, and performance variance.Note that although primitive diagnosing of the above performance issues does not require full event traces, MPI communication traces are still required for several advanced root cause analysis of specific performance issues in state-of-the-art research . We evaluate these tools with the NAS Parallel Benchmarks (NPB) and the real-world application LULESH .Specifically, we use class B input for NPB-16 processors, class D for NPB-1,024 processors, and -s 40 -i 400 for LULESH in our evaluation according to the evaluation scale. For HPCToolkit, we use -t -e REALTIME -e PAPI TOT INS to enable the trace function and performance metric collection function. For TAU, we enable the auto throttling function, callpath, and communication matrix collection.For TAU, we enable the auto throttling function, callpath, and communication matrix collection. For Scalasca, to generate the filter file and trace configuration (e.g., max buffer size), we first use scalasca -analyze and scalascaexamine to profile the target benchmarks and applications. We use the same input datasets for all the performance analysis tools to ensure a fair comparison. The performance analysis tools can be evaluated with two aspects: data collection and analysis capabilities.We evaluate the performance analysis tools based on the following criteria:
1) Data Collection: For both primitive and advanced performance analysis, developers first need to collect enough performance data from the target program execution. In this paper, we call the ability to collect different types of performance data as abundance of data collection. The abundance of data collection is essential for supporting various useful and important performance analysis tasks.For large-scale homogeneous clusters, we investigate whether the performance analysis tools can collect the following performance data: • CPU Performance Counter: The ability to collect CPU performance counter data, often represented as PAPI or Linux perf events. It can provide deep insights into the hardware performance bottlenecks of the target program (e.g., top-down microarchitecture analysis ).Besides, the time and storage overhead of data collection is another important aspect for evaluating the performance analysis tools. Specifically, time overhead is measured as the execution time of the target program with and without the performance analysis tools. Storage overhead is measured as the storage space required to store the collected performance data.Storage overhead is measured as the storage space required to store the collected performance data. The overhead of data collection is essential for minimizing the impact of the performance analysis tools on the target program execution. The higher time overhead leads to significant time and commercial costs for performance analysis and limits the applicability of the performance analysis tools at a large scale.The higher storage overhead leads to the difficulty of storing and analyzing the collected performance data, which may further result in unexpected fails due to exceeding the storage capacity (e.g., maximum 1 TB storage budgets adopted in our evaluated homebuilt HPC cluster). 2) Analysis Capabilities: For performance analysis capabilities of the evaluated performance analysis tools, it is difficult to provide a quantitative metric for comparison.Instead, we provide the pros and cons of the evaluated performance analysis tools based on the following common performance analysis tasks for large-scale HPC programs: We evaluate the performance analysis tools with the trace analysis capabilities with their built-in visualization GUI interface for intuitive comparison. • Hotspot Analysis -Hotspots indicate functions or code regions that consume the most significant time or resources.For a fair comparison, we run the default hotspot analysis within each evaluated performance tool and provide the top few functions reported by these tools. Apparently, for the same program execution with the same input at the same scale, the analysis results of the top 10 hotspots should be similar, which gains the most attention for developers to further investigate the performance optimization opportunities.• Scalability Analysis -Scalability analysis aims to identify the causes of poor performance scalability of target program execution at different scales. Poor scalability can lead to low utilization of large-scale computation resources and even the inability to obtain higher performance even running with more nodes. We evaluate the performance analysis tools with 16 and 1024 processes to evaluate the tool's ability to analyze the scalability issues.• Performance Variance Analysis -Performance variance indicates the significant performance slowdown of the different execution instances of the same program. For a fair comparison, we run the program with and without injected disturbances to evaluate their ability to identify the performance variance. For each evaluated analysis capability, we qualitatively investigate the intuitiveness, accuracy, and completeness of the analysis results provided by the performance analysis tools.Specifically, the analysis results should be intuitive for developers to understand the performance issues of the target program. The analysis results should be accurate to provide reliable guidance for optimizing the performance of the target program. Besides, the analysis results should also be actionable to provide optimization guidance for the target program, such as providing problematic code locations, calling contexts, and comprehensive diagnosis of root causes.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
【论文 Automatic_Code_Generation_for_High-Performance_Graph_Algorithms】
--------------------------------------------------
>>> 实验评价 章节 <<<
V. EVALUATION
In this section, we present the performance of automatically generated code for some of the sparse linear-algebra kernels and the graph algorithms. We compare our performance against LAGraph which contains an assortment of graph algorithms implemented using linear algebra. LAGraph employs the SuiteSparse:GraphBLAS library for sparse linear algebra kernels.LAGraph employs the SuiteSparse:GraphBLAS library for sparse linear algebra kernels. To show the performance benefit of our work, we evaluate two sets of benchmarks: 1) simple sparse kernels commonly used in graph algorithm which consists of sparse matrixsparse matrix multiplication (SpGEMM) and sparse matrixsparse matrix elementwise multiplication operations, 2) two representative graph algorithms TC and BFS.Triangle Counting (TC) algorithm counts the number of triangles given an input undirected graph G. This problem was also part of the GraphChallenge competition . A triangle is defined to be a set of three mutually adjacent Algorithm 2: BFS expressed in linear algebra. Input: Graph, A, represented as an adjacency matrix; f , the frontier vector. s, the source vertex; n, the number of vertices Output: l, a vector of visited vertices' level.s, the source vertex; n, the number of vertices Output: l, a vector of visited vertices' level. The naive way to count the number of triangles in a graph represented by adjacency matrix A is to perform the following operation: A 3 and take a trace of the resulting matrix. The final answer is obtained by dividing the obtained scalar by 6 to account for already counted triangles. The naive way is extremely computationally expensive since A 3 is most likely to become dense.The naive way is extremely computationally expensive since A 3 is most likely to become dense. There are various other linear algebra-based algorithms that propose better implementations as compared to the naive approach. For instance, element-wise multiplication (represented as . * ) can be used instead of the second matrix multiplication operation in the naive approach, i.e., (A 2 ). * A. This is followed by performing a reduction operation across the matrix to obtain the final triangle count.The algorithm with element-wise operation is similar to performing the matrix multiplication with A as a mask. In this way, the calculations in A 2 that are subsequently masked out by the element-wise operation are not performed in the first place. Multiple algorithms for triangle counting exist -the linear algebra formulation of the tested TC algorithms is described in Algorithm 1.These formulations include two linear algebra operations, a matrix-matrix multiplication, and an elementwise multiplication, followed by a reduction. As discussed earlier, the element-wise operation in each expression can be replaced by a masking operation. Finally, some algorithms utilize the lower and upper triangular parts of the adjacency matrix to limit the computational complexity of the problem.Breadth-First Search (BFS) algorithm traverses nodes of the graph structure to understand a particular property, such as the level of reachability starting from a source vertex. The search starts from the source vertex and reaches all vertices at the current depth level before moving to vertices at the next level. We describe the linear algebra formulation of BFS in Algorithm 2.We describe the linear algebra formulation of BFS in Algorithm 2. There are mainly two linear algebra operations: a masking operation that assigns levels to the level vector under the mask of f , the frontier vector, including visited vertices at the current level; and a sparse vector-sparse matrix multiplication operation, where the visited vertices are updated across iterations, and a masking operation, which concentrates the search on unvisited vertices.We perform all experiments on an Intel Xeon Skylake Gold 6126 processor with 192 GB DRAM memory. We Fig. : Performance of semiring in our compiler as normalized to SuiteSparse:GraphBLAS when the output matrix is in a jumbled state. use llvm-13 with optimization level −O3 for compiling the LAGraph and SuiteSparse:GraphBLAS (version 7.3.2) packages. The code generated by our compiler is lowered to LLVM-IR using MLIR. The mlir-cpu-runner utility is used to run the LLVM-IR code on the CPU.The mlir-cpu-runner utility is used to run the LLVM-IR code on the CPU. The generated LLVM-IR code is further optimized using LLVM level −O3 optimizations. This includes the ability of the LLVM backend to apply vectorizations when possible. That said, we do not fully explore the opportunity of MLIR to generate vectorized code using the vector dialect and this is part of future work. The sparse inputs used for this paper are from SuiteSparse, and their characteristics are listed in Table .The inputs rma10 and scircuit are not used in the triangle counting evaluation because they are not symmetric. All the sparse inputs are stored in the CSR format. The output of this work is also produced in the CSR format. All results reported are the average of 10 runs. Unless otherwise noted, the evaluation uses sequential execution. Results for parallel execution are reported in Figure .Results for parallel execution are reported in Figure . We perform a detailed performance evaluation of the generated code by our compiler against the SuiteSparse:GraphBLAS library for common kernels utilized in most graph algorithms. We also evaluate various triangle counting and breadth-first  Semiring Performance. Several graph algorithms can be represented using semirings instead of traditional linear algebra operator to improve performance efficiency.To evaluate the performance of the semirings operation, we make a comparison between our compiler and SuiteSparse:GraphBLAS library for combination of SpGEMM with multiple operation pairs (semirings) and different sparse inputs. In this evaluation, the workspace transformation is applied to improve data locality and avoid irregular access to sparse data structures. Figure shows the performance of semiring in the compiler when the output is in the jumbled state.Figure shows the performance of semiring in the compiler when the output is in the jumbled state. If the matrix is returned as jumbled, the column indices in any given row may appear out of order . The sort is left pending. Some graph algorithms can tolerate jumbled matrices on input, so it is faster to generate jumbled output to be given as input to the subsequent operation. As shown in Figure , our work performs better than SuiteSparse:GraphBLAS for all the sparse inputs, up to 3.7× speedup.The plus-times semiring is another representation of sparse matrix-sparse matrix multiplication. The plus-pair semiring replaces the multiplication operation with pair operation in sparse matrix operation contributing to improved performance since the pair operation is a trivial operation as we operate on non-zero elements. Figure shows the performance of the same set of benchmarks while the output is in a unjumbled state in which the indices always appear in ascending order.The performance of sparse operations depends on the performance of the sorting algorithm if the resulting matrix must have indices sorted in each row. Our compiler currently uses the standard C++ quicksort algorithm (std::qsort) as compared to the advanced sorting algorithm implemented in SuiteSparse:GraphBLAS. Hence, the performance of the compiler significantly drops (1.75×) as compared to the jumbled case in Figure . We plan to improve the sorting algorithm in future work. The rest Fig.We plan to improve the sorting algorithm in future work. The rest Fig. : Performance of masked SpGEMM (i.e., B<A> = A * A ) as compared to SuiteSparse:GraphBLAS. of the paper represents the result when the output matrix is in an unjumbled state. Overall Performance.Overall Performance. First, we evaluate the performance of sparse matrix-sparse matrix operation with plus-times semiring (i.e., SpGEMM) using the input matrix as a mask inside both our compiler and SuiteSparse:GraphBLAS when all the optimizations are enabled in the compiler. Figure illustrates the speedup obtained by code generated by the compiler as compared to library-based realization of the same SpGEMM operations.The figure shows that the our performance is better than SuiteSparse:GraphBLAS across various inputs, and the compiler obtains up to 2.19× speedup, with 1.48× geometric mean speedup. Masking optimization avoids unneeded computations based on the requirements of the graph algorithms. Specifically, masking intervenes in the basic sparse vector-sparse matrix multiplication that is performed for each row of the other input matrix.At each iteration, the corresponding sparse row from the mask matrix is converted to an intermediate dense vector to support random O(1) access to the elements in the mask. This accelerates the skipping of computations that do not need to be performed. The workspace transformation also provide some additional speedup as compared to SuiteSparse:GraphBLAS, which is evaluated further in this Section.Next, we evaluate the performance of four different Trian- In our experiments, we evaluate the implementation of these algorithms with the plus-pair semiring instead of SpGEMM operation (i.e., plus-times semiring) and with masking instead of the element-wise multiplication operation. These experiments demonstrate the benefit of all optimizations proposed in this paper.These experiments demonstrate the benefit of all optimizations proposed in this paper. The cost to determine the strict lower and upper triangular parts of the input matrix is not included in the performance evaluations. Figure shows the performance comparison of all four Triangle Counting algorithms implemented within our compiler and LAGraph with masking.It shows that our work can achieve up to 2.52× speedup, and 1.91×, 1.54×, 1.65×, and 1.68× geometric mean speedup over LAGraph for Burkhardt, Cohen, SandiaLL, and Sandi-aUU algorithms across all input matrices, respectively. The performance breakdown of various optimizations proposed inside the compiler is discussed later in this Section.In the results in Figure , when the input matrices have a relatively high density (e.g., bcsstk17), we do observe diminishing returns for algorithms that use sparser matrices such as lower Fig. A quantification of the performance gain obtained by applying our workspace and masking optimizations in a kernel that consists of the SpGEMM and element-wise multiplication operations (i.e., (A * A) . * A ). WS stands for Workspace Transformation. and upper triangular.* A ). WS stands for Workspace Transformation. and upper triangular. We attribute this to our masking implementation that is based on the push method . The pushbased masking is more suitable for masks with higher density, whereas, pull-based masking is suitable for sparser masks. We plan to investigate our design choices in future work. Moreover, we expect to have better performance as we use an advanced sorting algorithm for unjumbled output matrices.Next, Figure illustrates the performance comparison of BFS implementation between our work and LAGraph. It shows that our work can achieve up to 9.05× speedup over LAGraph and geometric mean 2.57× speedup for all input matrices. The main speedup comes from our use of the workspace transformation. The major computation in the BFS level algorithm involves finding the next frontier in each iteration (see algorithm 2). It is achieved by performing a sparse vectormatrix multiplication with masking.It is achieved by performing a sparse vectormatrix multiplication with masking. Workspace transformation can avoid the expensive insertion into the middle of sparse data structures and performs asymptotically faster. Parallel Performance. Figure shows our parallel performance of four Triangle Counting algorithm with masking compared with LAGraph. All experiments use 24 threads.All experiments use 24 threads. It shows that our work can achieve up to 4.63× speedup over LAGraph among all used input matrices, besides up to 2.02× speedup among the two large inputs Orkut and LiveJournal. Our work also achieves 2.40×, 1.41×, 1.36×, and 1.48× geometric mean speedup over LAGraph for Burkhardt, Cohen, SandiaLL, and SandiaUU algorithms among all input matrices, respectively.The results demonstrate that the compiler can achieve high-performance parallelization, thanks to the twophase computation. Performance Benefit Breakdown. This section discusses the performance gains obtained by each proposed optimization, including the workspace transformation, semiring, and masking. The base case is implemented as a kernel that consists of the SpGEMM operations followed by the element-wise multiplication operation.This base version does not include any of the optimizations proposed in this paper. Then, we Fig. : Performance breakdown of triangle counting algorithm SandiaLL implemented by our compiler. evaluate the performance gain of each of the optimizations. First, we apply the workspace transformation to improve data locality for sparse linear algebra operations. The masking optimization is then applied to eliminate the element-wise multiplication operation that succeeds the SpGEMM operation.The masking optimization improves the performance by skipping computations that are not needed since they will result in multiplication by zero in the element-wise multiplication operation. Figure shows the performance progression as incrementally the workspace and masking optimizations are applied to the base case. The workspace transformation has 20.60× geometric mean speedup over the base case across all inputs. The masking operation can be seen to add another 1.86× speedup.The masking operation can be seen to add another 1.86× speedup. It can be clearly seen that the proposed optimizations are important and lead to substantial gains compared to the base case, e.g., in most cases over 90% of the speedup is due to the workspace and masking optimizations. We highlight that the masking optimization is also important for low memory usage since we had difficulty running the relatively larger LiveJournal and Orkut inputs on our system.We also profile the performance breakdown of the proposed optimizations for various triangle counting algorithms. Figure shows the results of SandiaLL only for brevity. Other algorithms show the same trend. The base cases for these algorithms are shown in Algorithm 1, whereby a SpGEMM is followed by an element-wise multiplication operation, including reduction.As done earlier, the SpGEMM and element-wise multiplication operations can have the workspace transformation and masking optimizations applied incrementally. An additional performance advantage can be gained by replacing the SpGEMM operation with a semiring of plus-pair. Specifically, replacing the multiplication operation in SpGEMM with a pair operation tends to bring in a performance advantage of around 5% across all inputs for four triangle counting algorithms.Note that semiring operations are essential to support state-of-the-art graph algorithms , , . Although the performance advantage is dependent on the sparsity of the input matrices, the proposed optimizations can be safely and effectively applied to achieve some benefit across multiple application domains that utilize sparse computation.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
