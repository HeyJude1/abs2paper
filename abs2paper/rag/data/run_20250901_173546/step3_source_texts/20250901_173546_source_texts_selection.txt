选定的最相关原文章节
================================================================================
【论文 Simulation_of_Large-Scale_HPC_Storage_Systems_Challenges_and_Methodologies】
--------------------------------------------------
>>> 方法 章节 <<<
IV. THE FIVES SIMULATOR A. FIVES conceptual architecture
We consider that an HPC system comprises three main conceptual elements as depicted in Figure : a Job Manager, an Orchestrator and an Infrastructure. The Job Manager receives user requests with compute resources and time demands, creates a job for each request, and submits jobs for execution to the Orchestrator. The Orchestrator implements scheduling policies by which jobs are assigned to particular hardware resources in the Infrastructure.We describe these conceptual elements in more details hereafter. 1) Infrastructure: The Infrastructure represents a simulated hardware platform, such as the one depicted in Figure . In this example the compute partition consists of homogeneous compute nodes interconnected via a Dragonfly network topology, and is defined by the number of compute nodes, their compute speed and number of cores, and the bandwidth and latency of the network links.Other network topologies are already available in SimGrid, and more can be manually implemented as needed. The storage partition comprises homogeneous storage nodes interconnected via a star topology, and is defined by the number of storage nodes and the bandwidth and latency  2) Job Manager: The Job Manager is in charge of interpreting the resource demands in each user request, creating jobs, and sending these jobs to the Orchestrator.We define a job as a set of compute and I/O operations caused by the execution of one or more applications. These are to be executed on a set of resources on the Infrastructure (specified as a desired number of nodes and cores) that are requested for a given time period. A reservation corresponds to a job for which the requested resources have been allocated. During a reservation one or more applications may run, each with a known start and end timestamp within the bounds of the reservation.3) Orchestrator: The Orchestrator is responsible for scheduling submitted jobs and their I/O operations. It thus must employ both a job scheduling algorithm and a striping policy for allocating storage resources and distributing data to disks.Although users can provide their own implementations of these algorithms, we have already implemented several algorithms in FIVES, described in the next section, which correspond to a large spectrum of relevant use cases (i.e., classical job scheduling algorithms, the striping policy of the Lustre file system). FIVES is implemented using the WRENCH and Sim-Grid state-of-the-art simulation frameworks.FIVES is implemented using the WRENCH and Sim-Grid state-of-the-art simulation frameworks. SimGrid provides foundational simulation abstractions for sequential concurrent processes that use compute, network, and storage hardware resources, using scalable and validated simulation models. WRENCH builds on top of SimGrid to implement high-level simulation abstractions of "Services" to which "Jobs" can be submitted and perform "Actions."Using these abstractions, simulators of complex workloads and systems can be implemented with minimal effort. 1) Main Simulation Components: Figure depicts the simulation components used to implement the conceptual architecture described in Section IV-A, and shows the workflow of the simulation. The figure indicates which components are native to WRENCH and which are developed in FIVES. The entry point component of FIVES is the Controller, which manages job executions throughout the simulation.It takes as input a dataset that specifies the jobs whose executions are to be simulated on the HPC platform, with an arrival date for each job. At the onset of the simulation, the Controller creates a set of jobs to simulate based on the job dataset. It submits each job for execution to the Batch Compute Service configured to use a particular job scheduling algorithm.WRENCH already comes with several such algorithms, and in this work we use its (default) conservative backfilling implementation . The Controller acknowledges job completions until the execution of the entire workload has been simulated, and outputs timestamped job execution, I/O operation and I/O resource usage events. The Controller incurs no load during the simulation, but requires a few seconds before and after, while instantiating the jobs and computing the final metrics.The jobs submitted by the Controller are executed on a simulated HPC system such as the one shown in Figure . SimGrid provides a powerful API for describing arbitrary hardware platforms to be simulated that includes compute, network, and storage resources. Using this API, the FIVES user can implement the Infrastructure conceptual element described in the previous section for any target hardware configuration.Building on top of SimGrid, WRENCH offers high-level services implemented in simulation that manages the simulated hardware resources: a Compute Service manages compute nodes (called Compute Hosts), while a Simple Storage Service manages storage nodes (called Storage Hosts). However, the Simple Storage Service implemented in WRENCH, which answers file read/write/delete/copy requests over the network, can only run on a single host and manage file systems on disks attached to that host.To get around this limitation, which makes the framework unsuitable for modeling distributed storage systems, we introduce a new type of storage service that we have contributed to WRENCH: the Compound Storage Service (CSS). 2) Compound Storage Service: A CSS (see Figure ) aggregates and provides a high-level interface to multiple Simple Storage Services on multiple hosts.Files stored on the CSS are transparently distributed and/or striped across any subset of the Simple Storage Services, and the CSS keeps track of the location of each file and file stripe. It intercepts requests for read, write, copy or delete operations in order to redirect them to the appropriate Simple Storage Services. The CSS abstraction implements all core mechanisms for simulating a parallel file system.The CSS abstraction implements all core mechanisms for simulating a parallel file system. Custom policies, such as the striping policy, are provided through the Allocator component. This makes it possible to use the generic CSS abstraction and instantiate it to simulate a particular parallel file systems, as explained in Section V-B3. Once a simulator has been implemented, it must be instantiated to be representative of a real-world system of interest.For this study, we selected Theta at the Argonne National Laboratory, a 11.7-PetaFLOPS Cray XC40 HPC system that ran for almost 6 years until the end of 2023, producing a significant number of major scientific results. Theta featured 4,392 compute nodes (Intel KNL) interconnected via an Aries network with a Dragonfly topology. The choice of Theta was motivated by its hosting of a 10 PB Lustre file system, the most widespread storage system on Top500 machines to date.In addition, unless explicitly specified at compile time, applications running on Theta were monitored with the Darshan I/O monitoring tool, which has yielded a valuable dataset with years of I/O execution traces. An aggregated version of these traces is publicly available online . A. Background 1) Darshan: Darshan is a monitoring tool for recording the I/O performed by an application with low overhead.Information is collected at a high level of detail, down to the granularity of the data chunks written by a process. Darshan is deployed on several top-tier supercomputers, such as those at Argonne National Laboratory or at NERSC, where the majority of applications are monitored. However, for reasons specific to these institutions (confidentiality, data control), raw data is not publicly distributed.Only aggregated data is available, providing a global view of job I/Os but omitting information such as the number of files written or the number of processes that took part in I/O. Overall, for a given job, the information made available is job start and end times, volumes of data read and written, and time spent performing I/O (plus other information not relevant to this work).2) The Lustre Parallel File System: Lustre is an opensource parallel file system (PFS) that has been maintained and evolved for over twenty years. As depicted in Figure , a Lustre file system consists of I/O servers called OSS (Object Storage Servers) and disks called OST (Object Storage Targets).The number of OSTs managed by an OSS is variable ranging from one on the Theta supercomputer at Argonne National Laboratory, to three on Frontier at Oak Ridge National Laboratory, or higher on other systems. Following the same model, metadata is managed by metadata servers (MDS) and targets (MDT). All storage resources are accessed via so-called LNET nodes, which correspond to the I/O forwarding nodes found in many HPC systems.These LNET nodes are generally part of the interconnection network and access the Lustre file system via a dedicated network interface. Fig. : Lustre architecture Data written to a Lustre file system is striped across the OSTs, allowing performance gain by aggregating bandwidth from multiple OSTs and possibly OSSs. Striping a file is a two steps process. First, Lustre creates an ordered list of usable OSTs.Striping a file is a two steps process. First, Lustre creates an ordered list of usable OSTs. Second, Lustre distributes file parts on a subset of this list according to a striping layout. Selecting and ordering which OST(s) will be used is achieved using one of two allocation strategies: round-robin or weighted. The roundrobin strategy is used in most cases.The roundrobin strategy is used in most cases. The weighted strategy implements a bias towards selecting the least used OSTs first and is triggered on rare occasions, when free-space imbalance between OSTs is above some threshold.Lustre's default striping layout takes two main parameters: the stripe count which defines the number of OSTs on which the stripes will be distributed and the stripe size, which defines the granularity at which the data is divided into chunks of identical size inside the stripes. The stripe count and the stripe size have default values, although they can be changed by the user, on a per-file or per-directory basis. On Theta, the stripe count was set to 1 and the stripe size was set to 1MB.On Theta, the stripe count was set to 1 and the stripe size was set to 1MB. Section V-B3 describes our implementation of the Lustre's striping policy in the FIVES Allocator component. Given the characteristics of the platform to be simulated and the information available regarding the jobs whose executions are to be reproduced on that platform, one must make various decisions regarding how the simulator should be instantiated.In what follows we discuss the three main decision axes when using FIVES to simulate the Theta platform and workload. 1) Job execution: When submitting a job to the Batch Compute Service, FIVES uses the real-world walltime and resource requirements of each reservation and respects the actual interval between subsequent job submissions, as available in the job trace. The start date of a job, however, is conditioned by the conservative backfilling job scheduling algorithm implemented in WRENCH.These start dates can thus be different from start dates that were driven by the specific batch scheduler configuration deployed on the real-world platform. Each job is defined by a runtime that we estimate based on the information present in the workload trace, and bound by the reservation walltime. Each I/O phase is subdivided in a variable number of individual read or write WRENCH actions, executed collectively from a subset of the job's compute nodes.Each action internally spawns multiple actual reads and writes to the storage system, depending on the job's I/O volume. The typical number of simulated reads or writes for a job can be approximated as F × H × S H , where F (1 ≤ F ≤ 10 2 ) is the number of files, H (1 ≤ H ≤ 10 1 ) is the number of participating compute nodes and S H (1 ≤ S H ≤ 10 2 ) is the number of stripes accessed by each host, similar for all hosts ±1.S H is bounded by the number of OSTs in use and the number of file chunks allowed on each OST for simulation scalability reasons (see Section V-B3). This approximation is an underestimation of the number of I/O accesses that jobs performed on the real system. Despite this underestimation, our results demonstrate that our simulations are in line with real-world behaviors. We assume that all reads and writes from a job are on the PFS.We assume that all reads and writes from a job are on the PFS. Some might be local since the Theta compute nodes feature node-local SSD, but the workload trace does not distinguish between local and non-local I/O. Furthermore, for a given job, the trace does not specify the number of files (F ) and the number of compute nodes that are involved in I/O operations (H). We consider that F and H are parameters that must be calibrated based on observed real-world job executions.We assume that, for all jobs, the ratio of the total I/O volume to the total number of files, and the ratio of the number of compute nodes involved in I/O operations to the number of compute nodes allocated to the job, are based on fixed constants for reads and writes. In other words, we need to calibrate only 4 parameters (2 for reads, and 2 for writes), instead of 4n parameters where n is the number of jobs, which would render the calibration problem intractable due to high dimensionality.This choice leads to a smaller range of possible I/O behaviors among the simulated jobs, which at the moment we consider better than to allow more randomness, as our data doesn't currently provide us with details on the matter. Making assumptions and abstracting away certain I/O behaviors is unavoidable because the information contained in I/O monitoring datasets is not comprehensive.Some of these assumptions, however, cause simulated workloads to be less heterogeneous than their real-world counterparts. 2) Disk contention model: SimGrid implements a default naive fair-share model for disk bandwidth, but users are advised to provide their own model for bandwidth degradation as a function of the number of concurrent I/O operations.This degradation depends on the mix of read and write operations and occurs both on HDDs and SSDs, with various behaviors that depend on many architecture-and workloadspecific characteristics , . Producing a general such model is outside the scope of this paper. Instead, we developed an empirical model derived from experiments we conducted on Seagate ST1000NX0443 SATA HDDs.The results (omitted due to space limitations) show that going from a single access to concurrent accesses causes an initial sharp decrease in bandwidth. But as concurrency increases, contention from each additional I/O operation has less impact and the bandwidth deterioration curve flattens out. It turns out that this behavior is modeled accurately (for our experimental results) using a model that includes a logarithmic component.Specifically, in FIVES, we model the instantaneous bandwidth (read or write) of a disk as:
bw = bw max * 1 C + log n ,
where bw max is the maximum achievable disk bandwidth, n is the number of ongoing concurrent I/O operations, and C is a constant. Difference choices for the C value makes the model more linear or more logarithmic. Values for bw max and C must then be calibrated based on observed real-world job executions.Values for bw max and C must then be calibrated based on observed real-world job executions. 3) Striping Policy: The Allocator component provides the Compound Storage Service with a striping policy. Since Theta's storage system is a Lustre PFS, we have implemented in FIVES the internal striping policy based on Lustre's open source code.It handles both round-robin and weighted allocation strategies, with recommended default parameters for choosing between strategies, and accepts stripe size and stripe count parameters. It is important to note that the Darshan data source used in this study does not contain the stripe count and stripe size parameters used for each file.In our simulations, we resort to calibrating these values based on the following rules:
• stripe count: All jobs with a mean I/O bandwidth (bw job ) below a calibrated threshold use the same static default value for all I/Os (set to 1 in this paper as it is the default stripe count on Theta). Jobs above the threshold use a dynamic value computed as alt stripe count × bw job /threshold. The alt stripe count and the threshold values are different for reads and writes.The alt stripe count and the threshold values are different for reads and writes. • stripe size: This value is manually configured in the range [50, 100] MB. For simulation scalability reasons, this is higher than Lustre's default value of (1MB) to limit the number of simulation objects being created. For the same reason, FIVES can be configured so that the number of file parts on each OST is bounded by a user-specified values (F OST ).If using the stripe size default value in conjunction with a large file size L ends up exceeding F OST , the stripe size is instead set to L / (F OST × stripe count) on a per-job basis. We pick F OST so that one invocation of FIVES in our experimental evaluations takes at most 20min on a 2.5GHz core.Each simulation model must abstract away some of the details of the real-world component it simulates for two reasons that can apply simultaneously: (i) because some of the details of the real-world system and/or workload are unknown and (ii) because of simulation scalability concerns.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
【论文 Automatic_Code_Generation_for_High-Performance_Graph_Algorithms】
--------------------------------------------------
>>> 实验评价 章节 <<<
V. EVALUATION
In this section, we present the performance of automatically generated code for some of the sparse linear-algebra kernels and the graph algorithms. We compare our performance against LAGraph which contains an assortment of graph algorithms implemented using linear algebra. LAGraph employs the SuiteSparse:GraphBLAS library for sparse linear algebra kernels.LAGraph employs the SuiteSparse:GraphBLAS library for sparse linear algebra kernels. To show the performance benefit of our work, we evaluate two sets of benchmarks: 1) simple sparse kernels commonly used in graph algorithm which consists of sparse matrixsparse matrix multiplication (SpGEMM) and sparse matrixsparse matrix elementwise multiplication operations, 2) two representative graph algorithms TC and BFS.Triangle Counting (TC) algorithm counts the number of triangles given an input undirected graph G. This problem was also part of the GraphChallenge competition . A triangle is defined to be a set of three mutually adjacent Algorithm 2: BFS expressed in linear algebra. Input: Graph, A, represented as an adjacency matrix; f , the frontier vector. s, the source vertex; n, the number of vertices Output: l, a vector of visited vertices' level.s, the source vertex; n, the number of vertices Output: l, a vector of visited vertices' level. The naive way to count the number of triangles in a graph represented by adjacency matrix A is to perform the following operation: A 3 and take a trace of the resulting matrix. The final answer is obtained by dividing the obtained scalar by 6 to account for already counted triangles. The naive way is extremely computationally expensive since A 3 is most likely to become dense.The naive way is extremely computationally expensive since A 3 is most likely to become dense. There are various other linear algebra-based algorithms that propose better implementations as compared to the naive approach. For instance, element-wise multiplication (represented as . * ) can be used instead of the second matrix multiplication operation in the naive approach, i.e., (A 2 ). * A. This is followed by performing a reduction operation across the matrix to obtain the final triangle count.The algorithm with element-wise operation is similar to performing the matrix multiplication with A as a mask. In this way, the calculations in A 2 that are subsequently masked out by the element-wise operation are not performed in the first place. Multiple algorithms for triangle counting exist -the linear algebra formulation of the tested TC algorithms is described in Algorithm 1.These formulations include two linear algebra operations, a matrix-matrix multiplication, and an elementwise multiplication, followed by a reduction. As discussed earlier, the element-wise operation in each expression can be replaced by a masking operation. Finally, some algorithms utilize the lower and upper triangular parts of the adjacency matrix to limit the computational complexity of the problem.Breadth-First Search (BFS) algorithm traverses nodes of the graph structure to understand a particular property, such as the level of reachability starting from a source vertex. The search starts from the source vertex and reaches all vertices at the current depth level before moving to vertices at the next level. We describe the linear algebra formulation of BFS in Algorithm 2.We describe the linear algebra formulation of BFS in Algorithm 2. There are mainly two linear algebra operations: a masking operation that assigns levels to the level vector under the mask of f , the frontier vector, including visited vertices at the current level; and a sparse vector-sparse matrix multiplication operation, where the visited vertices are updated across iterations, and a masking operation, which concentrates the search on unvisited vertices.We perform all experiments on an Intel Xeon Skylake Gold 6126 processor with 192 GB DRAM memory. We Fig. : Performance of semiring in our compiler as normalized to SuiteSparse:GraphBLAS when the output matrix is in a jumbled state. use llvm-13 with optimization level −O3 for compiling the LAGraph and SuiteSparse:GraphBLAS (version 7.3.2) packages. The code generated by our compiler is lowered to LLVM-IR using MLIR. The mlir-cpu-runner utility is used to run the LLVM-IR code on the CPU.The mlir-cpu-runner utility is used to run the LLVM-IR code on the CPU. The generated LLVM-IR code is further optimized using LLVM level −O3 optimizations. This includes the ability of the LLVM backend to apply vectorizations when possible. That said, we do not fully explore the opportunity of MLIR to generate vectorized code using the vector dialect and this is part of future work. The sparse inputs used for this paper are from SuiteSparse, and their characteristics are listed in Table .The inputs rma10 and scircuit are not used in the triangle counting evaluation because they are not symmetric. All the sparse inputs are stored in the CSR format. The output of this work is also produced in the CSR format. All results reported are the average of 10 runs. Unless otherwise noted, the evaluation uses sequential execution. Results for parallel execution are reported in Figure .Results for parallel execution are reported in Figure . We perform a detailed performance evaluation of the generated code by our compiler against the SuiteSparse:GraphBLAS library for common kernels utilized in most graph algorithms. We also evaluate various triangle counting and breadth-first  Semiring Performance. Several graph algorithms can be represented using semirings instead of traditional linear algebra operator to improve performance efficiency.To evaluate the performance of the semirings operation, we make a comparison between our compiler and SuiteSparse:GraphBLAS library for combination of SpGEMM with multiple operation pairs (semirings) and different sparse inputs. In this evaluation, the workspace transformation is applied to improve data locality and avoid irregular access to sparse data structures. Figure shows the performance of semiring in the compiler when the output is in the jumbled state.Figure shows the performance of semiring in the compiler when the output is in the jumbled state. If the matrix is returned as jumbled, the column indices in any given row may appear out of order . The sort is left pending. Some graph algorithms can tolerate jumbled matrices on input, so it is faster to generate jumbled output to be given as input to the subsequent operation. As shown in Figure , our work performs better than SuiteSparse:GraphBLAS for all the sparse inputs, up to 3.7× speedup.The plus-times semiring is another representation of sparse matrix-sparse matrix multiplication. The plus-pair semiring replaces the multiplication operation with pair operation in sparse matrix operation contributing to improved performance since the pair operation is a trivial operation as we operate on non-zero elements. Figure shows the performance of the same set of benchmarks while the output is in a unjumbled state in which the indices always appear in ascending order.The performance of sparse operations depends on the performance of the sorting algorithm if the resulting matrix must have indices sorted in each row. Our compiler currently uses the standard C++ quicksort algorithm (std::qsort) as compared to the advanced sorting algorithm implemented in SuiteSparse:GraphBLAS. Hence, the performance of the compiler significantly drops (1.75×) as compared to the jumbled case in Figure . We plan to improve the sorting algorithm in future work. The rest Fig.We plan to improve the sorting algorithm in future work. The rest Fig. : Performance of masked SpGEMM (i.e., B<A> = A * A ) as compared to SuiteSparse:GraphBLAS. of the paper represents the result when the output matrix is in an unjumbled state. Overall Performance.Overall Performance. First, we evaluate the performance of sparse matrix-sparse matrix operation with plus-times semiring (i.e., SpGEMM) using the input matrix as a mask inside both our compiler and SuiteSparse:GraphBLAS when all the optimizations are enabled in the compiler. Figure illustrates the speedup obtained by code generated by the compiler as compared to library-based realization of the same SpGEMM operations.The figure shows that the our performance is better than SuiteSparse:GraphBLAS across various inputs, and the compiler obtains up to 2.19× speedup, with 1.48× geometric mean speedup. Masking optimization avoids unneeded computations based on the requirements of the graph algorithms. Specifically, masking intervenes in the basic sparse vector-sparse matrix multiplication that is performed for each row of the other input matrix.At each iteration, the corresponding sparse row from the mask matrix is converted to an intermediate dense vector to support random O(1) access to the elements in the mask. This accelerates the skipping of computations that do not need to be performed. The workspace transformation also provide some additional speedup as compared to SuiteSparse:GraphBLAS, which is evaluated further in this Section.Next, we evaluate the performance of four different Trian- In our experiments, we evaluate the implementation of these algorithms with the plus-pair semiring instead of SpGEMM operation (i.e., plus-times semiring) and with masking instead of the element-wise multiplication operation. These experiments demonstrate the benefit of all optimizations proposed in this paper.These experiments demonstrate the benefit of all optimizations proposed in this paper. The cost to determine the strict lower and upper triangular parts of the input matrix is not included in the performance evaluations. Figure shows the performance comparison of all four Triangle Counting algorithms implemented within our compiler and LAGraph with masking.It shows that our work can achieve up to 2.52× speedup, and 1.91×, 1.54×, 1.65×, and 1.68× geometric mean speedup over LAGraph for Burkhardt, Cohen, SandiaLL, and Sandi-aUU algorithms across all input matrices, respectively. The performance breakdown of various optimizations proposed inside the compiler is discussed later in this Section.In the results in Figure , when the input matrices have a relatively high density (e.g., bcsstk17), we do observe diminishing returns for algorithms that use sparser matrices such as lower Fig. A quantification of the performance gain obtained by applying our workspace and masking optimizations in a kernel that consists of the SpGEMM and element-wise multiplication operations (i.e., (A * A) . * A ). WS stands for Workspace Transformation. and upper triangular.* A ). WS stands for Workspace Transformation. and upper triangular. We attribute this to our masking implementation that is based on the push method . The pushbased masking is more suitable for masks with higher density, whereas, pull-based masking is suitable for sparser masks. We plan to investigate our design choices in future work. Moreover, we expect to have better performance as we use an advanced sorting algorithm for unjumbled output matrices.Next, Figure illustrates the performance comparison of BFS implementation between our work and LAGraph. It shows that our work can achieve up to 9.05× speedup over LAGraph and geometric mean 2.57× speedup for all input matrices. The main speedup comes from our use of the workspace transformation. The major computation in the BFS level algorithm involves finding the next frontier in each iteration (see algorithm 2). It is achieved by performing a sparse vectormatrix multiplication with masking.It is achieved by performing a sparse vectormatrix multiplication with masking. Workspace transformation can avoid the expensive insertion into the middle of sparse data structures and performs asymptotically faster. Parallel Performance. Figure shows our parallel performance of four Triangle Counting algorithm with masking compared with LAGraph. All experiments use 24 threads.All experiments use 24 threads. It shows that our work can achieve up to 4.63× speedup over LAGraph among all used input matrices, besides up to 2.02× speedup among the two large inputs Orkut and LiveJournal. Our work also achieves 2.40×, 1.41×, 1.36×, and 1.48× geometric mean speedup over LAGraph for Burkhardt, Cohen, SandiaLL, and SandiaUU algorithms among all input matrices, respectively.The results demonstrate that the compiler can achieve high-performance parallelization, thanks to the twophase computation. Performance Benefit Breakdown. This section discusses the performance gains obtained by each proposed optimization, including the workspace transformation, semiring, and masking. The base case is implemented as a kernel that consists of the SpGEMM operations followed by the element-wise multiplication operation.This base version does not include any of the optimizations proposed in this paper. Then, we Fig. : Performance breakdown of triangle counting algorithm SandiaLL implemented by our compiler. evaluate the performance gain of each of the optimizations. First, we apply the workspace transformation to improve data locality for sparse linear algebra operations. The masking optimization is then applied to eliminate the element-wise multiplication operation that succeeds the SpGEMM operation.The masking optimization improves the performance by skipping computations that are not needed since they will result in multiplication by zero in the element-wise multiplication operation. Figure shows the performance progression as incrementally the workspace and masking optimizations are applied to the base case. The workspace transformation has 20.60× geometric mean speedup over the base case across all inputs. The masking operation can be seen to add another 1.86× speedup.The masking operation can be seen to add another 1.86× speedup. It can be clearly seen that the proposed optimizations are important and lead to substantial gains compared to the base case, e.g., in most cases over 90% of the speedup is due to the workspace and masking optimizations. We highlight that the masking optimization is also important for low memory usage since we had difficulty running the relatively larger LiveJournal and Orkut inputs on our system.We also profile the performance breakdown of the proposed optimizations for various triangle counting algorithms. Figure shows the results of SandiaLL only for brevity. Other algorithms show the same trend. The base cases for these algorithms are shown in Algorithm 1, whereby a SpGEMM is followed by an element-wise multiplication operation, including reduction.As done earlier, the SpGEMM and element-wise multiplication operations can have the workspace transformation and masking optimizations applied incrementally. An additional performance advantage can be gained by replacing the SpGEMM operation with a semiring of plus-pair. Specifically, replacing the multiplication operation in SpGEMM with a pair operation tends to bring in a performance advantage of around 5% across all inputs for four triangle counting algorithms.Note that semiring operations are essential to support state-of-the-art graph algorithms , , . Although the performance advantage is dependent on the sparsity of the input matrices, the proposed optimizations can be safely and effectively applied to achieve some benefit across multiple application domains that utilize sparse computation.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
================================================================================
