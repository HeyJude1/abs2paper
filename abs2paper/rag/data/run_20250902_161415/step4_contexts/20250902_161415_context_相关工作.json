{
  "section_name": "相关工作",
  "context": "### RelatedWork 总结\n**总结1** (来源: 3577193.3593712):\n## 相关工作总结  \n\n1. **现有方法一：基于多模型调优的方法（如BLISS）**  \n   - **核心思想**：通过为大规模应用调优多个模型来实现数据复用，以提高采样效率并减少建模开销。  \n   - **主要局限性**：难以在小数据集和完整性能范围之间进行泛化，且多模型调优成本较高。  \n\n2. **现有方法二：基于成本模型的替代方法**  \n   - **核心思想**：利用成本模型替代昂贵的信息源，并通过迁移学习（TL）按需泛化信息。  \n   - **主要局限性**：在预算有限的情况下，成本模型可能与目标问题相关性较低，且需要重新构建模型。  \n\n3. **现有方法三：基于机器学习的优化方法（如GPTune）**  \n   - **核心思想**：采用机器学习技术（如GPTune）进行最优解投影，优化少量样本的迁移学习预算。  \n   - **主要局限性**：在新任务中需要进行盲评估以建立迁移关系，增加了计算开销。  \n\n4. **现有方法四：多目标优化方法（如Active Harmony、ANGEL、ParEGO）**  \n   - **核心思想**：通过优化代理帕累托...\n\n**总结2** (来源: 3577193.3593712):\n## 相关工作总结  \n\n1. **现有方法一：基于多模型调优的方法（如BLISS）**  \n   - **核心思想**：通过为大规模应用调优多个模型来实现数据复用，以提高采样效率并减少建模开销。  \n   - **主要局限性**：难以在小数据集和完整性能范围之间进行泛化，且多模型调优成本较高。  \n\n2. **现有方法二：基于成本模型的替代方法**  \n   - **核心思想**：利用成本模型替代昂贵的信息源，并通过迁移学习（TL）按需泛化信息。  \n   - **主要局限性**：在预算有限的情况下，成本模型可能与目标问题相关性较低，且需要重新构建模型。  \n\n3. **现有方法三：基于机器学习的优化方法（如GPTune）**  \n   - **核心思想**：采用机器学习技术（如GPTune）进行最优解投影，优化少量样本的迁移学习预算。  \n   - **主要局限性**：在新任务中需要进行盲评估以建立迁移关系，增加了计算开销。  \n\n4. **现有方法四：多目标优化方法（如Active Harmony、ANGEL、ParEGO）**  \n   - **核心思想**：通过优化代理帕累托...\n\n**总结3** (来源: 3688609):\n相关工作总结：\n\n1. 现有方法一：简化设计空间探索方法\n核心思想: 通过减少DLAS（Deep Learning Acceleration Stack）中探索的参数数量或固定某些层参数，来降低设计空间复杂度。\n主要局限性: \n- 忽略压缩技术或算法的影响（如Hadidi et al.）\n- 过度依赖特定编译器框架和加速器（如VTA），缺乏通用性\n- 无法捕捉跨堆栈层的交互效应\n\n2. 现有方法二：DNN加速参数综述研究\n核心思想: 全面梳理DNN加速相关参数（如Sze et al.）\n主要局限性:\n- 缺乏将复杂参数体系转化为简洁结构的框架\n- 未提供有效的设计空间探索方法论\n- 对NP-hard优化问题仅提供原则性启发式方法\n\n3. 现有方法三：单一优化技术研究\n核心思想: 针对特定技术（如剪枝、量化）进行独立优化：\n(3.1) 剪枝技术：\n核心思想: 通过权重剪枝/通道剪枝减少模型复杂度\n局限性:\n- 稀疏计算在GPU上效率低下（因不规则数据访问）\n- 重训练成本被低估（Blalock et al.指出）\n- 实际速度提升常低于预期（仅达预期30%）\n\n(3.2) 量化技术：\n核...\n\n### Challenges 总结\n**总结1** (来源: 3577193.3593712):\n### 核心挑战总结：\n\n#### 挑战一：**高维参数空间的搜索成本过高**  \n**分析**:  \n- **具体内容**: 论文指出，即使是简单的内核（如Polybench的3mm内核）也可能涉及10个可调参数（如循环分块大小、循环交换顺序、内存管理策略等），产生376,320种独特配置组合。通过暴力搜索（brute-force）评估所有配置的实证成本过高，因为每个评估需要编译、执行并收集性能数据，耗时显著。  \n- **根源**: 问题源于参数空间的组合爆炸性增长（组合优化问题的NP难特性）与实证评估的高成本（需实际运行程序）。现有技术（如网格搜索或随机搜索）无法高效处理此类高维空间。  \n\n#### 挑战二：**输入规模变化导致的性能最优配置不稳定性**  \n**分析**:  \n- **具体内容**: 输入规模（如矩阵大小）的变化会显著改变最优参数配置。例如，小规模输入可能需要特定的内存打包技术，而中等规模输入则不需要；性能提升倍数也从1.13×到14.94×不等。这使得为不同输入规模独立调优成为必要，进一步增加了调优负担。  \n- **根源**: 问题源于计算任务的性能对输入...\n\n**总结2** (来源: 3577193.3593712):\n### 核心挑战总结：\n\n#### 挑战一：**高维参数空间的搜索成本过高**  \n**分析**:  \n- **具体内容**: 论文指出，即使是简单的内核（如Polybench的3mm内核）也可能涉及10个可调参数（如循环分块大小、循环交换顺序、内存管理策略等），产生376,320种独特配置组合。通过暴力搜索（brute-force）评估所有配置的实证成本过高，因为每个评估需要编译、执行并收集性能数据，耗时显著。  \n- **根源**: 问题源于参数空间的组合爆炸性增长（组合优化问题的NP难特性）与实证评估的高成本（需实际运行程序）。现有技术（如网格搜索或随机搜索）无法高效处理此类高维空间。  \n\n#### 挑战二：**输入规模变化导致的性能最优配置不稳定性**  \n**分析**:  \n- **具体内容**: 输入规模（如矩阵大小）的变化会显著改变最优参数配置。例如，小规模输入可能需要特定的内存打包技术，而中等规模输入则不需要；性能提升倍数也从1.13×到14.94×不等。这使得为不同输入规模独立调优成为必要，进一步增加了调优负担。  \n- **根源**: 问题源于计算任务的性能对输入...\n\n**总结3** (来源: 3688609):\n### 核心挑战总结：\n\n#### 挑战一：**跨栈优化的复杂性**  \n**分析**:  \n- **具体内容**: 论文指出，深度神经网络（DNN）的加速需要协调机器学习（如模型架构、压缩技术）和系统（如算法、硬件）多个层次的优化，但各层之间的选择存在强耦合性。例如，硬件资源限制（如CPU内存）要求模型压缩和软件算法必须适配，而新型DNN操作（如深度可分离卷积）需要定制的硬件支持。  \n- **根源**: 这种复杂性源于DNN部署的“全栈”特性：每一层的优化（如模型剪枝）需依赖下层支持（如稀疏计算算法），而现有研究往往孤立探索单层优化，缺乏跨层协同设计的通用框架。此外，不同领域（机器学习与系统）的研究者缺乏共同语言，导致优化脱节。\n\n#### 挑战二：**设计空间爆炸与评估成本高昂**  \n**分析**:  \n- **具体内容**: 论文通过实验发现，即使少量参数组合（如4种模型×3种压缩技术×2种硬件）也会产生大量结果，且性能表现非线性。例如，MobileNetV2在特定硬件上最优算法从GEMM变为直接卷积仅因引入“调优”参数。  \n- **根源**: DNN加速涉及多个NP难问题...\n\n### Baseline 总结\n**总结1** (来源: 3656019.3676889):\n根据论文内容，以下是Baseline选取策略的总结：\n\n---\n\n### Baseline选取总结  \n1. **对比方法**:  \n   - **PipeCheck**（基于𝜇spec的流水线验证工具）  \n   - **Herd**（内存一致性模型验证工具）  \n   - **传统硬件描述语言（HDL）方法**（如Verilog、VHDL、Chisel、Bluespec的手动实现）  \n   - **微架构描述语言**（如Teapot、PDL等领域专用语言）  \n\n2. **选取理由**:  \n   - **PipeCheck和Herd**：作为当前主流的**验证工具**，它们通过形式化方法或litmus测试验证现有流水线是否符合目标内存一致性模型（MCM），但均属于“事后验证”而非“正确性构造生成”。作者选择它们作为Baseline以凸显PipeGen的**主动生成优势**。  \n   - **传统HDL方法**：代表工业界实际开发流程中的手动实现方式，用于对比自动化工具（PipeGen）在减少人工错误和提升效率方面的价值。  \n   - **微架构描述语言**（如PDL）：与P...\n\n**总结2** (来源: 3656019.3676889):\n根据论文内容，以下是Baseline选取策略的总结：\n\n---\n\n### Baseline选取总结  \n1. **对比方法**:  \n   - **PipeCheck**（基于𝜇spec的流水线验证工具）  \n   - **Herd**（内存一致性模型验证工具）  \n   - **传统硬件描述语言（HDL）方法**（如Verilog、VHDL、Chisel、Bluespec的手动实现）  \n   - **微架构描述语言**（如Teapot、PDL等领域专用语言）  \n\n2. **选取理由**:  \n   - **PipeCheck和Herd**：作为当前主流的**验证工具**，它们通过形式化方法或litmus测试验证现有流水线是否符合目标内存一致性模型（MCM），但均属于“事后验证”而非“正确性构造生成”。作者选择它们作为Baseline以凸显PipeGen的**主动生成优势**。  \n   - **传统HDL方法**：代表工业界实际开发流程中的手动实现方式，用于对比自动化工具（PipeGen）在减少人工错误和提升效率方面的价值。  \n   - **微架构描述语言**（如PDL）：与P...\n\n**总结3** (来源: HPC_Application_Parameter_Autotuning_on_Edge_Devices_A_Bandit_Learning_Approach):\n### Baseline选取总结：\n\n1. **对比方法**:  \n   - BLISS（Bayesian Learning-based Iterative Software System）\n\n2. **选取理由**:  \n   - **SOTA代表性**：BLISS是当前最先进的（SOTA）基于机器学习的优化方法，采用贝叶斯优化（BO）来减少调优开销，并通过构建多样化的简化模型池加速收敛。选择它能够直接对比LASP与前沿方法的性能差异。  \n   - **技术路线对比**：BLISS依赖复杂的代理模型预测和计算密集型优化，而LASP专注于轻量级设计（适合资源受限的边缘设备）。这种对比凸显了两种技术路线的优劣（如BLISS的精度优势 vs. LASP的资源效率）。  \n   - **实验验证需求**：作者通过分析BLISS与LASP在CPU/内存占用上的差异（在MAXN和5W两种功耗模式下），证明LASP更适合边缘场景的动态性需求，从而强化了论文的贡献——轻量化自适应调优的实用性。  \n\n**补充说明**：  \n论文虽未明确列出其他经典基线（如随机搜索、遗传算法等），但通过强调与BLI...\n\n\n### 研究趋势分析\n**Challenges 趋势**:\n- 技术趋势: 泛化能力技术广泛应用\n- 研究模式:  在43/5篇论文中被提及(860.0%), n在39/5篇论文中被提及(780.0%), '在36/5篇论文中被提及(720.0%)\n\n",
  "context_length": 5052
}