"""
ç»“è®ºå¤„ç†æ¨¡å—ï¼Œè´Ÿè´£å¤„ç†å’Œæ±‡æ€»è®ºæ–‡å¤„ç†ç»“æœ
"""

import os
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
import sys

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def add_paper_result(paper_name: str, result: str, results_list: Optional[List] = None) -> List[Dict[str, str]]:
    """
    å°†è®ºæ–‡ç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
    
    Args:
        paper_name: è®ºæ–‡åç§°
        result: æ ‡ç­¾ç»“æœ
        results_list: å·²æœ‰çš„ç»“æœåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°åˆ—è¡¨
        
    Returns:
        æ›´æ–°åçš„ç»“æœåˆ—è¡¨
    """
    if results_list is None:
        results_list = []
    
    results_list.append({
        "paper": paper_name,
        "labels": result.strip()
    })
    
    return results_list


def save_results(results_list: List[Dict[str, str]], output_dir: str) -> Dict[str, int]:
    """
    ä¿å­˜ç»“æœåˆ°æ–‡ä»¶å¹¶è¿›è¡Œå…³é”®è¯ç»Ÿè®¡
    
    Args:
        results_list: ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        å…³é”®è¯è®¡æ•°å­—å…¸
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results_file = os.path.join(output_dir, "paper_labels_results.json")
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results_list, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“Š è®ºæ–‡æ ‡ç­¾ç»“æœå·²ä¿å­˜è‡³ {results_file}")
    
    # æå–å…³é”®è¯å¹¶è®¡æ•°
    keyword_counts = extract_keywords_count(results_list)
    
    # ä¿å­˜å…³é”®è¯ç»Ÿè®¡
    keywords_file = os.path.join(output_dir, "keyword_counts.json")
    with open(keywords_file, "w", encoding="utf-8") as f:
        json.dump(keyword_counts, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“Š å…³é”®è¯ç»Ÿè®¡å·²ä¿å­˜è‡³ {keywords_file}")
    
    return keyword_counts


def extract_keywords_count(results_list: List[Dict[str, str]]) -> Dict[str, int]:
    """
    ä»ç»“æœåˆ—è¡¨ä¸­æå–å…³é”®è¯å¹¶ç»Ÿè®¡
    
    Args:
        results_list: ç»“æœåˆ—è¡¨
        
    Returns:
        å…³é”®è¯è®¡æ•°å­—å…¸
    """
    keyword_counts = {}
    
    for result in results_list:
        labels = result.get("labels", "").split("ï¼Œ")
        for label in labels:
            label = label.strip()
            if label:
                keyword_counts[label] = keyword_counts.get(label, 0) + 1
    
    # æŒ‰è®¡æ•°æ’åº
    sorted_counts = dict(sorted(
        keyword_counts.items(), 
        key=lambda x: x[1], 
        reverse=True
    ))
    
    return sorted_counts


def generate_keywords_summary(keyword_counts: Dict[str, int], threshold: int = 3) -> str:
    """
    æ ¹æ®å…³é”®è¯ç»Ÿè®¡ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    
    Args:
        keyword_counts: å…³é”®è¯è®¡æ•°å­—å…¸
        threshold: æœ€å°å‡ºç°æ¬¡æ•°é˜ˆå€¼
        
    Returns:
        æ‘˜è¦æŠ¥å‘Šæ–‡æœ¬
    """
    filtered_keywords = {k: v for k, v in keyword_counts.items() if v >= threshold}
    total_papers = sum(keyword_counts.values()) // max(len(keyword_counts), 1)
    
    report = []
    report.append(f"# è®ºæ–‡å…³é”®è¯åˆ†ææŠ¥å‘Š")
    report.append(f"\n## åŸºæœ¬ç»Ÿè®¡")
    report.append(f"- æ€»è®ºæ–‡æ•°: {total_papers}")
    report.append(f"- å”¯ä¸€å…³é”®è¯æ•°: {len(keyword_counts)}")
    report.append(f"- å‡ºç°{threshold}æ¬¡ä»¥ä¸Šçš„å…³é”®è¯: {len(filtered_keywords)}")
    
    report.append(f"\n## çƒ­é—¨å…³é”®è¯ (å‡ºç°{threshold}æ¬¡ä»¥ä¸Š)")
    report.append(f"| å…³é”®è¯ | å‡ºç°æ¬¡æ•° | å æ¯” |")
    report.append(f"|--------|----------|------|")
    
    for keyword, count in filtered_keywords.items():
        percentage = count / total_papers * 100
        report.append(f"| {keyword} | {count} | {percentage:.1f}% |")
    
    return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤„ç†è®ºæ–‡æ ‡ç­¾ç»“æœå¹¶ç”Ÿæˆæ±‡æ€»")
    parser.add_argument("--results_dir", type=str, required=True,
                      help="åŒ…å«è®ºæ–‡æ ‡ç­¾ç»“æœçš„ç›®å½•")
    parser.add_argument("--threshold", type=int, default=3,
                      help="å…³é”®è¯æœ€å°å‡ºç°æ¬¡æ•°é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    results_file = os.path.join(args.results_dir, "paper_labels_results.json")
    if not os.path.exists(results_file):
        logger.error(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {results_file}")
        return
    
    with open(results_file, "r", encoding="utf-8") as f:
        results_list = json.load(f)
    
    # æå–å…³é”®è¯å¹¶è®¡æ•°
    keyword_counts = extract_keywords_count(results_list)
    
    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    report = generate_keywords_summary(keyword_counts, args.threshold)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(args.results_dir, "keywords_report.md")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    logger.info(f"ğŸ“Š å…³é”®è¯åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³ {report_file}")


if __name__ == "__main__":
    main()
