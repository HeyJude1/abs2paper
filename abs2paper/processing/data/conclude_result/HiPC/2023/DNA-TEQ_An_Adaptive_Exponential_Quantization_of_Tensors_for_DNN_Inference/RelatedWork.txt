相关工作总结：

1、现有方法一：聚类方法（Clustering）
核心思想: 使用K-means等方法将权重压缩为K个聚类中心，用索引代替原始权重值。
主要局限性: 仅减少存储需求，不减少计算量或计算成本。

2、现有方法二：剪枝方法（Pruning）
核心思想: 通过移除不重要的参数来减小模型规模和计算量。
主要局限性: 剪枝后的模型变得稀疏，需要专用硬件才能高效执行；可能损失精度，需要重新训练恢复。

3、现有方法三：均匀量化（Uniform Quantization）
核心思想: 将数值精度降低到8-16位以减少计算成本。
主要局限性: 当尝试将精度降至8位以下时，由于不考虑张量分布特性，在复杂神经网络上会导致严重的精度下降。

4、现有方法四：混合精度量化（Mixed Precision Quantization）
核心思想: 根据权重/激活值对量化误差的敏感性设置不同位宽。
主要局限性: 虽然能降低位宽(<8b)，但在确定每层精度后仍使用均匀量化，可能影响模型精度。

5、现有方法五：对数非均匀量化（Base-2 Logarithmic Non-uniform Quantization）
核心思想: 基于CNN权重/激活值的以2为底对数表示进行非均匀量化。
主要局限性: 即使简单网络中也会产生不可忽视的精度下降；部分工作只量化权重以避免动态量化开销。

6、现有方法六：APoT量化方案
核心思想: 针对DNN中钟形和长尾分布的权重/激活值，将量化级别约束为二的幂次项之和。
主要局限性: 需要重新训练来恢复精度；仅限于特定网络结构(如ResNet)。

7、现有方法七：Mokey压缩方法
核心思想: Transformer专用后训练压缩方法，通过4位索引字典和16位定点质心实现压缩。
主要局限性: 仅适用于Transformer模型；需要在昂贵的后处理阶段计算异常值；无法适配其他DNN架构。

研究缺口总结：
1. 现有方法普遍无法同时兼顾存储和计算成本的降低
2. 多数非均匀量化方案会导致明显的精度损失
3. Transformer专用方法缺乏通用性
4. 后处理方法通常需要昂贵的计算开销
5. 许多方案需要重新训练才能恢复精度