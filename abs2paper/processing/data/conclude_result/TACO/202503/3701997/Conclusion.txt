结论与展望总结：

1、结论回顾: 
- 提出MemoriaNova框架，包含BTSearch和GenEFlow两种创新算法，用于优化边缘设备分布式深度学习中的内存和推理延迟。
- BTSearch通过优化DAG结构模型的算子执行顺序，显著降低内存开销（最高减少12%），并扩大延迟优化的搜索空间。
- GenEFlow从整体模型角度优化分布式推理的通信延迟，利用遗传算法配置算子布局，实现推理延迟降低33.9%。

2、工作局限性: 
（注：原文未明确提及具体局限性，此部分需根据其他章节补充或标注为"未明确说明"）

3、未来工作: 
- 研究如何将高内存需求的大语言模型部署到内存受限的边缘设备
- 进一步优化大语言模型在边缘设备上的推理性能

问题背景补充说明（根据结论反推）：
该研究针对边缘计算环境中分布式深度学习的两大核心挑战：
1. 内存效率问题：DAG结构模型在边缘设备上的内存开销优化
2. 通信延迟问题：分布式推理任务中跨设备通信的延迟优化
研究动机源于边缘设备资源受限性与大型模型部署需求之间的矛盾，特别是随着大语言模型普及带来的新挑战。