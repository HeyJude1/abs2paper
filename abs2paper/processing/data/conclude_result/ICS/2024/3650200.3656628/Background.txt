问题背景总结：  
1、研究领域: **边缘人工智能（Edge AI）与分布式深度学习**  
2、核心问题: **如何在资源受限的异构边缘设备环境中高效并行化基于Transformer的深度神经网络（DNN）推理，以满足服务级目标（SLOs）的延迟要求？**  
3、研究动机:  
   - **理论价值**：Transformer模型（如BERT、ViT）因计算密集性（参数量大、FLOPs高）与边缘设备（如ARM Cortex-A72）的有限资源不匹配，导致推理延迟超标。现有并行化方法（如CoEdge、Megatron-LM）仅针对同质GPU环境或CNN架构，缺乏对Transformer独特结构（自注意力机制、MLP块）和动态异构环境的适配。  
   - **实践价值**：边缘AI市场预计达31亿美元（2027年），实时应用（如心跳检测、语音识别）依赖低延迟推理。现有方案因忽略网络状态动态性和设备异构性，无法保证SLOs。  
4、潜在应用:  
   - **实时边缘智能服务**：医疗监测、个性化语音交互等需低延迟响应的场景。  
   - **物联网（IoT）设备协同计算**：在Raspberry Pi等非GPU设备上部署高性能Transformer模型。