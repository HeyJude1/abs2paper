方法概述：

1、方法名称: Task-aware Decoding (TaD)

2、核心思想: TaD通过构建一个知识向量（knowledge vector）来显式表示预训练语言模型在微调过程中学到的任务特定知识，并利用该向量增强微调后模型的输出概率分布，从而提升模型在下游任务中的表现。其核心直觉是：微调过程中模型输出的概率分布变化反映了从通用知识到任务特定知识的适应性转变，通过放大这种转变可以强化模型对下游任务的适应能力。

3、主要流程/组件

组件/步骤一: 知识向量构建（Constructing the Knowledge Vector）
- 功能：计算预训练模型θ和微调后模型ϕ在词汇表V上的对数概率差值向量V_K = log p_ϕ - log p_θ，该向量反映微调带来的概率分布偏移。通过约束函数C_t和惩罚系数λ消除低概率token的干扰，得到修正后的知识向量VK。

组件/步骤二: 知识概率转换（Knowledge Probability Conversion）
- 功能：对修正后的知识向量VK应用softmax函数，将其转换为概率分布p_K = softmax(VK)，表示微调对模型输出的影响强度。

组件/步骤三: 任务感知解码（Task-Aware Decoding）
- 功能：将微调模型的原始输出分布p_ϕ与知识分布p_K通过加权参数μ进行融合：p = (1-μ)p_ϕ + μp_K。通过调节μ控制任务知识的显性化程度，同时保留p_ϕ以避免错误抑制。

组件/步骤四: 基础解码策略集成（Basic Decoding Integration）
- 功能：将改进的概率分布p输入到贪婪搜索等基础解码策略中生成最终文本。TaD作为即插即用模块可与现有解码策略无缝结合。