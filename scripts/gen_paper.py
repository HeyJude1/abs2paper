#!/usr/bin/env python3
"""
è®ºæ–‡ç”ŸæˆRAGç³»ç»Ÿä¸»è„šæœ¬
åŸºäºç”¨æˆ·éœ€æ±‚ï¼Œé€šè¿‡RAGæµç¨‹ç”Ÿæˆå®Œæ•´è®ºæ–‡
"""

import os
import sys
import json
import argparse
import glob
from datetime import datetime
from typing import Dict, List, Optional, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from abs2paper.rag.summary_retriever import SummaryRetriever
from abs2paper.rag.crossPaper_analyzer import CrossPaperAnalyzer
from abs2paper.rag.sourceText_retriever import SourceTextRetriever
from abs2paper.rag.context_builder import ContextBuilder
from abs2paper.rag.paper_generator import PaperGenerator
from abs2paper.utils.llm_client import LLMClient

# å…¨å±€æ—¶é—´æˆ³
global_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# åŠ è½½é…ç½®
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(project_root, "config", "config.json")
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)

# ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºç‹¬ç«‹çš„ç»“æœç›®å½•
def get_run_data_dir():
    """è·å–æœ¬æ¬¡è¿è¡Œçš„æ•°æ®ç›®å½•"""
    config = load_config()
    rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
    base_data_dir = os.path.join(project_root, rag_data_base_path)
    run_data_dir = os.path.join(base_data_dir, f"run_{global_timestamp}")
    return run_data_dir

def convert_to_serializable(obj):
    """å°†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
    if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):
        # å¤„ç†åˆ—è¡¨æˆ–ç±»ä¼¼åˆ—è¡¨çš„å¯¹è±¡ï¼ˆå¦‚RepeatedScalarContainerï¼‰
        try:
            return list(obj)
        except:
            return str(obj)
    elif hasattr(obj, '__dict__'):
        # å¤„ç†å¤æ‚å¯¹è±¡
        return obj.__dict__
    else:
        return obj

def serialize_data(data):
    """é€’å½’å¤„ç†æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥JSONåºåˆ—åŒ–"""
    import json
    
    try:
        # å°è¯•ç›´æ¥åºåˆ—åŒ–ï¼Œå¦‚æœå¯ä»¥åˆ™è¿”å›
        json.dumps(data)
        return data
    except TypeError:
        pass
    
    if isinstance(data, dict):
        return {key: serialize_data(value) for key, value in data.items()}
    elif isinstance(data, (list, tuple)):
        return [serialize_data(item) for item in data]
    elif hasattr(data, '__iter__') and not isinstance(data, (str, bytes)):
        # å¤„ç†RepeatedScalarContainerç­‰protobufç±»å‹
        try:
            return [serialize_data(item) for item in data]
        except:
            return str(data)
    elif hasattr(data, '__dict__'):
        # å¤„ç†å¤æ‚å¯¹è±¡
        return serialize_data(data.__dict__)
    else:
        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºåŸºæœ¬ç±»å‹
        try:
            return str(data)
        except:
            return None

def save_json_and_txt(data: Dict[str, Any], json_path: str, txt_path: str, 
                      text_extract_func=None):
    """ä¿å­˜JSONå’ŒTXTæ–‡ä»¶çš„é€šç”¨å‡½æ•°"""
    # åºåˆ—åŒ–æ•°æ®
    serializable_data = serialize_data(data)
    
    # ä¿å­˜JSON
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜TXT
    if text_extract_func:
        text_content = text_extract_func(data)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(text_content)

def extract_summaries_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤1ç»“æœä¸­æå–æ€»ç»“æ–‡æœ¬"""
    text_lines = []
    text_lines.append(f"ç”¨æˆ·éœ€æ±‚ï¼š{data.get('user_requirement', '')}\n")
    text_lines.append("=" * 80 + "\n")
    
    relevant_summaries = data.get('relevant_summaries', {})
    for summary_type, summaries in relevant_summaries.items():
        text_lines.append(f"ã€{summary_type.upper()} ç±»å‹æ€»ç»“ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        for i, summary in enumerate(summaries):
            text_lines.append(f"æ€»ç»“ {i+1}ï¼š\n")
            text_lines.append(f"è®ºæ–‡IDï¼š{summary.get('paper_id', '')}\n")
            text_lines.append(f"ç›¸å…³åº¦å¾—åˆ†ï¼š{summary.get('score', 0):.4f}\n")
            text_lines.append(f"æ¥æºç« èŠ‚ï¼š{', '.join(summary.get('source_sections', []))}\n")
            text_lines.append(f"ä¸»é¢˜æ ‡ç­¾ï¼š{', '.join(summary.get('topics', []))}\n")
            text_lines.append("æ€»ç»“å†…å®¹ï¼š\n")
            text_lines.append(summary.get('summary_text', '') + "\n")
            text_lines.append("\n" + "~" * 30 + "\n")
        
        text_lines.append("\n" + "=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_analysis_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤2ç»“æœä¸­æå–åˆ†ææ–‡æœ¬"""
    text_lines = []
    text_lines.append("è·¨è®ºæ–‡åŒç±»å‹åˆ†æç»“æœ\n")
    text_lines.append("=" * 80 + "\n")
    
    cross_paper_analysis = data.get('cross_paper_analysis', {})
    for analysis_type, analysis_data in cross_paper_analysis.items():
        text_lines.append(f"ã€{analysis_type.upper()} åˆ†æã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        # åˆ†ææ€»ç»“
        if 'summary' in analysis_data:
            text_lines.append("ğŸ“Š åˆ†ææ€»ç»“ï¼š\n")
            text_lines.append(f"{analysis_data['summary']}\n\n")
        
        # å‘å±•è¶‹åŠ¿
        if 'trends' in analysis_data:
            text_lines.append("ğŸ“ˆ å‘å±•è¶‹åŠ¿ï¼š\n")
            trends = analysis_data['trends']
            if isinstance(trends, list):
                for i, trend in enumerate(trends, 1):
                    text_lines.append(f"  {i}. {str(trend)}\n")
            else:
                text_lines.append(f"  â€¢ {str(trends)}\n")
            text_lines.append("\n")
        
        # å¸¸è§æ–¹æ³•
        if 'common_approaches' in analysis_data:
            text_lines.append("ğŸ”§ å¸¸è§æ–¹æ³•ï¼š\n")
            approaches = analysis_data['common_approaches']
            if isinstance(approaches, list):
                for i, approach in enumerate(approaches, 1):
                    text_lines.append(f"  {i}. {str(approach)}\n")
            else:
                text_lines.append(f"  â€¢ {str(approaches)}\n")
            text_lines.append("\n")
        
        # å‘ç°çš„æ¨¡å¼ 
        if 'patterns' in analysis_data:
            text_lines.append("ğŸ” å‘ç°çš„æ¨¡å¼ï¼š\n")
            patterns = analysis_data['patterns']
            if isinstance(patterns, list):
                for i, pattern in enumerate(patterns[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæœ€é‡è¦çš„æ¨¡å¼
                    text_lines.append(f"  {i}. {str(pattern)}\n")
            else:
                text_lines.append(f"  â€¢ {str(patterns)}\n")
            text_lines.append("\n")
        
        # ä¸»é¢˜èšç±»ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
        if 'topic_clusters' in analysis_data:
            text_lines.append("ğŸ“š ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼š\n")
            clusters = analysis_data['topic_clusters']
            if isinstance(clusters, dict):
                # è¿‡æ»¤å’Œæ•´ç†èšç±»ç»“æœï¼Œåªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„èšç±»
                meaningful_clusters = {}
                for cluster_name, papers in clusters.items():
                    # è¿‡æ»¤æ‰å•å­—ç¬¦æˆ–æ— æ„ä¹‰çš„èšç±»å
                    if len(cluster_name) > 2 and cluster_name not in ['[', "'", ' ', '(', ')', ',', '-']:
                        if isinstance(papers, list) and len(papers) >= 2:  # è‡³å°‘2ç¯‡è®ºæ–‡çš„èšç±»æ‰æœ‰æ„ä¹‰
                            if isinstance(papers[0], dict):
                                paper_count = len(set(p.get('paper_id', str(p)) for p in papers))
                            else:
                                paper_count = len(set(str(p) for p in papers))
                            meaningful_clusters[cluster_name] = paper_count
                
                # æŒ‰è®ºæ–‡æ•°é‡æ’åºï¼Œæ˜¾ç¤ºå‰10ä¸ª
                sorted_clusters = sorted(meaningful_clusters.items(), key=lambda x: x[1], reverse=True)[:10]
                for i, (topic, count) in enumerate(sorted_clusters, 1):
                    text_lines.append(f"  {i}. {topic}ï¼ˆ{count}ç¯‡è®ºæ–‡æ¶‰åŠï¼‰\n")
            text_lines.append("\n")
        
        text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_source_texts_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤3ç»“æœä¸­æå–æºæ–‡æœ¬"""
    text_lines = []
    text_lines.append("é€‰å®šçš„æœ€ç›¸å…³åŸæ–‡ç« èŠ‚\n")
    text_lines.append("=" * 80 + "\n")
    
    selected_source_texts = data.get('selected_source_texts', {})
    for paper_id, sections in selected_source_texts.items():
        text_lines.append(f"ã€è®ºæ–‡ {paper_id}ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        for section_name, section_content in sections.items():
            text_lines.append(f">>> {section_name} ç« èŠ‚ <<<\n")
            if isinstance(section_content, list):
                text_lines.append("".join(section_content) + "\n")
            else:
                text_lines.append(str(section_content) + "\n")
            text_lines.append("\n" + "~" * 30 + "\n")
        
        text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_contexts_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤4ç»“æœä¸­æå–ç»“æ„åŒ–ä¸Šä¸‹æ–‡æ–‡æœ¬"""
    text_lines = []
    text_lines.append("ç»“æ„åŒ–RAGä¸Šä¸‹æ–‡\n")
    text_lines.append("=" * 80 + "\n")
    
    # å¤„ç†å®Œæ•´çš„structured_contextsæ•°æ®
    if 'structured_contexts' in data:
        structured_contexts = data['structured_contexts']
        for section_name, context in structured_contexts.items():
            text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
            text_lines.append("-" * 50 + "\n")
            if isinstance(context, str) and context.strip():
                text_lines.append(context + "\n\n")
            else:
                text_lines.append("(ä¸Šä¸‹æ–‡å†…å®¹ä¸ºç©º)\n\n")
            text_lines.append("=" * 80 + "\n")
    
    # å¤„ç†å•ä¸ªsectionçš„æ•°æ®
    elif 'section_name' in data and 'context' in data:
        section_name = data['section_name']
        context = data['context']
        context_length = data.get('context_length', 0)
        
        text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        text_lines.append(f"ä¸Šä¸‹æ–‡é•¿åº¦ï¼š{context_length} å­—ç¬¦\n\n")
        
        if isinstance(context, str) and context.strip():
            text_lines.append("ä¸Šä¸‹æ–‡å†…å®¹ï¼š\n")
            text_lines.append(context + "\n\n")
        else:
            text_lines.append("(ä¸Šä¸‹æ–‡å†…å®¹ä¸ºç©º)\n\n")
    
    # å¤„ç†æ—§æ ¼å¼çš„æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
    else:
        for section_name, context_data in data.items():
            if isinstance(context_data, dict):
                text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
                text_lines.append("-" * 50 + "\n")
                
                # æ€»ç»“ä¿¡æ¯
                if 'summary_info' in context_data:
                    text_lines.append("ğŸ“‹ ç›¸å…³æ€»ç»“ä¿¡æ¯ï¼š\n")
                    for summary_type, summaries in context_data['summary_info'].items():
                        text_lines.append(f"  {summary_type}ï¼š\n")
                        for summary in summaries:
                            paper_id = summary.get('paper_id', '')
                            summary_text = summary.get('summary_text', '')
                            text_lines.append(f"    - {paper_id}: {summary_text[:200]}...\n")
                        text_lines.append("\n")
                
                # è¶‹åŠ¿ä¿¡æ¯
                if 'trend_info' in context_data:
                    text_lines.append("ğŸ“ˆ è¶‹åŠ¿åˆ†æï¼š\n")
                    text_lines.append(context_data['trend_info'] + "\n\n")
                
                # åŸæ–‡ä¿¡æ¯
                if 'source_text_info' in context_data:
                    text_lines.append("ğŸ“„ ç›¸å…³åŸæ–‡ï¼š\n")
                    text_lines.append(context_data['source_text_info'] + "\n\n")
                
                text_lines.append("=" * 80 + "\n")
            elif isinstance(context_data, str) and context_data.strip():
                text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
                text_lines.append("-" * 50 + "\n")
                text_lines.append(context_data + "\n\n")
                text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def analyze_user_requirement(user_input: str) -> str:
    """
    åˆ†æç”¨æˆ·è¾“å…¥ï¼Œæå–æ ‡å‡†åŒ–çš„è®ºæ–‡ç”Ÿæˆéœ€æ±‚
    
    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        
        Returns:
        standardized_requirement: æ ‡å‡†åŒ–çš„éœ€æ±‚æè¿°
    """
    print(f"ğŸ” åˆ†æç”¨æˆ·éœ€æ±‚ï¼š{user_input}")
    
    # åŠ è½½é…ç½®å’Œprompt
    config = load_config()
    paper_prompt_path = config["data_paths"]["paper_prompt"]["path"].lstrip('/')
    prompt_dir = os.path.join(project_root, paper_prompt_path)
    
    # åŠ è½½ç”¨æˆ·éœ€æ±‚åˆ†æprompt
    user_need_prompt_path = os.path.join(prompt_dir, "userNeed_prompt")
    try:
        with open(user_need_prompt_path, 'r', encoding='utf-8') as f:
            base_prompt = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°ç”¨æˆ·éœ€æ±‚åˆ†æpromptæ–‡ä»¶: {user_need_prompt_path}")
        return user_input  # å¦‚æœæ²¡æœ‰promptæ–‡ä»¶ï¼Œç›´æ¥è¿”å›åŸå§‹è¾“å…¥
    
    # æ„å»ºå®Œæ•´prompt
    full_prompt = f"{base_prompt}\n\nç”¨æˆ·è¾“å…¥ï¼š{user_input}"
    
    # è°ƒç”¨LLMåˆ†æ
    llm_client = LLMClient()
    analysis_result = llm_client.get_completion(full_prompt)
    
    try:
        # å°è¯•è§£æJSONç»“æœ
        import json
        parsed_result = json.loads(analysis_result)
        standardized_requirement = parsed_result.get("standardized_requirement", user_input)
        
        print(f"âœ… éœ€æ±‚åˆ†æå®Œæˆ")
        print(f"ğŸ“‹ æ ‡å‡†åŒ–éœ€æ±‚ï¼š{standardized_requirement}")
        print(f"ğŸ”¬ ç ”ç©¶é¢†åŸŸï¼š{parsed_result.get('research_field', 'N/A')}")
        print(f"ğŸ¯ å…³é”®ä¸»é¢˜ï¼š{', '.join(parsed_result.get('key_topics', []))}")
        
        # ä¿å­˜ç”¨æˆ·éœ€æ±‚åˆ†æç»“æœåˆ°step0_needç›®å½•
        save_user_need_analysis_result(user_input, analysis_result, parsed_result, standardized_requirement)
        
        return standardized_requirement
        
    except (json.JSONDecodeError, Exception) as e:
        print(f"âš ï¸ è§£æéœ€æ±‚åˆ†æç»“æœå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥: {e}")
        
        # å³ä½¿è§£æå¤±è´¥ä¹Ÿä¿å­˜åŸå§‹åˆ†æç»“æœ
        save_user_need_analysis_result(user_input, analysis_result, {}, user_input)
        
        return user_input

def save_user_need_analysis_result(user_input: str, analysis_result: str, parsed_result: dict, standardized_requirement: str):
    """
    ä¿å­˜ç”¨æˆ·éœ€æ±‚åˆ†æç»“æœåˆ°step0_needç›®å½•
    
    Args:
        user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
        analysis_result: LLMåŸå§‹åˆ†æç»“æœ
        parsed_result: è§£æåçš„JSONç»“æœ
        standardized_requirement: æ ‡å‡†åŒ–éœ€æ±‚
    """
    # åˆ›å»ºstep0_needç›®å½•
    run_data_dir = get_run_data_dir()
    step0_dir = os.path.join(run_data_dir, "step0_need")
    os.makedirs(step0_dir, exist_ok=True)
    
    # æ„å»ºå®Œæ•´çš„åˆ†æç»“æœæ•°æ®
    result_data = {
        "user_input": user_input,
        "analysis_result": analysis_result,
        "parsed_result": parsed_result,
        "standardized_requirement": standardized_requirement,
        "analysis_timestamp": global_timestamp
    }
    
    # ä¿å­˜JSONæ ¼å¼
    json_path = os.path.join(step0_dir, f"{global_timestamp}_user_need_analysis.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜TXTæ ¼å¼
    txt_path = os.path.join(step0_dir, f"{global_timestamp}_user_need_analysis.txt")
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write("ç”¨æˆ·éœ€æ±‚åˆ†æç»“æœ\n")
        f.write("=" * 80 + "\n")
        f.write(f"åˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
        
        f.write("ã€ç”¨æˆ·åŸå§‹è¾“å…¥ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write(f"{user_input}\n\n")
        
        f.write("ã€LLMåˆ†æç»“æœã€‘\n")
        f.write("-" * 50 + "\n")
        f.write(f"{analysis_result}\n\n")
        
        f.write("ã€æ ‡å‡†åŒ–éœ€æ±‚ã€‘\n")
        f.write("-" * 50 + "\n")
        f.write(f"{standardized_requirement}\n\n")
        
        if parsed_result:
            f.write("ã€ç»“æ„åŒ–åˆ†æã€‘\n")
            f.write("-" * 50 + "\n")
            f.write(f"ç ”ç©¶é¢†åŸŸï¼š{parsed_result.get('research_field', 'N/A')}\n")
            f.write(f"å…³é”®ä¸»é¢˜ï¼š{', '.join(parsed_result.get('key_topics', []))}\n")
            f.write(f"æŠ€æœ¯æ–¹å‘ï¼š{parsed_result.get('technical_approach', 'N/A')}\n")
            f.write(f"é‡ç‚¹å…³æ³¨ï¼š{', '.join(parsed_result.get('focus_areas', []))}\n")
            f.write(f"åº”ç”¨é¢†åŸŸï¼š{parsed_result.get('application_domain', 'N/A')}\n")
    
    print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚åˆ†æç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ï¿½ï¿½ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")

def step1_retrieve_summaries(user_requirement: str) -> Dict[str, Any]:
    """æ­¥éª¤1ï¼šå¤šç±»å‹æ€»ç»“å¹¶è¡Œæ£€ç´¢"""
    print(f"ğŸ”„ æ­¥éª¤1ï¼šå¤šç±»å‹æ€»ç»“å¹¶è¡Œæ£€ç´¢...")
    print(f"ğŸ“ ç”¨æˆ·åŸå§‹è¾“å…¥ï¼š{user_requirement}")
    
    # å…ˆåˆ†æç”¨æˆ·éœ€æ±‚ï¼Œæå–æ ‡å‡†åŒ–éœ€æ±‚
    standardized_requirement = analyze_user_requirement(user_requirement)
    
    retriever = SummaryRetriever()
    relevant_summaries = retriever.parallel_retrieve_summaries(
        user_requirement=standardized_requirement,
        top_k_per_type=5
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = retriever.get_retrieval_statistics(relevant_summaries)
    
    result = {
        "user_requirement": user_requirement,  # ä¿å­˜åŸå§‹è¾“å…¥
        "standardized_requirement": standardized_requirement,  # ä¿å­˜æ ‡å‡†åŒ–éœ€æ±‚
        "top_k_per_type": 5,
        "relevant_summaries": relevant_summaries,
        "statistics": stats
    }
    
    # ä¿å­˜ç»“æœåˆ°step1ç›®å½•
    run_data_dir = get_run_data_dir()
    step1_dir = os.path.join(run_data_dir, "step1_summaries")
    os.makedirs(step1_dir, exist_ok=True)
    
    json_path = os.path.join(step1_dir, f"{global_timestamp}_summaries_retrieval.json")
    txt_path = os.path.join(step1_dir, f"{global_timestamp}_summaries_retrieval.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_summaries_text)
    
    print(f"âœ… æ­¥éª¤1å®Œæˆï¼å…±æ£€ç´¢åˆ° {stats['total_summaries']} ä¸ªæ€»ç»“ï¼Œæ¶µç›– {stats['types_found']} ç§ç±»å‹")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼š{stats['type_counts']}")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step2_analyze_cross_paper() -> Dict[str, Any]:
    """æ­¥éª¤2ï¼šè·¨è®ºæ–‡åŒç±»å‹åˆ†æ"""
    print(f"ğŸ”„ æ­¥éª¤2ï¼šè·¨è®ºæ–‡åŒç±»å‹åˆ†æ...")
    
    # åŠ è½½æ­¥éª¤1çš„ç»“æœ
    run_data_dir = get_run_data_dir()
    step1_file = os.path.join(run_data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    
    if not os.path.exists(step1_file):
        # æŸ¥æ‰¾æœ€æ–°çš„step1æ–‡ä»¶ï¼ˆåœ¨æ‰€æœ‰runç›®å½•ä¸­ï¼‰
        config = load_config()
        rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
        base_data_dir = os.path.join(project_root, rag_data_base_path)
        step1_files = glob.glob(os.path.join(base_data_dir, "run_*", "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1ç»“æœï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    
    analyzer = CrossPaperAnalyzer()
    cross_paper_analysis = analyzer.analyze_cross_paper_patterns(
        step1_result['relevant_summaries']
    )
    
    result = {
        "cross_paper_analysis": cross_paper_analysis,
        "input_summaries_count": sum(len(summaries) for summaries in step1_result['relevant_summaries'].values())
    }
    
    # ä¿å­˜ç»“æœåˆ°step2ç›®å½•
    step2_dir = os.path.join(run_data_dir, "step2_analysis")
    os.makedirs(step2_dir, exist_ok=True)
    
    json_path = os.path.join(step2_dir, f"{global_timestamp}_cross_paper_analysis.json")
    txt_path = os.path.join(step2_dir, f"{global_timestamp}_cross_paper_analysis.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_analysis_text)
    
    print(f"âœ… æ­¥éª¤2å®Œæˆï¼åˆ†æäº† {len(cross_paper_analysis)} ç§æ€»ç»“ç±»å‹çš„è·¨è®ºæ–‡æ¨¡å¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step3_select_source_texts() -> Dict[str, Any]:
    """æ­¥éª¤3ï¼šæ ¹æ®æ€»ç»“è·å–å¯¹åº”åŸæ–‡ç« èŠ‚"""
    print(f"ğŸ”„ æ­¥éª¤3ï¼šæ ¹æ®æ€»ç»“è·å–å¯¹åº”åŸæ–‡ç« èŠ‚...")
    
    # åŠ è½½æ­¥éª¤1çš„ç»“æœ
    run_data_dir = get_run_data_dir()
    step1_file = os.path.join(run_data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    
    if not os.path.exists(step1_file):
        # æŸ¥æ‰¾æœ€æ–°çš„step1æ–‡ä»¶ï¼ˆåœ¨æ‰€æœ‰runç›®å½•ä¸­ï¼‰
        config = load_config()
        rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
        base_data_dir = os.path.join(project_root, rag_data_base_path)
        step1_files = glob.glob(os.path.join(base_data_dir, "run_*", "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1ç»“æœï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    
    retriever = SourceTextRetriever()
    selected_source_texts = retriever.select_most_relevant_source_texts(
        step1_result['relevant_summaries']
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    total_papers = len(selected_source_texts)
    total_sections = sum(len(sections) for sections in selected_source_texts.values())
    total_chunks = sum(
        len(content) if isinstance(content, list) else 1
        for sections in selected_source_texts.values()
        for content in sections.values()
    )
    
    result = {
        "selected_source_texts": selected_source_texts,
        "statistics": {
            "total_papers": total_papers,
            "total_sections": total_sections,
            "total_chunks": total_chunks,
            "paper_details": {
                paper_id: {
                    "sections": list(sections.keys()),
                    "section_count": len(sections)
                }
                for paper_id, sections in selected_source_texts.items()
            }
        }
    }
    
    # ä¿å­˜ç»“æœåˆ°step3ç›®å½•
    step3_dir = os.path.join(run_data_dir, "step3_source_texts")
    os.makedirs(step3_dir, exist_ok=True)
    
    json_path = os.path.join(step3_dir, f"{global_timestamp}_source_texts_selection.json")
    txt_path = os.path.join(step3_dir, f"{global_timestamp}_source_texts_selection.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_source_texts_text)
    
    print(f"âœ… æ­¥éª¤3å®Œæˆï¼é€‰æ‹©äº† {total_papers} ç¯‡è®ºæ–‡çš„ {total_sections} ä¸ªç« èŠ‚")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step4_build_contexts() -> Dict[str, Any]:
    """æ­¥éª¤4ï¼šæŒ‰ç”Ÿæˆè®ºæ–‡éƒ¨åˆ†æ„å»ºç»“æ„åŒ–RAGä¸Šä¸‹æ–‡"""
    print(f"ğŸ”„ æ­¥éª¤4ï¼šæŒ‰ç”Ÿæˆè®ºæ–‡éƒ¨åˆ†æ„å»ºç»“æ„åŒ–RAGä¸Šä¸‹æ–‡...")
    
    run_data_dir = get_run_data_dir()
    
    # åŠ è½½å‰é¢æ­¥éª¤çš„ç»“æœ
    step1_file = os.path.join(run_data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    step2_file = os.path.join(run_data_dir, "step2_analysis", f"{global_timestamp}_cross_paper_analysis.json")
    step3_file = os.path.join(run_data_dir, "step3_source_texts", f"{global_timestamp}_source_texts_selection.json")
    
    # å¦‚æœæ‰¾ä¸åˆ°å½“å‰è¿è¡Œçš„æ–‡ä»¶ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
    config = load_config()
    rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
    base_data_dir = os.path.join(project_root, rag_data_base_path)
    
    if not os.path.exists(step1_file):
        step1_files = glob.glob(os.path.join(base_data_dir, "run_*", "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1æ–‡ä»¶ï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    if not os.path.exists(step2_file):
        step2_files = glob.glob(os.path.join(base_data_dir, "run_*", "step2_analysis", "*_cross_paper_analysis.json"))
        if step2_files:
            step2_file = max(step2_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤2æ–‡ä»¶ï¼š{os.path.basename(step2_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤2çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤2")
    
    if not os.path.exists(step3_file):
        step3_files = glob.glob(os.path.join(base_data_dir, "run_*", "step3_source_texts", "*_source_texts_selection.json"))
        if step3_files:
            step3_file = max(step3_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤3æ–‡ä»¶ï¼š{os.path.basename(step3_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤3çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤3")
    
    # åŠ è½½æ•°æ®
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    with open(step2_file, 'r', encoding='utf-8') as f:
        step2_result = json.load(f)
    with open(step3_file, 'r', encoding='utf-8') as f:
        step3_result = json.load(f)
    
    builder = ContextBuilder()
    structured_contexts = builder.build_structured_contexts(
        relevant_summaries=step1_result['relevant_summaries'],
        cross_paper_insights=step2_result['cross_paper_analysis'],
        selected_source_texts=step3_result['selected_source_texts']
    )
    
    result = {
        "structured_contexts": structured_contexts,
        "context_sections": list(structured_contexts.keys()),
        "total_contexts": len(structured_contexts)
    }
    
    # ä¿å­˜ç»“æœåˆ°step4ç›®å½•
    step4_dir = os.path.join(run_data_dir, "step4_contexts")
    os.makedirs(step4_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡
    json_path = os.path.join(step4_dir, f"{global_timestamp}_structured_contexts.json")
    txt_path = os.path.join(step4_dir, f"{global_timestamp}_structured_contexts.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_contexts_text)
    
    # ä¸ºæ¯ä¸ªéƒ¨åˆ†å•ç‹¬ä¿å­˜ä¸Šä¸‹æ–‡
    for section_name, context in structured_contexts.items():
        section_json_path = os.path.join(step4_dir, f"{global_timestamp}_context_{section_name}.json")
        section_txt_path = os.path.join(step4_dir, f"{global_timestamp}_context_{section_name}.txt")
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„sectionæ•°æ®
        section_data = {
            "section_name": section_name,
            "context": context,
            "context_length": len(context) if isinstance(context, str) else len(str(context))
        }
        
        save_json_and_txt(section_data, section_json_path, section_txt_path, extract_contexts_text)
    
    print(f"âœ… æ­¥éª¤4å®Œæˆï¼æ„å»ºäº† {len(structured_contexts)} ä¸ªéƒ¨åˆ†çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step5_generate_paper(user_requirement: str) -> Dict[str, Any]:
    """æ­¥éª¤5ï¼šé¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹"""
    print(f"ğŸ”„ æ­¥éª¤5ï¼šé¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹...")
    if user_requirement:
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}")
    else:
        print(f"ğŸ“ åŸºäºå‰é¢æ­¥éª¤ç»“æœç”Ÿæˆè®ºæ–‡")
    
    run_data_dir = get_run_data_dir()
    
    # åŠ è½½æ­¥éª¤4çš„ç»“æœ
    step4_file = os.path.join(run_data_dir, "step4_contexts", f"{global_timestamp}_structured_contexts.json")
    
    if not os.path.exists(step4_file):
        # æŸ¥æ‰¾æœ€æ–°çš„æ­¥éª¤4æ–‡ä»¶ï¼ˆåœ¨æ‰€æœ‰runç›®å½•ä¸­ï¼‰
        config = load_config()
        rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
        base_data_dir = os.path.join(project_root, rag_data_base_path)
        step4_files = glob.glob(os.path.join(base_data_dir, "run_*", "step4_contexts", "*_structured_contexts.json"))
        if step4_files:
            step4_file = max(step4_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤4ç»“æœï¼š{os.path.basename(step4_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤4çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤4")
    
    with open(step4_file, 'r', encoding='utf-8') as f:
        step4_result = json.load(f)
    
    # å¦‚æœç”¨æˆ·éœ€æ±‚ä¸ºç©ºï¼Œå°è¯•ä»ä¹‹å‰çš„æ­¥éª¤ä¸­è·å–
    if not user_requirement:
        # å°è¯•ä»æ­¥éª¤1ç»“æœä¸­è·å–ç”¨æˆ·éœ€æ±‚
        config = load_config()
        rag_data_base_path = config["data_paths"]["rag_data_base"]["path"].lstrip('/')
        base_data_dir = os.path.join(project_root, rag_data_base_path)
        step1_files = glob.glob(os.path.join(base_data_dir, "run_*", "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            with open(step1_file, 'r', encoding='utf-8') as f:
                step1_result = json.load(f)
                user_requirement = step1_result.get('user_requirement', 'åŸºäºæ·±åº¦å­¦ä¹ çš„ç ”ç©¶')
    
    generator = PaperGenerator()
    generated_sections = generator.generate_paper_sequentially(
        paper_section_contexts=step4_result['structured_contexts'],
        user_requirement=user_requirement
    )
    
    # è·å–å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
    enhanced_stats = get_enhanced_generation_statistics(generated_sections, run_data_dir)
    
    # æ„å»ºå®Œæ•´çš„è®ºæ–‡æ•°æ®ç»“æ„
    generated_paper = {
        "title": "åŸºäºRAGçš„è®ºæ–‡ç”Ÿæˆ",
        "user_requirement": user_requirement,
        "sections": generated_sections,
        "statistics": enhanced_stats,
        "generation_timestamp": global_timestamp
    }
    
    # ä¿å­˜ç»“æœåˆ°step5ç›®å½•
    step5_dir = os.path.join(run_data_dir, "step5_generated")
    os.makedirs(step5_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´è®ºæ–‡
    json_path = os.path.join(step5_dir, f"{global_timestamp}_generated_paper.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(generated_paper, f, ensure_ascii=False, indent=2)
    
    # ä¸ºæ¯ä¸ªéƒ¨åˆ†å•ç‹¬ä¿å­˜
    for section, content in generated_sections.items():
        section_json_path = os.path.join(step5_dir, f"{global_timestamp}_generated_{section}.json")
        section_txt_path = os.path.join(step5_dir, f"{global_timestamp}_generated_{section}.txt")
        
        section_data = {section: content}
        with open(section_json_path, 'w', encoding='utf-8') as f:
            json.dump(section_data, f, ensure_ascii=False, indent=2)
        
        with open(section_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"ã€{section}ã€‘\n")
            f.write("=" * 80 + "\n")
            f.write(content + "\n")
    
    print(f"âœ… æ­¥éª¤5å®Œæˆï¼ç”Ÿæˆäº†å®Œæ•´è®ºæ–‡ï¼ŒåŒ…å« {len(generated_sections)} ä¸ªéƒ¨åˆ†")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    
    return generated_paper

def get_enhanced_generation_statistics(generated_sections: Dict[str, str], run_data_dir: str) -> Dict[str, Any]:
    """è·å–å¢å¼ºçš„ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
    # åŸºç¡€ç»Ÿè®¡
    total_words = sum(len(content.replace(' ', '').replace('\n', '')) for content in generated_sections.values())
    
    stats = {
        "total_sections": len(generated_sections),
        "total_words": total_words,
        "section_word_counts": {
            section: len(content.replace(' ', '').replace('\n', ''))
            for section, content in generated_sections.items()
        },
        "sections_generated": list(generated_sections.keys())
    }
    
    # ç»Ÿè®¡æ£€ç´¢åˆ°çš„è®ºæ–‡æ•°é‡
    retrieved_papers = get_all_retrieved_papers(run_data_dir)
    stats["retrieved_papers_count"] = len(retrieved_papers)
    stats["retrieved_papers"] = retrieved_papers
    
    # ç»Ÿè®¡åŸæ–‡å‚è€ƒ
    source_references = get_source_references(run_data_dir)
    stats["source_references"] = source_references
    
    return stats

def get_all_retrieved_papers(run_data_dir: str) -> list:
    """ç»Ÿè®¡æ‰€æœ‰æ­¥éª¤ä¸­æ£€ç´¢åˆ°çš„è®ºæ–‡"""
    all_papers = set()
    
    # ä»step1è·å–æ£€ç´¢åˆ°çš„è®ºæ–‡
    step1_file = os.path.join(run_data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    if os.path.exists(step1_file):
        with open(step1_file, 'r', encoding='utf-8') as f:
            step1_data = json.load(f)
            for summary_type, summaries in step1_data.get('relevant_summaries', {}).items():
                for summary in summaries:
                    paper_id = summary.get('paper_id', '')
                    if paper_id:
                        all_papers.add(paper_id)
    
    return list(all_papers)

def get_source_references(run_data_dir: str) -> list:
    """è·å–åŸæ–‡å‚è€ƒä¿¡æ¯"""
    references = []
    
    # ä»step3è·å–åŸæ–‡ä¿¡æ¯
    step3_file = os.path.join(run_data_dir, "step3_source_texts", f"{global_timestamp}_source_texts_selection.json")
    if os.path.exists(step3_file):
        with open(step3_file, 'r', encoding='utf-8') as f:
            step3_data = json.load(f)
            for paper_id, sections in step3_data.get('selected_source_texts', {}).items():
                reference = {
                    "paper_id": paper_id,
                    "sections_used": list(sections.keys()),
                    "purpose": "æ–¹æ³•å’Œå®éªŒå‚è€ƒ"
                }
                references.append(reference)
    
    return references

def generate_final_markdown(generated_paper: Dict[str, Any], user_requirement: str = None) -> str:
    """ç”Ÿæˆæœ€ç»ˆçš„markdownå†…å®¹"""
    # ç”Ÿæˆæ ‡é¢˜å’Œæ—¶é—´ä¿¡æ¯
    title = generated_paper.get('title', 'åŸºäºRAGçš„è®ºæ–‡ç”Ÿæˆ')
    time_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
    
    if user_requirement:
        header = f"""# {title}

> **ç”Ÿæˆæ—¶é—´**: {time_str}  
> **ç”¨æˆ·éœ€æ±‚**: {user_requirement}

---

"""
    else:
        header = f"""# {title}

> **ç”Ÿæˆæ—¶é—´**: {time_str}  
> **åŸºäºRAGç³»ç»Ÿç”Ÿæˆ**

---

"""
    
    # æ·»åŠ å„ä¸ªéƒ¨åˆ†
    content_parts = [header]
    section_order = ['å¼•è¨€', 'ç›¸å…³å·¥ä½œ', 'æ–¹æ³•', 'å®éªŒè¯„ä»·', 'æ€»ç»“']
    
    for section in section_order:
        if section in generated_paper.get('sections', {}):
            content_parts.append(f"## {section}\n\n{generated_paper['sections'][section]}\n\n---\n\n")
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if 'statistics' in generated_paper:
        stats_content = generate_statistics_section(generated_paper['statistics'])
        content_parts.append(stats_content)
    
    return "".join(content_parts)

def generate_statistics_section(stats: Dict[str, Any]) -> str:
    """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯éƒ¨åˆ†"""
    lines = ["## ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯\n\n"]
    
    # åŸºç¡€ç»Ÿè®¡
    lines.append(f"- **æ€»å­—æ•°**: {stats.get('total_words', 'N/A')}\n")
    lines.append(f"- **ç”Ÿæˆéƒ¨åˆ†**: {len(stats.get('sections_generated', []))} ä¸ª\n")
    
    # æ£€ç´¢ç»Ÿè®¡
    retrieved_count = stats.get('retrieved_papers_count', 0)
    lines.append(f"- **æ£€ç´¢åˆ°çš„è®ºæ–‡æ•°é‡**: {retrieved_count} ç¯‡\n")
    
    # åŸæ–‡å‚è€ƒ
    source_refs = stats.get('source_references', [])
    if source_refs:
        lines.append(f"- **åŸæ–‡å‚è€ƒ**: {len(source_refs)} ç¯‡\n")
        for ref in source_refs:
            paper_id = ref.get('paper_id', 'Unknown')
            sections = ', '.join(ref.get('sections_used', []))
            lines.append(f"  - {paper_id} (ä½¿ç”¨ç« èŠ‚: {sections})\n")
    else:
        lines.append("- **åŸæ–‡å‚è€ƒ**: æ— \n")
    
    # å„éƒ¨åˆ†å­—æ•°ç»Ÿè®¡
    section_counts = stats.get('section_word_counts', {})
    if section_counts:
        lines.append("- **å„éƒ¨åˆ†å­—æ•°**:\n")
        for section, count in section_counts.items():
            lines.append(f"  - {section}: {count} å­—\n")
    
    return "".join(lines)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®ºæ–‡ç”Ÿæˆ RAG ç³»ç»Ÿ')
    parser.add_argument('user_requirement', nargs='?', help='ç”¨æˆ·éœ€æ±‚æè¿°ï¼ˆæ­¥éª¤5å¿…éœ€ï¼‰')
    parser.add_argument('--step', type=int, choices=[1, 2, 3, 4, 5], help='è¿è¡ŒæŒ‡å®šæ­¥éª¤')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶åï¼ˆä»…å¯¹æ­¥éª¤5ç”Ÿæ•ˆï¼‰')
    args = parser.parse_args()

    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # è®¾ç½®paperGenè¾“å‡ºç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    paper_gen_dir = os.path.join(script_dir, "paperGen")
    os.makedirs(paper_gen_dir, exist_ok=True)
    
    try:
        if args.step:
            # è¿è¡ŒæŒ‡å®šæ­¥éª¤
            if args.step == 1:
                if not args.user_requirement:
                    print("âŒ é”™è¯¯ï¼šæ­¥éª¤1éœ€è¦ç”¨æˆ·éœ€æ±‚å‚æ•°")
                    sys.exit(1)
                step1_retrieve_summaries(args.user_requirement)
            elif args.step == 2:
                step2_analyze_cross_paper()
            elif args.step == 3:
                step3_select_source_texts()
            elif args.step == 4:
                step4_build_contexts()
            elif args.step == 5:
                # æ­¥éª¤5ä¸å†éœ€è¦ç”¨æˆ·éœ€æ±‚å‚æ•°ï¼Œç›´æ¥åŸºäºå‰é¢æ­¥éª¤çš„ç»“æœç”Ÿæˆè®ºæ–‡
                generated_paper = step5_generate_paper("")
                
                # ç”Ÿæˆæœ€ç»ˆçš„markdownæ–‡ä»¶å¹¶ä¿å­˜åˆ°paperGenç›®å½•ï¼Œä½¿ç”¨é»˜è®¤å‘½å
                final_output_name = f"generated_paper_{timestamp}.md"
                final_output_path = os.path.join(paper_gen_dir, final_output_name)
                
                # ç”Ÿæˆmarkdownå†…å®¹
                markdown_content = generate_final_markdown(generated_paper)
                
                # ä¿å­˜æœ€ç»ˆmarkdownæ–‡ä»¶
                with open(final_output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                print(f"ğŸ‰ è®ºæ–‡ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“„ æœ€ç»ˆè®ºæ–‡å·²ä¿å­˜åˆ°ï¼š{final_output_path}")
            
        else:
            # è¿è¡Œå®Œæ•´æµç¨‹
            if not args.user_requirement:
                # å…è®¸ç”¨æˆ·åœ¨æ§åˆ¶å°è¾“å…¥éœ€æ±‚
                print("ğŸ“ è¯·è¾“å…¥æ‚¨çš„è®ºæ–‡ç”Ÿæˆéœ€æ±‚ï¼š")
                print("ğŸ’¡ ç¤ºä¾‹ï¼š")
                print("   - åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»æ–¹æ³•ç ”ç©¶")
                print("   - è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–")
                print("   - æ¨èç³»ç»Ÿä¸­çš„ååŒè¿‡æ»¤ç®—æ³•æ”¹è¿›")
                print()
                
                user_input = input("ğŸ¯ æ‚¨çš„éœ€æ±‚: ").strip()
                if not user_input:
                    print("âŒ é”™è¯¯ï¼šéœ€æ±‚ä¸èƒ½ä¸ºç©º")
                    sys.exit(1)
                args.user_requirement = user_input
    
            print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„è®ºæ–‡ç”Ÿæˆ RAG æµç¨‹...")
            
            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            step1_retrieve_summaries(args.user_requirement)
            step2_analyze_cross_paper()
            step3_select_source_texts()
            step4_build_contexts()
            generated_paper = step5_generate_paper(args.user_requirement)
            
            # ç”Ÿæˆæœ€ç»ˆè¾“å‡º
            final_output_name = args.output if args.output else f"generated_paper_{timestamp}.md"
            if args.output:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶åï¼Œå¤„ç†æ ¼å¼
                if not final_output_name.endswith('.md'):
                    final_output_name = final_output_name + f"_{timestamp}.md"
                else:
                    # åœ¨.mdå‰æ·»åŠ æ—¶é—´æˆ³
                    final_output_name = final_output_name.replace('.md', f'_{timestamp}.md')
                
            final_output_path = os.path.join(paper_gen_dir, final_output_name)
            
            # ç”Ÿæˆmarkdownå†…å®¹ï¼ˆåŒä¸Šï¼‰
            markdown_content = generate_final_markdown(generated_paper, args.user_requirement)
            
            with open(final_output_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            print(f"ğŸ‰ å®Œæ•´ RAG æµç¨‹æ‰§è¡Œå®Œæˆï¼")
            print(f"ğŸ“„ æœ€ç»ˆè®ºæ–‡å·²ä¿å­˜åˆ°ï¼š{final_output_path}")
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 