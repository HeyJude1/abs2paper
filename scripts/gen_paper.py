#!/usr/bin/env python3
"""
è®ºæ–‡ç”ŸæˆRAGç³»ç»Ÿä¸»è„šæœ¬
åŸºäºç”¨æˆ·éœ€æ±‚ï¼Œé€šè¿‡RAGæµç¨‹ç”Ÿæˆå®Œæ•´è®ºæ–‡
"""

import os
import sys
import json
import argparse
import glob
from datetime import datetime
from typing import Dict, List, Optional, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from abs2paper.rag.summary_retriever import SummaryRetriever
from abs2paper.rag.cross_paper_analyzer import CrossPaperAnalyzer
from abs2paper.rag.source_text_retriever import SourceTextRetriever
from abs2paper.rag.context_builder import ContextBuilder
from abs2paper.rag.paper_generator import PaperGenerator

# å…¨å±€æ—¶é—´æˆ³
global_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

def convert_to_serializable(obj):
    """å°†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
    if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):
        # å¤„ç†åˆ—è¡¨æˆ–ç±»ä¼¼åˆ—è¡¨çš„å¯¹è±¡ï¼ˆå¦‚RepeatedScalarContainerï¼‰
        try:
            return list(obj)
        except:
            return str(obj)
    elif hasattr(obj, '__dict__'):
        # å¤„ç†å¤æ‚å¯¹è±¡
        return obj.__dict__
    else:
        return obj

def serialize_data(data):
    """é€’å½’å¤„ç†æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥JSONåºåˆ—åŒ–"""
    import json
    
    try:
        # å°è¯•ç›´æ¥åºåˆ—åŒ–ï¼Œå¦‚æœå¯ä»¥åˆ™è¿”å›
        json.dumps(data)
        return data
    except TypeError:
        pass
    
    if isinstance(data, dict):
        return {key: serialize_data(value) for key, value in data.items()}
    elif isinstance(data, (list, tuple)):
        return [serialize_data(item) for item in data]
    elif hasattr(data, '__iter__') and not isinstance(data, (str, bytes)):
        # å¤„ç†RepeatedScalarContainerç­‰protobufç±»å‹
        try:
            return [serialize_data(item) for item in data]
        except:
            return str(data)
    elif hasattr(data, '__dict__'):
        # å¤„ç†å¤æ‚å¯¹è±¡
        return serialize_data(data.__dict__)
    else:
        # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºåŸºæœ¬ç±»å‹
        try:
            return str(data)
        except:
            return None

def save_json_and_txt(data: Dict[str, Any], json_path: str, txt_path: str, 
                      text_extract_func=None):
    """ä¿å­˜JSONå’ŒTXTæ–‡ä»¶çš„é€šç”¨å‡½æ•°"""
    # åºåˆ—åŒ–æ•°æ®
    serializable_data = serialize_data(data)
    
    # ä¿å­˜JSON
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_data, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜TXT
    if text_extract_func:
        text_content = text_extract_func(data)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(text_content)

def extract_summaries_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤1ç»“æœä¸­æå–æ€»ç»“æ–‡æœ¬"""
    text_lines = []
    text_lines.append(f"ç”¨æˆ·éœ€æ±‚ï¼š{data.get('user_requirement', '')}\n")
    text_lines.append("=" * 80 + "\n")
    
    relevant_summaries = data.get('relevant_summaries', {})
    for summary_type, summaries in relevant_summaries.items():
        text_lines.append(f"ã€{summary_type.upper()} ç±»å‹æ€»ç»“ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        for i, summary in enumerate(summaries):
            text_lines.append(f"æ€»ç»“ {i+1}ï¼š\n")
            text_lines.append(f"è®ºæ–‡IDï¼š{summary.get('paper_id', '')}\n")
            text_lines.append(f"ç›¸å…³åº¦å¾—åˆ†ï¼š{summary.get('score', 0):.4f}\n")
            text_lines.append(f"æ¥æºç« èŠ‚ï¼š{', '.join(summary.get('source_sections', []))}\n")
            text_lines.append(f"ä¸»é¢˜æ ‡ç­¾ï¼š{', '.join(summary.get('topics', []))}\n")
            text_lines.append("æ€»ç»“å†…å®¹ï¼š\n")
            text_lines.append(summary.get('summary_text', '') + "\n")
            text_lines.append("\n" + "~" * 30 + "\n")
        
        text_lines.append("\n" + "=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_analysis_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤2ç»“æœä¸­æå–åˆ†ææ–‡æœ¬"""
    text_lines = []
    text_lines.append("è·¨è®ºæ–‡åŒç±»å‹åˆ†æç»“æœ\n")
    text_lines.append("=" * 80 + "\n")
    
    cross_paper_analysis = data.get('cross_paper_analysis', {})
    for analysis_type, analysis_data in cross_paper_analysis.items():
        text_lines.append(f"ã€{analysis_type.upper()} åˆ†æã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        # åˆ†ææ€»ç»“
        if 'summary' in analysis_data:
            text_lines.append("ğŸ“Š åˆ†ææ€»ç»“ï¼š\n")
            text_lines.append(f"{analysis_data['summary']}\n\n")
        
        # å‘å±•è¶‹åŠ¿
        if 'trends' in analysis_data:
            text_lines.append("ğŸ“ˆ å‘å±•è¶‹åŠ¿ï¼š\n")
            trends = analysis_data['trends']
            if isinstance(trends, list):
                for i, trend in enumerate(trends, 1):
                    text_lines.append(f"  {i}. {str(trend)}\n")
            else:
                text_lines.append(f"  â€¢ {str(trends)}\n")
            text_lines.append("\n")
        
        # å¸¸è§æ–¹æ³•
        if 'common_approaches' in analysis_data:
            text_lines.append("ğŸ”§ å¸¸è§æ–¹æ³•ï¼š\n")
            approaches = analysis_data['common_approaches']
            if isinstance(approaches, list):
                for i, approach in enumerate(approaches, 1):
                    text_lines.append(f"  {i}. {str(approach)}\n")
            else:
                text_lines.append(f"  â€¢ {str(approaches)}\n")
            text_lines.append("\n")
        
        # å‘ç°çš„æ¨¡å¼ 
        if 'patterns' in analysis_data:
            text_lines.append("ğŸ” å‘ç°çš„æ¨¡å¼ï¼š\n")
            patterns = analysis_data['patterns']
            if isinstance(patterns, list):
                for i, pattern in enumerate(patterns[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªæœ€é‡è¦çš„æ¨¡å¼
                    text_lines.append(f"  {i}. {str(pattern)}\n")
            else:
                text_lines.append(f"  â€¢ {str(patterns)}\n")
            text_lines.append("\n")
        
        # ä¸»é¢˜èšç±»ï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
        if 'topic_clusters' in analysis_data:
            text_lines.append("ğŸ“š ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼š\n")
            clusters = analysis_data['topic_clusters']
            if isinstance(clusters, dict):
                # è¿‡æ»¤å’Œæ•´ç†èšç±»ç»“æœï¼Œåªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„èšç±»
                meaningful_clusters = {}
                for cluster_name, papers in clusters.items():
                    # è¿‡æ»¤æ‰å•å­—ç¬¦æˆ–æ— æ„ä¹‰çš„èšç±»å
                    if len(cluster_name) > 2 and cluster_name not in ['[', "'", ' ', '(', ')', ',', '-']:
                        if isinstance(papers, list) and len(papers) >= 2:  # è‡³å°‘2ç¯‡è®ºæ–‡çš„èšç±»æ‰æœ‰æ„ä¹‰
                            if isinstance(papers[0], dict):
                                paper_count = len(set(p.get('paper_id', str(p)) for p in papers))
                            else:
                                paper_count = len(set(str(p) for p in papers))
                            meaningful_clusters[cluster_name] = paper_count
                
                # æŒ‰è®ºæ–‡æ•°é‡æ’åºï¼Œæ˜¾ç¤ºå‰10ä¸ª
                sorted_clusters = sorted(meaningful_clusters.items(), key=lambda x: x[1], reverse=True)[:10]
                for i, (topic, count) in enumerate(sorted_clusters, 1):
                    text_lines.append(f"  {i}. {topic}ï¼ˆ{count}ç¯‡è®ºæ–‡æ¶‰åŠï¼‰\n")
            text_lines.append("\n")
        
        text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_source_texts_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤3ç»“æœä¸­æå–æºæ–‡æœ¬"""
    text_lines = []
    text_lines.append("é€‰å®šçš„æœ€ç›¸å…³åŸæ–‡ç« èŠ‚\n")
    text_lines.append("=" * 80 + "\n")
    
    selected_source_texts = data.get('selected_source_texts', {})
    for paper_id, sections in selected_source_texts.items():
        text_lines.append(f"ã€è®ºæ–‡ {paper_id}ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        
        for section_name, section_content in sections.items():
            text_lines.append(f">>> {section_name} ç« èŠ‚ <<<\n")
            if isinstance(section_content, list):
                text_lines.append("".join(section_content) + "\n")
            else:
                text_lines.append(str(section_content) + "\n")
            text_lines.append("\n" + "~" * 30 + "\n")
        
        text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def extract_contexts_text(data: Dict[str, Any]) -> str:
    """ä»æ­¥éª¤4ç»“æœä¸­æå–ç»“æ„åŒ–ä¸Šä¸‹æ–‡æ–‡æœ¬"""
    text_lines = []
    text_lines.append("ç»“æ„åŒ–RAGä¸Šä¸‹æ–‡\n")
    text_lines.append("=" * 80 + "\n")
    
    # å¤„ç†å®Œæ•´çš„structured_contextsæ•°æ®
    if 'structured_contexts' in data:
        structured_contexts = data['structured_contexts']
        for section_name, context in structured_contexts.items():
            text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
            text_lines.append("-" * 50 + "\n")
            if isinstance(context, str) and context.strip():
                text_lines.append(context + "\n\n")
            else:
                text_lines.append("(ä¸Šä¸‹æ–‡å†…å®¹ä¸ºç©º)\n\n")
            text_lines.append("=" * 80 + "\n")
    
    # å¤„ç†å•ä¸ªsectionçš„æ•°æ®
    elif 'section_name' in data and 'context' in data:
        section_name = data['section_name']
        context = data['context']
        context_length = data.get('context_length', 0)
        
        text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
        text_lines.append("-" * 50 + "\n")
        text_lines.append(f"ä¸Šä¸‹æ–‡é•¿åº¦ï¼š{context_length} å­—ç¬¦\n\n")
        
        if isinstance(context, str) and context.strip():
            text_lines.append("ä¸Šä¸‹æ–‡å†…å®¹ï¼š\n")
            text_lines.append(context + "\n\n")
        else:
            text_lines.append("(ä¸Šä¸‹æ–‡å†…å®¹ä¸ºç©º)\n\n")
    
    # å¤„ç†æ—§æ ¼å¼çš„æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
    else:
        for section_name, context_data in data.items():
            if isinstance(context_data, dict):
                text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
                text_lines.append("-" * 50 + "\n")
                
                # æ€»ç»“ä¿¡æ¯
                if 'summary_info' in context_data:
                    text_lines.append("ğŸ“‹ ç›¸å…³æ€»ç»“ä¿¡æ¯ï¼š\n")
                    for summary_type, summaries in context_data['summary_info'].items():
                        text_lines.append(f"  {summary_type}ï¼š\n")
                        for summary in summaries:
                            paper_id = summary.get('paper_id', '')
                            summary_text = summary.get('summary_text', '')
                            text_lines.append(f"    - {paper_id}: {summary_text[:200]}...\n")
                        text_lines.append("\n")
                
                # è¶‹åŠ¿ä¿¡æ¯
                if 'trend_info' in context_data:
                    text_lines.append("ğŸ“ˆ è¶‹åŠ¿åˆ†æï¼š\n")
                    text_lines.append(context_data['trend_info'] + "\n\n")
                
                # åŸæ–‡ä¿¡æ¯
                if 'source_text_info' in context_data:
                    text_lines.append("ğŸ“„ ç›¸å…³åŸæ–‡ï¼š\n")
                    text_lines.append(context_data['source_text_info'] + "\n\n")
                
                text_lines.append("=" * 80 + "\n")
            elif isinstance(context_data, str) and context_data.strip():
                text_lines.append(f"ã€{section_name} éƒ¨åˆ†çš„ä¸Šä¸‹æ–‡ã€‘\n")
                text_lines.append("-" * 50 + "\n")
                text_lines.append(context_data + "\n\n")
                text_lines.append("=" * 80 + "\n")
    
    return "".join(text_lines)

def step1_retrieve_summaries(user_requirement: str) -> Dict[str, Any]:
    """æ­¥éª¤1ï¼šå¤šç±»å‹æ€»ç»“å¹¶è¡Œæ£€ç´¢"""
    print(f"ğŸ”„ æ­¥éª¤1ï¼šå¤šç±»å‹æ€»ç»“å¹¶è¡Œæ£€ç´¢...")
    print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}")
    
    retriever = SummaryRetriever()
    relevant_summaries = retriever.parallel_retrieve_summaries(
        user_requirement=user_requirement,
        top_k_per_type=5
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = retriever.get_retrieval_statistics(relevant_summaries)
    
    result = {
        "user_requirement": user_requirement,
        "top_k_per_type": 5,
        "relevant_summaries": relevant_summaries,
        "statistics": stats
    }
    
    # ä¿å­˜ç»“æœåˆ°step1ç›®å½•
    data_dir = os.path.join(project_root, "abs2paper", "rag", "data")
    step1_dir = os.path.join(data_dir, "step1_summaries")
    os.makedirs(step1_dir, exist_ok=True)
    
    json_path = os.path.join(step1_dir, f"{global_timestamp}_summaries_retrieval.json")
    txt_path = os.path.join(step1_dir, f"{global_timestamp}_summaries_retrieval.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_summaries_text)
    
    print(f"âœ… æ­¥éª¤1å®Œæˆï¼å…±æ£€ç´¢åˆ° {stats['total_summaries']} ä¸ªæ€»ç»“ï¼Œæ¶µç›– {stats['types_found']} ç§ç±»å‹")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼š{stats['type_counts']}")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step2_analyze_cross_paper() -> Dict[str, Any]:
    """æ­¥éª¤2ï¼šè·¨è®ºæ–‡åŒç±»å‹åˆ†æ"""
    print(f"ğŸ”„ æ­¥éª¤2ï¼šè·¨è®ºæ–‡åŒç±»å‹åˆ†æ...")
    
    # åŠ è½½æ­¥éª¤1çš„ç»“æœ
    data_dir = os.path.join(project_root, "abs2paper", "rag", "data")
    step1_file = os.path.join(data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    
    if not os.path.exists(step1_file):
        # æŸ¥æ‰¾æœ€æ–°çš„step1æ–‡ä»¶
        step1_files = glob.glob(os.path.join(data_dir, "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1ç»“æœï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    
    analyzer = CrossPaperAnalyzer()
    cross_paper_analysis = analyzer.analyze_cross_paper_patterns(
        step1_result['relevant_summaries']
    )
    
    result = {
        "cross_paper_analysis": cross_paper_analysis,
        "input_summaries_count": sum(len(summaries) for summaries in step1_result['relevant_summaries'].values())
    }
    
    # ä¿å­˜ç»“æœåˆ°step2ç›®å½•
    step2_dir = os.path.join(data_dir, "step2_analysis")
    os.makedirs(step2_dir, exist_ok=True)
    
    json_path = os.path.join(step2_dir, f"{global_timestamp}_cross_paper_analysis.json")
    txt_path = os.path.join(step2_dir, f"{global_timestamp}_cross_paper_analysis.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_analysis_text)
    
    print(f"âœ… æ­¥éª¤2å®Œæˆï¼åˆ†æäº† {len(cross_paper_analysis)} ç§æ€»ç»“ç±»å‹çš„è·¨è®ºæ–‡æ¨¡å¼")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step3_select_source_texts() -> Dict[str, Any]:
    """æ­¥éª¤3ï¼šæ ¹æ®æ€»ç»“è·å–å¯¹åº”åŸæ–‡ç« èŠ‚"""
    print(f"ğŸ”„ æ­¥éª¤3ï¼šæ ¹æ®æ€»ç»“è·å–å¯¹åº”åŸæ–‡ç« èŠ‚...")
    
    # åŠ è½½æ­¥éª¤1çš„ç»“æœ
    data_dir = os.path.join(project_root, "abs2paper", "rag", "data")
    step1_file = os.path.join(data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    
    if not os.path.exists(step1_file):
        step1_files = glob.glob(os.path.join(data_dir, "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1ç»“æœï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    
    retriever = SourceTextRetriever()
    selected_source_texts = retriever.select_most_relevant_source_texts(
        step1_result['relevant_summaries']
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    total_papers = len(selected_source_texts)
    total_sections = sum(len(sections) for sections in selected_source_texts.values())
    total_chunks = sum(
        len(content) if isinstance(content, list) else 1
        for sections in selected_source_texts.values()
        for content in sections.values()
    )
    
    result = {
        "selected_source_texts": selected_source_texts,
        "statistics": {
            "total_papers": total_papers,
            "total_sections": total_sections,
            "total_chunks": total_chunks,
            "paper_details": {
                paper_id: {
                    "sections": list(sections.keys()),
                    "section_count": len(sections)
                }
                for paper_id, sections in selected_source_texts.items()
            }
        }
    }
    
    # ä¿å­˜ç»“æœåˆ°step3ç›®å½•
    step3_dir = os.path.join(data_dir, "step3_source_texts")
    os.makedirs(step3_dir, exist_ok=True)
    
    json_path = os.path.join(step3_dir, f"{global_timestamp}_source_texts_selection.json")
    txt_path = os.path.join(step3_dir, f"{global_timestamp}_source_texts_selection.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_source_texts_text)
    
    print(f"âœ… æ­¥éª¤3å®Œæˆï¼é€‰æ‹©äº† {total_papers} ç¯‡è®ºæ–‡çš„ {total_sections} ä¸ªç« èŠ‚")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step4_build_contexts() -> Dict[str, Any]:
    """æ­¥éª¤4ï¼šæŒ‰ç”Ÿæˆè®ºæ–‡éƒ¨åˆ†æ„å»ºç»“æ„åŒ–RAGä¸Šä¸‹æ–‡"""
    print(f"ğŸ”„ æ­¥éª¤4ï¼šæŒ‰ç”Ÿæˆè®ºæ–‡éƒ¨åˆ†æ„å»ºç»“æ„åŒ–RAGä¸Šä¸‹æ–‡...")
    
    data_dir = os.path.join(project_root, "abs2paper", "rag", "data")
    
    # åŠ è½½å‰é¢æ­¥éª¤çš„ç»“æœ
    step1_file = os.path.join(data_dir, "step1_summaries", f"{global_timestamp}_summaries_retrieval.json")
    step2_file = os.path.join(data_dir, "step2_analysis", f"{global_timestamp}_cross_paper_analysis.json")
    step3_file = os.path.join(data_dir, "step3_source_texts", f"{global_timestamp}_source_texts_selection.json")
    
    # å¦‚æœæ‰¾ä¸åˆ°å½“å‰æ—¶é—´æˆ³çš„æ–‡ä»¶ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ–‡ä»¶
    if not os.path.exists(step1_file):
        step1_files = glob.glob(os.path.join(data_dir, "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤1æ–‡ä»¶ï¼š{os.path.basename(step1_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤1çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤1")
    
    if not os.path.exists(step2_file):
        step2_files = glob.glob(os.path.join(data_dir, "step2_analysis", "*_cross_paper_analysis.json"))
        if step2_files:
            step2_file = max(step2_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤2æ–‡ä»¶ï¼š{os.path.basename(step2_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤2çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤2")
    
    if not os.path.exists(step3_file):
        step3_files = glob.glob(os.path.join(data_dir, "step3_source_texts", "*_source_texts_selection.json"))
        if step3_files:
            step3_file = max(step3_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤3æ–‡ä»¶ï¼š{os.path.basename(step3_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤3çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤3")
    
    # åŠ è½½æ•°æ®
    with open(step1_file, 'r', encoding='utf-8') as f:
        step1_result = json.load(f)
    with open(step2_file, 'r', encoding='utf-8') as f:
        step2_result = json.load(f)
    with open(step3_file, 'r', encoding='utf-8') as f:
        step3_result = json.load(f)
    
    builder = ContextBuilder()
    structured_contexts = builder.build_structured_contexts(
        relevant_summaries=step1_result['relevant_summaries'],
        cross_paper_insights=step2_result['cross_paper_analysis'],
        selected_source_texts=step3_result['selected_source_texts']
    )
    
    result = {
        "structured_contexts": structured_contexts,
        "context_sections": list(structured_contexts.keys()),
        "total_contexts": len(structured_contexts)
    }
    
    # ä¿å­˜ç»“æœåˆ°step4ç›®å½•
    step4_dir = os.path.join(data_dir, "step4_contexts")
    os.makedirs(step4_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡
    json_path = os.path.join(step4_dir, f"{global_timestamp}_structured_contexts.json")
    txt_path = os.path.join(step4_dir, f"{global_timestamp}_structured_contexts.txt")
    
    save_json_and_txt(result, json_path, txt_path, extract_contexts_text)
    
    # ä¸ºæ¯ä¸ªéƒ¨åˆ†å•ç‹¬ä¿å­˜ä¸Šä¸‹æ–‡
    for section_name, context in structured_contexts.items():
        section_json_path = os.path.join(step4_dir, f"{global_timestamp}_context_{section_name}.json")
        section_txt_path = os.path.join(step4_dir, f"{global_timestamp}_context_{section_name}.txt")
        
        # æ„å»ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„sectionæ•°æ®
        section_data = {
            "section_name": section_name,
            "context": context,
            "context_length": len(context) if isinstance(context, str) else len(str(context))
        }
        
        save_json_and_txt(section_data, section_json_path, section_txt_path, extract_contexts_text)
    
    print(f"âœ… æ­¥éª¤4å®Œæˆï¼æ„å»ºäº† {len(structured_contexts)} ä¸ªéƒ¨åˆ†çš„ç»“æ„åŒ–ä¸Šä¸‹æ–‡")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    print(f"ğŸ“„ æ–‡æœ¬ç‰ˆæœ¬ï¼š{txt_path}")
    
    return result

def step5_generate_paper(user_requirement: str) -> Dict[str, Any]:
    """æ­¥éª¤5ï¼šé¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹"""
    print(f"ğŸ”„ æ­¥éª¤5ï¼šé¡ºåºåŒ–è®ºæ–‡ç”Ÿæˆæµç¨‹...")
    if user_requirement:
        print(f"ğŸ“ ç”¨æˆ·éœ€æ±‚ï¼š{user_requirement}")
    else:
        print(f"ğŸ“ åŸºäºå‰é¢æ­¥éª¤ç»“æœç”Ÿæˆè®ºæ–‡")
    
    data_dir = os.path.join(project_root, "abs2paper", "rag", "data")
    
    # åŠ è½½æ­¥éª¤4çš„ç»“æœ
    step4_file = os.path.join(data_dir, "step4_contexts", f"{global_timestamp}_structured_contexts.json")
    
    if not os.path.exists(step4_file):
        step4_files = glob.glob(os.path.join(data_dir, "step4_contexts", "*_structured_contexts.json"))
        if step4_files:
            step4_file = max(step4_files, key=os.path.getctime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°çš„æ­¥éª¤4ç»“æœï¼š{os.path.basename(step4_file)}")
        else:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ­¥éª¤4çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ­¥éª¤4")
    
    with open(step4_file, 'r', encoding='utf-8') as f:
        step4_result = json.load(f)
    
    # å¦‚æœç”¨æˆ·éœ€æ±‚ä¸ºç©ºï¼Œå°è¯•ä»ä¹‹å‰çš„æ­¥éª¤ä¸­è·å–
    if not user_requirement:
        # å°è¯•ä»æ­¥éª¤1ç»“æœä¸­è·å–ç”¨æˆ·éœ€æ±‚
        step1_files = glob.glob(os.path.join(data_dir, "step1_summaries", "*_summaries_retrieval.json"))
        if step1_files:
            step1_file = max(step1_files, key=os.path.getctime)
            with open(step1_file, 'r', encoding='utf-8') as f:
                step1_result = json.load(f)
                user_requirement = step1_result.get('user_requirement', 'åŸºäºæ·±åº¦å­¦ä¹ çš„ç ”ç©¶')
    
    generator = PaperGenerator()
    generated_sections = generator.generate_paper_sequentially(
        paper_section_contexts=step4_result['structured_contexts'],
        user_requirement=user_requirement
    )
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = generator.get_generation_statistics(generated_sections)
    
    # æ„å»ºå®Œæ•´çš„è®ºæ–‡æ•°æ®ç»“æ„
    generated_paper = {
        "title": "åŸºäºRAGçš„è®ºæ–‡ç”Ÿæˆ",
        "user_requirement": user_requirement,
        "sections": generated_sections,
        "statistics": stats,
        "generation_timestamp": global_timestamp
    }
    
    # ä¿å­˜ç»“æœåˆ°step5ç›®å½•
    step5_dir = os.path.join(data_dir, "step5_generated")
    os.makedirs(step5_dir, exist_ok=True)
    
    # ä¿å­˜å®Œæ•´è®ºæ–‡
    json_path = os.path.join(step5_dir, f"{global_timestamp}_generated_paper.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(generated_paper, f, ensure_ascii=False, indent=2)
    
    # ä¸ºæ¯ä¸ªéƒ¨åˆ†å•ç‹¬ä¿å­˜
    for section, content in generated_sections.items():
        section_json_path = os.path.join(step5_dir, f"{global_timestamp}_generated_{section}.json")
        section_txt_path = os.path.join(step5_dir, f"{global_timestamp}_generated_{section}.txt")
        
        section_data = {section: content}
        with open(section_json_path, 'w', encoding='utf-8') as f:
            json.dump(section_data, f, ensure_ascii=False, indent=2)
        
        with open(section_txt_path, 'w', encoding='utf-8') as f:
            f.write(f"ã€{section}ã€‘\n")
            f.write("=" * 80 + "\n")
            f.write(content + "\n")
    
    print(f"âœ… æ­¥éª¤5å®Œæˆï¼ç”Ÿæˆäº†å®Œæ•´è®ºæ–‡ï¼ŒåŒ…å« {len(generated_sections)} ä¸ªéƒ¨åˆ†")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")
    
    return generated_paper

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®ºæ–‡ç”Ÿæˆ RAG ç³»ç»Ÿ')
    parser.add_argument('user_requirement', nargs='?', help='ç”¨æˆ·éœ€æ±‚æè¿°ï¼ˆæ­¥éª¤5å¿…éœ€ï¼‰')
    parser.add_argument('--step', type=int, choices=[1, 2, 3, 4, 5], help='è¿è¡ŒæŒ‡å®šæ­¥éª¤')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶åï¼ˆä»…å¯¹æ­¥éª¤5ç”Ÿæ•ˆï¼‰')
    args = parser.parse_args()

    # åˆ›å»ºæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # è®¾ç½®paperGenè¾“å‡ºç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    paper_gen_dir = os.path.join(script_dir, "paperGen")
    os.makedirs(paper_gen_dir, exist_ok=True)
    
    try:
        if args.step:
            # è¿è¡ŒæŒ‡å®šæ­¥éª¤
            if args.step == 1:
                if not args.user_requirement:
                    print("âŒ é”™è¯¯ï¼šæ­¥éª¤1éœ€è¦ç”¨æˆ·éœ€æ±‚å‚æ•°")
                    sys.exit(1)
                step1_retrieve_summaries(args.user_requirement)
            elif args.step == 2:
                step2_analyze_cross_paper()
            elif args.step == 3:
                step3_select_source_texts()
            elif args.step == 4:
                step4_build_contexts()
            elif args.step == 5:
                # æ­¥éª¤5ä¸å†éœ€è¦ç”¨æˆ·éœ€æ±‚å‚æ•°ï¼Œç›´æ¥åŸºäºå‰é¢æ­¥éª¤çš„ç»“æœç”Ÿæˆè®ºæ–‡
                generated_paper = step5_generate_paper("")
                
                # ç”Ÿæˆæœ€ç»ˆçš„markdownæ–‡ä»¶å¹¶ä¿å­˜åˆ°paperGenç›®å½•ï¼Œä½¿ç”¨é»˜è®¤å‘½å
                final_output_name = f"generated_paper_{timestamp}.md"
                final_output_path = os.path.join(paper_gen_dir, final_output_name)
                
                # ç”Ÿæˆmarkdownå†…å®¹
                markdown_content = f"""# {generated_paper.get('title', 'åŸºäºRAGçš„è®ºæ–‡ç”Ÿæˆ')}

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
> **åŸºäºRAGç³»ç»Ÿç”Ÿæˆ**

---

"""
                
                # æŒ‰ç…§é¡ºåºæ·»åŠ å„ä¸ªéƒ¨åˆ†
                section_order = ['å¼•è¨€', 'ç›¸å…³å·¥ä½œ', 'æ–¹æ³•', 'å®éªŒè¯„ä»·', 'æ€»ç»“']
                for section in section_order:
                    if section in generated_paper.get('sections', {}):
                        markdown_content += f"## {section}\n\n{generated_paper['sections'][section]}\n\n---\n\n"
                
                # æ·»åŠ ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                if 'statistics' in generated_paper:
                    markdown_content += "## ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯\n\n"
                    stats = generated_paper['statistics']
                    markdown_content += f"- **æ€»å­—æ•°**: {stats.get('total_words', 'N/A')}\n"
                    markdown_content += f"- **ç”Ÿæˆæ—¶é—´**: {stats.get('generation_time', 'N/A')}\n"
                    markdown_content += f"- **ä½¿ç”¨çš„è®ºæ–‡æ•°é‡**: {stats.get('papers_used', 'N/A')}\n"
                
                # ä¿å­˜æœ€ç»ˆmarkdownæ–‡ä»¶
                with open(final_output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                print(f"ğŸ‰ è®ºæ–‡ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“„ æœ€ç»ˆè®ºæ–‡å·²ä¿å­˜åˆ°ï¼š{final_output_path}")
        
        else:
            # è¿è¡Œå®Œæ•´æµç¨‹
            if not args.user_requirement:
                print("âŒ é”™è¯¯ï¼šè¯·æä¾›ç”¨æˆ·éœ€æ±‚")
                print("ç¤ºä¾‹ï¼špython gen_paper.py 'åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†ç±»æ–¹æ³•ç ”ç©¶'")
                sys.exit(1)
            
            print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„è®ºæ–‡ç”Ÿæˆ RAG æµç¨‹...")
            
            # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            step1_retrieve_summaries(args.user_requirement)
            step2_analyze_cross_paper()
            step3_select_source_texts()
            step4_build_contexts()
            generated_paper = step5_generate_paper(args.user_requirement)
            
            # ç”Ÿæˆæœ€ç»ˆè¾“å‡º
            final_output_name = args.output if args.output else f"generated_paper_{timestamp}.md"
            if args.output:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶åï¼Œå¤„ç†æ ¼å¼
                if not final_output_name.endswith('.md'):
                    final_output_name = final_output_name + f"_{timestamp}.md"
                else:
                    # åœ¨.mdå‰æ·»åŠ æ—¶é—´æˆ³
                    final_output_name = final_output_name.replace('.md', f'_{timestamp}.md')
                
            final_output_path = os.path.join(paper_gen_dir, final_output_name)
            
            # ç”Ÿæˆmarkdownå†…å®¹ï¼ˆåŒä¸Šï¼‰
            markdown_content = f"""# {generated_paper.get('title', 'åŸºäºRAGçš„è®ºæ–‡ç”Ÿæˆ')}

> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}  
> **ç”¨æˆ·éœ€æ±‚**: {args.user_requirement}

---

"""
            
            section_order = ['å¼•è¨€', 'ç›¸å…³å·¥ä½œ', 'æ–¹æ³•', 'å®éªŒè¯„ä»·', 'æ€»ç»“']
            for section in section_order:
                if section in generated_paper.get('sections', {}):
                    markdown_content += f"## {section}\n\n{generated_paper['sections'][section]}\n\n---\n\n"
            
            if 'statistics' in generated_paper:
                markdown_content += "## ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯\n\n"
                stats = generated_paper['statistics']
                markdown_content += f"- **æ€»å­—æ•°**: {stats.get('total_words', 'N/A')}\n"
                markdown_content += f"- **ç”Ÿæˆæ—¶é—´**: {stats.get('generation_time', 'N/A')}\n"
                markdown_content += f"- **ä½¿ç”¨çš„è®ºæ–‡æ•°é‡**: {stats.get('papers_used', 'N/A')}\n"
            
            with open(final_output_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            print(f"ğŸ‰ å®Œæ•´ RAG æµç¨‹æ‰§è¡Œå®Œæˆï¼")
            print(f"ğŸ“„ æœ€ç»ˆè®ºæ–‡å·²ä¿å­˜åˆ°ï¼š{final_output_path}")
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 